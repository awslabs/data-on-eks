"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["6584"],{1899:function(e,n,i){i.r(n),i.d(n,{frontMatter:()=>o,default:()=>p,toc:()=>c,metadata:()=>a,assets:()=>l,contentTitle:()=>r});var a=JSON.parse('{"id":"resources/mountpoint-s3","title":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","description":"What is Mountpoint-S3?","source":"@site/docs/resources/mountpoint-s3.md","sourceDirName":"resources","slug":"/resources/mountpoint-s3","permalink":"/data-on-eks/docs/resources/mountpoint-s3","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/resources/mountpoint-s3.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Mounpoint-S3 on EKS"},"sidebar":"resources","previous":{"title":"Introduction","permalink":"/data-on-eks/docs/resources/intro"},"next":{"title":"Mounpoint-S3 for Spark Workloads","permalink":"/data-on-eks/docs/resources/mountpoint-s3-for-spark"}}'),s=i(85893),t=i(50065);let o={sidebar_position:2,sidebar_label:"Mounpoint-S3 on EKS"},r="Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS",l={},c=[{value:"What is Mountpoint-S3?",id:"what-is-mountpoint-s3",level:2},{value:"Deploying Mountpoint-S3 on Amazon EKS:",id:"deploying-mountpoint-s3-on-amazon-eks",level:2},{value:"Step 1: Set Up IAM Role for S3 CSI Driver",id:"step-1-set-up-iam-role-for-s3-csi-driver",level:3},{value:"Step 2: Configure EKS Blueprints Addons",id:"step-2-configure-eks-blueprints-addons",level:3},{value:"Step 3: Define a PersistentVolume",id:"step-3-define-a-persistentvolume",level:3},{value:"Step 4: Create a PersistentVolumeClaim",id:"step-4-create-a-persistentvolumeclaim",level:3},{value:"Step 5: Using PVC with Pod Definition",id:"step-5-using-pvc-with-pod-definition",level:3},{value:"Example 1: Basic Pod Using PVC for Storage",id:"example-1-basic-pod-using-pvc-for-storage",level:4},{value:"Example 2: AI/ML Training Job Using PVC",id:"example-2-aiml-training-job-using-pvc",level:4},{value:"Next Steps:",id:"next-steps",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"mounpoint-s3-enhancing-amazon-s3-file-access-for-data--ai-workloads-on-amazon-eks",children:"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS"})}),"\n",(0,s.jsx)(n.h2,{id:"what-is-mountpoint-s3",children:"What is Mountpoint-S3?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/awslabs/mountpoint-s3",children:"Mountpoint-S3"})," is an open-source file client developed by AWS that translates file operations into S3 API calls, enabling your applications to interact with ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/",children:"Amazon S3"})," buckets as if they were local disks. Mountpoint for Amazon S3 is optimized for applications that need high read throughput to large objects, potentially from many clients at once, and to write new objects sequentially from a single client at a time. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads and AI/ML training."]}),"\n",(0,s.jsxs)(n.p,{children:["A key feature of Mountpoint-S3 is its compatibility with both ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/storage-classes/",children:"Amazon S3 Standrad"})," and ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/storage-classes/",children:"Amazon S3 Express One Zone"}),". S3 Express One Zone is a high-performance storage class designed for ",(0,s.jsx)(n.em,{children:"single-Availability Zone deployment"}),". It offers consistent single-digit millisecond data access, making it ideal for frequently accessed data and latency-sensitive applications. S3 Express One Zone is known for delivering data access speeds up to 10 times faster and at up to 50% lower request costs compared to S3 Standard. This storage class enables users to co-locate storage and compute resources in the same Availability Zone, optimizing performance and potentially reducing compute costs."]}),"\n",(0,s.jsxs)(n.p,{children:["The integration with S3 Express One Zone enhances Mountpoint-S3's capabilities, particularly for machine learning and analytics workloads, as it can be used with services like ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),",  ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/sagemaker/train/",children:"Amazon SageMaker Model Training"}),", ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/athena/",children:"Amazon Athena"}),", ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/emr/",children:"Amazon EMR"}),", and ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/prescriptive-guidance/latest/serverless-etl-aws-glue/aws-glue-data-catalog.html",children:"AWS Glue Data Catalog"}),". The automatic scaling of storage based on consumption in S3 Express One Zone simplifies management for low-latency workloads, making Mountpoint-S3 a highly effective tool for a wide range of data-intensive tasks and AI/ML training environments."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Mountpoint-S3 does not support file renaming operations, which may limit its applicability in scenarios where such functionality is essential."})}),"\n",(0,s.jsx)(n.h2,{id:"deploying-mountpoint-s3-on-amazon-eks",children:"Deploying Mountpoint-S3 on Amazon EKS:"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-set-up-iam-role-for-s3-csi-driver",children:"Step 1: Set Up IAM Role for S3 CSI Driver"}),"\n",(0,s.jsx)(n.p,{children:"Create an IAM role using Terraform with the necessary permissions for the S3 CSI driver. This step is critical for ensuring secure and efficient communication between EKS and S3."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-terraform",children:'\n#---------------------------------------------------------------\n# IRSA for Mountpoint for Amazon S3 CSI Driver\n#---------------------------------------------------------------\nmodule "s3_csi_driver_irsa" {\n  source                = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"\n  version               = "~> 5.34"\n  role_name_prefix      = format("%s-%s-", local.name, "s3-csi-driver")\n  role_policy_arns = {\n    # WARNING: Demo purpose only. Bring your own IAM policy with least privileges\n    s3_csi_driver = "arn:aws:iam::aws:policy/AmazonS3FullAccess"\n  }\n  oidc_providers = {\n    main = {\n      provider_arn               = module.eks.oidc_provider_arn\n      namespace_service_accounts = ["kube-system:s3-csi-driver-sa"]\n    }\n  }\n  tags = local.tags\n}\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-configure-eks-blueprints-addons",children:"Step 2: Configure EKS Blueprints Addons"}),"\n",(0,s.jsx)(n.p,{children:"Configure EKS Blueprints Addons Terraform module to utilize this role for the Amazon EKS Add-on of S3 CSI driver."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-terraform",children:'#---------------------------------------------------------------\n# EKS Blueprints Addons\n#---------------------------------------------------------------\nmodule "eks_blueprints_addons" {\n  source  = "aws-ia/eks-blueprints-addons/aws"\n  version = "~> 1.2"\n\n  cluster_name      = module.eks.cluster_name\n  cluster_endpoint  = module.eks.cluster_endpoint\n  cluster_version   = module.eks.cluster_version\n  oidc_provider_arn = module.eks.oidc_provider_arn\n\n  #---------------------------------------\n  # Amazon EKS Managed Add-ons\n  #---------------------------------------\n  eks_addons = {\n    aws-mountpoint-s3-csi-driver = {\n      service_account_role_arn = module.s3_csi_driver_irsa.iam_role_arn\n    }\n  }\n}\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-define-a-persistentvolume",children:"Step 3: Define a PersistentVolume"}),"\n",(0,s.jsx)(n.p,{children:"Specify the S3 bucket, region details and access modes in a PersistentVolume (PV) configuration. This step is crucial for defining how EKS interacts with the S3 bucket."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany\n  mountOptions:\n    - uid=1000\n    - gid=2000\n    - allow-other\n    - allow-delete\n    - region <ENTER_REGION>\n  csi:\n    driver: s3.csi.aws.com # required\n    volumeHandle: s3-csi-driver-volume\n    volumeAttributes:\n      bucketName: <ENTER_S3_BUCKET_NAME>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-create-a-persistentvolumeclaim",children:"Step 4: Create a PersistentVolumeClaim"}),"\n",(0,s.jsx)(n.p,{children:"Establish a PersistentVolumeClaim (PVC) to utilize the defined PV, specifying access modes and static provisioning requirements."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-claim\n  namespace: spark-team-a\nspec:\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany\n  storageClassName: "" # required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # ignored, required\n  volumeName: s3-pv\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-using-pvc-with-pod-definition",children:"Step 5: Using PVC with Pod Definition"}),"\n",(0,s.jsx)(n.p,{children:"After setting up the PersistentVolumeClaim (PVC), the next step is to utilize it within a Pod definition. This allows your applications running in Kubernetes to access the data stored in the S3 bucket. Below are examples demonstrating how to reference the PVC in a Pod definition for different scenarios."}),"\n",(0,s.jsx)(n.h4,{id:"example-1-basic-pod-using-pvc-for-storage",children:"Example 1: Basic Pod Using PVC for Storage"}),"\n",(0,s.jsxs)(n.p,{children:["This example shows a basic Pod definition that mounts the ",(0,s.jsx)(n.code,{children:"s3-claim"})," PVC to a directory within the container."]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, the PVC ",(0,s.jsx)(n.code,{children:"s3-claim"})," is mounted to the ",(0,s.jsx)(n.code,{children:"/data"})," directory of the nginx container. This setup allows the application running within the container to read and write data to the S3 bucket as if it were a local directory."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  namespace: spark-team-a\nspec:\n  containers:\n    - name: app-container\n      image: nginx  # Example image\n      volumeMounts:\n        - name: s3-storage\n          mountPath: "/data"  # The path where the S3 bucket will be mounted\n  volumes:\n    - name: s3-storage\n      persistentVolumeClaim:\n        claimName: s3-claim\n\n'})}),"\n",(0,s.jsx)(n.h4,{id:"example-2-aiml-training-job-using-pvc",children:"Example 2: AI/ML Training Job Using PVC"}),"\n",(0,s.jsx)(n.p,{children:"In AI/ML training scenarios, data accessibility and throughput are critical. This example demonstrates a Pod configuration for a machine learning training job that accesses datasets stored in S3."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training-pod\n  namespace: spark-team-a\nspec:\n  containers:\n    - name: training-container\n      image: ml-training-image  # Replace with your ML training image\n      volumeMounts:\n        - name: dataset-storage\n          mountPath: "/datasets"  # Mount path for training data\n  volumes:\n    - name: dataset-storage\n      persistentVolumeClaim:\n        claimName: s3-claim\n\n'})}),"\n",(0,s.jsxs)(n.admonition,{type:"warning",children:[(0,s.jsx)(n.p,{children:"Mountpoint S3, when used with Amazon S3 or S3 express, may not be suitable as a shuffle storage for Spark workloads. This limitation arises due to the nature of shuffle operations in Spark, which often involve multiple clients reading and writing to the same location simultaneously."}),(0,s.jsx)(n.p,{children:"Additionally, Mountpoint S3 does not support file renaming, a critical feature required for efficient shuffle operations in Spark. This lack of renaming capability can lead to operational challenges and potential performance bottlenecks in data processing tasks."})]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Explore the provided Terraform code snippet for detailed deployment instructions."}),"\n",(0,s.jsx)(n.li,{children:"Refer to the official Mountpoint-S3 documentation for further configuration options and limitations."}),"\n",(0,s.jsx)(n.li,{children:"Utilize Mountpoint-S3 to unlock high-performance, scalable S3 access within your EKS applications."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By understanding Mountpoint-S3's capabilities and limitations, you can make informed decisions to optimize your data-driven workloads on Amazon EKS."})]})}function p(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},50065:function(e,n,i){i.d(n,{Z:()=>r,a:()=>o});var a=i(67294);let s={},t=a.createContext(s);function o(e){let n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);