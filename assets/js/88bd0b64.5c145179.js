"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[2235],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>h});var a=t(7294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=a.createContext({}),d=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=d(e.components);return a.createElement(l.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(t),m=o,h=c["".concat(l,".").concat(m)]||c[m]||u[m]||i;return t?a.createElement(h,r(r({ref:n},p),{},{components:t})):a.createElement(h,r({ref:n},p))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=t.length,r=new Array(i);r[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:o,r[1]=s;for(var d=2;d<i;d++)r[d]=t[d];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},9962:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=t(7462),o=(t(7294),t(3905));const i={sidebar_position:4,sidebar_label:"Networking for Data"},r="Networking for Data",s={unversionedId:"bestpractices/networking/networking",id:"bestpractices/networking/networking",title:"Networking for Data",description:"VPC and IP Considerations",source:"@site/docs/bestpractices/networking/networking.md",sourceDirName:"bestpractices/networking",slug:"/bestpractices/networking/",permalink:"/data-on-eks/docs/bestpractices/networking/",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/bestpractices/networking/networking.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,sidebar_label:"Networking for Data"},sidebar:"bestpractices",previous:{title:"EMR on EKS Best Practices",permalink:"/data-on-eks/docs/bestpractices/emr-on-eks/"}},l={},d=[{value:"VPC and IP Considerations",id:"vpc-and-ip-considerations",level:2},{value:"Plan for a large amount of IP address usage in your EKS clusters.",id:"plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters",level:3},{value:"Consider using a secondary CIDR if your IP space is constrained.",id:"consider-using-a-secondary-cidr-if-your-ip-space-is-constrained",level:3},{value:"Tuning the VPC CNI",id:"tuning-the-vpc-cni",level:2},{value:"VPC CNI and EC2 Rate Throttling",id:"vpc-cni-and-ec2-rate-throttling",level:3},{value:"Avoid using <code>WARM_IP_TARGET</code> in large clusters, or cluster with a lot of churn",id:"avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn",level:3},{value:"Limit the number of IPs per node on large instance types with <code>MAX_ENI</code> and <code>max-pods</code>",id:"limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods",level:3},{value:"Maxpods with Karpenter",id:"maxpods-with-karpenter",level:4},{value:"Application",id:"application",level:2},{value:"DNS Lookups and ndots",id:"dns-lookups-and-ndots",level:3},{value:"Inter AZ Network Optimization",id:"inter-az-network-optimization",level:3}],p={toc:d},c="wrapper";function u(e){let{components:n,...i}=e;return(0,o.kt)(c,(0,a.Z)({},p,i,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"networking-for-data"},"Networking for Data"),(0,o.kt)("h2",{id:"vpc-and-ip-considerations"},"VPC and IP Considerations"),(0,o.kt)("h3",{id:"plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters"},"Plan for a large amount of IP address usage in your EKS clusters."),(0,o.kt)("p",null,"The AWS VPC CNI maintains a \u201cwarm pool\u201d of IP addresses on the EKS worker nodes to assign to Pods. When more IP addresses are needed for your Pods, the CNI must communicate with EC2 APIs to assign the addresses to your nodes. During periods of high churn or large scale out these EC2 API calls can be rate throttled, which will delay the provisioning of Pods and thus delay the execution of workloads. When designing the VPC for your environment plan for more IP addresses than just your pods to accommodate this warm pool."),(0,o.kt)("p",null,"With the default VPC CNI configuration larger nodes will consume more IP addresses. For example ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI"},"a ",(0,o.kt)("inlineCode",{parentName:"a"},"m5.8xlarge")," node")," that is running 10 pods will hold 60 IPs total (to satisfy ",(0,o.kt)("inlineCode",{parentName:"p"},"WARM_ENI=1"),"). However a ",(0,o.kt)("inlineCode",{parentName:"p"},"m5.16xlarge")," node would hold 100 IPs.\nConfiguring the VPC CNI to minimize this warm pool can increase the EC2 API calls from your nodes and increase the risk of rate throttling. Planning for this extra IP address usage can avoid rate throttling problems and managing the IP address usage."),(0,o.kt)("h3",{id:"consider-using-a-secondary-cidr-if-your-ip-space-is-constrained"},"Consider using a secondary CIDR if your IP space is constrained."),(0,o.kt)("p",null,"If you are working with a network that spans multiple connected VPCs or sites the routable address space may be limited.\nFor example, your VPC may be limited to small subnets like below. In this VPC we wouldn\u2019t be able to run more than one ",(0,o.kt)("inlineCode",{parentName:"p"},"m5.16xlarge")," node without adjusting the CNI configuration."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Init VPC",src:t(6450).Z,width:"1061",height:"531"})),(0,o.kt)("p",null,"You can add additional VPC CIDRs from a range that is not routable across VPCs (such as the RFC 6598 range, ",(0,o.kt)("inlineCode",{parentName:"p"},"100.64.0.0/10"),"). In this case we added ",(0,o.kt)("inlineCode",{parentName:"p"},"100.64.0.0/16"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"100.65.0.0/16"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"100.65.0.0/16")," to the VPC (as this is the maximum CIDR size), then created new subnets with those CIDRs.\nFinally we recreated the node groups in the new subnets, leaving the existing EKS cluster control plane in place."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"expanded VPC",src:t(5402).Z,width:"1071",height:"882"})),(0,o.kt)("p",null,"With this configuration you can still communicate with the EKS cluster control plane from connected VPCs but your nodes and pods have plenty of IP addresses to accommodate your workloads and the warm pool."),(0,o.kt)("h2",{id:"tuning-the-vpc-cni"},"Tuning the VPC CNI"),(0,o.kt)("h3",{id:"vpc-cni-and-ec2-rate-throttling"},"VPC CNI and EC2 Rate Throttling"),(0,o.kt)("p",null,"When an EKS worker node is launched it initially has a single ENI with a single IP address attached for the EC2 instance to communicate. As the VPC CNI launches it tries to provision a Warm Pool of IP addresses that can be assigned to Kubernetes Pods (",(0,o.kt)("a",{parentName:"p",href:"https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview"},"More details in the EKS Best Practices Guide"),")."),(0,o.kt)("p",null,"The VPC CNI must make AWS EC2 API calls (like ",(0,o.kt)("inlineCode",{parentName:"p"},"AssignPrivateIpV4Address")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"DescribeNetworkInterfaces"),") to assign those additional IPs and ENIs to the worker node. When the EKS cluster scales out the number of Nodes or Pods there could be a spike in the number of these EC2 API calls. This surge of calls could encounter rate throttling from the EC2 API to help the performance of the service, and to ensure fair usage for all Amazon EC2 customers. This rate throttling can cause the pool of IP address to be exhausted while the CNI tries to allocate more IPs."),(0,o.kt)("p",null,"These failures will cause errors like the one below, indicating that the provisioning of the container network namespace has failed because the VPC CNI could not provision an IP address."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "xxxxxxxxxxxxxxxxxxxxxx" network for pod "test-pod": networkPlugin cni failed to set up pod test-pod_default" network: add cmd: failed to assign an IP address to container\n')),(0,o.kt)("p",null,"This failure delays the launch of the Pod and adds pressure to the kubelet and worker node as this action is retried until the IP address is assigned. To avoid this delay you can configure the CNI to reduce the number of EC2 API calls needed."),(0,o.kt)("h3",{id:"avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn"},"Avoid using ",(0,o.kt)("inlineCode",{parentName:"h3"},"WARM_IP_TARGET")," in large clusters, or cluster with a lot of churn"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"WARM_IP_TARGET")," can help limit the \u201cwasted\u201d IPs for small clusters, or clusters that has very low pod churn. However, this environment variable on the VPC CNI needs to be carefully configured in large clusters as it may increase the number of EC2 API calls, increasing the risk and impact of rate throttling.  "),(0,o.kt)("p",null,"For clusters that have a lot of Pod churn, it is recommended to set ",(0,o.kt)("inlineCode",{parentName:"p"},"MINIMUM_IP_TARGET")," to a value slightly higher than the expected number of pods you plan to run on each node. This will allow the CNI to provision all of those IP addresses in a single (or few) calls."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-hcl"},'  [...]\n\n  # EKS Addons\n  cluster_addons = {\n    vpc-cni = {\n      configuration_values = jsonencode({\n        env = {\n          MINIMUM_IP_TARGET        = "30"\n        }\n      })\n    }\n  }\n\n  [...]\n')),(0,o.kt)("h3",{id:"limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods"},"Limit the number of IPs per node on large instance types with ",(0,o.kt)("inlineCode",{parentName:"h3"},"MAX_ENI")," and ",(0,o.kt)("inlineCode",{parentName:"h3"},"max-pods")),(0,o.kt)("p",null,"When using larger instance types such as ",(0,o.kt)("inlineCode",{parentName:"p"},"16xlarge")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"24xlarge")," the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI"},"number of IP addresses that can be assigned per ENI")," can be fairly large. For example, a ",(0,o.kt)("inlineCode",{parentName:"p"},"c5.18xlarge")," instance type with the default CNI configuration of ",(0,o.kt)("inlineCode",{parentName:"p"},"WARM_ENI=1")," would end up holding 100 IP addresses (50 IPs per ENI * 2 ENIs) when running a handful of pods."),(0,o.kt)("p",null,"For some workloads the CPU, Memory, or other resource will limit the number of Pods on that ",(0,o.kt)("inlineCode",{parentName:"p"},"c5.18xlarge")," before we need more than 50 IPs. In this case you may want to be able to run 30-40 pods maximum on that instance."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-hcl"},'  [...]\n\n  # EKS Addons\n  cluster_addons = {\n    vpc-cni = {\n      configuration_values = jsonencode({\n        env = {\n          MAX_ENI           = "1"\n        }\n      })\n    }\n  }\n\n  [...]\n')),(0,o.kt)("p",null,"Setting the ",(0,o.kt)("inlineCode",{parentName:"p"},"MAX_ENI=1")," option on the CNI and that this will limit the number of IP addresses each node is able to provision, but it does not limit the number of pod that kubernetes will try to schedule to the nodes. This can lead to a situation where pods are scheduled to nodes that are unable to provision more IP addresses."),(0,o.kt)("p",null,"To limit the IPs ",(0,o.kt)("em",{parentName:"p"},"and")," stop k8s from scheduling too many pods you will need to:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Update the CNI configuration environment variables to set ",(0,o.kt)("inlineCode",{parentName:"li"},"MAX_ENI=1")),(0,o.kt)("li",{parentName:"ol"},"Update the ",(0,o.kt)("inlineCode",{parentName:"li"},"--max-pods")," option for the kubelet on the worker nodes.")),(0,o.kt)("p",null,"To configure the --max-pods option you can update the userdata for your worker nodes to set this option ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh"},"via the --kubelet -extra-args in the bootstrap.sh script"),". By default this script configures the max-pods value for the kubelet, the --use-max-pods false` option disables this behavior when providing your own value:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-hcl"},'  eks_managed_node_groups = {\n    system = {\n      instance_types = ["m5.xlarge"]\n\n      min_size     = 0\n      max_size     = 5\n      desired_size = 3\n\n      pre_bootstrap_user_data = <<-EOT\n\n      EOT\n\n      bootstrap_extra_args = "--use-max-pods false --kubelet-extra-args \'--max-pods=<your_value>\'"\n\n    }  \n')),(0,o.kt)("p",null,"One problem is the number of IPs per ENI is different based on the Instance type (",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI"},"for example a ",(0,o.kt)("inlineCode",{parentName:"a"},"m5d.2xlarge")," can have 15 IPs per ENI, where a ",(0,o.kt)("inlineCode",{parentName:"a"},"m5d.4xlarge")," can hold 30 IPs per ENI"),"). This means hard-coding a value for ",(0,o.kt)("inlineCode",{parentName:"p"},"max-pods")," may cause problems if you change instance types or in mixed-instance environments."),(0,o.kt)("p",null,"In the EKS Optimized AMI releases there is ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/max-pods-calculator.sh"},"a script included that can be used to help calculate the AWS Recommended max-pods value"),". If you\u2019d like to automate this calculation for mixed isntances you will also need to update the userdata for your instances to use the ",(0,o.kt)("inlineCode",{parentName:"p"},"--instance-type-from-imds")," flag to autodiscover the instance type from instance metadata."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-hcl"},'  eks_managed_node_groups = {\n    system = {\n      instance_types = ["m5.xlarge"]\n\n      min_size     = 0\n      max_size     = 5\n      desired_size = 3\n\n      pre_bootstrap_user_data = <<-EOT\n        /etc/eks/max-pod-calc.sh --instance-type-from-imds \u2014cni-version 1.13.4 \u2014cni-max-eni 1\n      EOT\n\n      bootstrap_extra_args = "--use-max-pods false --kubelet-extra-args \'--max-pods=<your_value>\'"\n\n    }  \n')),(0,o.kt)("h4",{id:"maxpods-with-karpenter"},"Maxpods with Karpenter"),(0,o.kt)("p",null,"By default, Nodes provisioned by Karpenter will have the max pods on a node ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt"},"based on the node instance type"),". To configure the ",(0,o.kt)("inlineCode",{parentName:"p"},"--max-pods")," option as mentioned above by defining at the Provisioner level by specifying ",(0,o.kt)("inlineCode",{parentName:"p"},"maxPods")," within the ",(0,o.kt)("inlineCode",{parentName:"p"},".spec.kubeletConfiguration")," . This value will be used during Karpenter pod scheduling and passed through to ",(0,o.kt)("inlineCode",{parentName:"p"},"--max-pods")," on kubelet startup."),(0,o.kt)("p",null,"Below is the example Provisioner spec:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: default\nspec:\n  providerRef:\n    name: default\n  requirements:\n    - key: "karpenter.k8s.aws/instance-category"\n      operator: In\n      values: ["c", "m", "r"]\n    - key: "karpenter.sh/capacity-type" # If not included, the webhook for the AWS cloud provider will default to on-demand\n      operator: In\n      values: ["spot", "on-demand"]\n\n  # Karpenter provides the ability to specify a few additional Kubelet args.\n  # These are all optional and provide support for additional customization and use cases.\n  kubeletConfiguration:\n    maxPods: 30\n')),(0,o.kt)("h2",{id:"application"},"Application"),(0,o.kt)("h3",{id:"dns-lookups-and-ndots"},"DNS Lookups and ndots"),(0,o.kt)("p",null,"In ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"},"Kubernetes Pods with the default DNS configuration")," have a ",(0,o.kt)("inlineCode",{parentName:"p"},"resolv.conf")," file like so:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"nameserver 10.100.0.10\nsearch namespace.svc.cluster.local svc.cluster.local cluster.local ec2.internal\noptions ndots:5\n")),(0,o.kt)("p",null,"The domain names listed in the ",(0,o.kt)("inlineCode",{parentName:"p"},"search")," line are appended to DNS names that are not fully qualified domain names (FQDN). For example, if a pod tries to connect to a Kubernetes service using ",(0,o.kt)("inlineCode",{parentName:"p"},"servicename.namespace")," the domains would be appended in order until the DNS name matched the full kubernetes service name:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"servicename.namespace.namespace.svc.cluster.local   <--- Fails with NXDOMAIN\nservicename.namespace.svc.cluster.local        <-- Succeed\n")),(0,o.kt)("p",null,"Whether or not a domain is fully qualified is determined by the ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," option in the resolv.conf. This option defines the number of dots that must be in a domain name before the ",(0,o.kt)("inlineCode",{parentName:"p"},"search")," domains are skipped. These additional searches can add latency to connections to external resources like S3 and RDS endpoints."),(0,o.kt)("p",null,"The default ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," setting in Kubernetes is five, if your application isn\u2019t talking to other pods in the cluster, we can set the ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," to a low value like \u201c2\u201d. This is a good starting point, because it still allows your application to do service discovery within the same namespace and in other namespaces within the cluster, but allows a domain like ",(0,o.kt)("inlineCode",{parentName:"p"},"s3.us-east-2.amazonaws.com")," to be recognized as a FQDN (skipping the ",(0,o.kt)("inlineCode",{parentName:"p"},"search")," domains)."),(0,o.kt)("p",null,"Here\u2019s an example pod manifest from the Kubernetes documentation with ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," set to \u201c2\u201d:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: dns-example\nspec:\n  containers:\n    - name: test\n      image: nginx\n  dnsConfig:\n    options:\n      - name: ndots\n        value: "2"\n')),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"While setting ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," to \u201c2\u201d in your pod deployment is a reasonable place to start, this will not universally work in all situations and shouldn\u2019t be applied across the entire cluster. The ",(0,o.kt)("inlineCode",{parentName:"p"},"ndots")," configuration needs to be configured at the Pod or Deployment level. Reducing this setting at the Cluster level CoreDNS configuration is not recommended.  ")),(0,o.kt)("h3",{id:"inter-az-network-optimization"},"Inter AZ Network Optimization"),(0,o.kt)("p",null,"Some workloads may need to exchange data between Pods in the cluster, like Spark executors during the shuffle stage.\nIf the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front. Hence, for these workloads, it is recommended to colocate executors or worker pods in the same AZ. Colocating workloads in the same AZ serves two main purposes:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Reduce inter-AZ traffic costs"),(0,o.kt)("li",{parentName:"ul"},"Reduce network latency between executors/Pods")),(0,o.kt)("p",null,"To have pods co-located on the same AZ, we can use ",(0,o.kt)("inlineCode",{parentName:"p"},"podAffinity")," based scheduling constraints. The scheduling constraint ",(0,o.kt)("inlineCode",{parentName:"p"},"preferredDuringSchedulingIgnoredDuringExecution")," can be enforced in the Pod spec. For example, ins Spark we can use a custom template for our driver and executor pods:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  executor:\n    affinity:\n      podAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n              matchExpressions:\n              - key: sparkoperator.k8s.io/app-name\n                operator: In\n                values:\n                - <<spark-app-name>>\n          topologyKey: topology.kubernetes.io/zone\n          ...\n")),(0,o.kt)("p",null,"You can also leverage Kubernetes Topology Aware Routing to have Kubernetes services route traffic in more efficient means once pods have been created: ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/"},"https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/")),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Having all executors located in a single AZ, means that AZ will be a ",(0,o.kt)("em",{parentName:"p"},"single point of failure"),". This is a trade off you should consider between lowering network cost and latency, and the event of an AZ failure interrupting workloads.\nIf your workload is running on instances with constrained capacity you may consider using multiple AZs to avoid Insufficient Capacity errors.")))}u.isMDXComponent=!0},5402:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/expanded-vpc-64f8e8bd4f836b1ea5a45f679a209d6d.png"},6450:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/init-vpc-d6c609e16a67746f8c78568ae8d2295e.png"}}]);