"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9018],{5234:(e,r,a)=>{a.d(r,{Ay:()=>i,RM:()=>t});var n=a(4848),s=a(8453);const t=[];function o(e){const r={code:"code",em:"em",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(r.p,{children:["Once our sample data is uploaded you can run the Spark job. You will need to\nreplace the ",(0,n.jsx)(r.em,{children:"<S3_BUCKET>"})," placeholders in this file with the name of the bucket\ncreated earlier. You can get that value by running ",(0,n.jsx)(r.code,{children:"echo $S3_BUCKET"}),"."]}),"\n",(0,n.jsx)(r.p,{children:"To do this automatically you can run the following, which will create a .old\nbackup file and do the replacement for you."})]})}function i(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(o,{...e})}):o(e)}},1825:(e,r,a)=>{a.d(r,{Ay:()=>i,RM:()=>t});var n=a(4848),s=a(8453);const t=[];function o(e){const r={code:"code",em:"em",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(r.p,{children:["Run the ",(0,n.jsx)(r.em,{children:"taxi-trip-execute.sh"})," script with the following input. You will use the ",(0,n.jsx)(r.em,{children:"S3_BUCKET"})," variable created earlier. Additionally, you must change YOUR_REGION_HERE with the region of your choice, ",(0,n.jsx)(r.em,{children:"us-west-2"})," for example."]}),"\n",(0,n.jsx)(r.p,{children:"This script will download some example taxi trip data and create duplicates of\nit in order to increase the size a bit. This will take a bit of time and will\nrequire a relatively fast internet connection."}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/scripts/\nchmod +x taxi-trip-execute.sh\ntaxi-trip-execute.sh ${S3_BUCKET} YOUR_REGION_HERE\n"})}),"\n",(0,n.jsx)(r.p,{children:"You can return to the blueprint directory and continue with the example"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator\n"})})]})}function i(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(o,{...e})}):o(e)}},4:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>u,contentTitle:()=>h,default:()=>f,frontMatter:()=>d,metadata:()=>p,toc:()=>m});var n=a(4848),s=a(8453),t=a(1470),o=a(9365),i=a(2450),l=a(1825),c=a(5234);a(1432);const d={sidebar_position:2,sidebar_label:"Spark Operator with YuniKorn",hide_table_of_contents:!0},h="Spark Operator with YuniKorn",p={id:"blueprints/data-analytics/spark-operator-yunikorn",title:"Spark Operator with YuniKorn",description:"Introduction",source:"@site/docs/blueprints/data-analytics/spark-operator-yunikorn.md",sourceDirName:"blueprints/data-analytics",slug:"/blueprints/data-analytics/spark-operator-yunikorn",permalink:"/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/data-analytics/spark-operator-yunikorn.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"Spark Operator with YuniKorn",hide_table_of_contents:!0},sidebar:"blueprints",previous:{title:"Introduction",permalink:"/data-on-eks/docs/blueprints/data-analytics/"},next:{title:"Spark Operator on EKS with IPv6",permalink:"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6"}},u={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Put sample data in S3",id:"put-sample-data-in-s3",level:2},...l.RM,{value:"NVMe Ephemeral SSD disk for Spark shuffle storage",id:"nvme-ephemeral-ssd-disk-for-spark-shuffle-storage",level:2},...c.RM,{value:"EBS Dynamic PVC for shuffle storage",id:"ebs-dynamic-pvc-for-shuffle-storage",level:2},...c.RM,{value:"Apache YuniKorn Gang Scheduling with NVMe based SSD disk for shuffle storage",id:"apache-yunikorn-gang-scheduling-with-nvme-based-ssd-disk-for-shuffle-storage",level:2},...c.RM,{value:"Using Karpenter Nodepool weights for running Spark Jobs on both AWS Graviton and Intel EC2 Instances",id:"using-karpenter-nodepool-weights-for-running-spark-jobs-on-both-aws-graviton-and-intel-ec2-instances",level:2},{value:"Solution",id:"solution",level:4}];function g(e){const r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"spark-operator-with-yunikorn",children:"Spark Operator with YuniKorn"})}),"\n",(0,n.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsx)(r.p,{children:"The EKS Cluster design for the Data on EKS blueprint is optimized for running Spark applications with Spark Operator and Apache YuniKorn as the batch scheduler. This blueprint leverage Karpenter to scale the worker nodes, AWS for FluentBit is employed for logging, and a combination of Prometheus, Amazon Managed Prometheus, and open source Grafana are used for observability. Additionally, the Spark History Server Live UI is configured for monitoring running Spark jobs through an NLB and NGINX ingress controller."}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Spark workloads with Karpenter"})}),children:[(0,n.jsx)(r.p,{children:"Using Karpenter as the autoscaler, eliminates the need for Managed Node Groups and Cluster Autoscaler for the Spark workloads. In this design, Karpenter and its Nodepools are responsible for creating both On-Demand and Spot instances, dynamically selecting instance types based on user demands. Karpenter offers improved performance compared to Cluster Autoscaler, with more efficient node scaling and faster response times. Karpenter's key features include its ability to scale from zero, optimizing resource utilization and reducing costs when there is no demand for resources. Additionally, Karpenter supports multiple Nodepools, allowing for greater flexibility in defining the required infrastructure for different workload types, such as compute, memory, and GPU-intensive tasks. Furthermore, Karpenter integrates seamlessly with Kubernetes, providing automatic, real-time adjustments to the cluster size based on observed workloads and scaling events. This enables a more efficient and cost-effective EKS cluster design that adapts to the ever-changing demands of Spark applications and other workloads."}),(0,n.jsx)(r.p,{children:(0,n.jsx)(r.img,{alt:"img.png",src:a(5178).A+"",width:"1398",height:"901"})}),(0,n.jsx)(r.p,{children:"The blueprint configures the Karpenter Nodepools and Ec2 classes in the tabs below."}),(0,n.jsxs)(t.A,{children:[(0,n.jsxs)(o.A,{value:"spark-memory-optimized",label:"spark-memory-optimized",children:[(0,n.jsx)(r.p,{children:"This NodePool uses the r5d instance type, from xlarge to 8xlarge sizes, which are great for Spark jobs require more memory."}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L177-L223",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]}),(0,n.jsxs)(o.A,{value:"spark-graviton-memory-optimized",label:"spark-graviton-memory-optimized",children:[(0,n.jsx)(r.p,{children:"This NodePool uses the r6g, r6gd, r7g, r7gd, and r8g instance type, from 4xlarge to 16xlarge sizes"}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L117-L170",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]}),(0,n.jsxs)(o.A,{value:"spark-compute-optimized",label:"spark-compute-optimized",children:[(0,n.jsx)(r.p,{children:"This NodePool uses the C5d instance type, from 4xlarge to 24xlarge sizes, which are great for Spark jobs require more CPU time."}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L63-L110",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]}),(0,n.jsxs)(o.A,{value:"spark-vertical-ebs-scale",label:"spark-vertical-ebs-scale",children:[(0,n.jsx)(r.p,{children:"This NodePool uses a broad range of EC2 instance types, and in the bootstrapping the instances create and mount a secondary EBS volume. This volume size is scaled based on the number of cores on the Ec2 instance.\nThis provides a secondary storage location that can be used for Spark workloads, reducing the load on the root volume of the instance and avoiding impact to system daemons or the kubelet. As larger nodes can accept more pods the bootstrapping creates a larger volume for the larger instances."}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L230-L355",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]})]})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"NVMe SSD Instance Storage for Spark Shuffle data"})}),children:[(0,n.jsx)(r.p,{children:'It is important to note that the NodePools in this EKS Cluster design utilize NVMe SSD instance storage for each node to serve as shuffle storage for Spark workloads. These high-performance storage options are available with all "d" type instances.'}),(0,n.jsx)(r.p,{children:"The use of NVMe SSD instance storage as shuffle storage for Spark brings numerous advantages. First, it provides low-latency and high-throughput data access, significantly improving Spark's shuffle performance. This results in faster job completion times and enhanced overall application performance. Second, the use of local SSD storage reduces the reliance on remote storage systems, such as EBS volumes, which can become a bottleneck during shuffle operations. This also reduces the costs associated with provisioning and managing additional EBS volumes for shuffle data. Finally, by leveraging NVMe SSD storage, the EKS cluster design offers better resource utilization and increased performance, allowing Spark applications to process larger datasets and tackle more complex analytics workloads more efficiently. This optimized storage solution ultimately contributes to a more scalable and cost-effective EKS cluster tailored for running Spark workloads on Kubernetes."}),(0,n.jsxs)(r.p,{children:["The NVMe SSD instance storage is configured by the bootstrapping scripts when the instance launches (",(0,n.jsxs)(r.a,{href:"https://karpenter.sh/docs/concepts/nodeclasses/#specinstancestorepolicy",children:["the Karpenter NodePools are configured with ",(0,n.jsx)(r.code,{children:"instanceStorePolicy: RAID0"})]}),"). The NVMe devices are combined into a single RAID0 (striped) array then mounted to ",(0,n.jsx)(r.code,{children:"/mnt/k8s-disks/0"}),". This directory is further linked with ",(0,n.jsx)(r.code,{children:"/var/lib/kubelet"}),",",(0,n.jsx)(r.code,{children:" /var/lib/containerd"})," and ",(0,n.jsx)(r.code,{children:"/var/log/pods"}),", ensuring that all data written to those locations is stored on the NVMe devices. Because data written inside of a pod will be written to one of these directories the Pods benefit from high-performance storage without having to leverage hostPath mounts or PersistentVolumes"]})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Spark Operator"})}),children:[(0,n.jsx)(r.p,{children:"The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes."}),(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"a SparkApplication controller that watches events of creation, updates, and deletion of SparkApplication objects and acts on the watch events,"}),"\n",(0,n.jsx)(r.li,{children:"a submission runner that runs spark-submit for submissions received from the controller,"}),"\n",(0,n.jsx)(r.li,{children:"a Spark pod monitor that watches for Spark pods and sends pod status updates to the controller,"}),"\n",(0,n.jsx)(r.li,{children:"a Mutating Admission Webhook that handles customizations for Spark driver and executor pods based on the annotations on the pods added by the controller,"}),"\n",(0,n.jsx)(r.li,{children:"and also a command-line tool named sparkctl for working with the operator."}),"\n"]}),(0,n.jsx)(r.p,{children:"The following diagram shows how different components of Spark Operator add-on interact and work together."}),(0,n.jsx)(r.p,{children:(0,n.jsx)(r.img,{alt:"img.png",src:a(7129).A+"",width:"960",height:"540"})})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Deploying the Solution"})}),children:[(0,n.jsxs)(r.p,{children:["In this ",(0,n.jsx)(r.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator",children:"example"}),", you will provision the following resources required to run Spark Jobs with open source Spark Operator and Apache YuniKorn."]}),(0,n.jsx)(r.p,{children:"This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC."}),(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"Creates a new sample VPC, 2 Private Subnets, 2 Public Subnets, and 2 subnets in the RFC6598 space (100.64.0.0/10) for EKS Pods."}),"\n",(0,n.jsx)(r.li,{children:"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets"}),"\n",(0,n.jsx)(r.li,{children:"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with Managed Node Groups for benchmarking and core services, and Karpenter NodePools for Spark workloads."}),"\n",(0,n.jsx)(r.li,{children:"Deploys Metrics server, Spark-operator, Apache Yunikorn, Karpenter, Cluster Autoscaler, Grafana, AMP and Prometheus server."}),"\n"]}),(0,n.jsx)(r.h3,{id:"prerequisites",children:"Prerequisites"}),(0,n.jsx)(r.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,n.jsx)(r.h3,{id:"deploy",children:"Deploy"}),(0,n.jsx)(r.p,{children:"Clone the repository."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\ncd data-on-eks\nexport DOEKS_HOME=$(pwd)\n"})}),(0,n.jsxs)(r.p,{children:["If DOEKS_HOME is ever unset, you can always set it manually using ",(0,n.jsx)(r.code,{children:"export DATA_ON_EKS=$(pwd)"})," from your data-on-eks directory."]}),(0,n.jsxs)(r.p,{children:["Navigate into one of the example directories and run ",(0,n.jsx)(r.code,{children:"install.sh"})," script."]}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator\nchmod +x install.sh\n./install.sh\n"})}),(0,n.jsx)(r.p,{children:"Now create an S3_BUCKET variable that holds the name of the bucket created\nduring the install. This bucket will be used in later examples to store output\ndata. If S3_BUCKET is ever unset, you can run the following commands again."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)\necho $S3_BUCKET\n"})})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Execute Sample Spark job with Karpenter"})}),children:[(0,n.jsx)(r.p,{children:"Navigate to example directory and submit the Spark job."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/karpenter\nkubectl apply -f pyspark-pi-job.yaml\n"})}),(0,n.jsx)(r.p,{children:"Monitor the job status using the below command.\nYou should see the new nodes triggered by the karpenter and the YuniKorn will schedule one driver pod and 2 executor pods on this node."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"kubectl get pods -n spark-team-a -w\n"})}),(0,n.jsx)(r.p,{children:"You can check the status of the SparkApplication if the pods are already completed:"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"kubectl kubectl describe sparkapplication pyspark-pi-karpenter -n spark-team-a\n"})}),(0,n.jsx)(r.p,{children:"You can try the following examples to leverage multiple Karpenter Nodepools, EBS as Dynamic PVC instead of SSD and YuniKorn Gang Scheduling."}),(0,n.jsx)(r.h2,{id:"put-sample-data-in-s3",children:"Put sample data in S3"}),(0,n.jsx)(l.Ay,{}),(0,n.jsx)(r.h2,{id:"nvme-ephemeral-ssd-disk-for-spark-shuffle-storage",children:"NVMe Ephemeral SSD disk for Spark shuffle storage"}),(0,n.jsx)(r.p,{children:"Example PySpark job that uses NVMe based ephemeral SSD disk for Driver and Executor shuffle storage"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/karpenter/\n"})}),(0,n.jsx)(c.Ay,{filename:"./nvme-ephemeral-storage.yaml"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"sed -i.old s/\\<S3_BUCKET\\>/${S3_BUCKET}/g ./nvme-ephemeral-storage.yaml\n"})}),(0,n.jsx)(r.p,{children:"Now that the bucket name is in place you can create the Spark job."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"kubectl apply -f nvme-ephemeral-storage.yaml\n"})}),(0,n.jsx)(r.h2,{id:"ebs-dynamic-pvc-for-shuffle-storage",children:"EBS Dynamic PVC for shuffle storage"}),(0,n.jsx)(r.p,{children:"Example PySpark job that uses EBS ON_DEMAND volumes using Dynamic PVCs for Driver and Executor shuffle storage"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/karpenter/\n"})}),(0,n.jsx)(c.Ay,{filename:"./ebs-storage-dynamic-pvc.yaml"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"sed -i.old s/\\<S3_BUCKET\\>/${S3_BUCKET}/g ./ebs-storage-dynamic-pvc.yaml\n"})}),(0,n.jsx)(r.p,{children:"Now that the bucket name is in place you can create the Spark job."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"kubectl apply -f ebs-storage-dynamic-pvc.yaml\n"})}),(0,n.jsx)(r.h2,{id:"apache-yunikorn-gang-scheduling-with-nvme-based-ssd-disk-for-shuffle-storage",children:"Apache YuniKorn Gang Scheduling with NVMe based SSD disk for shuffle storage"}),(0,n.jsx)(r.p,{children:"Gang Scheduling Spark jobs using Apache YuniKorn and Spark Operator"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/karpenter/\n"})}),(0,n.jsx)(c.Ay,{filename:"./nvme-storage-yunikorn-gang-scheduling.yaml"}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"sed -i.old s/\\<S3_BUCKET\\>/${S3_BUCKET}/g ./nvme-storage-yunikorn-gang-scheduling.yaml\n"})}),(0,n.jsx)(r.p,{children:"Now that the bucket name is in place you can create the Spark job."}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"kubectl apply -f nvme-storage-yunikorn-gang-scheduling.yaml\n"})})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Karpenter Nodepool weights with Graviton and Intel"})}),children:[(0,n.jsx)(r.h2,{id:"using-karpenter-nodepool-weights-for-running-spark-jobs-on-both-aws-graviton-and-intel-ec2-instances",children:"Using Karpenter Nodepool weights for running Spark Jobs on both AWS Graviton and Intel EC2 Instances"}),(0,n.jsxs)(r.p,{children:["Customers often seek to leverage AWS Graviton instances for running Spark jobs due to their cost savings and performance improvements over traditional Intel instances. However, a common challenge is the availability of Graviton instances in specific regions or availability zones, especially during times of high demand. To address this, a ",(0,n.jsx)(r.a,{href:"https://karpenter.sh/docs/concepts/scheduling/#weighted-nodepools",children:"fallback strategy"})," to equivalent Intel instances is desirable."]}),(0,n.jsx)(r.h4,{id:"solution",children:"Solution"}),(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Step 1: Create a Multi-Architecture Spark Docker Image"}),"\nFirst, ensure that your Spark job can run on both AWS Graviton (ARM architecture) and Intel (AMD architecture) instances by creating a multi-architecture Docker image. You can find a sample ",(0,n.jsx)(r.a,{href:"../../../../analytics/terraform/spark-k8s-operator/examples/docker/Dockerfile",children:"Dockerfile"})," and ",(0,n.jsx)(r.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator/examples/docker",children:"instructions for building and pushing this image to Amazon Elastic Container Registry (ECR) in the examples directory"}),"."]}),(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Step 2: Deploy Two Karpenter Nodepools with weights"}),"\nDeploy two separate Karpenter Nodepools: one configured for Graviton instances and the other for Intel instances."]}),(0,n.jsxs)(r.p,{children:["Graviton Nodepool (ARM): Set the weight of the Graviton Nodepool to ",(0,n.jsx)(r.code,{children:"100"}),". This prioritizes Graviton instances for your Spark workloads."]}),(0,n.jsxs)(r.p,{children:["Intel Nodepool (AMD): Set the weight of the Intel Nodepool to ",(0,n.jsx)(r.code,{children:"50"}),". This ensures that Karpenter will fall back to the Intel Nodepool when Graviton instances are either unavailable or reach their maximum CPU capacity."]}),(0,n.jsx)(r.p,{children:"The memory optimized Karpenter NodePools are configured with these weights."}),(0,n.jsxs)(t.A,{children:[(0,n.jsxs)(o.A,{value:"spark-memory-optimized",label:"spark-memory-optimized",children:[(0,n.jsx)(r.p,{children:"This NodePool uses the r5d instance type, from xlarge to 8xlarge sizes, which are great for Spark jobs require more memory."}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L177-L223",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]}),(0,n.jsxs)(o.A,{value:"spark-graviton-memory-optimized",label:"spark-graviton-memory-optimized",children:[(0,n.jsx)(r.p,{children:"This NodePool uses the r6g, r6gd, r7g, r7gd, and r8g instance type, from 4xlarge to 16xlarge sizes"}),(0,n.jsxs)(r.p,{children:["To view the Karpenter configuration ",(0,n.jsxs)(r.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/addons.tf#L117-L170",children:["review the ",(0,n.jsx)(r.code,{children:"addons.tf"})," file here"]})]})]})]}),(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.strong,{children:"Step 3: Use a label selector that targets both Nodepools"}),"\nSince both nodepools have the label ",(0,n.jsx)(r.code,{children:"multiArch: Spark"})," we can configure our Spark job with a NodeSelector that matches that label. This will allow Karpenter to provision nodes from both of memory-optimized nodepools, and it will start with the Graviton instances due to the weights configured above."]}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-yaml",children:"    nodeSelector:\n      multiArch: Spark\n"})})]}),"\n",(0,n.jsxs)(i.A,{header:(0,n.jsx)(r.h2,{children:(0,n.jsx)(r.span,{children:"Cleanup"})}),children:[(0,n.jsx)(r.admonition,{type:"caution",children:(0,n.jsx)(r.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"})}),(0,n.jsxs)(r.p,{children:["This script will cleanup the environment using ",(0,n.jsx)(r.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator && chmod +x cleanup.sh\n./cleanup.sh\n"})})]})]})}function f(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(g,{...e})}):g(e)}},9365:(e,r,a)=>{a.d(r,{A:()=>o});a(6540);var n=a(4164);const s={tabItem:"tabItem_Ymn6"};var t=a(4848);function o(e){let{children:r,hidden:a,className:o}=e;return(0,t.jsx)("div",{role:"tabpanel",className:(0,n.A)(s.tabItem,o),hidden:a,children:r})}},1470:(e,r,a)=>{a.d(r,{A:()=>y});var n=a(6540),s=a(4164),t=a(3104),o=a(6347),i=a(205),l=a(7485),c=a(1682),d=a(679);function h(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:r}=e;return!!r&&"object"==typeof r&&"value"in r}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:r,children:a}=e;return(0,n.useMemo)((()=>{const e=r??function(e){return h(e).map((e=>{let{props:{value:r,label:a,attributes:n,default:s}}=e;return{value:r,label:a,attributes:n,default:s}}))}(a);return function(e){const r=(0,c.XI)(e,((e,r)=>e.value===r.value));if(r.length>0)throw new Error(`Docusaurus error: Duplicate values "${r.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[r,a])}function u(e){let{value:r,tabValues:a}=e;return a.some((e=>e.value===r))}function m(e){let{queryString:r=!1,groupId:a}=e;const s=(0,o.W6)(),t=function(e){let{queryString:r=!1,groupId:a}=e;if("string"==typeof r)return r;if(!1===r)return null;if(!0===r&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:r,groupId:a});return[(0,l.aZ)(t),(0,n.useCallback)((e=>{if(!t)return;const r=new URLSearchParams(s.location.search);r.set(t,e),s.replace({...s.location,search:r.toString()})}),[t,s])]}function g(e){const{defaultValue:r,queryString:a=!1,groupId:s}=e,t=p(e),[o,l]=(0,n.useState)((()=>function(e){let{defaultValue:r,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(r){if(!u({value:r,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${r}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return r}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:r,tabValues:t}))),[c,h]=m({queryString:a,groupId:s}),[g,f]=function(e){let{groupId:r}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(r),[s,t]=(0,d.Dv)(a);return[s,(0,n.useCallback)((e=>{a&&t.set(e)}),[a,t])]}({groupId:s}),b=(()=>{const e=c??g;return u({value:e,tabValues:t})?e:null})();(0,i.A)((()=>{b&&l(b)}),[b]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!u({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),f(e)}),[h,f,t]),tabValues:t}}var f=a(2303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=a(4848);function k(e){let{className:r,block:a,selectedValue:n,selectValue:o,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,t.a_)(),d=e=>{const r=e.currentTarget,a=l.indexOf(r),s=i[a].value;s!==n&&(c(r),o(s))},h=e=>{let r=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=l.indexOf(e.currentTarget)+1;r=l[a]??l[0];break}case"ArrowLeft":{const a=l.indexOf(e.currentTarget)-1;r=l[a]??l[l.length-1];break}}r?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":a},r),children:i.map((e=>{let{value:r,label:a,attributes:t}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:n===r?0:-1,"aria-selected":n===r,ref:e=>l.push(e),onKeyDown:h,onClick:d,...t,className:(0,s.A)("tabs__item",b.tabItem,t?.className,{"tabs__item--active":n===r}),children:a??r},r)}))})}function j(e){let{lazy:r,children:a,selectedValue:t}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(r){const e=o.find((e=>e.props.value===t));return e?(0,n.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:o.map(((e,r)=>(0,n.cloneElement)(e,{key:r,hidden:e.props.value!==t})))})}function v(e){const r=g(e);return(0,x.jsxs)("div",{className:(0,s.A)("tabs-container",b.tabList),children:[(0,x.jsx)(k,{...r,...e}),(0,x.jsx)(j,{...r,...e})]})}function y(e){const r=(0,f.A)();return(0,x.jsx)(v,{...e,children:h(e.children)},String(r))}},2450:(e,r,a)=>{a.d(r,{A:()=>m});var n=a(6540),s=a(5556),t=a.n(s),o=a(4164);const i="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=a(4848);function u(e){let{children:r,header:a}=e;const[s,t]=(0,n.useState)(!1);return(0,p.jsxs)("div",{className:i,children:[(0,p.jsxs)("div",{className:(0,o.A)(l,{[h]:s}),onClick:()=>{t(!s)},children:[a,(0,p.jsx)("span",{className:(0,o.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,p.jsx)("div",{className:d,children:r})]})}u.propTypes={children:t().node.isRequired,header:t().node.isRequired};const m=u},5178:(e,r,a)=>{a.d(r,{A:()=>n});const n=a.p+"assets/images/eks-spark-operator-karpenter-e045e7b7342edb390b25147dc4c98dd6.png"},7129:(e,r,a)=>{a.d(r,{A:()=>n});const n=a.p+"assets/images/spark-operator-6752098849b2e90ded1f19770c70f101.png"}}]);