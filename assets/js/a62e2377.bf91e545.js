"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[4338],{3330:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>p,frontMatter:()=>l,metadata:()=>c,toc:()=>d});var i=s(4848),a=s(8453),r=s(2450);const l={title:"DeepSeek-R1 on EKS",sidebar_position:1},t="DeepSeek-R1 on EKS with Ray and vLLM",c={id:"gen-ai/inference/GPUs/ray-vllm-deepseek",title:"DeepSeek-R1 on EKS",description:"In this guide, we'll explore deploying DeepSeek-R1-Distill-Llama-8B model inference using Ray with a vLLM backend on Amazon EKS.",source:"@site/docs/gen-ai/inference/GPUs/ray-vllm-deepseek.md",sourceDirName:"gen-ai/inference/GPUs",slug:"/gen-ai/inference/GPUs/ray-vllm-deepseek",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/ray-vllm-deepseek",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/GPUs/ray-vllm-deepseek.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"DeepSeek-R1 on EKS",sidebar_position:1},sidebar:"genai",previous:{title:"Inference on EKS",permalink:"/data-on-eks/docs/category/inference-on-eks"},next:{title:"RayServe with vLLM",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve"}},o={},d=[{value:"Understanding the GPU Memory Requirements",id:"understanding-the-gpu-memory-requirements",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Deploying DeepSeek-R1-Distill-Llama-8B with RayServe and vLLM",id:"deploying-deepseek-r1-distill-llama-8b-with-rayserve-and-vllm",level:2},{value:"Test the DeepSeek-R1 Model",id:"test-the-deepseek-r1-model",level:2},{value:"Deploy Open Web Ui",id:"deploy-open-web-ui",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"deepseek-r1-on-eks-with-ray-and-vllm",children:"DeepSeek-R1 on EKS with Ray and vLLM"})}),"\n",(0,i.jsxs)(n.p,{children:["In this guide, we'll explore deploying ",(0,i.jsx)(n.a,{href:"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",children:"DeepSeek-R1-Distill-Llama-8B"})," model inference using ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/getting_started.html",children:"Ray"})," with a ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," backend on ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(2316).A+"",width:"1024",height:"1024"})}),"\n",(0,i.jsx)(n.h2,{id:"understanding-the-gpu-memory-requirements",children:"Understanding the GPU Memory Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["Deploying an 8B parameter model like ",(0,i.jsx)(n.code,{children:"DeepSeek-R1-Distill-Llama"})," requires careful memory planning. Each model parameter typically consumes 2 bytes (",(0,i.jsx)(n.code,{children:"BF16"})," precision), meaning the full model weights require around ",(0,i.jsx)(n.code,{children:"14.99 GiB"})," GPU memory. Below is the actual memory usage observed during deployment:"]}),"\n",(0,i.jsx)(n.p,{children:"Log sample from Ray deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-log",children:"INFO model_runner.py:1115] Loading model weights took 14.99 GiB\nINFO worker.py:266] vLLM instance can use total GPU memory (22.30 GiB) x utilization (0.90) = 20.07 GiB\nINFO worker.py:266] Model weights: 14.99 GiB | Activation memory: 0.85 GiB | KV Cache: 4.17 GiB\n"})}),"\n",(0,i.jsxs)(n.p,{children:["G5 instances provide single ",(0,i.jsx)(n.code,{children:"A10G"})," GPUs with ",(0,i.jsx)(n.code,{children:"24 GiB"})," memory, ideal for running one large LLM inference process per instance. For this deployment, we use ",(0,i.jsx)(n.code,{children:"G5.4xlarge"}),", which has 1x NVIDIA A10G GPU (24 GiB), 16 vCPUs and 64 GiB RAM."]}),"\n",(0,i.jsx)(n.p,{children:"Using vLLM, we optimize memory utilization, enabling us to maximize inference speed while preventing out-of-memory (OOM) crashes."}),"\n",(0,i.jsxs)(r.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Deploying EKS Cluster and Addons"})}),children:[(0,i.jsx)(n.p,{children:"Our tech stack includes:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"})," \u2013 A managed Kubernetes service that simplifies deploying, managing, and scaling containerized applications using Kubernetes on AWS."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/getting_started.html",children:"Ray"})," \u2013 An open-source distributed computing framework that enables scalable and efficient execution of machine learning inference workloads."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," \u2013 A high-throughput and memory-efficient inference and serving engine for large language models (LLMs), optimized for GPU execution.\nAWSLABS.GITHUB.IO"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," \u2013 An open-source Kubernetes cluster autoscaler that dynamically provisions and manages compute resources, such as G5 instances, to improve application availability and cluster efficiency"]}),"\n"]}),"\n"]}),(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,i.jsx)(n.p,{children:"Before we begin, ensure you have all the necessary prerequisites in place to make the deployment process smooth. Make sure you have installed the following tools on your machine:"}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["To simplify the demo process, we assume the use of an IAM role with administrative privileges due to the complexity of creating minimal IAM roles for each blueprint that may create various AWS services. However, for production deployments, it is strongly advised to create an IAM role with only the necessary permissions. Employing tools such as ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/iam/access-analyzer/",children:"IAM Access Analyzer"})," can assist in ensuring a least-privilege approach."]})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,i.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,i.jsx)(n.p,{children:"Clone the repository"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Important Note:"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step1"}),": Ensure that you update the region in the ",(0,i.jsx)(n.code,{children:"variables.tf"})," file before deploying the blueprint.\nAdditionally, confirm that your local region setting matches the specified region to prevent any discrepancies."]}),(0,i.jsxs)(n.p,{children:["For example, set your ",(0,i.jsx)(n.code,{children:'export AWS_DEFAULT_REGION="<REGION>"'})," to the desired region:"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step2"}),": Run the installation script."]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/jark-stack/terraform && chmod +x install.sh\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,i.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,i.jsx)(n.p,{children:"Once the installation finishes, verify the Amazon EKS Cluster."}),(0,i.jsx)(n.p,{children:"Creates k8s config file to authenticate with EKS."}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 update-kubeconfig --name jark-stack\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                                           STATUS   ROLES    AGE    VERSION\nip-100-64-118-130.us-west-2.compute.internal   Ready    <none>   3h9m   v1.30.0-eks-036c24b\nip-100-64-127-174.us-west-2.compute.internal   Ready    <none>   9h     v1.30.0-eks-036c24b\nip-100-64-132-168.us-west-2.compute.internal   Ready    <none>   9h     v1.30.0-eks-036c24b\n"})}),(0,i.jsx)(n.p,{children:"Verify the Karpenter autosclaer Nodepools"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get nodepools\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                NODECLASS\ng5-gpu-karpenter    g5-gpu-karpenter\nx86-cpu-karpenter   x86-cpu-karpenter\n"})}),(0,i.jsx)(n.p,{children:"Verify the NVIDIA Device plugin"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n nvidia-device-plugin\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                                                              READY   STATUS    RESTARTS   AGE\nnvidia-device-plugin-gpu-feature-discovery-b4clk                  1/1     Running   0          3h13m\nnvidia-device-plugin-node-feature-discovery-master-568b49722ldt   1/1     Running   0          9h\nnvidia-device-plugin-node-feature-discovery-worker-clk9b          1/1     Running   0          3h13m\nnvidia-device-plugin-node-feature-discovery-worker-cwg28          1/1     Running   0          9h\nnvidia-device-plugin-node-feature-discovery-worker-ng52l          1/1     Running   0          9h\nnvidia-device-plugin-p56jj                                        1/1     Running   0          3h13m\n"})}),(0,i.jsxs)(n.p,{children:["Verify ",(0,i.jsx)(n.a,{href:"https://github.com/ray-project/kuberay",children:"Kuberay Operator"})," which is used to create Ray Clusters"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n kuberay-operator\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-7894df98dc-447pm   1/1     Running   0          9h\n"})})]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-deepseek-r1-distill-llama-8b-with-rayserve-and-vllm",children:"Deploying DeepSeek-R1-Distill-Llama-8B with RayServe and vLLM"}),"\n",(0,i.jsxs)(n.p,{children:["With the EKS cluster deployed and all necessary components in place, we can now proceed with deploying ",(0,i.jsx)(n.code,{children:"DeepSeek-R1-Distill-Llama-8B"})," using ",(0,i.jsx)(n.code,{children:"RayServe"})," and ",(0,i.jsx)(n.code,{children:"vLLM"}),". This guide outlines the steps to export the Hugging Face Hub token, create a Docker image (if required), and deploy the RayServe cluster."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Step1: Export the Hugging Face Hub Token"})}),"\n",(0,i.jsx)(n.p,{children:"Before deploying the model, you need to authenticate with Hugging Face to access the required model files. Follow these steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Create a Hugging Face account (if you do not already have one)."}),"\n",(0,i.jsx)(n.li,{children:"Generate an access token:"}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to Hugging Face Settings \u2192 Access Tokens."}),"\n",(0,i.jsx)(n.li,{children:"Create a new token with read permissions."}),"\n",(0,i.jsx)(n.li,{children:"Copy the generated token."}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsx)(n.li,{children:"Export the token as an environment variable in your terminal:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Note: The token must be base64-encoded before being used in Kubernetes secrets."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Step2: Create a Docker image"})}),"\n",(0,i.jsx)(n.p,{children:"To deploy the model efficiently, you need a Docker image that includes Ray, vLLM, and Hugging Face dependencies. Follow these steps:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use the provided Dockerfile:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"gen-ai/inference/vllm-ray-gpu-deepseek/Dockerfile\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"This Dockerfile is based on a Ray image and includes vLLM and Hugging Face libraries. No additional packages are required for this deployment."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Build and push the Docker image to Amazon ECR"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"OR"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use a pre-built image (for PoC deployments):"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"If you want to skip building and pushing a custom image, you can use the public ECR image:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"public.ecr.aws/data-on-eks/ray-2.41.0-py310-cu118-vllm0.7.0"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Note: If using a custom image, replace the image reference in the RayServe YAML file with your ECR image URI."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Step3: Deploy RayServe Cluster"})}),"\n",(0,i.jsx)(n.p,{children:"RayServe cluster is defined in a YAML configuration file that includes multiple resources:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Namespace for isolating the deployment."}),"\n",(0,i.jsx)(n.li,{children:"Secret for securely storing the Hugging Face Hub token."}),"\n",(0,i.jsx)(n.li,{children:"ConfigMap containing the serving script (OpenAI-compatible API interface)."}),"\n",(0,i.jsxs)(n.li,{children:["RayServe definition that includes:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A Ray head pod deployed on an x86 node."}),"\n",(0,i.jsx)(n.li,{children:"Ray worker pods deployed on GPU instances (g5.4xlarge)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deployment Steps"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Note: Ensure that the image: field in ",(0,i.jsx)(n.code,{children:"ray-vllm-deepseek.yml"})," is correctly set to either your custom ECR image URI or the default public ECR image."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Navigate to the directory containing the RayServe configuration and Apply the configuration using kubectl"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"cd gen-ai/inference/vllm-ray-gpu-deepseek/\nenvsubst < ray-vllm-deepseek.yml | kubectl apply -f -\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/rayserve-vllm created\nsecret/hf-token created\nconfigmap/vllm-serve-script created\nrayservice.ray.io/vllm created\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Step4: Monitor the deployment"})}),"\n",(0,i.jsx)(n.p,{children:"To monitor the deployment and check the status of the pods, run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pod -n rayserve-vllm\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Note: The image pull process may take up to 8 minutes on the first deployment. Subsequent updates will leverage the local cache. This can be optimized by building leaner images containing only necessary dependencies."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                                           READY   STATUS            RESTARTS   AGE\nvllm-raycluster-7qwlm-head-vkqsc               2/2     Running           0          8m47s\nvllm-raycluster-7qwlm-worker-gpu-group-vh2ng   0/1     PodInitializing   0          8m47s\n"})}),"\n",(0,i.jsx)(n.p,{children:"This deployment also creates a DeepSeek-R1 service with multiple ports:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"8265"})," - Ray Dashboard"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"8000"})," - DeepSeek-R1 model endpoint"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Run the following command to verify the services:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n rayserve-vllm\n\nNAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE\nvllm             ClusterIP   172.20.208.16    <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   48m\nvllm-head-svc    ClusterIP   172.20.239.237   <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   37m\nvllm-serve-svc   ClusterIP   172.20.196.195   <none>        8000/TCP                                        37m\n"})}),"\n",(0,i.jsx)(n.p,{children:"To access the Ray dashboard, you can port-forward the relevant port to your local machine:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n rayserve-vllm port-forward svc/vllm 8265:8265\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can then access the web UI at ",(0,i.jsx)(n.a,{href:"http://localhost:8265",children:"http://localhost:8265"}),", which displays the deployment of jobs and actors within the Ray ecosystem."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Model deploymen takes around 4 mins"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(1413).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(4062).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(2375).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.h2,{id:"test-the-deepseek-r1-model",children:"Test the DeepSeek-R1 Model"}),"\n",(0,i.jsx)(n.p,{children:"Now it's time to test the DeepSeek-R1-Distill-Llama-8B chat model."}),"\n",(0,i.jsxs)(n.p,{children:["First, execute a port forward to the ",(0,i.jsx)(n.code,{children:"vllm-serve-svc"})," Service using kubectl:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n rayserve-vllm port-forward svc/vllm-serve-svc 8000:8000\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Run a test inference request:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:'curl -X POST http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d \'{\n    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",\n    "messages": [{"role": "user", "content": \u201cExplain about DeepSeek model?\u201d}],\n    "stream": false\n}\'\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{"id":"chatcmpl-b86feed9-1482-4d1c-981d-085651d12813","object":"chat.completion","created":1739001265,"model":"deepseek-ai/DeepSeek-R1-Distill-Llama-8B","choices":[{"index":0,"message":{"role":"assistant","content":"<think>\\n\\n</think>\\n\\nDeepSeek is a powerful AI search engine developed by the Chinese Company DeepSeek Inc. It is designed to solve complex STEM (Science, Technology, Engineering, and Mathematics) problems through precise reasoning and efficient computation. The model works bymidtTeX, combining large-scale dataset and strong reasoning capabilities to provide accurate and reliable answers.\\n\\n### Key Features:\\n1. **AI-powered Search**: DeepSeek uses advanced AI techniques to understand and analyze vast amounts of data, providing more accurate and relevant search results compared to traditional search engines.\\n2. **Reasoning and Problem-solving**: The model is equipped with strong reasoning capabilities, enabling it to solve complex STEM problems, answer research-level questions, and assist in decision-making.\\n3. **Customization**: DeepSeek can be tailored to specific domains or industries, allowing it to be adapted for various use cases such as academic research, business analysis, and technical problem-solving.\\n4. **Efficiency**: The model is highly efficient, fast, and scalable, making it suitable for a wide range of applications and handling large-scale data processing tasks.\\n5. **Domain Expertise**: It can be trained on domain-specific data and knowledge, making it highly specialized in particular fields like mathematics, programming, or engineering.\\n\\n### Applications:\\n- **Education and Research**: Assisting students and researchers with complex STEM problems and research questions.\\n- **Business Analysis**: aiding in market research, data analysis, and strategic decision-making.\\n- **Technical Support**: solving technical issues and providing troubleshooting assistance.\\n- **Custom Problem Solving**: addressing specific challenges in various fields by leveraging domain-specific knowledge.\\n\\nDeepSeek is a valuable tool for any individual or organizationengaged in STEM fields or requires advanced AI-powered search and reasoning capabilities.","tool_calls":[]},"logprobs":null,"finish_reason":"stop","stop_reason":null}],"usage":{"prompt_tokens":10,"total_tokens":359,"completion_tokens":349,"prompt_tokens_details":null},"prompt_logprobs":null}%\n'})}),"\n",(0,i.jsx)(n.h2,{id:"deploy-open-web-ui",children:"Deploy Open Web Ui"}),"\n",(0,i.jsx)(n.p,{children:"Now, let's deploy the open-source Open WebUI, which provides a ChatGPT-style chat interface to interact with the DeepSeek model deployed on EKS. Open WebUI will use the model service to send requests and receive responses."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deploy Open WebUI"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Verify the YAML file ",(0,i.jsx)(n.code,{children:"gen-ai/inference/vllm-ray-gpu-deepseek/open-webui.yaml"})," for Open WebUI. This is deployed as a container in EKS, and it communicates with the model service."]}),"\n",(0,i.jsx)(n.li,{children:"Apply the Open WebUI deployment:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd gen-ai/inference/vllm-ray-gpu-deepseek/\nkubectl apply -f open-webui.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/openai-webui created\ndeployment.apps/open-webui created\nservice/open-webui created\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Accessing the Open WebUI"})}),"\n",(0,i.jsx)(n.p,{children:"To open the web UI, port-forward the Open WebUI service:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n open-webui port-forward svc/open-webui 8080:80\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Then, open a browser and navigate to: ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})]}),"\n",(0,i.jsx)(n.p,{children:"You will see a registration page. Register with your name, email, and password."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(7208).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(5073).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(4906).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(2483).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(2439).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.p,{children:"After submitting a request, you can monitor the GPU and CPU usage returning to normal:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(4765).A+"",width:"3024",height:"1652"})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Model Initialization & Memory Allocation"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Once deployed, the model automatically detects CUDA and initializes its execution environment."}),"\n",(0,i.jsx)(n.li,{children:"GPU memory is allocated dynamically, with 90% utilization reserved for model weights (14.99 GiB), activation memory (0.85 GiB), and KV Cache (4.17 GiB)."}),"\n",(0,i.jsx)(n.li,{children:"Expect some initial delay during the first model load, as weights are fetched and optimized for inference."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Inference Execution & Optimization"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The model supports multiple tasks but defaults to text generation (generate)."}),"\n",(0,i.jsx)(n.li,{children:"Flash Attention is enabled, reducing memory overhead and improving inference speed."}),"\n",(0,i.jsx)(n.li,{children:"CUDA Graph Capture is applied, allowing for faster repeated inferences\u2014but if OOM issues arise, decreasing gpu_memory_utilization or enabling eager execution can help."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Token Generation & Performance Metrics"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The model will initially show 0 tokens/sec for prompt throughput, as it waits for input."}),"\n",(0,i.jsx)(n.li,{children:"Once inference starts, token generation throughput stabilizes at ~29 tokens/sec."}),"\n",(0,i.jsx)(n.li,{children:"GPU KV Cache utilization starts at ~12.5% and increases as more tokens are processed\u2014ensuring smoother text generation over time."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. System Resource Utilization"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Expect 8 CPU and 8 CUDA blocks handling parallel execution."}),"\n",(0,i.jsx)(n.li,{children:"Inference concurrency is limited to 4 requests for 8192 tokens per request, meaning simultaneous requests may be queued if the model is fully utilized."}),"\n",(0,i.jsx)(n.li,{children:"If encountering memory spikes, lowering max_num_seqs will help reduce GPU strain."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"5. Monitoring & Observability"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You can track average prompt throughput, generation speed, and GPU KV Cache usage in the logs."}),"\n",(0,i.jsx)(n.li,{children:"If inference slows down, check the logs for pending or swapped requests, which may indicate memory pressure or scheduling delays."}),"\n",(0,i.jsx)(n.li,{children:"Real-time observability (e.g., tracing request latency) is disabled by default, but can be enabled for deeper monitoring."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What to Expect Post-Deployment?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The model will take a few minutes to initialize due to memory profiling and CUDA graph optimization."}),"\n",(0,i.jsx)(n.li,{children:"Once running, you should see stable throughput of ~29 tokens/sec with efficient memory usage."}),"\n",(0,i.jsx)(n.li,{children:"If performance dips, adjust KV Cache size, decrease memory utilization, or enable eager execution for better stability."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.p,{children:"Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed."}),"\n",(0,i.jsx)(n.p,{children:"Delete the RayCluster"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-gpu\n\nkubectl delete -f open-webui.yaml\n\nkubectl delete -f ray-vllm-deepseek.yml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/jark-stack/terraform/monitoring\n\nkubectl delete -f serviceMonitor.yaml\nkubectl delete -f podMonitor.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"Destroy the EKS Cluster and resources"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export AWS_DEAFULT_REGION="DEPLOYED_EKS_CLUSTER_REGION>"\n\ncd data-on-eks/ai-ml/jark-stack/terraform/ && chmod +x cleanup.sh\n./cleanup.sh\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},2450:(e,n,s)=>{s.d(n,{A:()=>g});var i=s(6540),a=s(5556),r=s.n(a),l=s(4164);const t="collapsibleContent_q3kw",c="header_QCEw",o="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=s(4848);function u(e){let{children:n,header:s}=e;const[a,r]=(0,i.useState)(!1);return(0,p.jsxs)("div",{className:t,children:[(0,p.jsxs)("div",{className:(0,l.A)(c,{[h]:a}),onClick:()=>{r(!a)},children:[s,(0,p.jsx)("span",{className:(0,l.A)(o,{[h]:a}),children:a?"\ud83d\udc47":"\ud83d\udc48"})]}),a&&(0,p.jsx)("div",{className:d,children:n})]})}u.propTypes={children:r().node.isRequired,header:r().node.isRequired};const g=u},2316:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek0-ed611393c3c4141ff84ea14d53c33e7e.png"},1413:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek1-31265aec978c1e6084c7c9adc170de6b.png"},4062:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek2-2a3aba15bcddd8e49634a76619fda1d9.png"},2375:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek3-f3d2396b2289097de0e9364dd68e7bac.png"},7208:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek4-8f9ea401c2d4ff6caa442bd212a1ddf9.png"},5073:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek5-5baa439343a2a8e3b6e510d91a5e49c7.png"},4906:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek6-58038057a768d69e3b794eaa9992bc2d.png"},2483:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek7-116771ae68c4a4b1ad4dc080dfe2fd35.png"},2439:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek8-a5c39c8973e342a944719b2defce0f3e.png"},4765:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/dseek9-542bf9038eed5c0ff6d29051c051d21a.png"},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>t});var i=s(6540);const a={},r=i.createContext(a);function l(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);