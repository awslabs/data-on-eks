"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["9836"],{90558:function(e,n,a){a.r(n),a.d(n,{frontMatter:()=>c,default:()=>g,toc:()=>p,metadata:()=>r,assets:()=>h,contentTitle:()=>d});var r=JSON.parse('{"id":"blueprints/data-analytics/ray-data-processing","title":"Distributed Data Processing with Ray Data","description":"What is Ray Data?","source":"@site/docs/blueprints/data-analytics/ray-data-processing.md","sourceDirName":"blueprints/data-analytics","slug":"/blueprints/data-analytics/ray-data-processing","permalink":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/data-analytics/ray-data-processing.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"sidebar_label":"Ray Data on EKS","mermaid":true},"sidebar":"blueprints","previous":{"title":"DataHub on EKS","permalink":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks"},"next":{"title":"Superset on EKS","permalink":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks"}}'),s=a(85893),t=a(50065),i=a(47902),l=a(5525),o=a(96912);let c={sidebar_position:7,sidebar_label:"Ray Data on EKS",mermaid:!0},d="Distributed Data Processing with Ray Data",h={},p=[{value:"What is Ray Data?",id:"what-is-ray-data",level:2},{value:"Why Ray Data? Is this an Alternative Tool to Spark?",id:"why-ray-data-is-this-an-alternative-tool-to-spark",level:2},{value:"Problem Statement",id:"problem-statement",level:3},{value:"Log Snippet in S3 Before Processing",id:"log-snippet-in-s3-before-processing",level:3},{value:"\uD83D\uDCCB Architecture Overview",id:"-architecture-overview",level:2},{value:"How Ray Data Transforms Log Processing",id:"how-ray-data-transforms-log-processing",level:3},{value:"Key Features of Ray Data Processing",id:"key-features-of-ray-data-processing",level:3},{value:"\uD83D\uDCCA <strong>Schema Extraction and Parsing</strong>",id:"-schema-extraction-and-parsing",level:4},{value:"\uD83D\uDD0D <strong>Intelligent Filtering and Querying</strong>",id:"-intelligent-filtering-and-querying",level:4},{value:"\uD83C\uDFAF <strong>Metadata Management</strong>",id:"-metadata-management",level:4},{value:"\uD83D\uDE80 Getting Started",id:"-getting-started",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Enable Ray Data Processing",id:"step-1-enable-ray-data-processing",level:3},{value:"Step 2: Verify KubeRay Operator Installation",id:"step-2-verify-kuberay-operator-installation",level:3},{value:"Step 3: Configure Ray Job",id:"step-3-configure-ray-job",level:3},{value:"Step 4: Deploy the Ray Cluster &amp; Execute Ray Job",id:"step-4-deploy-the-ray-cluster--execute-ray-job",level:3},{value:"\uD83D\uDCCA Monitoring the RayJob Deployment",id:"-monitoring-the-rayjob-deployment",level:2},{value:"Check Job Status",id:"check-job-status",level:3},{value:"Check RayJob Logs",id:"check-rayjob-logs",level:4},{value:"Access Ray Dashboard",id:"access-ray-dashboard",level:3},{value:"\u2705 Data Verification",id:"-data-verification",level:2},{value:"S3 Bucket Structure",id:"s3-bucket-structure",level:3},{value:"Log Snippet After RayData Processing",id:"log-snippet-after-raydata-processing",level:3},{value:"Option 1: Query Iceberg Tables",id:"option-1-query-iceberg-tables",level:3},{value:"Option 2: Use AWS CLI",id:"option-2-use-aws-cli",level:3},{value:"\uD83E\uDDF9 Cleanup",id:"-cleanup",level:2},{value:"\uD83C\uDF1F Scale Your Data Pipeline",id:"-scale-your-data-pipeline",level:2}];function u(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"distributed-data-processing-with-ray-data",children:"Distributed Data Processing with Ray Data"})}),"\n",(0,s.jsx)(n.h2,{id:"what-is-ray-data",children:"What is Ray Data?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/data/data.html",children:"Ray Data"})," is a scalable, framework-agnostic data processing library built on top of Ray, designed for distributed data analytics and machine learning workloads. It provides:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Processing"}),": Parallel data processing across multiple Ray worker nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lazy Evaluation"}),": Operations are optimized and executed only when results are needed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rich Data Connectors"}),": Native support for various data sources including S3, databases, and file systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Efficient handling of large datasets that don't fit in memory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration with ML Libraries"}),": Seamless integration with pandas, NumPy, and PyArrow"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"why-ray-data-is-this-an-alternative-tool-to-spark",children:"Why Ray Data? Is this an Alternative Tool to Spark?"}),"\n",(0,s.jsxs)(n.p,{children:["Ray Data is ",(0,s.jsx)(n.strong,{children:"complementary to Spark"}),", not a direct replacement. While both are distributed data processing frameworks, they serve different use cases:"]}),"\n",(0,s.jsx)(n.mermaid,{value:'graph LR\n    subgraph AS["<b>Apache Spark</b>"]\n        A[Large-scale Data Analytics]\n        B[Distributed SQL Analytics]\n        C[Batch/Stream Processing]\n    end\n\n    subgraph RD["<b>Ray Data</b>"]\n        D[ML Data Preprocessing]\n        E[Real-time Inference]\n        F[Iterative Workloads]\n        G[Python-native Processing]\n    end\n\n    subgraph CU["<b>Common Use Cases</b>"]\n        H[Data Transformation]\n        I[Distributed Computing]\n    end\n\n    style A fill:#37474f,color:#fff\n    style B fill:#37474f,color:#fff\n    style C fill:#37474f,color:#fff\n    style D fill:#FF6B6B,stroke:#FF4444,stroke-width:3px,color:#fff\n    style E fill:#4ECDC4,stroke:#45B7AA,stroke-width:3px,color:#fff\n    style F fill:#FFE66D,stroke:#FFD93D,stroke-width:3px,color:#333\n    style G fill:#A8E6CF,stroke:#7FD4A0,stroke-width:3px,color:#333\n    style H fill:#455a64,color:#fff\n    style I fill:#455a64,color:#fff'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ray Data excels when you need:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python-native data processing with familiar pandas/NumPy APIs"}),"\n",(0,s.jsx)(n.li,{children:"Tight integration with machine learning pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Real-time or streaming data processing"}),"\n",(0,s.jsx)(n.li,{children:"Complex iterative algorithms"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Spark remains ideal for:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large-scale ETL operations"}),"\n",(0,s.jsx)(n.li,{children:"Complex SQL-based analytics"}),"\n",(0,s.jsx)(n.li,{children:"Enterprise data warehouse workloads"}),"\n",(0,s.jsx)(n.li,{children:"Cross-language support (Scala, Java, Python, R)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,s.jsx)(n.p,{children:"When Apache Spark applications run on Kubernetes, they generate extensive logs that are captured by Fluent Bit and written to S3. However, these logs present several challenges for data engineers:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unstructured Format"}),": Spark logs are written as raw text files without a consistent schema"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No Query Capability"}),": Engineers cannot easily query logs using SQL-based tools like Amazon Athena"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metadata Enrichment"}),": Fluent Bit adds Kubernetes metadata as JSON, creating mixed formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Issues"}),": Scanning raw log files for troubleshooting is time-consuming and expensive"]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    subgraph UL["<b>Unstructured Logs</b>"]\n        Spark[Spark Applications] --\x3e|Raw Logs| FB[Fluent Bit]\n        FB --\x3e|Enriched JSON| S3Raw[S3 Raw Logs]\n        S3Raw --\x3e|\u274C No Schema| Query[Cannot Query Efficiently]\n    end\n\n    style Spark fill:#ff9800,color:#fff\n    style FB fill:#2196F3,color:#fff\n    style S3Raw fill:#f44336,color:#fff\n    style Query fill:#9e9e9e,color:#fff'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": Use Ray Data to periodically process these unstructured logs, apply a consistent schema, and write them to Apache Iceberg tables. This enables:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 SQL queries via Amazon Athena"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Structured data with defined schema"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Efficient columnar storage format"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Time-travel and versioning capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"log-snippet-in-s3-before-processing",children:"Log Snippet in S3 Before Processing"}),"\n",(0,s.jsx)(n.p,{children:"Here's what Spark logs look like when written to S3 by Fluent Bit:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "log": "2024-01-15 14:23:45 INFO SparkContext: Running Spark version 3.5.0\\n",\n  "stream": "stdout",\n  "time": "2024-01-15T14:23:45.123456Z",\n  "kubernetes": {\n    "pod_name": "spark-driver-abc123",\n    "namespace_name": "spark-team-a",\n    "pod_id": "12345678-1234-1234-1234-123456789012",\n    "labels": {\n      "spark-role": "driver",\n      "spark-app-id": "spark-application-12345"\n    },\n    "container_name": "spark-driver",\n    "container_image": "spark:3.5.0"\n  }\n}\n{\n  "log": "2024-01-15 14:23:46 INFO ResourceUtils: Using Spark\'s default log4j profile\\n",\n  "stream": "stdout",\n  "time": "2024-01-15T14:23:46.234567Z",\n  "kubernetes": {\n    "pod_name": "spark-driver-abc123",\n    "namespace_name": "spark-team-a",\n    "pod_id": "12345678-1234-1234-1234-123456789012",\n    "labels": {\n      "spark-role": "driver",\n      "spark-app-id": "spark-application-12345"\n    },\n    "container_name": "spark-driver",\n    "container_image": "spark:3.5.0"\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Challenges:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each log line is wrapped in JSON with Kubernetes metadata"}),"\n",(0,s.jsxs)(n.li,{children:["The actual log message is embedded in the ",(0,s.jsx)(n.code,{children:"log"})," field"]}),"\n",(0,s.jsx)(n.li,{children:"No structured schema for querying specific log levels or components"}),"\n",(0,s.jsx)(n.li,{children:"Redundant metadata repeated for each log line"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Fluent Bit Enrichment",type:"info",children:(0,s.jsxs)(n.p,{children:["Fluent Bit automatically enriches each log line with Kubernetes metadata including pod name, namespace, labels, and container information. This enrichment is configured in the ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/helm-values/aws-for-fluentbit-values.yaml",children:"aws-for-fluentbit-values.yaml"})," file. While this metadata is valuable for debugging, it creates a mixed format that's difficult to query efficiently."]})}),"\n",(0,s.jsx)(n.h2,{id:"-architecture-overview",children:"\uD83D\uDCCB Architecture Overview"}),"\n",(0,s.jsx)(n.h3,{id:"how-ray-data-transforms-log-processing",children:"How Ray Data Transforms Log Processing"}),"\n",(0,s.jsx)(n.p,{children:"Ray Data periodically fetches new logs from S3, processes them in parallel, and writes structured data to Apache Iceberg tables. The solution includes:"}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    S3[S3 Spark Logs] --\x3e RayData[\u26A1 Ray Data Processing]\n    RayData --\x3e Iceberg[Apache Iceberg Tables in S3]\n    Glue[AWS Glue Catalog] --\x3e Iceberg\n    Meta[Metadata Tracking] --\x3e RayData\n\n    subgraph EKS["<b>EKS Cluster</b>"]\n        RayData\n        subgraph RC["<b>Ray Cluster</b>"]\n            Head[Ray Head Node]\n            Workers[Ray Worker Nodes]\n        end\n    end\n\n    subgraph SC["<b>\u2601\uFE0F Storage</b>"]\n        S3\n        Iceberg\n    end\n\n    style S3 fill:#FF9900,stroke:#232F3E,stroke-width:2px,color:#fff\n    style Glue fill:#FF9900,stroke:#232F3E,stroke-width:2px,color:#fff\n    style RayData fill:#028CF0,stroke:#232F3E,stroke-width:2px,color:#fff\n    style Iceberg fill:#3FCBFF,stroke:#232F3E,stroke-width:2px,color:#fff'}),"\n",(0,s.jsx)(n.h3,{id:"key-features-of-ray-data-processing",children:"Key Features of Ray Data Processing"}),"\n",(0,s.jsxs)(n.h4,{id:"-schema-extraction-and-parsing",children:["\uD83D\uDCCA ",(0,s.jsx)(n.strong,{children:"Schema Extraction and Parsing"})]}),"\n",(0,s.jsx)(n.p,{children:"Ray Data intelligently extracts structured fields from unstructured logs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDD50 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"timestamp"})})," - Parsed from the log message"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDFF7\uFE0F ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"log_level"})})," - Extracted levels (INFO, WARN, ERROR, DEBUG)"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDD27 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"component"})})," - Spark component (SparkContext, ResourceUtils, etc.)"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDCDD ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"message"})})," - The actual log content"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDFE0 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"pod_name"})})," & ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"namespace"})})," - From Kubernetes metadata"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDC77 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"spark_role"})})," - Driver or Executor identification"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDD94 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"application_id"})})," - Unique Spark application identifier"]}),"\n"]}),"\n",(0,s.jsxs)(n.h4,{id:"-intelligent-filtering-and-querying",children:["\uD83D\uDD0D ",(0,s.jsx)(n.strong,{children:"Intelligent Filtering and Querying"})]}),"\n",(0,s.jsx)(n.p,{children:"Once processed, you can easily query logs using SQL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Find all ERROR logs for a specific application\nSELECT timestamp, component, message\nFROM spark_logs\nWHERE log_level = 'ERROR'\n  AND application_id = 'spark-application-12345'\n  AND timestamp > '2024-01-15 00:00:00'\nORDER BY timestamp DESC;\n\n-- Analyze log patterns by component\nSELECT component, log_level, COUNT(*) as count\nFROM spark_logs\nWHERE namespace = 'spark-team-a'\nGROUP BY component, log_level\nORDER BY count DESC;\n\n-- Track application lifecycle events\nSELECT timestamp, message\nFROM spark_logs\nWHERE component = 'SparkContext'\n  AND (message LIKE '%Starting%' OR message LIKE '%Stopping%')\nORDER BY timestamp;\n"})}),"\n",(0,s.jsxs)(n.h4,{id:"-metadata-management",children:["\uD83C\uDFAF ",(0,s.jsx)(n.strong,{children:"Metadata Management"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Idempotent Processing"})," - Tracks processed folders to avoid reprocessing"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDCCB ",(0,s.jsx)(n.strong,{children:"Metadata Table"})," - Maintains processing history and state"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDD04 ",(0,s.jsx)(n.strong,{children:"Auto-Discovery"})," - Automatically finds new log folders in S3"]}),"\n",(0,s.jsxs)(n.li,{children:["\u26A1 ",(0,s.jsx)(n.strong,{children:"Incremental Updates"})," - Processes only new data for efficiency"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-getting-started",children:"\uD83D\uDE80 Getting Started"}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before deploying this blueprint, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"S3 bucket with Spark application logs"}),": Follow the ",(0,s.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn",children:"Spark Operator blueprint"})," to generate Spark logs.\n",(0,s.jsx)(n.strong,{children:"Note:"})," Execute the steps in ",(0,s.jsx)(n.strong,{children:"Put sample data in S3"})," section of the ",(0,s.jsx)(n.strong,{children:"Execute Sample Spark job with Karpenter"})," step to populate the S3 bucket with Spark Application Logs."]}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"AWS CLI configured"})," with appropriate permissions"]}),"\n",(0,s.jsx)(n.li,{children:"\u2705 kubectl"}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Terraform installed"})," (>= 1.0)"]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{title:"Generate Spark Logs First",type:"tip",children:[(0,s.jsxs)(n.p,{children:["The Ray Data pipeline processes Spark application logs. Make sure you've run the taxi-trip example from the ",(0,s.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn#put-sample-data-in-s3",children:"Spark Operator blueprint"})," to populate your S3 bucket with logs."]}),(0,s.jsxs)(n.p,{children:["\uD83D\uDCC1 ",(0,s.jsx)(n.strong,{children:"Spark Logfile Structure in S3:"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"s3://${S3_BUCKET}/\n\u2514\u2500\u2500 spark-application-logs/\n    \u2514\u2500\u2500 spark-team-a/\n        \u251C\u2500\u2500 spark-application-1234567890-driver/\n        \u2502   \u2514\u2500\u2500 stdout\n        \u251C\u2500\u2500 spark-application-1234567890-exec-1/\n        \u2502   \u2514\u2500\u2500 stdout\n        \u2514\u2500\u2500 spark-application-1234567890-exec-2/\n            \u2514\u2500\u2500 stdout\n"})}),(0,s.jsxs)(n.p,{children:["Each ",(0,s.jsx)(n.code,{children:"stdout"})," file contains JSON-formatted logs with Kubernetes metadata enrichment from Fluent Bit."]})]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-enable-ray-data-processing",children:"Step 1: Enable Ray Data Processing"}),"\n",(0,s.jsxs)(n.p,{children:["Deploy the EKS cluster with Ray Data components by enabling the ",(0,s.jsx)(n.code,{children:"enable_raydata"})," variable. This will install:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"KubeRay Operator"})," - Manages Ray clusters on Kubernetes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ray Custom Resources"})," - RayJob and RayCluster CRDs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Resources"})," - IAM roles, S3 access policies, and Glue database"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ray Data Pipeline"})," - Namespace, service accounts, and RBAC"]}),"\n"]}),"\n",(0,s.jsxs)(i.Z,{children:[(0,s.jsx)(l.Z,{value:"terraform",label:"Using Terraform",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/spark-k8s-operator\n\n# Deploy EKS cluster with Ray Data support enabled\nexport TF_VAR_enable_raydata=true\n\nterraform init\nterraform plan\nterraform apply -auto-approve\n"})})}),(0,s.jsx)(l.Z,{value:"install-script",label:"Using Install Script",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/spark-k8s-operator\n\n# Set environment variables and run install script\nexport TF_VAR_enable_raydata=true\n\n./install.sh\n"})})})]}),"\n",(0,s.jsx)(n.admonition,{title:"Deployment Time",type:"info",children:(0,s.jsx)(n.p,{children:"The full deployment takes approximately 20-25 minutes to create the EKS cluster, install operators, and configure all Ray Data components."})}),"\n",(0,s.jsx)(n.p,{children:"This deployment creates:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDFAF ",(0,s.jsx)(n.strong,{children:"KubeRay Operator"})," for Ray job orchestration"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDD10 ",(0,s.jsx)(n.strong,{children:"Ray Service Account"})," with IRSA (IAM Roles for Service Accounts)"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDCDD ",(0,s.jsx)(n.strong,{children:"IAM Roles"})," with S3 and Glue permissions"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDCCA ",(0,s.jsx)(n.strong,{children:"AWS Glue Database"})," for Iceberg catalog"]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDF10 ",(0,s.jsx)(n.strong,{children:"Kubernetes Namespace"})," (",(0,s.jsx)(n.code,{children:"raydata"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-verify-kuberay-operator-installation",children:"Step 2: Verify KubeRay Operator Installation"}),"\n",(0,s.jsx)(n.p,{children:"Confirm that the KubeRay Operator is running successfully:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get po -n kuberay-operator\n"})}),"\n",(0,s.jsx)(n.p,{children:"Expected output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-74fcdcc6bf-gpl5p   1/1     Running   0          10h\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-configure-ray-job",children:"Step 3: Configure Ray Job"}),"\n",(0,s.jsx)(n.p,{children:"Navigate to the example directory and update the S3 configuration in the deployment script."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd examples/raydata-sparklogs-processing-job\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Replace  ",(0,s.jsx)(n.strong,{children:"S3_BUCKET"}),", ",(0,s.jsx)(n.strong,{children:"CLUSTER_NAME"})," and ",(0,s.jsx)(n.strong,{children:"AWS_REGION"})," variables in the ",(0,s.jsx)(n.code,{children:"execute-rayjob.sh"})," shell script before running."]}),"\n",(0,s.jsx)(n.h3,{id:"step-4-deploy-the-ray-cluster--execute-ray-job",children:"Step 4: Deploy the Ray Cluster & Execute Ray Job"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Make script executable\nchmod +x execute-rayjob.sh\n\n# Deploy the processing job\n./execute-rayjob.sh deploy\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-monitoring-the-rayjob-deployment",children:"\uD83D\uDCCA Monitoring the RayJob Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"check-job-status",children:"Check Job Status"}),"\n",(0,s.jsx)(n.p,{children:"Monitor your Ray job with these commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Monitor job progress in real-time\n./execute-rayjob.sh monitor\n\n# Check current status\n./execute-rayjob.sh status\n\n# View processing logs\n./execute-rayjob.sh logs\n\n"})}),"\n",(0,s.jsx)(n.h4,{id:"check-rayjob-logs",children:"Check RayJob Logs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"2025-07-27 22:04:46,324 - spark-log-processor - INFO - \u2705 Successfully processed 1287 records from spark-fb094270bf654473b372d0f773e86687\n2025-07-27 22:04:46,324 - spark-log-processor - INFO - \uD83C\uDFAF Processing Summary:\n2025-07-27 22:04:46,324 - spark-log-processor - INFO -   \uD83D\uDCCA Total records processed: 1287\n2025-07-27 22:04:46,324 - spark-log-processor - INFO -   \u2705 Successful folders: 1\n2025-07-27 22:04:46,324 - spark-log-processor - INFO -   \u274C Failed folders: 0\n2025-07-27 22:04:46,324 - spark-log-processor - INFO -   \u2705 Successfully processed: ['spark-fb094270bf654473b372d0f773e86687']\n2025-07-27 22:04:46,324 - spark-log-processor - INFO - \u2705 Metadata-driven incremental processing completed\n"})}),"\n",(0,s.jsxs)(n.admonition,{title:"What's Happening Behind the Scenes?",type:"tip",children:[(0,s.jsx)(n.p,{children:"When you deploy the RayJob, the following automated process occurs:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDE80 Ray Cluster Initialization"})," - KubeRay Operator creates a Ray cluster with head and worker nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDD0D S3 Discovery"})," - Ray Data scans the configured S3 bucket path for folders matching ",(0,s.jsx)(n.code,{children:"spark-*"})," pattern"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDCCA Metadata Check"})," - Queries the Iceberg metadata table to identify unprocessed folders"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDCE5 Parallel Processing"})," - Ray workers read JSON log files from S3 in parallel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDD04 Data Transformation"})," - Extracts structured fields from JSON logs (timestamp, log level, component, etc.)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u270D\uFE0F Iceberg Writing"})," - Writes transformed data to Apache Iceberg tables with ACID guarantees"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83D\uDCDD Metadata Update"})," - Records processing status in metadata table for idempotency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uD83C\uDFAF Completion"})," - Shuts down Ray cluster after successful processing"]}),"\n"]}),(0,s.jsxs)(n.p,{children:["The entire process is ",(0,s.jsx)(n.strong,{children:"idempotent"})," - you can safely re-run it without duplicating data, as it only processes new log folders."]})]}),"\n",(0,s.jsx)(n.h3,{id:"access-ray-dashboard",children:"Access Ray Dashboard"}),"\n",(0,s.jsxs)(o.Z,{header:(0,s.jsx)(n.h3,{children:"\uD83C\uDFA8 Ray Dashboard Access"}),children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Get dashboard access info\n./execute-rayjob.sh dashboard\n\n# Port forward to local machine\nkubectl port-forward svc/spark-log-processor-head-svc 8265:8265 -n raydata\n"})}),(0,s.jsxs)(n.p,{children:["Open ",(0,s.jsx)(n.a,{href:"http://localhost:8265",children:"http://localhost:8265"})," to view:"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\uD83D\uDCC8 Job execution progress"}),"\n",(0,s.jsx)(n.li,{children:"\uD83D\uDCBB Resource utilization"}),"\n",(0,s.jsx)(n.li,{children:"\u26A1 Task-level metrics"}),"\n",(0,s.jsx)(n.li,{children:"\uD83C\uDF10 Cluster topology"}),"\n"]})]}),"\n",(0,s.jsx)(n.h2,{id:"-data-verification",children:"\u2705 Data Verification"}),"\n",(0,s.jsx)(n.h3,{id:"s3-bucket-structure",children:"S3 Bucket Structure"}),"\n",(0,s.jsxs)(n.p,{children:["Ray Data uses the ",(0,s.jsx)(n.strong,{children:"same S3 bucket"})," for both input Spark logs and output Iceberg data, organized in separate paths:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"s3://your-spark-logs-bucket/\n\u251C\u2500\u2500 spark-application-logs/           # \uD83D\uDCE5 Input: Raw Spark logs from Fluent Bit\n\u2502   \u2514\u2500\u2500 spark-team-a/\n\u2502       \u251C\u2500\u2500 spark-application-1234567890-driver/\n\u2502       \u2502   \u2514\u2500\u2500 stdout                # JSON logs with Kubernetes metadata\n\u2502       \u251C\u2500\u2500 spark-application-1234567890-exec-1/\n\u2502       \u2502   \u2514\u2500\u2500 stdout\n\u2502       \u2514\u2500\u2500 spark-application-1234567890-exec-2/\n\u2502           \u2514\u2500\u2500 stdout\n\u2502\n\u2514\u2500\u2500 iceberg-warehouse/                # \uD83D\uDCE4 Output: Processed Iceberg data\n    \u2514\u2500\u2500 raydata_spark_logs.db/\n        \u2514\u2500\u2500 spark_logs/\n            \u251C\u2500\u2500 metadata/             # Iceberg metadata files\n            \u2502   \u251C\u2500\u2500 00000-xxx.metadata.json\n            \u2502   \u251C\u2500\u2500 snap-xxx.avro     # Snapshots for time travel\n            \u2502   \u2514\u2500\u2500 version-hint.text\n            \u2514\u2500\u2500 data/                 # Actual data in Parquet format\n                \u251C\u2500\u2500 00000-0-xxx.parquet\n                \u251C\u2500\u2500 00001-0-xxx.parquet\n                \u2514\u2500\u2500 ...\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Same Bucket, Different Paths",type:"tip",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Path"}),": ",(0,s.jsx)(n.code,{children:"s3://bucket/spark-application-logs/"})," - Contains raw JSON logs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output Path"}),": ",(0,s.jsx)(n.code,{children:"s3://bucket/iceberg-warehouse/"})," - Contains structured Parquet files"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage Format"}),": Iceberg uses efficient columnar Parquet format with metadata for ACID transactions"]}),"\n"]})}),"\n",(0,s.jsx)(n.p,{children:"In AWS S3 Console, it should looks like below:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"s3",src:a(88667).Z+"",width:"2198",height:"866"})}),"\n",(0,s.jsx)(n.h3,{id:"log-snippet-after-raydata-processing",children:"Log Snippet After RayData Processing"}),"\n",(0,s.jsx)(n.p,{children:"Here's how the data transformation looks before and after Ray Data processing:"}),"\n",(0,s.jsxs)(i.Z,{children:[(0,s.jsxs)(l.Z,{value:"before",label:"Before Processing (Raw S3 JSON)",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Raw Fluent Bit logs in S3"})," - Each log line wrapped in JSON with redundant metadata:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "log": "2024-01-15 14:23:45 INFO SparkContext: Running Spark version 3.5.0\\n",\n  "stream": "stdout",\n  "time": "2024-01-15T14:23:45.123456Z",\n  "kubernetes": {\n    "pod_name": "spark-driver-abc123",\n    "namespace_name": "spark-team-a",\n    "pod_id": "12345678-1234-1234-1234-123456789012",\n    "labels": {\n      "spark-role": "driver",\n      "spark-app-id": "spark-application-12345"\n    },\n    "container_name": "spark-driver",\n    "container_image": "spark:3.5.0"\n  }\n}\n{\n  "log": "2024-01-15 14:23:46 ERROR TaskSchedulerImpl: Lost executor 1: Container killed\\n",\n  "stream": "stderr",\n  "time": "2024-01-15T14:23:46.234567Z",\n  "kubernetes": {\n    "pod_name": "spark-executor-def456",\n    "namespace_name": "spark-team-a",\n    "labels": {\n      "spark-role": "executor",\n      "spark-app-id": "spark-application-12345"\n    }\n  }\n}\n'})})]}),(0,s.jsxs)(l.Z,{value:"after",label:"After Processing (Structured Iceberg)",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Processed data in Apache Iceberg"})," - Clean, structured schema optimized for querying:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Query the processed data\nSELECT * FROM raydata_spark_logs.spark_logs LIMIT 2;\n"})}),(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"timestamp"}),(0,s.jsx)(n.th,{children:"log_level"}),(0,s.jsx)(n.th,{children:"component"}),(0,s.jsx)(n.th,{children:"message"}),(0,s.jsx)(n.th,{children:"pod_name"}),(0,s.jsx)(n.th,{children:"namespace"}),(0,s.jsx)(n.th,{children:"spark_role"}),(0,s.jsx)(n.th,{children:"application_id"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2024-01-15 14:23:45"}),(0,s.jsx)(n.td,{children:"INFO"}),(0,s.jsx)(n.td,{children:"SparkContext"}),(0,s.jsx)(n.td,{children:"Running Spark version 3.5.0"}),(0,s.jsx)(n.td,{children:"spark-driver-abc123"}),(0,s.jsx)(n.td,{children:"spark-team-a"}),(0,s.jsx)(n.td,{children:"driver"}),(0,s.jsx)(n.td,{children:"spark-application-12345"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2024-01-15 14:23:46"}),(0,s.jsx)(n.td,{children:"ERROR"}),(0,s.jsx)(n.td,{children:"TaskSchedulerImpl"}),(0,s.jsx)(n.td,{children:"Lost executor 1: Container killed"}),(0,s.jsx)(n.td,{children:"spark-executor-def456"}),(0,s.jsx)(n.td,{children:"spark-team-a"}),(0,s.jsx)(n.td,{children:"executor"}),(0,s.jsx)(n.td,{children:"spark-application-12345"})]})]})]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2705 Key Improvements:"})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structured Fields"})," - Easy to query specific log levels, components, and time ranges"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deduplicated Metadata"})," - No redundant Kubernetes metadata per log line"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Columnar Storage"})," - Efficient storage and query performance with Parquet format"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema Evolution"})," - Add new fields without breaking existing queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ACID Transactions"})," - Consistent reads even during concurrent processing"]}),"\n"]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"option-1-query-iceberg-tables",children:"Option 1: Query Iceberg Tables"}),"\n",(0,s.jsx)(n.p,{children:"Use the built-in data verification script provided in the blueprint that automatically sets up a Python virtual environment and all required dependencies:"}),"\n",(0,s.jsx)(n.p,{children:"from your environment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Make script executable\nchmod +x verify-iceberg-data.sh\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Replace  ",(0,s.jsx)(n.strong,{children:"S3_BUCKET"})," and ",(0,s.jsx)(n.strong,{children:"AWS_REGION"})," variables in the ",(0,s.jsx)(n.code,{children:"verify-iceberg-data.sh"})," shell script before running."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./verify-iceberg-data.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"The script automatically..."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Creates an isolated Python virtual environment"}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 Installs PyIceberg and all dependencies (",(0,s.jsx)(n.code,{children:"pyiceberg[glue,s3fs]==0.7.0"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Connects to AWS Glue catalog and Iceberg tables"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Performs comprehensive data validation"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Cleans up temporary files and environment after completion"}),"\n"]}),"\n",(0,s.jsx)(o.Z,{header:(0,s.jsx)(n.h4,{children:"\uD83D\uDCCB Sample Script Output"}),children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\uD83D\uDD0D Connecting to Iceberg catalog...\n\u2705 Connected to Iceberg catalog in region: us-west-2\n\uD83D\uDCCA Loading table: raydata_spark_logs.spark_logs\n\u2705 Table loaded successfully\n\n\uD83D\uDCCB Table Schema:\n  - timestamp: timestamp (optional)\n  - log_level: string (optional)\n  - message: string (optional)\n  - pod_name: string (optional)\n  - namespace_name: string (optional)\n  - app: string (optional)\n  - spark_app_selector: string (optional)\n  - queue: string (optional)\n  - spark_app_name: string (optional)\n  - spark_role: string (optional)\n  - spark_version: string (optional)\n  - submission_id: string (optional)\n  - container_name: string (optional)\n  - container_image: string (optional)\n\n\uD83D\uDD0D Scanning table data...\n\u2705 SUCCESS! Found 1287 records in Iceberg table\n\n\uD83D\uDCCB Data Summary:\n   \uD83D\uDCCA Total Records: 1287\n   \uD83D\uDCC5 Date Range: 2025-07-08 19:52:43.079161 to 2025-07-08 20:00:29.393901\n   \uD83D\uDCF1 Unique Pods: 5\n\n\uD83D\uDCC8 Log Level Distribution:\n   INFO: 1269\n   WARN: 14\n   ERROR: 4\n\n\uD83D\uDCDD Sample Records:\n\n  Record 1:\n    timestamp: 2025-07-08 19:52:43.079161\n    log_level: WARN\n    message: Unable to load native-hadoop library for your platform... using builtin-java classes where applicabl...\n\n  Record 2:\n    timestamp: 2025-07-08 19:52:43.460063\n    log_level: WARN\n    message: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.proper...\n\n  Record 3:\n    timestamp: 2025-07-08 19:52:46.170113\n    log_level: INFO\n    message: Running Spark version 3.5.3\n\n\uD83C\uDF89 VERIFICATION SUCCESSFUL!\n\u2705 Ray Data successfully processed and stored Spark logs in Iceberg format\n\u2705 Data is accessible and queryable via PyIceberg\n\u2705 You can now query this data using Amazon Athena or other SQL tools\n[SUCCESS] Verification completed successfully!\n==== Cleaning Up ====\n[INFO] Removed verification script\n[INFO] Removed virtual environment\n[SUCCESS] Cleanup completed\n"})})}),"\n",(0,s.jsx)(n.h3,{id:"option-2-use-aws-cli",children:"Option 2: Use AWS CLI"}),"\n",(0,s.jsx)(n.p,{children:"Check table metadata without querying data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# View Iceberg table in Glue catalog\naws glue get-table \\\n  --database-name raydata_spark_logs \\\n  --name spark_logs \\\n  --query 'Table.StorageDescriptor.Location'\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-cleanup",children:"\uD83E\uDDF9 Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"To clean up resources:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Remove Ray job only (preserve infrastructure)\n./execute-rayjob.sh cleanup\n\n# Remove all infrastructure\ncd analytics/terraform/spark-k8s-operator\nterraform destroy -var="enable_raydata_processing=true"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-scale-your-data-pipeline",children:"\uD83C\uDF1F Scale Your Data Pipeline"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Processing"}),": Adjust Ray worker counts in ",(0,s.jsx)(n.code,{children:"rayjob.yaml"})," for larger workloads"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add Analytics"}),": Create dashboards using Amazon QuickSight or Grafana"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automate"}),": Schedule regular processing with Kubernetes CronJobs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extend"}),": Process other data types like metrics, events, or application data"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Learn More",type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\uD83D\uDCDA ",(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/data/data.html",children:"Ray Data Documentation"})]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83E\uDDCA ",(0,s.jsx)(n.a,{href:"https://iceberg.apache.org/",children:"Apache Iceberg Documentation"})]}),"\n",(0,s.jsxs)(n.li,{children:["\uD83C\uDFAF ",(0,s.jsx)(n.a,{href:"https://ray-project.github.io/kuberay/",children:"KubeRay Documentation"})]}),"\n",(0,s.jsxs)(n.li,{children:["\u2601\uFE0F ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html",children:"AWS Glue Catalog"})]}),"\n"]})}),"\n",(0,s.jsx)(n.p,{children:"This blueprint demonstrates how Ray Data and Apache Iceberg can work together to build scalable, reliable data processing pipelines on Amazon EKS. The combination provides a modern data lake architecture with distributed processing capabilities, ACID transactions, and intelligent metadata management."}),"\n",(0,s.jsx)("style",{children:`
.feature-grid {
display: grid;
grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
gap: 1.5rem;
margin: 2rem 0;
}

.feature-card {
padding: 1.5rem;
border: 1px solid var(--ifm-color-emphasis-300);
border-radius: 8px;
background: var(--ifm-background-surface-color);
transition: transform 0.2s, box-shadow 0.2s;
}

.feature-card:hover {
transform: translateY(-2px);
box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

.feature-card h3 {
margin-top: 0;
margin-bottom: 0.5rem;
font-size: 1.2rem;
}

.feature-card p {
margin: 0;
color: var(--ifm-color-content-secondary);
}
`})]})}function g(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},88667:function(e,n,a){a.d(n,{Z:()=>r});let r=a.p+"assets/images/raydata-s3-struc-6517cacfc21b52505f3a4056b536153c.png"},5525:function(e,n,a){a.d(n,{Z:()=>t});var r=a(85893);a(67294);var s=a(67026);function t({children:e,hidden:n,className:a}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,s.Z)("tabItem_Ymn6",a),hidden:n,children:e})}},47902:function(e,n,a){a.d(n,{Z:()=>f});var r=a(85893),s=a(67294),t=a(67026),i=a(69599),l=a(16550),o=a(32e3),c=a(4520),d=a(38341),h=a(76009);function p(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}var g=a(7227);function x({className:e,block:n,selectedValue:a,selectValue:s,tabValues:l}){let o=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.o5)(),d=e=>{let n=e.currentTarget,r=l[o.indexOf(n)].value;r!==a&&(c(n),s(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{let a=o.indexOf(e.currentTarget)+1;n=o[a]??o[0];break}case"ArrowLeft":{let a=o.indexOf(e.currentTarget)-1;n=o[a]??o[o.length-1]}}n?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.Z)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:s})=>(0,r.jsx)("li",{role:"tab",tabIndex:a===e?0:-1,"aria-selected":a===e,ref:e=>{o.push(e)},onKeyDown:h,onClick:d,...s,className:(0,t.Z)("tabs__item","tabItem_LNqP",s?.className,{"tabs__item--active":a===e}),children:n??e},e))})}function m({lazy:e,children:n,selectedValue:a}){let i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=i.find(e=>e.props.value===a);return e?(0,s.cloneElement)(e,{className:(0,t.Z)("margin-top--md",e.props.className)}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==a}))})}function j(e){let n=function(e){let{defaultValue:n,queryString:a=!1,groupId:r}=e,t=function(e){let{values:n,children:a}=e;return(0,s.useMemo)(()=>{let e=n??p(a).map(({props:{value:e,label:n,attributes:a,default:r}})=>({value:e,label:n,attributes:a,default:r})),r=(0,d.lx)(e,(e,n)=>e.value===n.value);if(r.length>0)throw Error(`Docusaurus error: Duplicate values "${r.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,a])}(e),[i,g]=(0,s.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let a=n.find(e=>e.default)??n[0];if(!a)throw Error("Unexpected error: 0 tabValues");return a.value})({defaultValue:n,tabValues:t})),[x,m]=function({queryString:e=!1,groupId:n}){let a=(0,l.k6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c._X)(r),(0,s.useCallback)(e=>{if(!r)return;let n=new URLSearchParams(a.location.search);n.set(r,e),a.replace({...a.location,search:n.toString()})},[r,a])]}({queryString:a,groupId:r}),[j,f]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[a,r]=(0,h.Nk)(n);return[a,(0,s.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),b=(()=>{let e=x??j;return u({value:e,tabValues:t})?e:null})();return(0,o.Z)(()=>{b&&g(b)},[b]),{selectedValue:i,selectValue:(0,s.useCallback)(e=>{if(!u({value:e,tabValues:t}))throw Error(`Can't select invalid tab value=${e}`);g(e),m(e),f(e)},[m,f,t]),tabValues:t}}(e);return(0,r.jsxs)("div",{className:(0,t.Z)("tabs-container","tabList__CuJ"),children:[(0,r.jsx)(x,{...n,...e}),(0,r.jsx)(m,{...n,...e})]})}function f(e){let n=(0,g.Z)();return(0,r.jsx)(j,{...e,children:p(e.children)},String(n))}},96912:function(e,n,a){a.d(n,{Z:()=>d});var r=a(85893),s=a(67294),t=a(45697),i=a.n(t),l=a(67026);let o="expanded_iGsi";function c({children:e,header:n}){let[a,t]=(0,s.useState)(!1);return(0,r.jsxs)("div",{className:"collapsibleContent_q3kw",children:[(0,r.jsxs)("div",{className:(0,l.Z)("header_QCEw",{[o]:a}),onClick:()=>{t(!a)},children:[n,(0,r.jsx)("span",{className:(0,l.Z)("icon_PckA",{[o]:a}),children:a?"\uD83D\uDC47":"\uD83D\uDC48"})]}),a&&(0,r.jsx)("div",{className:"content_qLC1",children:e})]})}c.propTypes={children:i().node.isRequired,header:i().node.isRequired};let d=c},50065:function(e,n,a){a.d(n,{Z:()=>l,a:()=>i});var r=a(67294);let s={},t=r.createContext(s);function i(e){let n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);