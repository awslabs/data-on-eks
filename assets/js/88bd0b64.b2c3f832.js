"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["1039"],{86303:function(e,n,s){s.r(n),s.d(n,{frontMatter:()=>a,default:()=>h,toc:()=>l,metadata:()=>o,assets:()=>d,contentTitle:()=>r});var o=JSON.parse('{"id":"bestpractices/networking/networking","title":"Networking for Data","description":"VPC and IP Considerations","source":"@site/docs/bestpractices/networking/networking.md","sourceDirName":"bestpractices/networking","slug":"/bestpractices/networking/","permalink":"/data-on-eks/docs/bestpractices/networking/","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/bestpractices/networking/networking.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"VPC Configuration"},"sidebar":"bestpractices","previous":{"title":"Networking","permalink":"/data-on-eks/docs/category/networking"},"next":{"title":"Scalability","permalink":"/data-on-eks/docs/category/scalability"}}'),t=s(85893),i=s(50065);let a={sidebar_label:"VPC Configuration"},r="Networking for Data",d={},l=[{value:"VPC and IP Considerations",id:"vpc-and-ip-considerations",level:2},{value:"Default VPC CNI Configuration",id:"default-vpc-cni-configuration",level:3},{value:"Plan for a large amount of IP address usage in your EKS clusters.",id:"plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters",level:3},{value:"Consider using a secondary CIDR if your IP space is constrained.",id:"consider-using-a-secondary-cidr-if-your-ip-space-is-constrained",level:3},{value:"Tuning the VPC CNI",id:"tuning-the-vpc-cni",level:2},{value:"VPC CNI and EC2 Rate Throttling",id:"vpc-cni-and-ec2-rate-throttling",level:3},{value:"Avoid using <code>WARM_IP_TARGET</code> in large clusters, or cluster with a lot of churn",id:"avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn",level:3},{value:"Limit the number of IPs per node on large instance types with <code>MAX_ENI</code> and <code>max-pods</code>",id:"limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods",level:3},{value:"Maxpods with Karpenter",id:"maxpods-with-karpenter",level:4},{value:"Application",id:"application",level:2},{value:"Scaling CoreDNS",id:"scaling-coredns",level:3},{value:"Default Behavior",id:"default-behavior",level:4},{value:"Remediation",id:"remediation",level:4},{value:"CoreDNS Monitoring",id:"coredns-monitoring",level:4},{value:"DNS Lookups and ndots",id:"dns-lookups-and-ndots",level:3},{value:"Inter AZ Network Optimization",id:"inter-az-network-optimization",level:3}];function c(e){let n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"networking-for-data",children:"Networking for Data"})}),"\n",(0,t.jsx)(n.h2,{id:"vpc-and-ip-considerations",children:"VPC and IP Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"default-vpc-cni-configuration",children:"Default VPC CNI Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["With the default VPC CNI configuration larger nodes will consume more IP addresses. For example a ",(0,t.jsxs)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI",children:[(0,t.jsx)(n.code,{children:"m5.8xlarge"})," node"]})," that is running 10 pods will hold 60 IPs total (to satisfy ",(0,t.jsx)(n.code,{children:"WARM_ENI_TARGET=1"}),"). However a ",(0,t.jsx)(n.code,{children:"m5.16xlarge"})," node would hold 100 IPs."]}),"\n",(0,t.jsx)(n.p,{children:"The AWS VPC CNI maintains this \u201Cwarm pool\u201D of IP addresses on the EKS worker nodes to assign to Pods. When more IP addresses are needed for your Pods, the CNI must communicate with EC2 APIs to assign the addresses to your nodes."}),"\n",(0,t.jsx)(n.h3,{id:"plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters",children:"Plan for a large amount of IP address usage in your EKS clusters."}),"\n",(0,t.jsx)(n.p,{children:"During periods of high churn or large scale out, these EC2 API calls can be rate throttled, which will delay the provisioning of Pods and thus delay the execution of workloads. Also, configuring the VPC CNI to minimize this warm pool can increase the EC2 API calls from your nodes and increase the risk of rate throttling."}),"\n",(0,t.jsx)(n.h3,{id:"consider-using-a-secondary-cidr-if-your-ip-space-is-constrained",children:"Consider using a secondary CIDR if your IP space is constrained."}),"\n",(0,t.jsxs)(n.p,{children:["If you are working with a network that spans multiple connected VPCs or sites the routable address space may be limited.\nFor example, your VPC may be limited to small subnets like below. In this VPC we wouldn\u2019t be able to run more than one ",(0,t.jsx)(n.code,{children:"m5.16xlarge"})," node without adjusting the CNI configuration."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Init VPC",src:s(97277).Z+"",width:"1061",height:"531"})}),"\n",(0,t.jsxs)(n.p,{children:["You can add additional VPC CIDRs from a range that is not routable across VPCs (such as the RFC 6598 range, ",(0,t.jsx)(n.code,{children:"100.64.0.0/10"}),"). In this case we added ",(0,t.jsx)(n.code,{children:"100.64.0.0/16"}),", ",(0,t.jsx)(n.code,{children:"100.65.0.0/16"}),", and ",(0,t.jsx)(n.code,{children:"100.66.0.0/16"})," to the VPC (as this is the maximum CIDR size), then created new subnets with those CIDRs.\nFinally we recreated the node groups in the new subnets, leaving the existing EKS cluster control plane in place."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"expanded VPC",src:s(87060).Z+"",width:"1071",height:"882"})}),"\n",(0,t.jsx)(n.p,{children:"With this configuration you can still communicate with the EKS cluster control plane from connected VPCs but your nodes and pods have plenty of IP addresses to accommodate your workloads and the warm pool."}),"\n",(0,t.jsx)(n.h2,{id:"tuning-the-vpc-cni",children:"Tuning the VPC CNI"}),"\n",(0,t.jsx)(n.h3,{id:"vpc-cni-and-ec2-rate-throttling",children:"VPC CNI and EC2 Rate Throttling"}),"\n",(0,t.jsxs)(n.p,{children:["When an EKS worker node is launched it initially has a single ENI with a single IP address attached for the EC2 instance to communicate. As the VPC CNI launches it tries to provision a Warm Pool of IP addresses that can be assigned to Kubernetes Pods (",(0,t.jsx)(n.a,{href:"https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview",children:"More details in the EKS Best Practices Guide"}),")."]}),"\n",(0,t.jsxs)(n.p,{children:["The VPC CNI must make AWS EC2 API calls (like ",(0,t.jsx)(n.code,{children:"AssignPrivateIpV4Address"})," and ",(0,t.jsx)(n.code,{children:"DescribeNetworkInterfaces"}),") to assign those additional IPs and ENIs to the worker node. When the EKS cluster scales out the number of Nodes or Pods there could be a spike in the number of these EC2 API calls. This surge of calls could encounter rate throttling from the EC2 API to help the performance of the service, and to ensure fair usage for all Amazon EC2 customers. This rate throttling can cause the pool of IP address to be exhausted while the CNI tries to allocate more IPs."]}),"\n",(0,t.jsx)(n.p,{children:"These failures will cause errors like the one below, indicating that the provisioning of the container network namespace has failed because the VPC CNI could not provision an IP address."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "xxxxxxxxxxxxxxxxxxxxxx" network for pod "test-pod": networkPlugin cni failed to set up pod test-pod_default" network: add cmd: failed to assign an IP address to container\n'})}),"\n",(0,t.jsx)(n.p,{children:"This failure delays the launch of the Pod and adds pressure to the kubelet and worker node as this action is retried until the IP address is assigned. To avoid this delay you can configure the CNI to reduce the number of EC2 API calls needed."}),"\n",(0,t.jsxs)(n.h3,{id:"avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn",children:["Avoid using ",(0,t.jsx)(n.code,{children:"WARM_IP_TARGET"})," in large clusters, or cluster with a lot of churn"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"WARM_IP_TARGET"})," can help limit the \u201Cwasted\u201D IPs for small clusters, or clusters that has very low pod churn. However, this environment variable on the VPC CNI needs to be carefully configured in large clusters, as it may increase the number of EC2 API calls for IP attachment and detachment operations by ipamd, increasing the risk and impact of rate throttling.\n(",(0,t.jsx)(n.code,{children:"ipamd"})," stands for IP Address Management Daemon, and ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/best-practices/networking.html#amazon-virtual-private-cloud-vpc-cni",children:"is a core component of VPC CNI"}),". It maintains a warm pool of available IPs for fast Pod startup times.)"]}),"\n",(0,t.jsxs)(n.p,{children:["For clusters that have a lot of Pod churn, it is recommended to set ",(0,t.jsx)(n.code,{children:"MINIMUM_IP_TARGET"})," to a value slightly higher than the expected number of pods you plan to run on each node. This will allow the CNI to provision all of those IP addresses in a single (or few) calls."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'  [...]\n\n  # EKS Addons\n  cluster_addons = {\n    vpc-cni = {\n      configuration_values = jsonencode({\n        env = {\n          MINIMUM_IP_TARGET        = "30"\n        }\n      })\n    }\n  }\n\n  [...]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For detailed information of tweaking VPC CNI variables, refer to ",(0,t.jsx)(n.a,{href:"https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md",children:"this documentation on github"}),"."]}),"\n",(0,t.jsxs)(n.h3,{id:"limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods",children:["Limit the number of IPs per node on large instance types with ",(0,t.jsx)(n.code,{children:"MAX_ENI"})," and ",(0,t.jsx)(n.code,{children:"max-pods"})]}),"\n",(0,t.jsxs)(n.p,{children:["When using larger instance types such as ",(0,t.jsx)(n.code,{children:"16xlarge"})," or ",(0,t.jsx)(n.code,{children:"24xlarge"})," the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI",children:"number of IP addresses that can be assigned per ENI"})," can be fairly large. For example, a ",(0,t.jsx)(n.code,{children:"c5.18xlarge"})," instance type with the default CNI configuration of ",(0,t.jsx)(n.code,{children:"WARM_ENI_TARGET=1"})," would end up holding 100 IP addresses (50 IPs per ENI * 2 ENIs) when running a handful of pods."]}),"\n",(0,t.jsxs)(n.p,{children:["For some workloads the CPU, Memory, or other resource will limit the number of Pods on that ",(0,t.jsx)(n.code,{children:"c5.18xlarge"})," before we need more than 50 IPs. In this case you may want to be able to run 30-40 pods maximum on that instance."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'  [...]\n\n  # EKS Addons\n  cluster_addons = {\n    vpc-cni = {\n      configuration_values = jsonencode({\n        env = {\n          MAX_ENI           = "1"\n        }\n      })\n    }\n  }\n\n  [...]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Setting the ",(0,t.jsx)(n.code,{children:"MAX_ENI=1"})," option on the CNI and that this will limit the number of IP addresses each node is able to provision, but it does not limit the number of pod that kubernetes will try to schedule to the nodes. This can lead to a situation where pods are scheduled to nodes that are unable to provision more IP addresses."]}),"\n",(0,t.jsxs)(n.p,{children:["To limit the IPs ",(0,t.jsx)(n.em,{children:"and"})," stop k8s from scheduling too many pods you will need to:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Update the CNI configuration environment variables to set ",(0,t.jsx)(n.code,{children:"MAX_ENI=1"})]}),"\n",(0,t.jsxs)(n.li,{children:["Update the ",(0,t.jsx)(n.code,{children:"--max-pods"})," option for the kubelet on the worker nodes."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["To configure the --max-pods option you can update the userdata for your worker nodes to set this option ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh",children:"via the --kubelet -extra-args in the bootstrap.sh script"}),". By default this script configures the max-pods value for the kubelet, the --use-max-pods false` option disables this behavior when providing your own value:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'  eks_managed_node_groups = {\n    system = {\n      instance_types = ["m5.xlarge"]\n\n      min_size     = 0\n      max_size     = 5\n      desired_size = 3\n\n      pre_bootstrap_user_data = <<-EOT\n\n      EOT\n\n      bootstrap_extra_args = "--use-max-pods false --kubelet-extra-args \'--max-pods=<your_value>\'"\n\n    }\n'})}),"\n",(0,t.jsxs)(n.p,{children:["One problem is the number of IPs per ENI is different based on the Instance type (",(0,t.jsxs)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI",children:["for example a ",(0,t.jsx)(n.code,{children:"m5d.2xlarge"})," can have 15 IPs per ENI, where a ",(0,t.jsx)(n.code,{children:"m5d.4xlarge"})," can hold 30 IPs per ENI"]}),"). This means hard-coding a value for ",(0,t.jsx)(n.code,{children:"max-pods"})," may cause problems if you change instance types or in mixed-instance environments."]}),"\n",(0,t.jsxs)(n.p,{children:["In the EKS Optimized AMI releases there is ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/max-pods-calculator.sh",children:"a script included that can be used to help calculate the AWS Recommended max-pods value"}),". If you\u2019d like to automate this calculation for mixed instances you will also need to update the userdata for your instances to use the ",(0,t.jsx)(n.code,{children:"--instance-type-from-imds"})," flag to autodiscover the instance type from instance metadata."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'  eks_managed_node_groups = {\n    system = {\n      instance_types = ["m5.xlarge"]\n\n      min_size     = 0\n      max_size     = 5\n      desired_size = 3\n\n      pre_bootstrap_user_data = <<-EOT\n        /etc/eks/max-pod-calc.sh --instance-type-from-imds \u2014cni-version 1.13.4 \u2014cni-max-eni 1\n      EOT\n\n      bootstrap_extra_args = "--use-max-pods false --kubelet-extra-args \'--max-pods=<your_value>\'"\n\n    }\n'})}),"\n",(0,t.jsx)(n.h4,{id:"maxpods-with-karpenter",children:"Maxpods with Karpenter"}),"\n",(0,t.jsxs)(n.p,{children:["By default, Nodes provisioned by Karpenter will have the max pods on a node ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt",children:"based on the node instance type"}),". To configure the ",(0,t.jsx)(n.code,{children:"--max-pods"})," option as mentioned above by defining at the Provisioner level by specifying ",(0,t.jsx)(n.code,{children:"maxPods"})," within the ",(0,t.jsx)(n.code,{children:".spec.kubeletConfiguration"})," . This value will be used during Karpenter pod scheduling and passed through to ",(0,t.jsx)(n.code,{children:"--max-pods"})," on kubelet startup."]}),"\n",(0,t.jsx)(n.p,{children:"Below is the example Provisioner spec:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: default\nspec:\n  providerRef:\n    name: default\n  requirements:\n    - key: "karpenter.k8s.aws/instance-category"\n      operator: In\n      values: ["c", "m", "r"]\n    - key: "karpenter.sh/capacity-type" # If not included, the webhook for the AWS cloud provider will default to on-demand\n      operator: In\n      values: ["spot", "on-demand"]\n\n  # Karpenter provides the ability to specify a few additional Kubelet args.\n  # These are all optional and provide support for additional customization and use cases.\n  kubeletConfiguration:\n    maxPods: 30\n'})}),"\n",(0,t.jsx)(n.h2,{id:"application",children:"Application"}),"\n",(0,t.jsx)(n.h3,{id:"scaling-coredns",children:"Scaling CoreDNS"}),"\n",(0,t.jsx)(n.h4,{id:"default-behavior",children:"Default Behavior"}),"\n",(0,t.jsxs)(n.p,{children:["Route 53 Resolver enforces a limit of 1024 packets per second per network interface for each EC2 instance, and this limit is not adjustable. In EKS clusters, CoreDNS runs with two replicas by default, with each replica on a separate EC2 instance. When DNS traffic exceeds 1024 packets per second for a CoreDNS replica, DNS requests will be throttled, resulting in ",(0,t.jsx)(n.code,{children:"unknownHostException"})," errors."]}),"\n",(0,t.jsx)(n.h4,{id:"remediation",children:"Remediation"}),"\n",(0,t.jsx)(n.p,{children:"To address the scalability of default coreDNS, consider implementing one of the following two options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html",children:"Enable coreDNS auto-scaling"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/",children:"Implement Node local cache"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["While scaling out ",(0,t.jsx)(n.code,{children:"CoreDNS"})," it is also crucial to distribute replicas across different nodes. Co-locating CoreDNS on same nodes, will again end up throttling the ENI, rendering additional replicas ineffective. In order to distribute ",(0,t.jsx)(n.code,{children:"CoreDNS"})," across nodes, apply node anti-affinity policy to the pods:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"affinity:\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchExpressions:\n        - key: k8s-app\n          operator: In\n          values:\n          - kube-dns\n      topologyKey: kubernetes.io/hostname\n"})}),"\n",(0,t.jsx)(n.h4,{id:"coredns-monitoring",children:"CoreDNS Monitoring"}),"\n",(0,t.jsxs)(n.p,{children:["It is recommended to continuously monitor CoreDNS metrics. Refer to ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/best-practices/monitoring_eks_workloads_for_network_performance_issues.html#_monitoring_coredns_traffic_for_dns_throttling_issues",children:"EKS Networking Best Practices"})," for detailed information."]}),"\n",(0,t.jsx)(n.h3,{id:"dns-lookups-and-ndots",children:"DNS Lookups and ndots"}),"\n",(0,t.jsxs)(n.p,{children:["In ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/",children:"Kubernetes Pods with the default DNS configuration"})," have a ",(0,t.jsx)(n.code,{children:"resolv.conf"})," file like so:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"nameserver 10.100.0.10\nsearch namespace.svc.cluster.local svc.cluster.local cluster.local ec2.internal\noptions ndots:5\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The domain names listed in the ",(0,t.jsx)(n.code,{children:"search"})," line are appended to DNS names that are not fully qualified domain names (FQDN). For example, if a pod tries to connect to a Kubernetes service using ",(0,t.jsx)(n.code,{children:"servicename.namespace"})," the domains would be appended in order until the DNS name matched the full kubernetes service name:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"servicename.namespace.namespace.svc.cluster.local   <--- Fails with NXDOMAIN\nservicename.namespace.svc.cluster.local        <-- Succeed\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Whether or not a domain is fully qualified is determined by the ",(0,t.jsx)(n.code,{children:"ndots"})," option in the resolv.conf. This option defines the number of dots that must be in a domain name before the ",(0,t.jsx)(n.code,{children:"search"})," domains are skipped. These additional searches can add latency to connections to external resources like S3 and RDS endpoints."]}),"\n",(0,t.jsxs)(n.p,{children:["The default ",(0,t.jsx)(n.code,{children:"ndots"})," setting in Kubernetes is five, if your application isn\u2019t talking to other pods in the cluster, we can set the ",(0,t.jsx)(n.code,{children:"ndots"})," to a low value like \u201C2\u201D. This is a good starting point, because it still allows your application to do service discovery within the same namespace and in other namespaces within the cluster, but allows a domain like ",(0,t.jsx)(n.code,{children:"s3.us-east-2.amazonaws.com"})," to be recognized as a FQDN (skipping the ",(0,t.jsx)(n.code,{children:"search"})," domains)."]}),"\n",(0,t.jsxs)(n.p,{children:["Here\u2019s an example pod manifest from the Kubernetes documentation with ",(0,t.jsx)(n.code,{children:"ndots"})," set to \u201C2\u201D:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: dns-example\nspec:\n  containers:\n    - name: test\n      image: nginx\n  dnsConfig:\n    options:\n      - name: ndots\n        value: "2"\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["While setting ",(0,t.jsx)(n.code,{children:"ndots"})," to \u201C2\u201D in your pod deployment is a reasonable place to start, this will not universally work in all situations and shouldn\u2019t be applied across the entire cluster. The ",(0,t.jsx)(n.code,{children:"ndots"})," configuration needs to be configured at the Pod or Deployment level. Reducing this setting at the Cluster level CoreDNS configuration is not recommended."]})}),"\n",(0,t.jsx)(n.h3,{id:"inter-az-network-optimization",children:"Inter AZ Network Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Some workloads may need to exchange data between Pods in the cluster, like Spark executors during the shuffle stage.\nIf the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front. Hence, for these workloads, it is recommended to colocate executors or worker pods in the same AZ. Colocating workloads in the same AZ serves two main purposes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reduce inter-AZ traffic costs"}),"\n",(0,t.jsx)(n.li,{children:"Reduce network latency between executors/Pods"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["To have pods co-located on the same AZ, we can use ",(0,t.jsx)(n.code,{children:"podAffinity"})," based scheduling constraints. The scheduling constraint ",(0,t.jsx)(n.code,{children:"preferredDuringSchedulingIgnoredDuringExecution"})," can be enforced in the Pod spec. For example, ins Spark we can use a custom template for our driver and executor pods:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"spec:\n  executor:\n    affinity:\n      podAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n              matchExpressions:\n              - key: sparkoperator.k8s.io/app-name\n                operator: In\n                values:\n                - <<spark-app-name>>\n          topologyKey: topology.kubernetes.io/zone\n          ...\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can also leverage Kubernetes Topology Aware Routing to have Kubernetes services route traffic in more efficient means once pods have been created: ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/",children:"https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/"})]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Having all executors located in a single AZ, means that AZ will be a ",(0,t.jsx)(n.em,{children:"single point of failure"}),". This is a trade off you should consider between lowering network cost and latency, and the event of an AZ failure interrupting workloads.\nIf your workload is running on instances with constrained capacity you may consider using multiple AZs to avoid Insufficient Capacity errors."]})})]})}function h(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},87060:function(e,n,s){s.d(n,{Z:()=>o});let o=s.p+"assets/images/expanded-vpc-64f8e8bd4f836b1ea5a45f679a209d6d.png"},97277:function(e,n,s){s.d(n,{Z:()=>o});let o=s.p+"assets/images/init-vpc-d6c609e16a67746f8c78568ae8d2295e.png"},50065:function(e,n,s){s.d(n,{Z:()=>r,a:()=>a});var o=s(67294);let t={},i=o.createContext(t);function a(e){let n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);