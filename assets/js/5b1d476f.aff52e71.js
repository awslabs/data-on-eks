"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[2326],{5933:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var s=t(4848),i=t(8453),r=t(2450);const a={title:"Llama-3-8B with vLLM on Inferentia2",sidebar_position:1,description:"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance."},o="Serving LLMs with RayServe and vLLM on AWS Neuron",l={id:"gen-ai/inference/Neuron/vllm-ray-inf2",title:"Llama-3-8B with vLLM on Inferentia2",description:"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance.",source:"@site/docs/gen-ai/inference/Neuron/vllm-ray-inf2.md",sourceDirName:"gen-ai/inference/Neuron",slug:"/gen-ai/inference/Neuron/vllm-ray-inf2",permalink:"/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/Neuron/vllm-ray-inf2.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Llama-3-8B with vLLM on Inferentia2",sidebar_position:1,description:"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance."},sidebar:"genai",previous:{title:"NVIDIA NIM LLM on Amazon EKS",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/nvidia-nim-llama3"},next:{title:"Mistral-7B on Inferentia2",permalink:"/data-on-eks/docs/gen-ai/inference/Neuron/Mistral-7b-inf2"}},c={},d=[{value:"What is AWS Neuron?",id:"what-is-aws-neuron",level:3},{value:"What is vLLM?",id:"what-is-vllm",level:3},{value:"What is RayServe?",id:"what-is-rayserve",level:3},{value:"What is Llama-3-8B Instruct?",id:"what-is-llama-3-8b-instruct",level:3},{value:"Why AWS Accelerators?",id:"why-aws-accelerators",level:3},{value:"Solution Architecture",id:"solution-architecture",level:2},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Verify Neuron Plugin",id:"verify-neuron-plugin",level:3},{value:"Verify Neuron Scheduler",id:"verify-neuron-scheduler",level:3},{value:"Deploying the Ray Cluster with Llama3 Model",id:"deploying-the-ray-cluster-with-llama3-model",level:2},{value:"To Test the Llama3 Model",id:"to-test-the-llama3-model",level:3},{value:"Observability",id:"observability",level:2},{value:"Observability with AWS CloudWatch and Neuron Monitor",id:"observability-with-aws-cloudwatch-and-neuron-monitor",level:3},{value:"Open WebUI Deployment",id:"open-webui-deployment",level:2},{value:"Performance Benchmarking with LLMPerf Tool",id:"performance-benchmarking-with-llmperf-tool",level:2},{value:"Performance Benchmarking Metrics",id:"performance-benchmarking-metrics",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",...(0,i.R)(),...e.components},{Details:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsxs)(n.p,{children:["Note: Use of this Llama-3 Instruct model is governed by the Meta license.\nIn order to download the model weights and tokenizer, please visit the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3-8B",children:"website"})," and accept the license before requesting access."]})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects."})}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"serving-llms-with-rayserve-and-vllm-on-aws-neuron",children:"Serving LLMs with RayServe and vLLM on AWS Neuron"})}),"\n",(0,s.jsxs)(n.p,{children:["Welcome to the comprehensive guide on deploying LLMs on Amazon Elastic Kubernetes Service (EKS) using ",(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/index.html",children:"Ray Serve"})," and AWS Neuron."]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-aws-neuron",children:"What is AWS Neuron?"}),"\n",(0,s.jsxs)(n.p,{children:["In this tutorial, you'll leverage ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/machine-learning/neuron/",children:"AWS Neuron"}),", a powerful SDK that optimizes deep learning performance on AWS Inferentia and Trainium accelerators. Neuron seamlessly integrates with frameworks like PyTorch and TensorFlow, providing a comprehensive toolkit for developing, profiling, and deploying high-performance machine learning models on specialized EC2 instances such as Inf1, Inf2, Trn1, and Trn1n."]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-vllm",children:"What is vLLM?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM"})," is a high-performance library for LLM inference and serving, designed to maximize throughput and minimize latency. At its core, vLLM utilizes ",(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html",children:"PagedAttention"}),", an innovative attention algorithm that dramatically improves memory efficiency, allowing for optimal utilization of GPU resources. This open-source solution offers seamless integration through its Python API and OpenAI-compatible server, enabling developers to deploy and scale large language models like Llama 3 with unprecedented efficiency in production environments."]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-rayserve",children:"What is RayServe?"}),"\n",(0,s.jsx)(n.p,{children:"Ray Serve is a scalable model serving library built on top of Ray, designed for deploying machine learning models and AI applications with features like framework-agnostic deployment, model composition, and built-in scaling. You will also encounter RayService, which is a Kubernetes custom resource that's part of the KubeRay project, used to deploy and manage Ray Serve applications on Kubernetes clusters."}),"\n",(0,s.jsx)(n.h3,{id:"what-is-llama-3-8b-instruct",children:"What is Llama-3-8B Instruct?"}),"\n",(0,s.jsx)(n.p,{children:"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety."}),"\n",(0,s.jsxs)(n.p,{children:["More information on Llama3 sizes and model architecture can be found ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",children:"here"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"why-aws-accelerators",children:"Why AWS Accelerators?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scalability and Availability"})}),"\n",(0,s.jsxs)(n.p,{children:["One of the key challenges in deploying large language models (",(0,s.jsx)(n.code,{children:"LLMs"}),") like Llama-3 is the scalability and availability of suitable hardware. Traditional ",(0,s.jsx)(n.code,{children:"GPU"})," instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively."]}),"\n",(0,s.jsxs)(n.p,{children:["In contrast, ",(0,s.jsx)(n.code,{children:"Trn1/Inf2"})," instances, such as ",(0,s.jsx)(n.code,{children:"trn1.32xlarge"}),", ",(0,s.jsx)(n.code,{children:"trn1n.32xlarge"}),", ",(0,s.jsx)(n.code,{children:"inf2.24xlarge"})," and ",(0,s.jsx)(n.code,{children:"inf2.48xlarge"}),", are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your ",(0,s.jsx)(n.code,{children:"Llama-3"})," models as needed, without resource bottlenecks or delays."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cost Optimization"})}),"\n",(0,s.jsxs)(n.p,{children:["Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing. ",(0,s.jsx)(n.strong,{children:"Trn1/Inf2"})," instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Boost"})}),"\n",(0,s.jsx)(n.p,{children:"While Llama-3 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-3's inference speeds. This translates to faster response times and improved user experiences when deploying Llama-3 on Trn1/Inf2 instances."}),"\n",(0,s.jsx)(n.h2,{id:"solution-architecture",children:"Solution Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["In this section, we will delve into the architecture of our solution, which combines Llama-3 model, ",(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/index.html",children:"Ray Serve"})," and ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/inf2/",children:"Inferentia2"})," on Amazon EKS."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Llama-3-inf2",src:t(8236).A+"",width:"1642",height:"834"})}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-solution",children:"Deploying the Solution"}),"\n",(0,s.jsxs)(n.p,{children:["To get started with deploying ",(0,s.jsx)(n.code,{children:"Llama-3-8B-instruct"})," on ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),", we will cover the necessary prerequisites and guide you through the deployment process step by step."]}),"\n",(0,s.jsxs)(n.p,{children:["This includes setting up the infrastructure using AWS Inferentia instances and deploying the ",(0,s.jsx)(n.strong,{children:"Ray cluster"}),"."]}),"\n",(0,s.jsxs)(r.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Prerequisites"})}),children:[(0,s.jsx)(n.p,{children:"Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free.\nEnsure that you have installed the following tools on your machine."}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,s.jsx)(n.p,{children:"Clone the repository:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,s.jsxs)(n.p,{children:["Navigate into the following directory and run ",(0,s.jsx)(n.code,{children:"install.sh"})," script:"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Important Note:"})," Ensure that you update the region in the ",(0,s.jsx)(n.code,{children:"variables.tf"})," file before deploying the blueprint.\nAdditionally, confirm that your local region setting matches the specified region to prevent any discrepancies.\nFor example, set your ",(0,s.jsx)(n.code,{children:'export AWS_DEFAULT_REGION="<REGION>"'})," to the desired region."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/trainium-inferentia/\n./install.sh\n"})}),(0,s.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,s.jsx)(n.p,{children:"Verify the Amazon EKS Cluster"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 describe-cluster --name trainium-inferentia\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Creates k8s config file to authenticate with EKS\naws eks --region us-west-2 update-kubeconfig --name trainium-inferentia\n\nkubectl get nodes # Output shows the EKS Managed Node group nodes\n"})}),(0,s.jsx)(n.p,{children:"Verify the Karpenter autoscaler Nodepools"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get nodepools\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"NAME              NODECLASS\ndefault           default\ninferentia-inf2   inferentia-inf2\ntrainium-trn1     trainium-trn1\n"})}),(0,s.jsx)(n.h3,{id:"verify-neuron-plugin",children:"Verify Neuron Plugin"}),(0,s.jsx)(n.p,{children:"Neuron device plugin exposes Neuron cores & devices to kubernetes as a resource. Verify the status of the plugin installed by the blueprint."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get ds neuron-device-plugin --namespace kube-system\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nneuron-device-plugin   1         1         1       1            1           <none>          15d\n"})}),(0,s.jsx)(n.h3,{id:"verify-neuron-scheduler",children:"Verify Neuron Scheduler"}),(0,s.jsx)(n.p,{children:"The Neuron scheduler extension is required for scheduling pods that require more than one Neuron core or device resource. Verify the status of the scheduler installed by the blueprint."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n kube-system | grep my-scheduler\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"my-scheduler-c6fc957d9-hzrf7  1/1     Running   0  2d1h\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-ray-cluster-with-llama3-model",children:"Deploying the Ray Cluster with Llama3 Model"}),"\n",(0,s.jsx)(n.p,{children:"In this tutorial, we leverage the KubeRay operator, which extends Kubernetes with custom resource definitions for Ray-specific constructs like RayCluster, RayJob, and RayService. The operator watches for user events related to these resources, automatically creates necessary Kubernetes artifacts to form Ray clusters, and continuously monitors cluster state to ensure the desired configuration matches the actual state. It handles lifecycle management including setup, dynamic scaling of worker groups, and teardown, abstracting away the complexity of managing Ray applications on Kubernetes."}),"\n",(0,s.jsx)(n.p,{children:"Each Ray cluster consists of a head node pod and a collection of worker node pods, with optional autoscaling support to size clusters according to workload requirements. KubeRay supports heterogeneous compute nodes (including GPUs) and running multiple Ray clusters with different Ray versions in the same Kubernetes cluster. Additionally, KubeRay can integrate with AWS Inferentia accelerators, enabling efficient deployment of large language models like Llama 3 on specialized hardware, potentially improving performance and cost-effectiveness for machine learning inference tasks."}),"\n",(0,s.jsxs)(n.p,{children:["Having deployed the EKS cluster with all the necessary components, we can now proceed with the steps to deploy ",(0,s.jsx)(n.code,{children:"NousResearch/Meta-Llama-3-8B-Instruct"})," using ",(0,s.jsx)(n.code,{children:"RayServe"})," and ",(0,s.jsx)(n.code,{children:"vLLM"})," on AWS Accelerators."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 1:"})," To deploy the RayService cluster, navigate to the directory containing the ",(0,s.jsx)(n.code,{children:"vllm-rayserve-deployment.yaml"})," file and execute the ",(0,s.jsx)(n.code,{children:"kubectl apply"})," command in your terminal.\nThis will apply the RayService configuration and deploy the cluster on your EKS setup."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2\n\nkubectl apply -f vllm-rayserve-deployment.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Optional Configuration"})}),"\n",(0,s.jsxs)(n.p,{children:["By default, an ",(0,s.jsx)(n.code,{children:"inf2.8xlarge"})," instance will be provisioned. If you would like to use ",(0,s.jsx)(n.code,{children:"inf2.48xlarge"}),", modify the file ",(0,s.jsx)(n.code,{children:"vllm-rayserve-deployment.yaml"})," to change ",(0,s.jsx)(n.code,{children:"resources"})," section under ",(0,s.jsx)(n.code,{children:"worker"})," container."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'limits:\n    cpu: "30"\n    memory: "110G"\n    aws.amazon.com/neuron: "1"\nrequests:\n    cpu: "30"\n    memory: "110G"\n    aws.amazon.com/neuron: "1"\n'})}),"\n",(0,s.jsx)(n.p,{children:"to the following:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'limits:\n    cpu: "90"\n    memory: "360G"\n    aws.amazon.com/neuron: "12"\nrequests:\n    cpu: "90"\n    memory: "360G"\n    aws.amazon.com/neuron: "12"\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 2:"})," Verify the deployment by running the following commands"]}),"\n",(0,s.jsx)(n.p,{children:"To ensure that the deployment has been successfully completed, run the following commands:"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["Deployment process may take up to ",(0,s.jsx)(n.strong,{children:"10 minutes"}),". The Head Pod is expected to be ready within 5 to 6 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface."]})}),"\n",(0,s.jsxs)(n.p,{children:["According to the RayServe configuration, you will have one Ray head pod running on an ",(0,s.jsx)(n.code,{children:"x86"})," instance and one worker pod running on a ",(0,s.jsx)(n.code,{children:"inf2"})," instance. You can modify the RayServe YAML file to run multiple replicas; however, be aware that each additional replica can potentially create new instances."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n vllm\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"NAME                                                      READY   STATUS    RESTARTS   AGE\nlm-llama3-inf2-raycluster-ksh7w-worker-inf2-group-dcs5n   1/1     Running   0          2d4h\nvllm-llama3-inf2-raycluster-ksh7w-head-4ck8f              2/2     Running   0          2d4h\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This deployment also configures a service with multiple ports. Port ",(0,s.jsx)(n.strong,{children:"8265"})," is designated for the Ray dashboard, and port ",(0,s.jsx)(n.strong,{children:"8000"})," is for the vLLM inference server endpoint."]}),"\n",(0,s.jsx)(n.p,{children:"Run the following command to verify the services:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n vllm\n\nNAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE\nvllm                         ClusterIP   172.20.23.54    <none>        8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP   2d4h\nvllm-llama3-inf2-head-svc    ClusterIP   172.20.18.130   <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   2d4h\nvllm-llama3-inf2-serve-svc   ClusterIP   172.20.153.10   <none>        8000/TCP                                        2d4h\n"})}),"\n",(0,s.jsx)(n.p,{children:"To access the Ray dashboard, you can port-forward the relevant port to your local machine:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n vllm port-forward svc/vllm 8265:8265\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You can then access the web UI at ",(0,s.jsx)(n.a,{href:"http://localhost:8265",children:"http://localhost:8265"}),", which displays the deployment of jobs and actors within the Ray ecosystem."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RayServe Deployment",src:t(2394).A+"",width:"3456",height:"1202"})}),"\n",(0,s.jsxs)(n.p,{children:["Once the deployment is complete, the Controller and Proxy status should be ",(0,s.jsx)(n.code,{children:"HEALTHY"})," and Application status should be ",(0,s.jsx)(n.code,{children:"RUNNING"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RayServe Deployment Logs",src:t(5325).A+"",width:"3450",height:"1224"})}),"\n",(0,s.jsx)(n.h3,{id:"to-test-the-llama3-model",children:"To Test the Llama3 Model"}),"\n",(0,s.jsxs)(n.p,{children:["Now it's time to test the ",(0,s.jsx)(n.code,{children:"Meta-Llama-3-8B-Instruct"})," chat model. We'll use a Python client script to send prompts to the RayServe inference endpoint and verify the outputs generated by the model."]}),"\n",(0,s.jsxs)(n.p,{children:["First, execute a port forward to the ",(0,s.jsx)(n.code,{children:"vllm-llama3-inf2-serve-svc"})," Service using kubectl:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"openai-client.py"})," uses the HTTP POST method to send a list of prompts to the inference endpoint for text completion and Q&A, targeting the vllm server."]}),"\n",(0,s.jsx)(n.p,{children:"To run the Python client application in a virtual environment, follow these steps:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install openai\npython3 openai-client.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"You will see an output something like below in the terminal:"}),"\n",(0,s.jsxs)(a,{children:[(0,s.jsx)("summary",{children:"Click to expand Python Client Terminal output"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Example 1 - Simple chat completion:\nHandling connection for 8000\nThe capital of India is New Delhi.\n\n\nExample 2 - Chat completion with different parameters:\nThe twin suns of Tatooine set slowly in the horizon, casting a warm orange glow over the bustling spaceport of Anchorhead. Amidst the hustle and bustle, a young farm boy named Anakin Skywalker sat atop a dusty speeder, his eyes fixed on the horizon as he dreamed of adventure beyond the desert planet.\n\nAs the suns dipped below the dunes, Anakin's uncle, Owen Lars, called out to him from the doorway of their humble moisture farm. \"Anakin, it's time to head back! Your aunt and I have prepared a special dinner in your honor.\"\n\nBut Anakin was torn. He had received a strange message from an unknown sender, hinting at a great destiny waiting for him. Against his uncle's warnings, Anakin decided to investigate further, sneaking away into the night to follow the mysterious clues.\n\nAs he rode his speeder through the desert, the darkness seemed to grow thicker, and the silence was broken only by the distant\n\n\nExample 3 - Streaming chat completion:\nI'd be happy to help you with that. Here we go:\n\n1...\n\n(Pause)\n\n2...\n\n(Pause)\n\n3...\n\n(Pause)\n\n4...\n\n(Pause)\n\n5...\n\n(Pause)\n\n6...\n\n(Pause)\n\n7...\n\n(Pause)\n\n8...\n\n(Pause)\n\n9...\n\n(Pause)\n\n10!\n\nLet me know if you have any other requests!\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,s.jsx)(n.h3,{id:"observability-with-aws-cloudwatch-and-neuron-monitor",children:"Observability with AWS CloudWatch and Neuron Monitor"}),"\n",(0,s.jsxs)(n.p,{children:["This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the addon utilizes the ",(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide",children:"Neuron Monitor plugin"})," to capture and report Neuron-specific metrics."]}),"\n",(0,s.jsx)(n.p,{children:"All metrics, including container insights and Neuron metrics such as Neuron Core utilization, NeuronCore memory usage are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"CloudWatch-neuron-monitor",src:t(1745).A+"",width:"3070",height:"1916"})}),"\n",(0,s.jsx)(n.h2,{id:"open-webui-deployment",children:"Open WebUI Deployment"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," is compatible only with models that work with the OpenAI API server and Ollama."]})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Deploy the WebUI"})}),"\n",(0,s.jsxs)(n.p,{children:["Deploy the ",(0,s.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," by running the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f openai-webui-deployment.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Port Forward to Access WebUI"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note"})," If you're running a port forward already to test the inference with python client, then press ",(0,s.jsx)(n.code,{children:"ctrl+c"})," to interrupt that."]}),"\n",(0,s.jsx)(n.p,{children:"Use kubectl port-forward to access the WebUI locally:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl port-forward svc/open-webui 8081:80 -n openai-webui\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Access the WebUI"})}),"\n",(0,s.jsxs)(n.p,{children:["Open your browser and go to ",(0,s.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"4. Sign Up"})}),"\n",(0,s.jsx)(n.p,{children:"Sign up using your name, email, and a dummy password."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"5. Start a New Chat"})}),"\n",(0,s.jsx)(n.p,{children:"Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"alt text",src:t(7298).A+"",width:"2848",height:"1442"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"6. Enter Test Prompt"})}),"\n",(0,s.jsx)(n.p,{children:"Enter your prompt, and you will see the streaming results, as shown below:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"alt text",src:t(5481).A+"",width:"1436",height:"611"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-benchmarking-with-llmperf-tool",children:"Performance Benchmarking with LLMPerf Tool"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/ray-project/llmperf/blob/main/README.md",children:"LLMPerf"})," is an open-source tool designed for benchmarking the performance of large language models (LLMs)."]}),"\n",(0,s.jsxs)(n.p,{children:["LLMPerf tool connects to the vllm service via port 8000 using the port forwarding setup above done using the command ",(0,s.jsx)(n.code,{children:"kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Execute the commands below in your terminal."}),"\n",(0,s.jsx)(n.p,{children:"Clone the LLMPerf repository:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/ray-project/llmperf.git\ncd llmperf\npip install -e .\npip install pandas\npip install ray\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Create the ",(0,s.jsx)(n.code,{children:"vllm_benchmark.sh"})," file using the command below:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cat << 'EOF' > vllm_benchmark.sh\n#!/bin/bash\nmodel=${1:-NousResearch/Meta-Llama-3-8B-Instruct}\nvu=${2:-1}\nexport OPENAI_API_KEY=EMPTY\nexport OPENAI_API_BASE=\"http://localhost:8000/v1\"\nexport TOKENIZERS_PARALLELISM=true\n#if you have more vllm servers, append the below line to the above\n#;http://localhost:8001/v1;http://localhost:8002/v1\"\nmax_requests=$(expr ${vu} \\* 8 )\ndate_str=$(date '+%Y-%m-%d-%H-%M-%S')\npython ./token_benchmark_ray.py \\\n       --model ${model} \\\n       --mean-input-tokens 512 \\\n       --stddev-input-tokens 20 \\\n       --mean-output-tokens 245 \\\n       --stddev-output-tokens 20 \\\n       --max-num-completed-requests ${max_requests} \\\n       --timeout 7200 \\\n       --num-concurrent-requests ${vu} \\\n       --results-dir \"vllm_bench_results/${date_str}\" \\\n       --llm-api openai \\\n       --additional-sampling-params '{}'\nEOF\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--mean-input-tokens"}),": specifies the average number of tokens in the input prompts"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--stddev-input-tokens"}),": specifies the variability in input token lengths for creating a more realistic testing environment"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--mean-output-tokens"}),": specifies the average number of tokens expected in the model's output to simulate realistic response lengths"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--stddev-output-tokens"}),": specifies the variability in output token lengths introducing diversity in response sizes"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--max-num-completed-requests"}),": sets the maximum number of requests to process"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"--num-concurrent-requests"}),": specifies the number of simultaneous requests to simulate parallel workload"]}),"\n",(0,s.jsxs)(n.p,{children:["The command below executes the benchmarking script with the specified model, ",(0,s.jsx)(n.code,{children:"NousResearch/Meta-Llama-3-8B-Instruct"}),", and sets the number of virtual users to 2. This results in the benchmark testing the model's performance with 2 concurrent requests, calculating a maximum of 16 requests to be processed."]}),"\n",(0,s.jsx)(n.p,{children:"Execute the command below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should see similar output like the following:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2\nNone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n2024-09-03 09:54:45,976\tINFO worker.py:1783 -- Started a local Ray instance.\n  0%|                                                                                                                                                                                                                                                    | 0/16 [00:00<?, ?it/s]Handling connection for 8000\nHandling connection for 8000\n 12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                              | 2/16 [00:17<02:00,  8.58s/it]Handling connection for 8000\nHandling connection for 8000\n 25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                 | 4/16 [00:33<01:38,  8.20s/it]Handling connection for 8000\nHandling connection for 8000\n 38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                   | 6/16 [00:47<01:17,  7.75s/it]Handling connection for 8000\nHandling connection for 8000\n 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                      | 8/16 [01:00<00:58,  7.36s/it]Handling connection for 8000\nHandling connection for 8000\n 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                        | 10/16 [01:15<00:43,  7.31s/it]Handling connection for 8000\nHandling connection for 8000\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                          | 12/16 [01:29<00:28,  7.20s/it]Handling connection for 8000\nHandling connection for 8000\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                             | 14/16 [01:45<00:15,  7.52s/it]Handling connection for 8000\nHandling connection for 8000\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [02:01<00:00,  7.58s/it]\n\\Results for token benchmark for NousResearch/Meta-Llama-3-8B-Instruct queried with the openai api.\n\ninter_token_latency_s\n    p25 = 0.051964785839225695\n    p50 = 0.053331799814278796\n    p75 = 0.05520852723583741\n    p90 = 0.05562424625711179\n    p95 = 0.05629651696856784\n    p99 = 0.057518213120178636\n    mean = 0.053548951905597324\n    min = 0.0499955879607504\n    max = 0.05782363715808134\n    stddev = 0.002070751885022901\nttft_s\n    p25 = 1.5284210312238429\n    p50 = 1.7579061459982768\n    p75 = 1.8209733433031943\n    p90 = 1.842437624989543\n    p95 = 1.852818323241081\n    p99 = 1.8528624982456676\n    mean = 1.5821313202395686\n    min = 0.928935999982059\n    max = 1.8528735419968143\n    stddev = 0.37523908630204694\nend_to_end_latency_s\n    p25 = 13.74749460403109\n    p50 = 14.441407957987394\n    p75 = 15.53337344751344\n    p90 = 16.104882833489683\n    p95 = 16.366086292022374\n    p99 = 16.395070491998922\n    mean = 14.528114874927269\n    min = 10.75658329098951\n    max = 16.40231654199306\n    stddev = 1.4182672949824733\nrequest_output_throughput_token_per_s\n    p25 = 18.111220396798153\n    p50 = 18.703139371912407\n    p75 = 19.243016652511997\n    p90 = 19.37836414194298\n    p95 = 19.571455249271224\n    p99 = 19.915057038539217\n    mean = 18.682678715983627\n    min = 17.198769813363445\n    max = 20.000957485856215\n    stddev = 0.725563381521316\nnumber_input_tokens\n    p25 = 502.5\n    p50 = 509.5\n    p75 = 516.5\n    p90 = 546.5\n    p95 = 569.25\n    p99 = 574.65\n    mean = 515.25\n    min = 485\n    max = 576\n    stddev = 24.054105678657024\nnumber_output_tokens\n    p25 = 259.75\n    p50 = 279.5\n    p75 = 291.75\n    p90 = 297.0\n    p95 = 300.5\n    p99 = 301.7\n    mean = 271.625\n    min = 185\n    max = 302\n    stddev = 29.257192847799555\nNumber Of Errored Requests: 0\nOverall Output Throughput: 35.827933968528434\nNumber Of Completed Requests: 16\nCompleted Requests Per Minute: 7.914131755588426\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can try generating benchmarking results with multiple concurrent requests to understand how performance varies with increasing number of concurrent requests:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2\n./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 4\n./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 8\n./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 16\n.\n.\n"})}),"\n",(0,s.jsx)(n.h3,{id:"performance-benchmarking-metrics",children:"Performance Benchmarking Metrics"}),"\n",(0,s.jsxs)(n.p,{children:["You can find the results of the benchmarking script  under ",(0,s.jsx)(n.code,{children:"vllm_bench_results"})," directory in the ",(0,s.jsx)(n.code,{children:"llmperf"})," directory. The results are stored in folders following a date-time naming convention. New folders are created every time the benchmarking script is executed."]}),"\n",(0,s.jsx)(n.p,{children:"You will find that the results for every execution of the benchmarking script comprise of 2 files in the format below:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"NousResearch-Meta-Llama-3-8B-Instruct_512_245_summary_32.json"})," - contains the summary of performance metrics across all the request/response pairs."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"NousResearch-Meta-Llama-3-8B-Instruct_512_245_individual_responses.json"})," - contains performance metrics for each and every request / response pair."]}),"\n",(0,s.jsx)(n.p,{children:"Each of these files contain the following Performance Benchmarking Metrics:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_inter_token_latency_s_*"}),": Also referred to as Token generation latency (TPOT).Inter-Token latency refers to the average time elapsed between generating consecutive output tokens by a large language model (LLM) during the decoding or generation phase"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_ttft_s_*"}),": Time taken to generate the first token (TTFT)"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_end_to_end_s_*"}),": End-to-End latency - total time taken from when a user submits an input prompt to when the complete output response is generated by the LLM"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_request_output_throughput_token_per_s_*"}),": Number of output tokens generated by a large language model (LLM) per second across all user requests or queries"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_number_input_tokens_*"}),": Number of input tokens in the requests (Input length)"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_number_output_tokens_*"}),": Number of output tokens in the requests (Output length)"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"In summary, when it comes to deploying and scaling Llama-3, AWS Trn1/Inf2 instances offer a compelling advantage.\nThey provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you're building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-3 on the AWS cloud."}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed."}),"\n",(0,s.jsx)(n.p,{children:"Delete the RayCluster"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2\n\nkubectl delete -f vllm-rayserve-deployment.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Destroy the EKS Cluster and resources"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/trainium-inferentia/\n\n./cleanup.sh\n"})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},2450:(e,n,t)=>{t.d(n,{A:()=>m});var s=t(6540),i=t(5556),r=t.n(i),a=t(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=t(4848);function p(e){let{children:n,header:t}=e;const[i,r]=(0,s.useState)(!1);return(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("div",{className:(0,a.A)(l,{[h]:i}),onClick:()=>{r(!i)},children:[t,(0,u.jsx)("span",{className:(0,a.A)(c,{[h]:i}),children:i?"\ud83d\udc47":"\ud83d\udc48"})]}),i&&(0,u.jsx)("div",{className:d,children:n})]})}p.propTypes={children:r().node.isRequired,header:r().node.isRequired};const m=p},1745:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/neuron-monitor-cwci-4ed152729b7e072f5b1d2f79c25d75b3.png"},7298:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/openweb-ui-ray-vllm-inf2-1-ca4458ab24555d56156a7a393d81b5eb.png"},5481:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/openweb-ui-ray-vllm-inf2-2-bd168c90b7849105772e8332dcad5f86.png"},2394:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/ray-dashboard-vllm-llama3-inf2-1ec06c016b0ffdf955c973406854f2f0.png"},5325:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/ray-logs-vllm-llama3-inf2-5c5963506ac6563dae4bd04a4f6a16bf.png"},8236:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/ray-vllm-inf2-f4743ae82b32bc6cfd823bcd97a29144.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);