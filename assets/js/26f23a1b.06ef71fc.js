"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[6226],{8053:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var r=i(4848),s=i(8453),o=i(2450);const t={sidebar_position:1,sidebar_label:"BioNeMo on EKS"},a="BioNeMo on EKS",l={id:"gen-ai/training/GPUs/bionemo",title:"bionemo",description:"The AI on EKS content is being migrated to a new repository.",source:"@site/docs/gen-ai/training/GPUs/bionemo.md",sourceDirName:"gen-ai/training/GPUs",slug:"/gen-ai/training/GPUs/bionemo",permalink:"/data-on-eks/docs/gen-ai/training/GPUs/bionemo",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/training/GPUs/bionemo.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,sidebar_label:"BioNeMo on EKS"},sidebar:"genai",previous:{title:"Training on EKS",permalink:"/data-on-eks/docs/category/training-on-eks"},next:{title:"Llama-3 with RayTrain on Trn1",permalink:"/data-on-eks/docs/gen-ai/training/Neuron/Llama-LoRA-Finetuning"}},d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Deploying BioNeMo on Kubernetes",id:"deploying-bionemo-on-kubernetes",level:2},{value:"Clone the repository",id:"clone-the-repository",level:4},{value:"Initialize Terraform",id:"initialize-terraform",level:4},{value:"Run the install script",id:"run-the-install-script",level:4},{value:"Install Kubeflow Training Operator",id:"install-kubeflow-training-operator",level:4},{value:"Run BioNeMo Training jobs",id:"run-bionemo-training-jobs",level:3},{value:"Step1: Initiate the Uniref50 Data Preparation Task",id:"step1-initiate-the-uniref50-data-preparation-task",level:4},{value:"Benefits of Distributed Training:",id:"benefits-of-distributed-training",level:4},{value:"Conclusion",id:"conclusion",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"AI on EKS"})," content ",(0,r.jsx)(n.strong,{children:"is being migrated"})," to a new repository.\n\ud83d\udd17 \ud83d\udc49 ",(0,r.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement",children:"Read the full migration announcement \xbb"})]})}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"bionemo-on-eks",children:"BioNeMo on EKS"})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsx)(n.p,{children:"This blueprint should be considered as experimental and should only be used for proof of concept."})}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://www.nvidia.com/en-us/clara/bionemo/",children:"NVIDIA BioNeMo"})," is a generative AI platform for drug discovery that simplifies and accelerates the training of models using your own data and scaling the deployment of models for drug discovery applications. BioNeMo offers the quickest path to both AI model development and deployment, accelerating the journey to AI-powered drug discovery. It has a growing community of users and contributors, and is actively maintained and developed by the NVIDIA."]}),"\n",(0,r.jsx)(n.p,{children:"Given its containerized nature, BioNeMo finds versatility in deployment across various environments such as Amazon Sagemaker, AWS ParallelCluster, Amazon ECS, and Amazon EKS. This solution, however, zeroes in on the specific deployment of BioNeMo on Amazon EKS."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Source: ",(0,r.jsx)(n.a,{href:"https://blogs.nvidia.com/blog/bionemo-on-aws-generative-ai-drug-discovery/",children:"https://blogs.nvidia.com/blog/bionemo-on-aws-generative-ai-drug-discovery/"})]})}),"\n",(0,r.jsx)(n.h2,{id:"deploying-bionemo-on-kubernetes",children:"Deploying BioNeMo on Kubernetes"}),"\n",(0,r.jsx)(n.p,{children:"This blueprint leverages three major components for its functionality. The NVIDIA Device Plugin facilitates GPU usage, FSx stores training data, and the Kubeflow Training Operator manages the actual training process."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.kubeflow.org/docs/components/training/",children:(0,r.jsx)(n.strong,{children:"Kubeflow Training Operator"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/NVIDIA/k8s-device-plugin",children:(0,r.jsx)(n.strong,{children:"NVIDIA Device Plugin"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html",children:(0,r.jsx)(n.strong,{children:"FSx for Lustre CSI Driver"})})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In this blueprint, we will deploy an Amazon EKS cluster and execute both a data preparation job and a distributed model training job."}),"\n",(0,r.jsxs)(o.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Pre-requisites"})}),children:[(0,r.jsx)(n.p,{children:"Ensure that you have installed the following tools on your local machine or the machine you are using to deploy the Terraform blueprint, such as Mac, Windows, or Cloud9 IDE:"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]})]}),"\n",(0,r.jsxs)(o.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Deploy the blueprint"})}),children:[(0,r.jsx)(n.h4,{id:"clone-the-repository",children:"Clone the repository"}),(0,r.jsx)(n.p,{children:"First, clone the repository containing the necessary files for deploying the blueprint. Use the following command in your terminal:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,r.jsx)(n.h4,{id:"initialize-terraform",children:"Initialize Terraform"}),(0,r.jsx)(n.p,{children:"Navigate into the directory specific to the blueprint you want to deploy. In this case, we're interested in the BioNeMo blueprint, so navigate to the appropriate directory using the terminal:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/bionemo\n"})}),(0,r.jsx)(n.h4,{id:"run-the-install-script",children:"Run the install script"}),(0,r.jsxs)(n.p,{children:["Use the provided helper script ",(0,r.jsx)(n.code,{children:"install.sh"})," to run the terraform init and apply commands. By default the script deploys EKS cluster to ",(0,r.jsx)(n.code,{children:"us-west-2"})," region. Update ",(0,r.jsx)(n.code,{children:"variables.tf"})," to change the region. This is also the time to update any other input variables or make any other changes to the terraform template."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./install .sh\n"})}),(0,r.jsx)(n.p,{children:"Update local kubeconfig so we can access kubernetes cluster"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks update-kubeconfig --name bionemo-on-eks #or whatever you used for EKS cluster name\n"})}),(0,r.jsx)(n.p,{children:"Since there is no helm chart for Training Operator, we have to manually install the package. If a helm chart gets build by training-operator team, we\nwill incorporate it to the terraform-aws-eks-data-addons repository."}),(0,r.jsx)(n.h4,{id:"install-kubeflow-training-operator",children:"Install Kubeflow Training Operator"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl apply -k "github.com/kubeflow/training-operator/manifests/overlays/standalone?ref=v1.7.0"\n'})})]}),"\n",(0,r.jsxs)(o.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Verify Deployment"})}),children:[(0,r.jsx)(n.p,{children:"First, lets verify that we have worker nodes running in the cluster."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                                           STATUS   ROLES    AGE   VERSION\nip-100-64-180-114.us-west-2.compute.internal   Ready    <none>   17m   v1.29.0-eks-5e0fdde\nip-100-64-19-70.us-west-2.compute.internal     Ready    <none>   16m   v1.29.0-eks-5e0fdde\nip-100-64-205-93.us-west-2.compute.internal    Ready    <none>   17m   v1.29.0-eks-5e0fdde\nip-100-64-235-15.us-west-2.compute.internal    Ready    <none>   16m   v1.29.0-eks-5e0fdde\nip-100-64-34-75.us-west-2.compute.internal     Ready    <none>   17m   v1.29.0-eks-5e0fdde\n...\n"})}),(0,r.jsx)(n.p,{children:"Next, lets verify all the pods are running."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -A\n"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAMESPACE              NAME                                                              READY   STATUS    RESTARTS   AGE\namazon-cloudwatch      aws-cloudwatch-metrics-4g9dm                                      1/1     Running   0          15m\namazon-cloudwatch      aws-cloudwatch-metrics-4ktjc                                      1/1     Running   0          15m\namazon-cloudwatch      aws-cloudwatch-metrics-5hj96                                      1/1     Running   0          15m\namazon-cloudwatch      aws-cloudwatch-metrics-k84p5                                      1/1     Running   0          15m\namazon-cloudwatch      aws-cloudwatch-metrics-rkt8f                                      1/1     Running   0          15m\nkube-system            aws-node-4pnpr                                                    2/2     Running   0          15m\nkube-system            aws-node-jrksf                                                    2/2     Running   0          15m\nkube-system            aws-node-lv7vn                                                    2/2     Running   0          15m\nkube-system            aws-node-q7cp9                                                    2/2     Running   0          14m\nkube-system            aws-node-zplq5                                                    2/2     Running   0          14m\nkube-system            coredns-86bd649884-8kwn9                                          1/1     Running   0          15m\nkube-system            coredns-86bd649884-bvltg                                          1/1     Running   0          15m\nkube-system            fsx-csi-controller-85d9ddfbff-7hgmn                               4/4     Running   0          16m\nkube-system            fsx-csi-controller-85d9ddfbff-lp28p                               4/4     Running   0          16m\nkube-system            fsx-csi-node-2tfgq                                                3/3     Running   0          16m\nkube-system            fsx-csi-node-jtdd6                                                3/3     Running   0          16m\nkube-system            fsx-csi-node-kj6tz                                                3/3     Running   0          16m\nkube-system            fsx-csi-node-pwp5x                                                3/3     Running   0          16m\nkube-system            fsx-csi-node-rl59r                                                3/3     Running   0          16m\nkube-system            kube-proxy-5nbms                                                  1/1     Running   0          15m\nkube-system            kube-proxy-dzjxz                                                  1/1     Running   0          15m\nkube-system            kube-proxy-j9bnp                                                  1/1     Running   0          15m\nkube-system            kube-proxy-p8xwq                                                  1/1     Running   0          15m\nkube-system            kube-proxy-pgqbb                                                  1/1     Running   0          15m\nkubeflow               training-operator-64c768746c-l5fbq                                1/1     Running   0          24s\nnvidia-device-plugin   neuron-device-plugin-gpu-feature-discovery-g4xx9                  1/1     Running   0          15m\nnvidia-device-plugin   neuron-device-plugin-gpu-feature-discovery-ggwjm                  1/1     Running   0          15m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-master-68bc46c9dbw8   1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-worker-6b94s          1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-worker-7jzsn          1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-worker-kt9fd          1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-worker-vlpdp          1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-node-feature-discovery-worker-wwnk6          1/1     Running   0          16m\nnvidia-device-plugin   neuron-device-plugin-nvidia-device-plugin-mslxx                   1/1     Running   0          15m\nnvidia-device-plugin   neuron-device-plugin-nvidia-device-plugin-phw2j                   1/1     Running   0          15m\n...\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Make sure training-operator, nvidia-device-plugin and fsx-csi-controller pods are running and healthy."})})]}),"\n",(0,r.jsx)(n.h3,{id:"run-bionemo-training-jobs",children:"Run BioNeMo Training jobs"}),"\n",(0,r.jsx)(n.p,{children:"Once you've ensured that all components are functioning properly, you can proceed to submit jobs to your clusters."}),"\n",(0,r.jsx)(n.h4,{id:"step1-initiate-the-uniref50-data-preparation-task",children:"Step1: Initiate the Uniref50 Data Preparation Task"}),"\n",(0,r.jsxs)(n.p,{children:["The first task, named the ",(0,r.jsx)(n.code,{children:"uniref50-job.yaml"}),", involves downloading and partitioning the data to enhance processing efficiency. This task specifically retrieves the ",(0,r.jsx)(n.code,{children:"uniref50 dataset"})," and organizes it within the FSx for Lustre Filesystem. This structured layout is designed for training, testing, and validation purposes. You can learn more about the uniref dataset ",(0,r.jsx)(n.a,{href:"https://www.uniprot.org/help/uniref",children:"here"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["To execute this job, navigate to the ",(0,r.jsx)(n.code,{children:"examples\\training"})," directory and deploy the ",(0,r.jsx)(n.code,{children:"uniref50-job.yaml"})," manifest using the following commands:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd examples/training\nkubectl apply -f uniref50-job.yaml\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"It's important to note that this task requires a significant amount of time, typically ranging from 50 to 60 hours."})}),"\n",(0,r.jsxs)(n.p,{children:["Run the below command to look for the pod ",(0,r.jsx)(n.code,{children:"uniref50-download-*"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,r.jsx)(n.p,{children:"To verify its progress, examine the logs generated by the corresponding pod:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs uniref50-download-xnz42\n\n[NeMo I 2024-02-26 23:02:20 preprocess:289] Download and preprocess of UniRef50 data does not currently use GPU. Workstation or CPU-only instance recommended.\n[NeMo I 2024-02-26 23:02:20 preprocess:115] Data processing can take an hour or more depending on system resources.\n[NeMo I 2024-02-26 23:02:20 preprocess:117] Downloading file from https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz...\n[NeMo I 2024-02-26 23:02:20 preprocess:75] Downloading file to /fsx/raw/uniref50.fasta.gz...\n[NeMo I 2024-02-26 23:08:33 preprocess:89] Extracting file to /fsx/raw/uniref50.fasta...\n[NeMo I 2024-02-26 23:12:46 preprocess:311] UniRef50 data processing complete.\n[NeMo I 2024-02-26 23:12:46 preprocess:313] Indexing UniRef50 dataset.\n[NeMo I 2024-02-26 23:16:21 preprocess:319] Writing processed dataset files to /fsx/processed...\n[NeMo I 2024-02-26 23:16:21 preprocess:255] Creating train split...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["After finishing this task, the processed dataset will be saved in the ",(0,r.jsx)(n.code,{children:"/fsx/processed"})," directory. Once this is done, we can move forward and start the ",(0,r.jsx)(n.code,{children:"pre-training"})," job by running the following command:"]}),"\n",(0,r.jsx)(n.p,{children:"Following this, we can proceed to execute the pre-training job by running:"}),"\n",(0,r.jsxs)(n.p,{children:["In this PyTorchJob YAML, the command ",(0,r.jsx)(n.code,{children:"python3 -m torch.distributed.run"})," plays a crucial role in orchestrating ",(0,r.jsx)(n.strong,{children:"distributed training"})," across multiple worker pods in your Kubernetes cluster."]}),"\n",(0,r.jsx)(n.p,{children:"It handles the following tasks:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Initializes a distributed backend (e.g., c10d, NCCL) for communication between worker processes.In our example it's using c10d. This is a commonly used distributed backend in PyTorch that can leverage different communication mechanisms like TCP or Infiniband depending on your environment."}),"\n",(0,r.jsx)(n.li,{children:"Sets up environment variables to enable distributed training within your training script."}),"\n",(0,r.jsx)(n.li,{children:"Launches your training script on all worker pods, ensuring each process participates in the distributed training."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd examples/training\nkubectl apply -f esm1nv_pretrain-job.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Run the below command to look for the pods ",(0,r.jsx)(n.code,{children:"esm1nv-pretraining-worker-*"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                           READY   STATUS    RESTARTS   AGE\nesm1nv-pretraining-worker-0    1/1     Running   0          13m\nesm1nv-pretraining-worker-1    1/1     Running   0          13m\nesm1nv-pretraining-worker-10   1/1     Running   0          13m\nesm1nv-pretraining-worker-11   1/1     Running   0          13m\nesm1nv-pretraining-worker-12   1/1     Running   0          13m\nesm1nv-pretraining-worker-13   1/1     Running   0          13m\nesm1nv-pretraining-worker-14   1/1     Running   0          13m\nesm1nv-pretraining-worker-15   1/1     Running   0          13m\nesm1nv-pretraining-worker-2    1/1     Running   0          13m\nesm1nv-pretraining-worker-3    1/1     Running   0          13m\nesm1nv-pretraining-worker-4    1/1     Running   0          13m\nesm1nv-pretraining-worker-5    1/1     Running   0          13m\nesm1nv-pretraining-worker-6    1/1     Running   0          13m\nesm1nv-pretraining-worker-7    1/1     Running   0          13m\nesm1nv-pretraining-worker-8    1/1     Running   0          13m\nesm1nv-pretraining-worker-9    1/1     Running   0          13m\n"})}),"\n",(0,r.jsxs)(n.p,{children:['We should see 16 pods running. We chose p3.16xlarge instances and each instance has 8 GPUs. In the pod definition we specified each job will leverage 1 gpu.\nSince we set up "nprocPerNode" to "8", each node will be responsible for 8 jobs. Since we have 2 nodes, total of 16 pods will start. For more details around distributed pytorch training see ',(0,r.jsx)(n.a,{href:"https://pytorch.org/docs/stable/distributed.html",children:"pytorch docs"}),"."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This training job can run for at least 3-4 days with 2 p3.16xlarge nodes."})}),"\n",(0,r.jsxs)(n.p,{children:["This configuration utilizes Kubeflow's PyTorch training Custom Resource Definition (CRD). Within this manifest, various parameters are available for customization. For detailed insights into each parameter and guidance on fine-tuning, you can refer to ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/bionemo-framework/latest/notebooks/model_training_esm1nv.html",children:"BioNeMo's documentation"}),"."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Based on the Kubeflow training operator documentation, if you do not specify the master replica pod explicitly, the first worker replica pod(worker-0) will be treated as the master pod."})}),"\n",(0,r.jsx)(n.p,{children:"To track the progress of this process, follow these steps:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs esm1nv-pretraining-worker-0\n\nEpoch 0:   7%|\u258b         | 73017/1017679 [00:38<08:12, 1918.0%\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Additionally, to monitor the usage of the GPUs, you have the option to connect to your nodes through the EC2 console using Session Manager and run ",(0,r.jsx)(n.code,{children:"nvidia-smi"})," command. If you want to have a more robust observability, you can refer to the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/latest/dcgm-exporter.html",children:"DCGM Exporter"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sh-4.2$ nvidia-smi\nThu Mar  7 16:31:01 2024\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:17.0 Off |                    0 |\n| N/A   51C    P0              80W / 300W |   3087MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla V100-SXM2-16GB           On  | 00000000:00:18.0 Off |                    0 |\n| N/A   44C    P0              76W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   2  Tesla V100-SXM2-16GB           On  | 00000000:00:19.0 Off |                    0 |\n| N/A   43C    P0              77W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   3  Tesla V100-SXM2-16GB           On  | 00000000:00:1A.0 Off |                    0 |\n| N/A   52C    P0              77W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   4  Tesla V100-SXM2-16GB           On  | 00000000:00:1B.0 Off |                    0 |\n| N/A   49C    P0              79W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   5  Tesla V100-SXM2-16GB           On  | 00000000:00:1C.0 Off |                    0 |\n| N/A   44C    P0              74W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   6  Tesla V100-SXM2-16GB           On  | 00000000:00:1D.0 Off |                    0 |\n| N/A   44C    P0              78W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   7  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n| N/A   50C    P0              79W / 300W |   3085MiB / 16384MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A   1552275      C   /usr/bin/python3                           3084MiB |\n|    1   N/A  N/A   1552277      C   /usr/bin/python3                           3082MiB |\n|    2   N/A  N/A   1552278      C   /usr/bin/python3                           3082MiB |\n|    3   N/A  N/A   1552280      C   /usr/bin/python3                           3082MiB |\n|    4   N/A  N/A   1552279      C   /usr/bin/python3                           3082MiB |\n|    5   N/A  N/A   1552274      C   /usr/bin/python3                           3082MiB |\n|    6   N/A  N/A   1552273      C   /usr/bin/python3                           3082MiB |\n|    7   N/A  N/A   1552276      C   /usr/bin/python3                           3082MiB |\n+---------------------------------------------------------------------------------------+\n"})}),"\n",(0,r.jsx)(n.h4,{id:"benefits-of-distributed-training",children:"Benefits of Distributed Training:"}),"\n",(0,r.jsx)(n.p,{children:"By distributing the training workload across multiple GPUs in your worker pods, you can train large models faster by leveraging the combined computational power of all GPUs. Handle larger datasets that might not fit on a single GPU's memory."}),"\n",(0,r.jsx)(n.h4,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsxs)(n.p,{children:["BioNeMo stands as a formidable generative AI tool tailored for the realm of drug discovery. In this illustrative example, we took the initiative to pretrain a custom model entirely from scratch, utilizing the extensive uniref50 dataset. However, it's worth noting that BioNeMo offers the flexibility to expedite the process by employing pretrained models directly ",(0,r.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/containers/bionemo-framework",children:"provided by NVidia"}),". This alternative approach can significantly streamline your workflow while maintaining the robust capabilities of the BioNeMo framework."]}),"\n",(0,r.jsxs)(o.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Cleanup"})}),children:[(0,r.jsxs)(n.p,{children:["Use the provided helper script ",(0,r.jsx)(n.code,{children:"cleanup.sh"})," to tear down EKS cluster and other AWS resources."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ../../\n./cleanup.sh\n"})})]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},2450:(e,n,i)=>{i.d(n,{A:()=>m});var r=i(6540),s=i(5556),o=i.n(s),t=i(4164);const a="collapsibleContent_q3kw",l="header_QCEw",d="icon_PckA",c="content_qLC1",h="expanded_iGsi";var u=i(4848);function p(e){let{children:n,header:i}=e;const[s,o]=(0,r.useState)(!1);return(0,u.jsxs)("div",{className:a,children:[(0,u.jsxs)("div",{className:(0,t.A)(l,{[h]:s}),onClick:()=>{o(!s)},children:[i,(0,u.jsx)("span",{className:(0,t.A)(d,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,u.jsx)("div",{className:c,children:n})]})}p.propTypes={children:o().node.isRequired,header:o().node.isRequired};const m=p},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);