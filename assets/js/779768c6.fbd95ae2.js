"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9913],{5162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(7294),r=a(6010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:a},t)}},4866:(e,t,a)=>{a.d(t,{Z:()=>w});var n=a(7462),r=a(7294),o=a(6010),l=a(2466),s=a(6550),i=a(1980),p=a(7392),d=a(12);function u(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}function c(e){const{values:t,children:a}=e;return(0,r.useMemo)((()=>{const e=t??u(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function m(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function k(e){let{queryString:t=!1,groupId:a}=e;const n=(0,s.k6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(n.location.search);t.set(o,e),n.replace({...n.location,search:t.toString()})}),[o,n])]}function f(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,o=c(e),[l,s]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[i,p]=k({queryString:a,groupId:n}),[u,f]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,o]=(0,d.Nk)(a);return[n,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:n}),h=(()=>{const e=i??u;return m({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{h&&s(h)}),[h]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var h=a(2389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:a,selectedValue:s,selectValue:i,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:u}=(0,l.o5)(),c=e=>{const t=e.currentTarget,a=d.indexOf(t),n=p[a].value;n!==s&&(u(t),i(n))},m=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=d.indexOf(e.currentTarget)+1;t=d[a]??d[0];break}case"ArrowLeft":{const a=d.indexOf(e.currentTarget)-1;t=d[a]??d[d.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>d.push(e),onKeyDown:m,onClick:c},l,{className:(0,o.Z)("tabs__item",g.tabItem,l?.className,{"tabs__item--active":s===t})}),a??t)})))}function N(e){let{lazy:t,children:a,selectedValue:n}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function y(e){const t=f(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",g.tabList)},r.createElement(b,(0,n.Z)({},e,t)),r.createElement(N,(0,n.Z)({},e,t)))}function w(e){const t=(0,h.Z)();return r.createElement(y,(0,n.Z)({key:String(t)},e))}},769:(e,t,a)=>{a.d(t,{Z:()=>m});var n=a(7294),r=a(5697),o=a.n(r),l=a(6010);const s="collapsibleContent_q3kw",i="header_QCEw",p="icon_PckA",d="content_qLC1",u="expanded_iGsi";function c(e){let{children:t,header:a}=e;const[r,o]=(0,n.useState)(!1);return n.createElement("div",{className:s},n.createElement("div",{className:(0,l.Z)(i,{[u]:r}),onClick:()=>{o(!r)}},a,n.createElement("span",{className:(0,l.Z)(p,{[u]:r})})),r&&n.createElement("div",{className:d},t))}c.propTypes={children:o().node.isRequired,header:o().node.isRequired};const m=c},8184:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>p,default:()=>f,frontMatter:()=>i,metadata:()=>d,toc:()=>c});var n=a(7462),r=(a(7294),a(3905)),o=(a(4866),a(5162),a(769)),l=a(814),s=a(7721);const i={sidebar_position:3,sidebar_label:"Airflow on EKS"},p="Self-managed Apache Airflow deployment on Amazon EKS",d={unversionedId:"blueprints/job-schedulers/self-managed-airflow",id:"blueprints/job-schedulers/self-managed-airflow",title:"Self-managed Apache Airflow deployment on Amazon EKS",description:"Introduction",source:"@site/docs/blueprints/job-schedulers/self-managed-airflow.md",sourceDirName:"blueprints/job-schedulers",slug:"/blueprints/job-schedulers/self-managed-airflow",permalink:"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/job-schedulers/self-managed-airflow.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"Airflow on EKS"},sidebar:"blueprints",previous:{title:"Amazon MWAA",permalink:"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow"},next:{title:"Argo Workflows on EKS",permalink:"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks"}},u={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Verify the resources",id:"verify-the-resources",level:2},{value:"Create kubectl config",id:"create-kubectl-config",level:3},{value:"Describe the EKS Cluster",id:"describe-the-eks-cluster",level:3},{value:"Verify the EFS PV and PVC created by this deployment",id:"verify-the-efs-pv-and-pvc-created-by-this-deployment",level:3},{value:"Verify the EFS Filesystem",id:"verify-the-efs-filesystem",level:3},{value:"Verify S3 bucket created for Airflow logs",id:"verify-s3-bucket-created-for-airflow-logs",level:3},{value:"Verify the Airflow deployment",id:"verify-the-airflow-deployment",level:3},{value:"Fetch Postgres RDS password",id:"fetch-postgres-rds-password",level:3},{value:"Login to Airflow Web UI",id:"login-to-airflow-web-ui",level:3},{value:"Execute Sample Airflow Job",id:"execute-sample-airflow-job",level:3},{value:"Create Kubernetes Default Connection from Airflow Web UI",id:"create-kubernetes-default-connection-from-airflow-web-ui",level:3},{value:"Execute Sample Spark Job DAG",id:"execute-sample-spark-job-dag",level:3}],m={toc:c},k="wrapper";function f(e){let{components:t,...i}=e;return(0,r.kt)(k,(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"self-managed-apache-airflow-deployment-on-amazon-eks"},"Self-managed Apache Airflow deployment on Amazon EKS"),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"This pattern deploys self-managed ",(0,r.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/"},"Apache Airflow")," deployment on EKS. This blueprint deploys Airflow on Amazon EKS managed node groups and leverages Karpenter to run the workloads."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Architecture")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"airflow-eks-architecture",src:a(4637).Z,width:"720",height:"405"})),(0,r.kt)("p",null,"This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults and only customize if you have viable alternative option available for replacement."),(0,r.kt)("p",null,"In terms of infrastructure, below are the resources that are created by this pattern:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"One managed node group"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Core Node group with 3 instances spanning multi-AZs for running Apache Airflow and other system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Apache Airflow core components (with airflow-core.tf):"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Amazon RDS PostgreSQL instance and security group for Airflow meta database."),(0,r.kt)("li",{parentName:"ul"},"Airflow namespace"),(0,r.kt)("li",{parentName:"ul"},"Kubernetes service accounts and AWS IAM roles for service account (IRSA) for Airflow Webserver, Airflow Scheduler, and Airflow Worker."),(0,r.kt)("li",{parentName:"ul"},"Amazon Elastic File System (EFS), EFS mounts, Kubernetes Storage Class for EFS, and Kubernetes Persistent Volume Claim for mounting Airflow DAGs for Airflow pods."),(0,r.kt)("li",{parentName:"ul"},"Amazon S3 log bucket for Airflow logs")))),(0,r.kt)("p",null,"AWS for FluentBit is employed for logging, and a combination of Prometheus, Amazon Managed Prometheus, and open source Grafana are used for observability. You can see the complete list of add-ons available below."),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as ",(0,r.kt)("inlineCode",{parentName:"p"},"core-node-group")," as provided by this pattern.")),(0,r.kt)("admonition",{type:"danger"},(0,r.kt)("p",{parentName:"admonition"},"We don't recommend removing critical add-ons (",(0,r.kt)("inlineCode",{parentName:"p"},"Amazon VPC CNI"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"CoreDNS"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"Kube-proxy"),").")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Add-on"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Enabled by default?"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Benefits"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Link"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Amazon VPC CNI"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"VPC CNI is available as an ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html"},"EKS add-on")," and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html"},"VPC CNI Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"CoreDNS"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"CoreDNS is available as an ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html"},"EKS add-on")," and is responsible for resolving DNS queries for spark application and for Kubernetes cluster"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html"},"EKS CoreDNS Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Kube-proxy"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Kube-proxy is available as an ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html"},"EKS add-on")," and it maintains network rules on your nodes and enables network communication to your spark application pods"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html"},"EKS kube-proxy Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Amazon EBS CSI driver"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"EBS CSI driver is available as an ",(0,r.kt)("a",{parentName:"td",href:"hhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html"},"EKS add-on")," and it allows EKS clusters to manage the lifecycle of EBS volumes"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html"},"EBS CSI Driver Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Amazon EFS CSI driver"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"The Amazon EFS Container Storage Interface (CSI) driver provides a CSI interface that allows Kubernetes clusters running on AWS to manage the lifecycle of Amazon EFS file systems."),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html"},"EFS CSI Driver Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Karpenter"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Karpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://karpenter.sh/"},"Karpenter Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Cluster Autoscaler"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Kubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as ",(0,r.kt)("inlineCode",{parentName:"td"},"core-node-group"),") in the cluster"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md"},"Cluster Autoscaler Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Cluster proportional autoscaler"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"This is responsible for scaling CoreDNS pods in your Kubernetes cluster"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/kubernetes-sigs/cluster-proportional-autoscaler"},"Cluster Proportional Autoscaler Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Metrics server"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Kubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html"},"EKS Metrics Server Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Prometheus"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Prometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://prometheus.io/docs/introduction/overview/"},"Prometheus Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Amazon Managed Prometheus"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"This is responsible for storing and scaling of EKS cluster and spark application metrics"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html"},"Amazon Managed Prometheus Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"Kubecost"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Kubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/cost-monitoring.html"},"EKS Kubecost Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"CloudWatch metrics"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"CloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-metrics-EKS.html"},"CloudWatch Container Insights Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"AWS for Fluent-bit"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"This can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/aws/aws-for-fluent-bit"},"AWS For Fluent-bit Documentation"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"AWS Load Balancer Controller"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Yes"),(0,r.kt)("td",{parentName:"tr",align:"left"},"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster."),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html"},"AWS Load Balancer Controller Documentation"))))),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"Ensure that you have installed the following tools on your machine."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"},"aws cli")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://Kubernetes.io/docs/tasks/tools/"},"kubectl")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://learn.hashicorp.com/tutorials/terraform/install-cli"},"terraform"))),(0,r.kt)("h2",{id:"deploying-the-solution"},"Deploying the Solution"),(0,r.kt)("p",null,"Clone the repository"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/awslabs/data-on-eks.git\n")),(0,r.kt)("p",null,"Navigate into self-managed-airflow directory and run install.sh script"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/schedulers/terraform/self-managed-airflow\nchmod +x install.sh\n./install.sh\n")),(0,r.kt)("h2",{id:"verify-the-resources"},"Verify the resources"),(0,r.kt)("h3",{id:"create-kubectl-config"},"Create kubectl config"),(0,r.kt)("p",null,"Update the placeholder for AWS region and run the below command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"mv ~/.kube/config ~/.kube/config.bk\naws eks update-kubeconfig --region <region>  --name self-managed-airflow\n")),(0,r.kt)("h3",{id:"describe-the-eks-cluster"},"Describe the EKS Cluster"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-cluster --name self-managed-airflow\n")),(0,r.kt)("h3",{id:"verify-the-efs-pv-and-pvc-created-by-this-deployment"},"Verify the EFS PV and PVC created by this deployment"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pvc -n airflow\n\nNAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nairflow-dags   Bound    pvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            efs-sc         73m\n\nkubectl get pv -n airflow\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE\npvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            Delete           Bound    airflow/airflow-dags           efs-sc                  74m\n\n")),(0,r.kt)("h3",{id:"verify-the-efs-filesystem"},"Verify the EFS Filesystem"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text\n')),(0,r.kt)("h3",{id:"verify-s3-bucket-created-for-airflow-logs"},"Verify S3 bucket created for Airflow logs"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bashell"},"aws s3 ls | grep airflow-logs-\n")),(0,r.kt)("h3",{id:"verify-the-airflow-deployment"},"Verify the Airflow deployment"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bashell"},"kubectl get deployment -n airflow\n\nNAME                READY   UP-TO-DATE   AVAILABLE   AGE\nairflow-pgbouncer   1/1     1            1           77m\nairflow-scheduler   2/2     2            2           77m\nairflow-statsd      1/1     1            1           77m\nairflow-triggerer   1/1     1            1           77m\nairflow-webserver   2/2     2            2           77m\n\n")),(0,r.kt)("h3",{id:"fetch-postgres-rds-password"},"Fetch Postgres RDS password"),(0,r.kt)("p",null,"Amazon Postgres RDS database password can be fetched from the Secrets manager"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Login to AWS console and open secrets manager"),(0,r.kt)("li",{parentName:"ul"},"Click on ",(0,r.kt)("inlineCode",{parentName:"li"},"postgres")," secret name"),(0,r.kt)("li",{parentName:"ul"},"Click on Retrieve secret value button to verify the Postgres DB master password")),(0,r.kt)("h3",{id:"login-to-airflow-web-ui"},"Login to Airflow Web UI"),(0,r.kt)("p",null,"This deployment creates an Ingress object with public LoadBalancer(internet-facing) for demo purpose\nFor production workloads, you can modify ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow-values.yaml")," to choose ",(0,r.kt)("inlineCode",{parentName:"p"},"internal")," LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port."),(0,r.kt)("p",null,"Execute the following command to get the ALB DNS name"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get ingress -n airflow\n\nNAME                      CLASS   HOSTS   ADDRESS                                                                PORTS   AGE\nairflow-airflow-ingress   alb     *       k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com   80      88m\n\n")),(0,r.kt)("p",null,"The above ALB URL will be different for you deployment. So use your URL and open it in a brower"),(0,r.kt)("p",null,"e.g., Open URL ",(0,r.kt)("inlineCode",{parentName:"p"},"http://k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com/")," in a browser"),(0,r.kt)("p",null,"By default, Airflow creates a default user with ",(0,r.kt)("inlineCode",{parentName:"p"},"admin")," and password as ",(0,r.kt)("inlineCode",{parentName:"p"},"admin")),(0,r.kt)("p",null,"Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user"),(0,r.kt)("h3",{id:"execute-sample-airflow-job"},"Execute Sample Airflow Job"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Login to Airflow WebUI"),(0,r.kt)("li",{parentName:"ul"},"Click on ",(0,r.kt)("inlineCode",{parentName:"li"},"DAGs")," link on the top of the page. This will show dags pre-created by the GitSync feature"),(0,r.kt)("li",{parentName:"ul"},"Execute the hello_world_scheduled_dag DAG by clicking on Play button (",(0,r.kt)("inlineCode",{parentName:"li"},">"),")"),(0,r.kt)("li",{parentName:"ul"},"Verify the DAG execution from ",(0,r.kt)("inlineCode",{parentName:"li"},"Graph")," link"),(0,r.kt)("li",{parentName:"ul"},"All the Tasks will go green after few minutes"),(0,r.kt)("li",{parentName:"ul"},"Click on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3")),(0,r.kt)(o.Z,{header:(0,r.kt)("h2",null,(0,r.kt)("span",null,"Airflow to run Spark workloads with Karpenter")),mdxType:"CollapsibleContent"},(0,r.kt)("p",null,(0,r.kt)("img",{alt:"img.png",src:a(914).Z,width:"4157",height:"2705"})),(0,r.kt)("p",null,"This option presents leverages Karpenter as the autoscaler, eliminating the need for Managed Node Groups and Cluster Autoscaler. In this design, Karpenter and its provisioner are responsible for creating both On-Demand and Spot instances, dynamically selecting instance types based on user demands. Karpenter offers improved performance compared to Cluster Autoscaler, with more efficient node scaling and faster response times. Karpenter's key features include its ability to scale from zero, optimizing resource utilization and reducing costs when there is no demand for resources. Additionally, Karpenter supports multiple provisioners, allowing for greater flexibility in defining the required infrastructure for different workload types, such as compute, memory, and GPU-intensive tasks. Furthermore, Karpenter integrates seamlessly with Kubernetes, providing automatic, real-time adjustments to the cluster size based on observed workloads and scaling events. This enables a more efficient and cost-effective EKS cluster design that adapts to the ever-changing demands of Spark applications and other workloads."),(0,r.kt)("p",null,"In this tutorial, you will use Karpenter provisioner that uses memory optimized instances. This template uses the AWS Node template with Userdata."),(0,r.kt)("details",null,(0,r.kt)("summary",null," To view Karpenter provisioner for memory optimized instances, Click to toggle content!"),(0,r.kt)(l.Z,{language:"yaml",mdxType:"CodeBlock"},s.Z)),(0,r.kt)("p",null,"To run Spark Jobs that can use this provisioner, you need to submit your jobs by adding ",(0,r.kt)("inlineCode",{parentName:"p"},"tolerations")," to your Spark Application manifest. Additionally, to ensure Spark Driver pods run only on ",(0,r.kt)("inlineCode",{parentName:"p"},"On-Demand")," nodes and Spark Executors run only on ",(0,r.kt)("inlineCode",{parentName:"p"},"Spot")," nodes, add the ",(0,r.kt)("inlineCode",{parentName:"p"},"karpenter.sh/capacity-type")," node selectors."),(0,r.kt)("p",null,"For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'    # Using Karpenter provisioner nodeSelectors and tolerations\n    nodeSelector:\n      NodeGroupType: "SparkComputeOptimized"\n      karpenter.sh/capacity-type: "on-demand"\n    tolerations:\n      - key: "spark-compute-optimized"\n        operator: "Exists"\n        effect: "NoSchedule"\n')),(0,r.kt)("h3",{id:"create-kubernetes-default-connection-from-airflow-web-ui"},"Create Kubernetes Default Connection from Airflow Web UI"),(0,r.kt)("p",null,"This step is critical for writing the Airflow to connect to EKS cluster."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Login to Airflow WebUI with ",(0,r.kt)("inlineCode",{parentName:"li"},"admin")," and password as ",(0,r.kt)("inlineCode",{parentName:"li"},"admin")," using ALB URL"),(0,r.kt)("li",{parentName:"ul"},"Select ",(0,r.kt)("inlineCode",{parentName:"li"},"Admin")," dropdown and Click on ",(0,r.kt)("inlineCode",{parentName:"li"},"Connections")),(0,r.kt)("li",{parentName:"ul"},'Click on "+" button to add a new record'),(0,r.kt)("li",{parentName:"ul"},"Enter Connection Id as ",(0,r.kt)("inlineCode",{parentName:"li"},"kubernetes_default"),", Connection Type as ",(0,r.kt)("inlineCode",{parentName:"li"},"Kubernetes Cluster Connection")," and tick the checkbox ",(0,r.kt)("strong",{parentName:"li"},"In cluster configuration")),(0,r.kt)("li",{parentName:"ul"},"Click on Save button")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Airflow AWS Connection",src:a(166).Z,width:"1848",height:"1408"})),(0,r.kt)("p",null,"Navigate to airflow directory and change the ",(0,r.kt)("strong",{parentName:"p"},"enable_airflow_spark_example")," variable to ",(0,r.kt)("inlineCode",{parentName:"p"},"true")," in ",(0,r.kt)("strong",{parentName:"p"},"variable.tf"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd schedulers/terraform/self-managed-airflow\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'variable "enable_airflow_spark_example" {\n  description = "Enable Apache Airflow and Spark Operator example"\n  type        = bool\n  default     = true\n}\n')),(0,r.kt)("p",null,"Change the ",(0,r.kt)("strong",{parentName:"p"},"enable_spark_operator")," variable to ",(0,r.kt)("inlineCode",{parentName:"p"},"true")," in ",(0,r.kt)("strong",{parentName:"p"},"addons.tf"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'variable "enable_airflow_spark_example" {\n  description = "Enable Apache Airflow and Spark Operator example"\n  type        = bool\n  default     = true\n}\n')),(0,r.kt)("p",null,"Run install.sh script"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/schedulers/terraform/self-managed-airflow\nchmod +x install.sh\n./install.sh\n")),(0,r.kt)("h3",{id:"execute-sample-spark-job-dag"},"Execute Sample Spark Job DAG"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Login to Airflow WebUI"),(0,r.kt)("li",{parentName:"ul"},"Click on ",(0,r.kt)("inlineCode",{parentName:"li"},"DAGs")," link on the top of the page. This will show dags pre-created by the GitSync feature"),(0,r.kt)("li",{parentName:"ul"},"Execute the ",(0,r.kt)("inlineCode",{parentName:"li"},"example_pyspark_pi_job")," DAG by clicking on Play button (",(0,r.kt)("inlineCode",{parentName:"li"},">"),")")),(0,r.kt)("details",null,(0,r.kt)("summary",null," To view example DAG, click to toggle content!"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'"""\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime, timedelta\n\n# [START import_module]\n# The DAG object; we\'ll need this to instantiate a DAG\nfrom airflow import DAG\n\n# Operators; we need this to operate!\nfrom airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator\nfrom airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor\n\n# [END import_module]\n\n# [START instantiate_dag]\n\nDAG_ID = "example_pyspark_pi_job"\n\nwith DAG(\n    DAG_ID,\n    default_args={"max_active_runs": 1},\n    description="submit spark-pi as sparkApplication on kubernetes",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n) as dag:\n    # [START SparkKubernetesOperator_DAG]\n    t1 = SparkKubernetesOperator(\n        task_id="pyspark_pi_submit",\n        namespace="spark-team-a",\n        application_file="example_pyspark_pi_job.yaml",\n        do_xcom_push=True,\n        dag=dag,\n    )\n\n    t2 = SparkKubernetesSensor(\n        task_id="pyspark_pi_monitor",\n        namespace="spark-team-a",\n        application_name="{{ task_instance.xcom_pull(task_ids=\'pyspark_pi_submit\')[\'metadata\'][\'name\'] }}",\n        dag=dag,\n    )\n    t1 >> t2\n'))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"DAG has two tasks, the first task is ",(0,r.kt)("inlineCode",{parentName:"li"},"SparkKubernetesOperator")," that uses sample SparkApplication ",(0,r.kt)("inlineCode",{parentName:"li"},"example_pyspark_pi_job.yaml")," file. The SparkKubernetesOperator task creates Airflow worker pods in ",(0,r.kt)("inlineCode",{parentName:"li"},"airflow")," namespace. Airflow worker triggers pre-installed Kubernetes Spark Operator in ",(0,r.kt)("inlineCode",{parentName:"li"},"spark-operator")," namespace. Kubernetes Spark Operator then uses ",(0,r.kt)("inlineCode",{parentName:"li"},"example_pyspark_pi_job.yaml")," to launch Spark Driver on ",(0,r.kt)("inlineCode",{parentName:"li"},"On-Demand")," nodes, Spark Executors on ",(0,r.kt)("inlineCode",{parentName:"li"},"Spot")," nodes in ",(0,r.kt)("inlineCode",{parentName:"li"},"spark-team-a")," namespace and submits the PySpark job to calculate the value of Pi.")),(0,r.kt)("details",null,(0,r.kt)("summary",null," To view sample SparkApplication with tolerations and node selectors, click to toggle content!"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'# NOTE: This example requires the following prerequisites before executing the jobs\n# 1. Ensure spark-team-a name space exists\n\n---\napiVersion: "sparkoperator.k8s.io/v1beta2"\nkind: SparkApplication\nmetadata:\n  name: pyspark-pi-karpenter\n  namespace: spark-team-a\nspec:\n  type: Python\n  pythonVersion: "3"\n  mode: cluster\n  image: "public.ecr.aws/r1l5w1y9/spark-operator:3.2.1-hadoop-3.3.1-java-11-scala-2.12-python-3.8-latest"\n  imagePullPolicy: Always\n  mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py\n  sparkVersion: "3.1.1"\n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 1\n    onFailureRetryInterval: 10\n    onSubmissionFailureRetries: 5\n    onSubmissionFailureRetryInterval: 20\n  driver:\n    cores: 1\n    coreLimit: "1200m"\n    memory: "512m"\n    labels:\n      version: 3.1.1\n    serviceAccount: spark-team-a\n    # Using Karpenter provisioner nodeSelectors and tolerations\n    nodeSelector:\n      NodeGroupType: "SparkComputeOptimized"\n      karpenter.sh/capacity-type: "on-demand"\n    tolerations:\n      - key: "spark-compute-optimized"\n        operator: "Exists"\n        effect: "NoSchedule"\n  executor:\n    cores: 1\n    instances: 2\n    memory: "512m"\n    serviceAccount: spark-team-a\n    labels:\n      version: 3.1.1\n    # Using Karpenter provisioner nodeSelectors and tolerations\n    nodeSelector:\n      NodeGroupType: "SparkComputeOptimized"\n      karpenter.sh/capacity-type: "spot"\n    tolerations:\n      - key: "spark-compute-optimized"\n        operator: "Exists"\n        effect: "NoSchedule"\n\n'))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The second task is ",(0,r.kt)("inlineCode",{parentName:"li"},"SparkKubernetesSensor")," that waits for the Kubernetes Spark Operator to complete the execution."),(0,r.kt)("li",{parentName:"ul"},"Verify the DAG execution from ",(0,r.kt)("inlineCode",{parentName:"li"},"Graph")," link"),(0,r.kt)("li",{parentName:"ul"},"All the Tasks will go green after few minutes"),(0,r.kt)("li",{parentName:"ul"},"Click on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3"))),(0,r.kt)(o.Z,{header:(0,r.kt)("h2",null,(0,r.kt)("span",null,"Cleanup")),mdxType:"CollapsibleContent"},(0,r.kt)("p",null,"This script will cleanup the environment using ",(0,r.kt)("inlineCode",{parentName:"p"},"-target")," option to ensure all the resources are deleted in correct order."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/schedulers/terraform/self-managed-airflow && chmod +x cleanup.sh\n./cleanup.sh\n"))),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment")))}f.isMDXComponent=!0},7721:(e,t,a)=>{a.d(t,{Z:()=>n});const n='apiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter # Same namespace as Karpenter add-on installed\nspec:\n  kubeletConfiguration:\n    containerRuntime: containerd\n    #    podsPerCore: 2\n    #    maxPods: 20\n  requirements:\n    - key: "topology.kubernetes.io/zone"\n      operator: In\n      values: [${azs}a] #Update the correct region and zones\n    - key: "karpenter.sh/capacity-type"\n      operator: In\n      values: ["spot", "on-demand"]\n    - key: "node.kubernetes.io/instance-type" #If not included, all instance types are considered\n      operator: In\n      values: ["c5d.large","c5d.xlarge","c5d.2xlarge","c5d.4xlarge","c5d.9xlarge"] # 1 NVMe disk\n    - key: "kubernetes.io/arch"\n      operator: In\n      values: ["amd64"]\n  limits:\n    resources:\n      cpu: 1000\n  providerRef:\n    name: spark-compute-optimized\n  labels:\n    type: karpenter\n    provisioner: spark-compute-optimized\n    NodeGroupType: SparkComputeOptimized\n  taints:\n    - key: spark-compute-optimized\n      value: \'true\'\n      effect: NoSchedule\n  ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set\n\n---\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter\nspec:\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  metadataOptions:\n    httpEndpoint: enabled\n    httpProtocolIPv6: disabled\n    httpPutResponseHopLimit: 2\n    httpTokens: required\n  subnetSelector:\n    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes\n  securityGroupSelector:                      # required, when not using launchTemplate\n    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes\n  #  instanceProfile: ""      # optional, if already set in controller args\n  #RAID0 config example\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    cat <<-EOF > /etc/profile.d/bootstrap.sh\n    #!/bin/sh\n\n    # Configure NVMe volumes in RAID0 configuration\n    # https://github.com/awslabs/amazon-eks-ami/blob/056e31f8c7477e893424abce468cb32bbcd1f079/files/bootstrap.sh#L35C121-L35C126\n    # Mount will be: /mnt/k8s-disks\n    export LOCAL_DISKS=\'raid0\'\n    EOF\n\n    # Source extra environment variables in bootstrap script\n    sed -i \'/^set -o errexit/a\\\\nsource /etc/profile.d/bootstrap.sh\' /etc/eks/bootstrap.sh\n\n    --BOUNDARY--\n\n  tags:\n    InstanceType: "spark-compute-optimized"    # optional, add tags for your own use\n'},4637:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/airflow-eks-architecture-bc3a09a223eedb676cbdb81c96b1acca.png"},914:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/airflow-k8sspark-example-4ae13592dfea21d12d8cb356602547a4.png"},166:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/kubernetes_default_conn-c0ab2d93f342940af96e2903de37e4d2.png"}}]);