"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["5426"],{56809:function(e,a,n){n.r(a),n.d(a,{frontMatter:()=>l,toc:()=>p,default:()=>h,metadata:()=>t,assets:()=>c,contentTitle:()=>o});var t=JSON.parse('{"id":"blueprints/data-analytics/spark-operator-beam","title":"Run Apache Beam pipelines with Spark on EKS","description":"Apache Beam (Beam) is a flexible programming model for building batch and streaming data processing pipelines.  With Beam, developers can write code once and run it on various execution engines, such as Apache Spark and Apache Flink. This flexibility allows organizations to leverage the strengths of different execution engines while maintaining a consistent codebase, reducing the complexity of managing multiple codebases and minimizing the risk of vendor lock-in.","source":"@site/docs/blueprints/data-analytics/spark-operator-beam.md","sourceDirName":"blueprints/data-analytics","slug":"/blueprints/data-analytics/spark-operator-beam","permalink":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/data-analytics/spark-operator-beam.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"sidebar_label":"Apache Beam on EKS","hide_table_of_contents":true},"sidebar":"blueprints","previous":{"title":"Spark Observability on EKS","permalink":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks"},"next":{"title":"DataHub on EKS","permalink":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks"}}'),r=n(74848),i=n(84429);n(84109),n(93703);var s=n(70366);n(60464);let l={sidebar_position:5,sidebar_label:"Apache Beam on EKS",hide_table_of_contents:!0},o="Run Apache Beam pipelines with Spark on EKS",c={},p=[{value:"Beam on Amazon EKS",id:"beam-on-amazon-eks",level:2},{value:"Solution overview",id:"solution-overview",level:2},{value:"Deploy Beam pipeline",id:"deploy-beam-pipeline",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Step 1: Build custom Docker Image with Spark and Beam SDK",id:"step-1-build-custom-docker-image-with-spark-and-beam-sdk",level:3},{value:"Step 2: Build and package the Beam pipeline with dependencies",id:"step-2-build-and-package-the-beam-pipeline-with-dependencies",level:3},{value:"Step 3: Create and run the pipeline as SparkApplication",id:"step-3-create-and-run-the-pipeline-as-sparkapplication",level:3},{value:"Step 4: Execute Spark Job",id:"step-4-execute-spark-job",level:3},{value:"Step 5: Monitor and review the pipeline job",id:"step-5-monitor-and-review-the-pipeline-job",level:3},{value:"Delete the ECR Repository",id:"delete-the-ecr-repository",level:2},{value:"Delete the EKS cluster",id:"delete-the-eks-cluster",level:2}];function d(e){let a={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"run-apache-beam-pipelines-with-spark-on-eks",children:"Run Apache Beam pipelines with Spark on EKS"})}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.a,{href:"https://beam.apache.org/get-started/beam-overview/",children:"Apache Beam (Beam)"})," is a flexible programming model for building batch and streaming data processing pipelines.  With Beam, developers can write code once and run it on various execution engines, such as ",(0,r.jsx)(a.em,{children:"Apache Spark"})," and ",(0,r.jsx)(a.em,{children:"Apache Flink"}),". This flexibility allows organizations to leverage the strengths of different execution engines while maintaining a consistent codebase, reducing the complexity of managing multiple codebases and minimizing the risk of vendor lock-in."]}),"\n",(0,r.jsx)(a.h2,{id:"beam-on-amazon-eks",children:"Beam on Amazon EKS"}),"\n",(0,r.jsx)(a.p,{children:"The Spark Operator for Kubernetes simplifies the deployment and management of Apache Spark on Kubernetes. By using the Spark Operator, we can directly submit Apache Beam pipelines as Spark Applications and deploy and manage them on EKS cluster, taking advantage of features such as automatic scaling and self-healing capabilities on the robust and managed infrastructure of EKS."}),"\n",(0,r.jsx)(a.h2,{id:"solution-overview",children:"Solution overview"}),"\n",(0,r.jsxs)(a.p,{children:["In this solution, we will show how to deploy your Beam pipeline, written in Python, on an EKS cluster with Spark Operator.  It uses the example pipeline from Apache Beam ",(0,r.jsx)(a.a,{href:"https://github.com/apache/beam/tree/master/sdks/python",children:"github repo"}),"."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.img,{alt:"BeamOnEKS",src:n(11594).A+"",width:"1094",height:"461"})}),"\n",(0,r.jsx)(a.h2,{id:"deploy-beam-pipeline",children:"Deploy Beam pipeline"}),"\n",(0,r.jsxs)(s.A,{header:(0,r.jsx)(a.h2,{children:(0,r.jsx)(a.span,{children:"Deploying the Spark-Operator-on-EKS solution"})}),children:[(0,r.jsxs)(a.p,{children:["In this ",(0,r.jsx)(a.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator",children:"example"}),", you will provision the following resources required to run Spark Jobs with open source Spark Operator."]}),(0,r.jsx)(a.p,{children:"It deploys an EKS Cluster running the Spark K8s Operator into a new VPC."}),(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Creates a new sample VPC, 2 Private Subnets, 2 Public Subnets, and 2 subnets in the RFC6598 space (100.64.0.0/10) for EKS Pods."}),"\n",(0,r.jsx)(a.li,{children:"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets"}),"\n",(0,r.jsx)(a.li,{children:"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with Managed Node Groups for benchmarking and core services, and Karpenter NodePools for Spark workloads."}),"\n",(0,r.jsx)(a.li,{children:"Deploys Metrics server, Spark-operator, Apache Yunikorn, Karpenter, Grafana and Prometheus server."}),"\n"]}),(0,r.jsx)(a.h3,{id:"prerequisites",children:"Prerequisites"}),(0,r.jsx)(a.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,r.jsx)(a.h3,{id:"deploy",children:"Deploy"}),(0,r.jsx)(a.p,{children:"Clone the repository."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\ncd data-on-eks\nexport DOEKS_HOME=$(pwd)\n"})}),(0,r.jsxs)(a.p,{children:["If DOEKS_HOME is ever unset, you can always set it manually using ",(0,r.jsx)(a.code,{children:"export DATA_ON_EKS=$(pwd)"})," from your data-on-eks directory."]}),(0,r.jsxs)(a.p,{children:["Navigate into one of the example directories and run ",(0,r.jsx)(a.code,{children:"install.sh"})," script."]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator\nchmod +x install.sh\n./install.sh\n"})}),(0,r.jsxs)(a.p,{children:["Now create an ",(0,r.jsx)(a.code,{children:"S3_BUCKET"})," variable that holds the name of the bucket created\nduring the install. This bucket will be used in later examples to store output\ndata. If S3_BUCKET is ever unset, you can run the following commands again."]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)\necho $S3_BUCKET\n"})})]}),"\n",(0,r.jsx)(a.h3,{id:"step-1-build-custom-docker-image-with-spark-and-beam-sdk",children:"Step 1: Build custom Docker Image with Spark and Beam SDK"}),"\n",(0,r.jsx)(a.p,{children:"Create a custom spark runtime image from the office spark base image, with a Python virtual environment and Apache Beam SDK pre-installed."}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:["Review the sample ",(0,r.jsx)(a.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/beam/Dockerfile",children:"Dockerfile"})]}),"\n",(0,r.jsx)(a.li,{children:"Customize the Dockerfile as needed for your environment"}),"\n",(0,r.jsx)(a.li,{children:"Build the Docker image and push the image to ECR"}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"cd examples/beam\naws ecr create-repository --repository-name beam-spark-repo --region us-east-1\ndocker build . --tag ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/beam-spark-repo:eks-beam-image --platform linux/amd64,linux/arm64\naws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com\ndocker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/beam-spark-repo:eks-beam-image\n"})}),"\n",(0,r.jsx)(a.p,{children:"We have created a docker image and published in ECR."}),"\n",(0,r.jsx)(a.h3,{id:"step-2-build-and-package-the-beam-pipeline-with-dependencies",children:"Step 2: Build and package the Beam pipeline with dependencies"}),"\n",(0,r.jsx)(a.p,{children:"With python 3.11 installed, create a Python virtual environment and install dependencies required for building the Beam pipeline:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"python3 -m venv build-environment && \\\nsource build-environment/bin/activate && \\\npython3 -m pip install --upgrade pip && \\\npython3 -m pip install apache_beam==2.58.0 \\\n    s3fs \\\n    boto3\n\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Download the ",(0,r.jsx)(a.a,{href:"https://raw.githubusercontent.com/apache/beam/master/sdks/python/apache_beam/examples/wordcount.py",children:"wordcount.py"})," example pipeline and the sample input file. The wordcount Python example demonstrates an Apache Beam pipeline with the following stages: read files, split words, map, group, and sum word counts, and write output to files."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"curl -O https://raw.githubusercontent.com/apache/beam/master/sdks/python/apache_beam/examples/wordcount.py\n\ncurl -O https://raw.githubusercontent.com/cs109/2015/master/Lectures/Lecture15b/sparklect/shakes/kinglear.txt\n"})}),"\n",(0,r.jsx)(a.p,{children:"Upload the input text file to the S3 bucket."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"aws s3 cp kinglear.txt s3://${S3_BUCKET}/\n"})}),"\n",(0,r.jsx)(a.p,{children:'To run an Apache Beam Python pipeline on Spark, you may package the pipeline and all its dependencies into a single jar file.  Use the below command to create the "fat" jar for the wordcount pipeline with all parameters, without actually executing the pipeline:'}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:'python3 wordcount.py --output_executable_path=./wordcountApp.jar \\ --runner=SparkRunner \\ --environment_type=PROCESS \\ --environment_config=\'{"command":"/opt/apache/beam/boot"}\' \\ --input=s3://${S3_BUCKET}/kinglear.txt \\ --output=s3://${S3_BUCKET}/output.txt\n'})}),"\n",(0,r.jsx)(a.p,{children:"Upload the jar file to the S3 bucket to be used by the spark application."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"aws s3 cp wordcountApp.jar s3://${S3_BUCKET}/app/\n"})}),"\n",(0,r.jsx)(a.h3,{id:"step-3-create-and-run-the-pipeline-as-sparkapplication",children:"Step 3: Create and run the pipeline as SparkApplication"}),"\n",(0,r.jsx)(a.p,{children:"In this step, we create the manifest file for the SparkApplication object to submit the Apache Beam pipeline as a Spark application. Run the below commands to create a BeamApp.yaml file substituting the ACCOUNT_ID and S3_BUCKET values from the build environment."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"envsubst < beamapp.yaml > beamapp.yaml\n"})}),"\n",(0,r.jsx)(a.p,{children:"This command will replace the env variables in file beamapp.yaml."}),"\n",(0,r.jsx)(a.h3,{id:"step-4-execute-spark-job",children:"Step 4: Execute Spark Job"}),"\n",(0,r.jsx)(a.p,{children:"Apply the YAML configuration file to create the SparkApplication on your EKS cluster to execute the Beam pipeline:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"kubectl apply -f beamapp.yaml\n"})}),"\n",(0,r.jsx)(a.h3,{id:"step-5-monitor-and-review-the-pipeline-job",children:"Step 5: Monitor and review the pipeline job"}),"\n",(0,r.jsx)(a.p,{children:"Monitor and review the pipeline job\nThe word count Beam pipeline may take a couple of minutes to execute.  There are a few ways to monitor its status and review job details."}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:"We can use the Spark history server to check the running job"}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"We used the spark-k8s-operator pattern to create the EKS cluster, which had already installed and configured a spark-history-server.  Run the command below to start port-forwarding, then click the Preview menu and select Preview Running Application:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:"kubectl port-forward svc/spark-history-server 8080:80 -n spark-history-server\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Open a new browser window and go to this address: ",(0,r.jsx)(a.a,{href:"http://127.0.0.1:8080/",children:"http://127.0.0.1:8080/"}),"."]}),"\n",(0,r.jsxs)(a.ol,{start:"2",children:["\n",(0,r.jsx)(a.li,{children:"Once the job completes successfully, in about 2 minutes, the output files (output.txt-*) containing words found in the input text and the count of each occurrence can be downloaded from the S3 bucket by running the below commands to copy the outputs to your build environment."}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sh",children:'mkdir job_output &&  cd job_output\naws s3 sync s3://$S3_BUCKET/ . --include "output.txt-*" --exclude "kinglear*" --exclude app/*\n'})}),"\n",(0,r.jsx)(a.p,{children:"Output looks like below:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"...\nparticular: 3\nwish: 2\nEither: 3\nbenison: 2\nDuke: 30\nContending: 1\nsay'st: 4\nattendance: 1\n...\n"})}),"\n",(0,r.jsxs)(s.A,{header:(0,r.jsx)(a.h2,{children:(0,r.jsx)(a.span,{children:"Cleanup"})}),children:[(0,r.jsx)(a.admonition,{type:"caution",children:(0,r.jsx)(a.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"})}),(0,r.jsx)(a.h2,{id:"delete-the-ecr-repository",children:"Delete the ECR Repository"}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"aws ecr delete-repository --repository-name beam-spark-repo --region us-east-1 --force\n"})}),(0,r.jsx)(a.h2,{id:"delete-the-eks-cluster",children:"Delete the EKS cluster"}),(0,r.jsxs)(a.p,{children:["This script will cleanup the environment using ",(0,r.jsx)(a.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator && chmod +x cleanup.sh\n./cleanup.sh\n"})})]})]})}function h(e={}){let{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},11594:function(e,a,n){n.d(a,{A:()=>t});let t=n.p+"assets/images/spark-operator-beam-6218f0e0622e80533e34df948ddac309.png"},93703:function(e,a,n){n.d(a,{A:()=>i});var t=n(74848);n(96540);var r=n(39836);function i({children:e,hidden:a,className:n}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,r.A)("tabItem_Ymn6",n),hidden:a,children:e})}},84109:function(e,a,n){n.d(a,{A:()=>k});var t=n(74848),r=n(96540),i=n(39836),s=n(16364),l=n(68251),o=n(56347),c=n(28004),p=n(25580),d=n(12213),h=n(75734);function u(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){let{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m({value:e,tabValues:a}){return a.some(a=>a.value===e)}var b=n(98864);function x({className:e,block:a,selectedValue:n,selectValue:r,tabValues:s}){let o=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.a_)(),p=e=>{let a=e.currentTarget,t=s[o.indexOf(a)].value;t!==n&&(c(a),r(t))},d=e=>{let a=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{let n=o.indexOf(e.currentTarget)+1;a=o[n]??o[0];break}case"ArrowLeft":{let n=o.indexOf(e.currentTarget)-1;a=o[n]??o[o.length-1]}}a?.focus()};return(0,t.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},e),children:s.map(({value:e,label:a,attributes:r})=>(0,t.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:p,...r,className:(0,i.A)("tabs__item","tabItem_LNqP",r?.className,{"tabs__item--active":n===e}),children:a??e},e))})}function g({lazy:e,children:a,selectedValue:n}){let s=(Array.isArray(a)?a:[a]).filter(Boolean);if(e){let e=s.find(e=>e.props.value===n);return e?(0,r.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,t.jsx)("div",{className:"margin-top--md",children:s.map((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==n}))})}function f(e){let a=function(e){let a,{defaultValue:n,queryString:t=!1,groupId:i}=e,s=function(e){let{values:a,children:n}=e;return(0,r.useMemo)(()=>{let e=a??u(n).map(({props:{value:e,label:a,attributes:n,default:t}})=>({value:e,label:a,attributes:n,default:t})),t=(0,d.XI)(e,(e,a)=>e.value===a.value);if(t.length>0)throw Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[a,n])}(e),[l,b]=(0,r.useState)(()=>(function({defaultValue:e,tabValues:a}){if(0===a.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:a}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${a.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let n=a.find(e=>e.default)??a[0];if(!n)throw Error("Unexpected error: 0 tabValues");return n.value})({defaultValue:n,tabValues:s})),[x,g]=function({queryString:e=!1,groupId:a}){let n=(0,o.W6)(),t=function({queryString:e=!1,groupId:a}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!a)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:e,groupId:a});return[(0,p.aZ)(t),(0,r.useCallback)(e=>{if(!t)return;let a=new URLSearchParams(n.location.search);a.set(t,e),n.replace({...n.location,search:a.toString()})},[t,n])]}({queryString:t,groupId:i}),[f,k]=function({groupId:e}){let a=e?`docusaurus.tab.${e}`:null,[n,t]=(0,h.Dv)(a);return[n,(0,r.useCallback)(e=>{a&&t.set(e)},[a,t])]}({groupId:i}),j=m({value:a=x??f,tabValues:s})?a:null;return(0,c.A)(()=>{j&&b(j)},[j]),{selectedValue:l,selectValue:(0,r.useCallback)(e=>{if(!m({value:e,tabValues:s}))throw Error(`Can't select invalid tab value=${e}`);b(e),g(e),k(e)},[g,k,s]),tabValues:s}}(e);return(0,t.jsxs)("div",{className:(0,i.A)(s.G.tabs.container,"tabs-container","tabList__CuJ"),children:[(0,t.jsx)(x,{...a,...e}),(0,t.jsx)(g,{...a,...e})]})}function k(e){let a=(0,b.A)();return(0,t.jsx)(f,{...e,children:u(e.children)},String(a))}},70366:function(e,a,n){n.d(a,{A:()=>p});var t=n(74848),r=n(96540),i=n(5556),s=n.n(i),l=n(39836);let o="expanded_iGsi";function c({children:e,header:a}){let[n,i]=(0,r.useState)(!1);return(0,t.jsxs)("div",{className:"collapsibleContent_q3kw",children:[(0,t.jsxs)("div",{className:(0,l.A)("header_QCEw",{[o]:n}),onClick:()=>{i(!n)},children:[a,(0,t.jsx)("span",{className:(0,l.A)("icon_PckA",{[o]:n}),children:n?"\u{1F447}":"\u{1F448}"})]}),n&&(0,t.jsx)("div",{className:"content_qLC1",children:e})]})}c.propTypes={children:s().node.isRequired,header:s().node.isRequired};let p=c}}]);