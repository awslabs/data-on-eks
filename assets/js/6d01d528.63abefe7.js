"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[4895],{5217:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var r=s(4848),a=s(8453),t=s(2450);const i={sidebar_position:5,sidebar_label:"Ray on EKS"},o="Ray on EKS",l={id:"blueprints/ai-ml/ray",title:"Ray on EKS",description:"The AI on EKS content is being migrated to a new repository.",source:"@site/docs/blueprints/ai-ml/ray.md",sourceDirName:"blueprints/ai-ml",slug:"/blueprints/ai-ml/ray",permalink:"/data-on-eks/docs/blueprints/ai-ml/ray",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/ai-ml/ray.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,sidebar_label:"Ray on EKS"},sidebar:"blueprints",previous:{title:"EMR NVIDIA Spark-RAPIDS",permalink:"/data-on-eks/docs/blueprints/ai-ml/emr-spark-rapids"},next:{title:"Data Analytics on EKS",permalink:"/data-on-eks/docs/category/data-analytics-on-eks"}},c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Ray on Kubernetes",id:"ray-on-kubernetes",level:2},{value:"Deploying the Example",id:"deploying-the-example",level:2},{value:"Clone the repository",id:"clone-the-repository",level:4},{value:"Initialize Terraform",id:"initialize-terraform",level:4},{value:"Run the install script",id:"run-the-install-script",level:4},{value:"XGBoost",id:"xgboost",level:5},{value:"PyTorch",id:"pytorch",level:5}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ray-on-eks",children:"Ray on EKS"})}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"AI on EKS"})," content ",(0,r.jsx)(n.strong,{children:"is being migrated"})," to a new repository.\n\ud83d\udd17 \ud83d\udc49 ",(0,r.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement",children:"Read the full migration announcement \xbb"})]})}),"\n",(0,r.jsxs)(n.admonition,{type:"danger",children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"DEPRECATION NOTICE"})}),(0,r.jsxs)(n.p,{children:["This blueprint will be deprecated and eventually removed from this GitHub repository on ",(0,r.jsx)(n.strong,{children:"October 27, 2024"}),", in favor of the ",(0,r.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/jark",children:(0,r.jsx)(n.strong,{children:"JARK stack"})}),". Please use the JARK stack blueprint instead."]})]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://www.ray.io/",children:"Ray"})," is an open-source framework for building scalable and distributed applications. It is designed to make it easy to write parallel and distributed Python applications by providing a simple and intuitive API for distributed computing. It has a growing community of users and contributors, and is actively maintained and developed by the Ray team at Anyscale, Inc."]}),"\n",(0,r.jsxs)(n.p,{children:["To deploy Ray in production across multiple machines users must first deploy ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/getting-started.html",children:(0,r.jsx)(n.strong,{children:"Ray Cluster"})}),". A Ray Cluster consists of head nodes and worker nodes which can be autoscaled using the built-in ",(0,r.jsx)(n.strong,{children:"Ray Autoscaler"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"RayCluster",src:s(2876).A+"",width:"773",height:"388"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Source: ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/key-concepts.html",children:"https://docs.ray.io/en/latest/cluster/key-concepts.html"})]})}),"\n",(0,r.jsx)(n.h2,{id:"ray-on-kubernetes",children:"Ray on Kubernetes"}),"\n",(0,r.jsxs)(n.p,{children:["Deploying Ray Cluster on Kubernetes including on Amazon EKS is supported via the ",(0,r.jsx)(n.a,{href:"https://ray-project.github.io/kuberay/",children:(0,r.jsx)(n.strong,{children:"KubeRay Operator"})}),". The operator provides a Kubernetes-native way to manage Ray clusters. The installation of KubeRay Operator involves deploying the operator and the CRDs for ",(0,r.jsx)(n.code,{children:"RayCluster"}),", ",(0,r.jsx)(n.code,{children:"RayJob"})," and ",(0,r.jsx)(n.code,{children:"RayService"})," as documented ",(0,r.jsx)(n.a,{href:"https://ray-project.github.io/kuberay/deploy/helm/",children:"here"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Deploying Ray on Kubernetes can provide several benefits:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Scalability: Kubernetes allows you to scale your Ray cluster up or down based on your workload requirements, making it easy to manage large-scale distributed applications."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Fault tolerance: Kubernetes provides built-in mechanisms for handling node failures and ensuring high availability of your Ray cluster."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Resource allocation: With Kubernetes, you can easily allocate and manage resources for your Ray workloads, ensuring that they have access to the necessary resources for optimal performance."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Portability: By deploying Ray on Kubernetes, you can run your workloads across multiple clouds and on-premises data centers, making it easy to move your applications as needed."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Monitoring: Kubernetes provides rich monitoring capabilities, including metrics and logging, making it easy to troubleshoot issues and optimize performance."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Overall, deploying Ray on Kubernetes can simplify the deployment and management of distributed applications, making it a popular choice for many organizations that need to run large-scale machine learning workloads."}),"\n",(0,r.jsxs)(n.p,{children:["Before moving forward with the deployment please make sure you have read the pertinent sections of the official ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"documentation"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"RayonK8s",src:s(3742).A+"",width:"1708",height:"686"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Source: ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html"})]})}),"\n",(0,r.jsx)(n.h2,{id:"deploying-the-example",children:"Deploying the Example"}),"\n",(0,r.jsxs)(n.p,{children:["In this ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/ai-ml/ray/terraform",children:"example"}),", you will provision Ray Cluster on Amazon EKS using the KubeRay Operator. The example also demonstrates the use of Karpenter of autoscaling of worker nodes for job specific Ray Clusters."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"RayOnEKS",src:s(4006).A+"",width:"1403",height:"1184"})}),"\n",(0,r.jsxs)(t.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Pre-requisites"})}),children:[(0,r.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.python.org/",children:"python3"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/master/ray-overview/installation.html#from-wheels",children:"ray"})}),"\n"]})]}),"\n",(0,r.jsxs)(t.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Deploy the EKS Cluster with KubeRay Operator"})}),children:[(0,r.jsx)(n.h4,{id:"clone-the-repository",children:"Clone the repository"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,r.jsx)(n.h4,{id:"initialize-terraform",children:"Initialize Terraform"}),(0,r.jsx)(n.p,{children:"Navigate into the example directory"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/ray/terraform\n"})}),(0,r.jsx)(n.h4,{id:"run-the-install-script",children:"Run the install script"}),(0,r.jsxs)(n.p,{children:["Use the provided helper script ",(0,r.jsx)(n.code,{children:"install.sh"})," to run the terraform init and apply commands. By default the script deploys EKS cluster to ",(0,r.jsx)(n.code,{children:"us-west-2"})," region. Update ",(0,r.jsx)(n.code,{children:"variables.tf"})," to change the region. This is also the time to update any other input variables or make any other changes to the terraform template."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./install .sh\n"})})]}),"\n",(0,r.jsxs)(t.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Verify Deployment"})}),children:[(0,r.jsx)(n.p,{children:"Update local kubeconfig so we can access kubernetes cluster"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks update-kubeconfig --name ray-cluster #or whatever you used for EKS cluster name\n"})}),(0,r.jsx)(n.p,{children:"First, lets verify that we have worker nodes running in the cluster."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                          STATUS   ROLES    AGE   VERSION\nip-10-1-26-241.ec2.internal   Ready    <none>   10h   v1.24.9-eks-49d8fe8\nip-10-1-4-21.ec2.internal     Ready    <none>   10h   v1.24.9-eks-49d8fe8\nip-10-1-40-196.ec2.internal   Ready    <none>   10h   v1.24.9-eks-49d8fe8\n"})})}),(0,r.jsx)(n.p,{children:"Next, lets verify all the pods are running."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n kuberay-operator\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                               READY   STATUS    RESTARTS        AGE\nkuberay-operator-7b5c85998-vfsjr   1/1     Running   1 (1h37m ago)   1h\n"})})}),(0,r.jsx)(n.p,{children:"At this point we are ready to deploy Ray Clusters."})]}),"\n",(0,r.jsxs)(t.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Deploy Ray Clusters and Workloads"})}),children:[(0,r.jsxs)(n.p,{children:["For convenience, we have packaged the helm chart deployent of Ray Cluster as a repeatable terraform ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/ai-ml/ray/terraform/modules/ray-cluster/",children:"module"}),". This allows us to codify organizational best practices and requirements for deploying Ray Clusters for multiple Data Science teams. The module also creates configuration needed for karpenter to be able to provision EC2 instances for Ray applications as and when they are needed for the duration of the job. This model can be replicated via GitOps tooling such as ArgoCD or Flux but is done here via terraform for demonstration purpose."]}),(0,r.jsx)(n.h5,{id:"xgboost",children:"XGBoost"}),(0,r.jsxs)(n.p,{children:["First, we will deploy a Ray Cluster for our ",(0,r.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/examples/ml-example.html#kuberay-ml-example",children:"XGBoost benchmark"})," sample job."]}),(0,r.jsx)(n.p,{children:"Go to the xgboost directory followed by terraform init, and plan."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd examples/xgboost\nterraform init\nterraform plan\n"})}),(0,r.jsx)(n.p,{children:"If the changes look good, lets apply them."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"terraform apply -auto-approve\n"})}),(0,r.jsxs)(n.p,{children:["As the RayCluster pod goes into the pending state, Karpenter will provision an EC2 instance based on the ",(0,r.jsx)(n.code,{children:"Provisioner"})," and ",(0,r.jsx)(n.code,{children:"AWSNodeTemplate"})," configuration we have provided. We can check that a new node has been created."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                          STATUS   ROLES    AGE     VERSION\n# New node appears\nip-10-1-13-204.ec2.internal   Ready    <none>   2m22s   v1.24.9-eks-49d8fe8\nip-10-1-26-241.ec2.internal   Ready    <none>   12h     v1.24.9-eks-49d8fe8\nip-10-1-4-21.ec2.internal     Ready    <none>   12h     v1.24.9-eks-49d8fe8\nip-10-1-40-196.ec2.internal   Ready    <none>   12h     v1.24.9-eks-49d8fe8\n"})})}),(0,r.jsx)(n.p,{children:"Wait until the RayCluster head node pods are provisioned."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n xgboost\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                         READY   STATUS    RESTARTS   AGE\nxgboost-kuberay-head-585d6   2/2     Running   0          5m42s\n"})})}),(0,r.jsx)(n.p,{children:"Now we are ready to run our sample training benchmark using for XGBoost. First, open another terminal and forward the Ray server to our localhost."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"kubectl port-forward service/xgboost-kuberay-head-svc -n xgboost 8265:8265\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Forwarding from 127.0.0.1:8265 -> 8265\nForwarding from [::1]:8265 -> 8265\n"})})}),(0,r.jsx)(n.p,{children:"Submit the ray job for XGBoost benchmark."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python job/xgboost_submit.py\n"})}),(0,r.jsxs)(n.p,{children:["You can open ",(0,r.jsx)(n.a,{href:"http://localhost:8265",children:"http://localhost:8265"})," in your browser to monitor job progress. If there are any failures during execution those can be viewed in the logs under the Jobs section."]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"RayDashboard",src:s(1905).A+"",width:"1920",height:"953"})}),(0,r.jsx)(n.p,{children:"As the job progresses, you will notice new Ray autoscaler will provision additional ray worker pods based on the autoscaling configuration defined in the RayCluster configuration. Those worker pods will initially remain in pending state. That will trigger karpenter to spin up new EC2 instances so the pending pods can be scheduled. After worker pods go to running state, the job will progress to completion."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                          STATUS    ROLES    AGE   VERSION\nip-10-1-1-241.ec2.internal    Unknown   <none>   1s\nip-10-1-10-211.ec2.internal   Unknown   <none>   1s\nip-10-1-13-204.ec2.internal   Ready     <none>   24m   v1.24.9-eks-49d8fe8\nip-10-1-26-241.ec2.internal   Ready     <none>   12h   v1.24.9-eks-49d8fe8\nip-10-1-3-64.ec2.internal     Unknown   <none>   7s\nip-10-1-4-21.ec2.internal     Ready     <none>   12h   v1.24.9-eks-49d8fe8\nip-10-1-40-196.ec2.internal   Ready     <none>   12h   v1.24.9-eks-49d8fe8\nip-10-1-7-167.ec2.internal    Unknown   <none>   1s\nip-10-1-9-112.ec2.internal    Unknown   <none>   1s\nip-10-1-9-172.ec2.internal    Unknown   <none>   1s\n"})})}),(0,r.jsxs)(n.p,{children:["Optionally, you can also use ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/eks-node-viewer",children:"eks-node-viewer"})," for visualizing dynamic node usage within the cluster."]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"EksNodeViewer",src:s(4169).A+"",width:"861",height:"330"})}),(0,r.jsx)(n.p,{children:"Once the benchmark is complete, the job log will display the results. You might see different results based on your configurations."}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Results: {'training_time': 1338.488839321999, 'prediction_time': 403.36653568099973}\n"})})}),(0,r.jsx)(n.h5,{id:"pytorch",children:"PyTorch"}),(0,r.jsx)(n.p,{children:"We can simultaneously deploy the PyTorch benchmark as well. We deploy a separate Ray Cluster with its own configuration for Karpenter workers. Different jobs can have different requirements for Ray Cluster such as a different version of Ray libraries or EC2 instance configuration such as making use of Spot market or GPU instances. We take advantage of node taints and tolerations in Ray Cluster pod specs to match the Ray Cluster configuration to Karpenter configuration thus taking advantage of the flexibility that Karpenter provides."}),(0,r.jsx)(n.p,{children:"Go to the PyTorch directory and run the terraform init and plan as before."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ../pytorch\nterraform init\nterraform plan\n"})}),(0,r.jsx)(n.p,{children:"Apply the changes."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"terraform apply -auto-approve\n"})}),(0,r.jsx)(n.p,{children:"Wait for the pytorch Ray Cluster head node pods to be ready."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n pytorch -w\n"})}),(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"NAME                         READY   STATUS    RESTARTS   AGE\npytorch-kuberay-head-9tx56   0/2     Pending   0          43s\n"})})}),(0,r.jsx)(n.p,{children:"Once running, we can forward the port for server, taking care that we forward it to another local port as 8265 may be occupied by the xgboost connection."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/pytorch-kuberay-head-svc -n pytorch 8266:8265\n"})}),(0,r.jsx)(n.p,{children:"We can then submit the job for PyTorch benchmark workload."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python job/pytorch_submit.py\n"})}),(0,r.jsxs)(n.p,{children:["You can open ",(0,r.jsx)(n.a,{href:"http://localhost:8266",children:"http://localhost:8266"})," to monitor the progress of the pytorch benchmark."]})]}),"\n",(0,r.jsxs)(t.A,{header:(0,r.jsx)(n.h3,{children:(0,r.jsx)(n.span,{children:"Teardown"})}),children:[(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsx)(n.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment."})}),(0,r.jsx)(n.p,{children:"Destroy the Ray Clusters for pytorch followed by xgboost."}),(0,r.jsx)(n.p,{children:"From the pytorch directory."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ../pytorch\nterraform destroy -auto-approve\n"})}),(0,r.jsx)(n.p,{children:"From the xgboost directory."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ../xgboost\nterraform destroy -auto-approve\n"})}),(0,r.jsxs)(n.p,{children:["Use the provided helper script ",(0,r.jsx)(n.code,{children:"cleanup.sh"})," to tear down EKS cluster and other AWS resources."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ../../\n./cleanup.sh\n"})})]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},2450:(e,n,s)=>{s.d(n,{A:()=>m});var r=s(6540),a=s(5556),t=s.n(a),i=s(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=s(4848);function u(e){let{children:n,header:s}=e;const[a,t]=(0,r.useState)(!1);return(0,p.jsxs)("div",{className:o,children:[(0,p.jsxs)("div",{className:(0,i.A)(l,{[h]:a}),onClick:()=>{t(!a)},children:[s,(0,p.jsx)("span",{className:(0,i.A)(c,{[h]:a}),children:a?"\ud83d\udc47":"\ud83d\udc48"})]}),a&&(0,p.jsx)("div",{className:d,children:n})]})}u.propTypes={children:t().node.isRequired,header:t().node.isRequired};const m=u},4169:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/eks-node-viewer-00ad3fc9c73a984f048dee4345f8d27d.png"},2876:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray-cluster-8928896ee110a20b193fa95f297970a5.svg"},1905:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray-dashboard-9f00fe9aadf91cf8a6fb2b53d9ae66b2.png"},4006:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray-on-eks-b55f9b97ff12f6073d1c005e1a75e142.png"},3742:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray_on_kubernetes-5882d71319bfc6b44d7da61e25a0c122.webp"},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var r=s(6540);const a={},t=r.createContext(a);function i(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);