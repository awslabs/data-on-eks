"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[5741],{5191:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var s=t(4848),i=t(8453),r=t(2450);const a={title:"NVIDIA Triton Server with vLLM",sidebar_position:2},o="Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM",l={id:"gen-ai/inference/GPUs/vLLM-NVIDIATritonServer",title:"NVIDIA Triton Server with vLLM",description:"The AI on EKS content is being migrated to a new repository.",source:"@site/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer.md",sourceDirName:"gen-ai/inference/GPUs",slug:"/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"NVIDIA Triton Server with vLLM",sidebar_position:2},sidebar:"genai",previous:{title:"RayServe with vLLM",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve"},next:{title:"Stable Diffusion on GPU",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/stablediffusion-gpus"}},d={},c=[{value:"What to Expect",id:"what-to-expect",level:2},{value:"Mistralai/Mistral-7B-Instruct-v0.2",id:"mistralaimistral-7b-instruct-v02",level:3},{value:"Meta-llama/Llama-2-7b-chat-hf",id:"meta-llamallama-2-7b-chat-hf",level:3},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"NVIDIA Triton Server with vLLM Backend",id:"nvidia-triton-server-with-vllm-backend",level:3},{value:"Verify Deployment",id:"verify-deployment",level:3},{value:"Testing Llama-2-7b Chat and Mistral-7b Chat Models",id:"testing-llama-2-7b-chat-and-mistral-7b-chat-models",level:3},{value:"Observability",id:"observability",level:2},{value:"Observability with AWS CloudWatch and Neuron Monitor",id:"observability-with-aws-cloudwatch-and-neuron-monitor",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"AI on EKS"})," content ",(0,s.jsx)(n.strong,{children:"is being migrated"})," to a new repository.\n\ud83d\udd17 \ud83d\udc49 ",(0,s.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement",children:"Read the full migration announcement \xbb"})]})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["The use of ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3-8B",children:"Meta-llama/Llama-2-7b-chat-hf"})," and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",children:"Mistralai/Mistral-7B-Instruct-v0.2"})," models requires access through a Hugging Face account."]})}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"deploying-multiple-large-language-models-with-nvidia-triton-server-and-vllm",children:"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM"})}),"\n",(0,s.jsxs)(n.p,{children:["In this pattern, we'll explore how to deploy multiple large language models (LLMs) using the ",(0,s.jsx)(n.a,{href:"https://github.com/triton-inference-server/server",children:"Triton Inference Server"})," and the ",(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," backend/engine. We'll demonstrate this process with two specific models: ",(0,s.jsx)(n.code,{children:"mistralai/Mistral-7B-Instruct-v0.2"})," and ",(0,s.jsx)(n.code,{children:"meta-llama/Llama-2-7b-chat-hf"}),". These models will be hosted on a ",(0,s.jsx)(n.strong,{children:"g5.24xlarge"})," multi-GPU instance, equipped with ",(0,s.jsx)(n.strong,{children:"4 GPUs"}),", with each model utilizing up to one GPU."]}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Triton Inference Server, when combined with the vLLM backend, offers a robust framework for deploying multiple large language models (LLMs). User applications interact with the inference service via REST API or gRPC, which is managed by NGINX and a Network Load Balancer (NLB) to efficiently distribute incoming requests to the Triton K8s Service. The Triton K8s Service is the core of our deployment, where the Triton Server processes inference requests. For this deployment, we use g5.24xlarge instances, each equipped with 4 GPUs, to run multiple models like Llama2-7b and Mistral7b. The Horizontal Pod Autoscaler (HPA) monitors custom metrics and dynamically scales Triton pods based on demand, ensuring efficient handling of varying loads. Prometheus and Grafana are used to collect and visualize metrics, providing insights into performance and aiding in autoscaling decisions."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"NVIDIA Triton Server",src:t(6288).A+"",width:"1920",height:"1080"})}),"\n",(0,s.jsx)(n.h2,{id:"what-to-expect",children:"What to Expect"}),"\n",(0,s.jsxs)(n.p,{children:["When you deploy everything as described, you can expect quick response times for your inference requests. Below is an example output from running the ",(0,s.jsx)(n.code,{children:"triton-client.py"})," script with the ",(0,s.jsx)(n.code,{children:"Llama-2-7b-chat-hf"})," and ",(0,s.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," models:"]}),"\n",(0,s.jsxs)(a,{children:[(0,s.jsx)("summary",{children:"Click to expand comparison results"}),(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Run 1: Llama2"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Run 2: Mistral7b"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt"}),(0,s.jsx)(n.td,{children:"python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["Loading inputs from ",(0,s.jsx)(n.code,{children:"prompts.txt"}),"..."]}),(0,s.jsxs)(n.td,{children:["Loading inputs from ",(0,s.jsx)(n.code,{children:"prompts.txt"}),"..."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 11: 0.00 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 3: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 15: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 14: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 3: 0.00 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 11: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 8: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 15: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 0: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 5: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 9: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 0: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 14: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 7: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 16: 0.00 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 13: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 19: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 9: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 4: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 16: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 10: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 18: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 6: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 4: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 1: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 8: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 7: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 1: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 18: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 6: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 12: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 12: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 2: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 17: 0.00 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 17: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 2: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 13: 0.01 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 19: 0.01 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model llama2 - Request 5: 0.02 ms"}),(0,s.jsx)(n.td,{children:"Model mistral7b - Request 10: 0.02 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["Storing results into ",(0,s.jsx)(n.code,{children:"llama2_results.txt"}),"..."]}),(0,s.jsxs)(n.td,{children:["Storing results into ",(0,s.jsx)(n.code,{children:"mistral_results.txt"}),"..."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Total time for all requests: 0.00 seconds (0.18 milliseconds)"}),(0,s.jsx)(n.td,{children:"Total time for all requests: 0.00 seconds (0.11 milliseconds)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"PASS: vLLM example"}),(0,s.jsx)(n.td,{children:"PASS: vLLM example"})]})]})]})]}),"\n",(0,s.jsx)(n.h1,{id:"triton-server-internals-and-backend-integration",children:"Triton Server Internals and Backend Integration"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Triton Inference Server is engineered for high-performance inference across a wide range of model types and deployment scenarios. The core strength of Triton lies in its support for various backends, which provide the flexibility and power needed to handle different types of models and workloads effectively."}),"\n",(0,s.jsx)(n.p,{children:"Once a request reaches the Triton K8s Service, it is processed by the Triton Server. The server supports dynamic batching, allowing multiple inference requests to be grouped together to optimize processing. This is particularly useful in scenarios with high throughput requirements, as it helps reduce latency and improve overall performance."}),"\n",(0,s.jsx)(n.p,{children:"Requests are then managed by scheduled queues, ensuring that each model's inference requests are processed in an orderly manner. The Triton Server supports selective and compute model loading, which means it can dynamically load models based on the current workload and resource availability. This feature is crucial for efficiently managing the resources in a multi-model deployment."}),"\n",(0,s.jsx)(n.p,{children:"The backbone of Triton\u2019s inference capabilities are its various backends, including TensorRT-LLM and vLLM:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"})}),": TensorRT-LLM backend optimizes large language model (LLM) inference on NVIDIA GPUs. Leveraging TensorRT's high-performance capabilities, it accelerates inference, providing low-latency and high-throughput performance. TensorRT is particularly well-suited for deep learning models that require intensive computational resources, making it ideal for real-time AI applications."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})}),": vLLM backend is specifically designed to handle various LLM workloads. It offers efficient memory management and execution pipelines tailored for large models. This backend ensures that memory resources are used optimally, allowing for the deployment of very large models without running into memory bottlenecks. vLLM is crucial for applications that need to serve multiple large models simultaneously, providing a robust and scalable solution."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"NVIDIA Triton Server",src:t(3627).A+"",width:"1920",height:"1080"})}),"\n",(0,s.jsx)(n.h3,{id:"mistralaimistral-7b-instruct-v02",children:"Mistralai/Mistral-7B-Instruct-v0.2"}),"\n",(0,s.jsx)(n.p,{children:"Mistralai/Mistral-7B-Instruct-v0.2 is a state-of-the-art large language model designed to provide high-quality, instructive responses. Trained on a diverse dataset, it excels in understanding and generating human-like text across a variety of topics. Its capabilities make it suitable for applications requiring detailed explanations, complex queries, and natural language understanding."}),"\n",(0,s.jsx)(n.h3,{id:"meta-llamallama-2-7b-chat-hf",children:"Meta-llama/Llama-2-7b-chat-hf"}),"\n",(0,s.jsx)(n.p,{children:"Meta-llama/Llama-2-7b-chat-hf is an advanced conversational AI model developed by Meta. It is optimized for chat applications, delivering coherent and contextually relevant responses. With its robust training on extensive dialogue datasets, this model excels in maintaining engaging and dynamic conversations, making it ideal for customer service bots, interactive agents, and other chat-based applications."}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-solution",children:"Deploying the Solution"}),"\n",(0,s.jsxs)(n.p,{children:["To get started with deploying both ",(0,s.jsx)(n.code,{children:"mistralai/Mistral-7B-Instruct-v0.2"})," and ",(0,s.jsx)(n.code,{children:"meta-llama/Llama-2-7b-chat-hf"})," on ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),", we will cover the necessary prerequisites and guide you through the deployment process step by step. This process includes setting up the infrastructure, deploying the NVIDIA Triton Inference Server, and creating the Triton client Python application that sends gRPC requests to the Triton server for inferencing."]}),"\n",(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsxs)(n.p,{children:["Important: Deploying on ",(0,s.jsx)(n.code,{children:"g5.24xlarge"})," instances, which are equipped with multiple GPUs, can be expensive. Ensure you carefully monitor and manage your usage to avoid unexpected costs. Consider setting budget alerts and usage limits to keep track of your expenditures."]})}),"\n",(0,s.jsxs)(r.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Prerequisites"})}),children:[(0,s.jsx)(n.p,{children:"Before we begin, ensure you have all the necessary prerequisites in place to make the deployment process smooth. Make sure you have installed the following tools on your machine:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,s.jsx)(n.p,{children:"Clone the repository"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,s.jsxs)(n.p,{children:["Navigate into one of the example directories and run ",(0,s.jsx)(n.code,{children:"install.sh"})," script"]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Important Note:"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step1"}),": Ensure that you update the region in the ",(0,s.jsx)(n.code,{children:"variables.tf"})," file before deploying the blueprint.\nAdditionally, confirm that your local region setting matches the specified region to prevent any discrepancies."]}),(0,s.jsxs)(n.p,{children:["For example, set your ",(0,s.jsx)(n.code,{children:'export AWS_DEFAULT_REGION="<REGION>"'})," to the desired region:"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step2"}),": To proceed, ensure you have access to both models using your Huggingface account:"]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"mistral7b-hg.png",src:t(4639).A+"",width:"2038",height:"1460"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"llma27b-hg.png",src:t(6579).A+"",width:"2038",height:"1460"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step3"}),": Next, set the environment variable TF_VAR_huggingface_token with your Huggingface account token:\n",(0,s.jsx)(n.code,{children:"export TF_VAR_huggingface_token=<your Huggingface token>"}),"."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step4"}),": Run the installation script."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/nvidia-triton-server/ && chmod +x install.sh\n./install.sh\n"})}),(0,s.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step5"}),": Once the installation finishes, verify the Amazon EKS Cluster"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Creates k8s config file to authenticate with EKS\naws eks --region us-west-2 update-kubeconfig --name nvidia-triton-server\n\nkubectl get nodes # Output shows the EKS worker nodes\n"})}),(0,s.jsxs)(n.p,{children:["You should see three nodes deployed by this installation: two ",(0,s.jsx)(n.code,{children:"m5.xlarge"})," and one ",(0,s.jsx)(n.code,{children:"g5.24xlarge"}),"."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"ip-100-64-190-174.us-west-2.compute.internal   Ready    <none>   11d     v1.29.3-eks-ae9a62a\nip-100-64-59-224.us-west-2.compute.internal    Ready    <none>   8m26s   v1.29.3-eks-ae9a62a\nip-100-64-59-227.us-west-2.compute.internal    Ready    <none>   11d     v1.29.3-eks-ae9a62a\n"})})]}),"\n",(0,s.jsx)(n.h3,{id:"nvidia-triton-server-with-vllm-backend",children:"NVIDIA Triton Server with vLLM Backend"}),"\n",(0,s.jsxs)(n.p,{children:["This blueprint uses ",(0,s.jsx)(n.a,{href:"https://github.com/aws-ia/terraform-aws-eks-data-addons/tree/main/helm-charts/nvidia-triton-server",children:"Triton helm chart"})," to install and configure the Triton server on Amazon EKS. The deployment is configured using the following Terraform code in the blueprint."]}),"\n",(0,s.jsxs)(a,{children:[(0,s.jsx)("summary",{children:"Click to expand the deployment code"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'module "triton_server_vllm" {\n  depends_on = [module.eks_blueprints_addons.kube_prometheus_stack]\n  source     = "aws-ia/eks-data-addons/aws"\n  version    = "~> 1.32.0" # ensure to update this to the latest/desired version\n\n  oidc_provider_arn = module.eks.oidc_provider_arn\n\n  enable_nvidia_triton_server = false\n\n  nvidia_triton_server_helm_config = {\n    version   = "1.0.0"\n    timeout   = 120\n    wait      = false\n    namespace = kubernetes_namespace_v1.triton.metadata[0].name\n    values = [\n      <<-EOT\n      replicaCount: 1\n      image:\n        repository: nvcr.io/nvidia/tritonserver\n        tag: "24.06-vllm-python-py3"\n      serviceAccount:\n        create: false\n        name: ${kubernetes_service_account_v1.triton.metadata[0].name}\n      modelRepositoryPath: s3://${module.s3_bucket.s3_bucket_id}/model_repository\n      environment:\n        - name: model_name\n          value: ${local.default_model_name}\n        - name: "LD_PRELOAD"\n          value: ""\n        - name: "TRANSFORMERS_CACHE"\n          value: "/home/triton-server/.cache"\n        - name: "shm-size"\n          value: "5g"\n        - name: "NCCL_IGNORE_DISABLED_P2P"\n          value: "1"\n        - name: tensor_parallel_size\n          value: "1"\n        - name: gpu_memory_utilization\n          value: "0.9"\n        - name: dtype\n          value: "auto"\n      secretEnvironment:\n        - name: "HUGGING_FACE_TOKEN"\n          secretName: ${kubernetes_secret_v1.huggingface_token.metadata[0].name}\n          key: "HF_TOKEN"\n      resources:\n        limits:\n          cpu: 6\n          memory: 25Gi\n          nvidia.com/gpu: 4\n        requests:\n          cpu: 6\n          memory: 25Gi\n          nvidia.com/gpu: 4\n      nodeSelector:\n        NodeGroupType: g5-gpu-karpenter\n        type: karpenter\n\n      tolerations:\n        - key: "nvidia.com/gpu"\n          operator: "Exists"\n          effect: "NoSchedule"\n      EOT\n    ]\n  }\n}\n\n'})})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," The container image that's being used for Triton server is ",(0,s.jsx)(n.code,{children:"nvcr.io/nvidia/tritonserver:24.02-vllm-python-py3"})," and is vLLM backend enabled. You can choose appropriate tags in the ",(0,s.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags",children:"NGC Catalog"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Repository"}),":\nThe Triton Inference Server serves models from one or more model repositories specified at server startup. Triton can access models from locally accessible file paths and cloud storage locations like Amazon S3."]}),"\n",(0,s.jsx)(n.p,{children:"The directories and files that compose a model repository must follow a required layout. The repository layout should be structured as follows:"}),"\n",(0,s.jsxs)(a,{children:[(0,s.jsx)("summary",{children:"Click to expand the model directory hierarchy"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"<model-repository-path>/\n  <model-name>/\n    [config.pbtxt]\n    [<output-labels-file> ...]\n    <version>/\n      <model-definition-file>\n    <version>/\n      <model-definition-file>\n\n  <model-name>/\n    [config.pbtxt]\n    [<output-labels-file> ...]\n    <version>/\n      <model-definition-file>\n    <version>/\n      <model-definition-file>\n    ...\n\n\n-------------\nExample:\n-------------\nmodel-repository/\n  mistral-7b/\n    config.pbtxt\n    1/\n      model.py\n  llama-2/\n    config.pbtxt\n    1/\n      model.py\n"})})]}),"\n",(0,s.jsxs)(n.p,{children:["For vLLM enabled Triton model, the model_repository can be found at ",(0,s.jsx)(n.code,{children:"gen-ai/inference/vllm-nvidia-triton-server-gpu/model_repository"})," location. During the deployment, the blueprint creates an S3 bucket and syncs the local ",(0,s.jsx)(n.code,{children:"model_repository"})," contents to the S3 bucket."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"model.py"}),": This script uses vLLM library as Triton backend framework and initializes a ",(0,s.jsx)(n.code,{children:"TritonPythonModel"})," class by loading the model configuration and configuring vLLM engine. The ",(0,s.jsx)(n.code,{children:"huggingface_hub"})," library's login function is used to establish access to the hugging face repository for model access. It then starts an asyncio event loop to process the received requests asynchronously. The script has several functions that processes the inference requests, issues the requests to vLLM backend and return the response."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"config.pbtxt"}),": This is a model configuration file that specifies parameters such as"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Name - The name of the model must match the ",(0,s.jsx)(n.code,{children:"name"})," of the model repository directory containing the model."]}),"\n",(0,s.jsxs)(n.li,{children:["max_batch_size - The ",(0,s.jsx)(n.code,{children:"max_batch_size"})," value indicates the maximum batch size that the model supports for the type of batching that can be exploited by Triton"]}),"\n",(0,s.jsxs)(n.li,{children:["Inputs and Outputs - Each model input and output must specify a name, datatype, and shape. An input shape indicates the shape of an input tensor expected by the model and by Triton in inference requests. An output shape indicates the shape of an output tensor produced by the model and returned by Triton in response to an inference request. Input and output shapes are specified by a combination of ",(0,s.jsx)(n.code,{children:"max_batch_size"})," and the dimensions specified by ",(0,s.jsx)(n.code,{children:"input dims"})," or ",(0,s.jsx)(n.code,{children:"output dims"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"verify-deployment",children:"Verify Deployment"}),"\n",(0,s.jsx)(n.p,{children:"To verify that the Triton Inference Server has been deployed successfully, run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get all -n triton-vllm\n"})}),"\n",(0,s.jsx)(n.p,{children:"Output below shows that there is one pod running the Triton server, which is hosting two models.\nThere is one service to interact with the models, and one ReplicaSet for Triton servers.\nDeployment will be horizontally scaled based on custom metrics and the HPA object."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"NAME                                                               READY   STATUS    RESTARTS   AGE\npod/nvidia-triton-server-triton-inference-server-c49bd559d-szlpf   1/1     Running   0          13m\n\nNAME                                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/nvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   <none>        8000/TCP,8001/TCP,8002/TCP   13m\nservice/nvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    <none>        8080/TCP                     13m\n\nNAME                                                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nvidia-triton-server-triton-inference-server   1/1     1            1           13m\n\nNAME                                                                     DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nvidia-triton-server-triton-inference-server-c49bd559d   1         1         1       13m\n\nNAME                                                                               REFERENCE                                                 TARGETS                        MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/nvidia-triton-server-triton-inference-server   Deployment/nvidia-triton-server-triton-inference-server   <unknown>/80%, <unknown>/80%   1         5         1          13m\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"This output indicates that the Triton server pods are running, the services are correctly set up, and the deployment is functioning as expected. The Horizontal Pod Autoscaler is also active, ensuring that the number of pods scales based on the specified metrics."}),"\n",(0,s.jsx)(n.h3,{id:"testing-llama-2-7b-chat-and-mistral-7b-chat-models",children:"Testing Llama-2-7b Chat and Mistral-7b Chat Models"}),"\n",(0,s.jsx)(n.p,{children:"It's time to test both the Llama-2-7b chat and Mistral-7b chat models. We will run the following commands with the same prompts to verify the outputs generated by both models."}),"\n",(0,s.jsx)(n.p,{children:"First, execute a port forward to the Triton-inference-server Service using kubectl:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n triton-vllm port-forward svc/nvidia-triton-server-triton-inference-server 8001:8001\n"})}),"\n",(0,s.jsx)(n.p,{children:"Next, run the Triton client for each model using the same prompts:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-nvidia-triton-server-gpu/triton-client\npython3 -m venv .venv\nsource .venv/bin/activate\npip install tritonclient[all]\npython3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt\n"})}),"\n",(0,s.jsx)(n.p,{children:"You will see an output something like below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt\nLoading inputs from `prompts.txt`...\nModel mistral7b - Request 3: 0.00 ms\nModel mistral7b - Request 14: 0.00 ms\nModel mistral7b - Request 11: 0.00 ms\nModel mistral7b - Request 15: 0.00 ms\nModel mistral7b - Request 5: 0.00 ms\nModel mistral7b - Request 0: 0.01 ms\nModel mistral7b - Request 7: 0.01 ms\nModel mistral7b - Request 13: 0.00 ms\nModel mistral7b - Request 9: 0.00 ms\nModel mistral7b - Request 16: 0.01 ms\nModel mistral7b - Request 18: 0.01 ms\nModel mistral7b - Request 4: 0.01 ms\nModel mistral7b - Request 8: 0.01 ms\nModel mistral7b - Request 1: 0.01 ms\nModel mistral7b - Request 6: 0.00 ms\nModel mistral7b - Request 12: 0.00 ms\nModel mistral7b - Request 17: 0.00 ms\nModel mistral7b - Request 2: 0.01 ms\nModel mistral7b - Request 19: 0.01 ms\nModel mistral7b - Request 10: 0.02 ms\nStoring results into `mistral_results.txt`...\nTotal time for all requests: 0.00 seconds (0.11 milliseconds)\nPASS: vLLM example\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Output for ",(0,s.jsx)(n.code,{children:"mistral_results.txt"})," should like below:"]}),"\n",(0,s.jsxs)(a,{children:[(0,s.jsx)("summary",{children:"Click to expand Mistral results partial output"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"<s>[INST]<<SYS>>\nKeep short answers of no more than 100 sentences.\n<</SYS>>\n\nWhat are the key differences between traditional machine learning models and very large language models (vLLM)?\n[/INST] Traditional machine learning models (MLMs) are trained on specific datasets and features to learn patterns and make predictions based on that data. They require labeled data for training and are limited by the size and diversity of the training data. MLMs can be effective for solving structured problems, such as image recognition or speech recognition.\n\nVery Large Language Models (vLLMs), on the other hand, are trained on vast amounts of text data using deep learning techniques. They learn to generate human-like text based on the input they receive. vLLMs can understand and generate text in a more contextually aware and nuanced way than MLMs. They can also perform a wider range of tasks, such as text summarization, translation, and question answering. However, vLLMs can be more computationally expensive and require large amounts of data and power to train. They also have the potential to generate inaccurate or biased responses if not properly managed.\n\n=========\n\n<s>[INST]<<SYS>>\nKeep short answers of no more than 100 sentences.\n<</SYS>>\n\nCan you explain how TensorRT optimizes LLM inference on NVIDIA hardware?\n[/INST] TensorRT is a deep learning inference optimization tool from NVIDIA. It utilizes dynamic and static analysis to optimize deep learning models for inference on NVIDIA GPUs. For Maximum Likelihood Modeling (LLM) inference, TensorRT applies the following optimizations:\n\n1. Model Optimization: TensorRT converts the LLM model into an optimized format, such as INT8 or FP16, which reduces memory usage and increases inference speed.\n\n2. Engine Generation: TensorRT generates a custom engine for the optimized model, which includes kernel optimizations for specific NVIDIA GPUs.\n\n3. Memory Optimization: TensorRT minimizes memory usage by using data layout optimizations, memory pooling, and other techniques.\n\n4. Execution Optimization: TensorRT optimizes the execution of the engine on the GPU by scheduling and managing thread execution, reducing latency and increasing throughput.\n\n5. I/O Optimization: TensorRT optimizes input and output data transfer between the host and the GPU, reducing the time spent on data transfer and increasing overall inference speed.\n\n6. Dynamic Batching: TensorRT dynamically batches input data to maximize GPU utilization and reduce latency.\n\n7. Multi-Streaming: TensorRT supports multi-streaming, allowing multiple inference requests to be processed concurrently, increasing overall throughput.\n\n8. Profiling and Monitoring: TensorRT provides profiling and monitoring tools to help developers identify performance bottlenecks and optimize their models further.\n\nOverall, TensorRT optimizes LLM inference on NVIDIA hardware by applying a combination of model, engine, memory, execution, I/O, dynamic batching, multi-streaming, and profiling optimizations.\n"})})]}),"\n",(0,s.jsxs)(n.p,{children:["Now, try to run the inference on the Llama-2-7b-chat model with the same prompts and observe the output under a new file called ",(0,s.jsx)(n.code,{children:"llama2_results.txt"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt\n"})}),"\n",(0,s.jsx)(n.p,{children:"Output should look like:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt\nLoading inputs from `prompts.txt`...\nModel llama2 - Request 11: 0.00 ms\nModel llama2 - Request 15: 0.02 ms\nModel llama2 - Request 3: 0.00 ms\nModel llama2 - Request 8: 0.03 ms\nModel llama2 - Request 5: 0.02 ms\nModel llama2 - Request 0: 0.00 ms\nModel llama2 - Request 14: 0.00 ms\nModel llama2 - Request 16: 0.01 ms\nModel llama2 - Request 19: 0.02 ms\nModel llama2 - Request 4: 0.01 ms\nModel llama2 - Request 1: 0.01 ms\nModel llama2 - Request 10: 0.01 ms\nModel llama2 - Request 9: 0.01 ms\nModel llama2 - Request 7: 0.01 ms\nModel llama2 - Request 18: 0.01 ms\nModel llama2 - Request 12: 0.00 ms\nModel llama2 - Request 2: 0.00 ms\nModel llama2 - Request 6: 0.00 ms\nModel llama2 - Request 17: 0.01 ms\nModel llama2 - Request 13: 0.01 ms\nStoring results into `llama2_results.txt`...\nTotal time for all requests: 0.00 seconds (0.18 milliseconds)\nPASS: vLLM example\n"})}),"\n",(0,s.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,s.jsx)(n.h3,{id:"observability-with-aws-cloudwatch-and-neuron-monitor",children:"Observability with AWS CloudWatch and Neuron Monitor"}),"\n",(0,s.jsxs)(n.p,{children:["This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the blueprint integrates GPU metrics using NVIDIA's DCGM plugin, which is essential for monitoring high-performance GPU workloads. For machine learning models running on AWS Inferentia or Trainium, the ",(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide",children:"Neuron Monitor plugin"})," is added to capture and report Neuron-specific metrics."]}),"\n",(0,s.jsx)(n.p,{children:"All metrics, including container insights, GPU performance, and Neuron metrics, are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively."}),"\n",(0,s.jsx)(n.p,{children:"In addition to deploying CloudWatch EKS addon, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability."}),"\n",(0,s.jsx)(n.p,{children:"First, let's verify the services deployed by the Kube Prometheus stack:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n monitoring\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should see output similar to this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"kubectl get svc -n monitoring\nNAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nkube-prometheus-stack-grafana                    ClusterIP   172.20.252.10    <none>        80/TCP              11d\nkube-prometheus-stack-kube-state-metrics         ClusterIP   172.20.34.181    <none>        8080/TCP            11d\nkube-prometheus-stack-operator                   ClusterIP   172.20.186.93    <none>        443/TCP             11d\nkube-prometheus-stack-prometheus                 ClusterIP   172.20.147.64    <none>        9090/TCP,8080/TCP   11d\nkube-prometheus-stack-prometheus-node-exporter   ClusterIP   172.20.171.165   <none>        9100/TCP            11d\nprometheus-operated                              ClusterIP   None             <none>        9090/TCP            11d\n"})}),"\n",(0,s.jsxs)(n.p,{children:["To expose the NVIDIA Triton server metrics, we have deployed a metrics service(",(0,s.jsx)(n.code,{children:"nvidia-triton-server-triton-inference-server-metrics"}),") on port ",(0,s.jsx)(n.code,{children:"8080"}),". Verify it by running"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n triton-vllm\n"})}),"\n",(0,s.jsx)(n.p,{children:"The output should be:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"kubectl get svc -n triton-vllm\nNAME                                                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nnvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   <none>        8000/TCP,8001/TCP,8002/TCP   34m\nnvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    <none>        8080/TCP                     34m\n"})}),"\n",(0,s.jsx)(n.p,{children:"This confirms that the NVIDIA Triton server metrics are being scraped by the Prometheus server. You can visualize these metrics using the Grafana dashboard."}),"\n",(0,s.jsx)(n.p,{children:"In the Grafana dashboard below, you can see several important metrics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Average GPU Power Usage"}),": This gauge shows the current power usage of the GPU, which is crucial for monitoring the efficiency and performance of your inference tasks."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compute Time (milliseconds)"}),": This bar graph displays the time taken to compute inference requests, helping identify any latency issues."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cumulative Inference Requests"}),": This graph shows the total number of inference requests processed over time, providing insights into the workload and performance trends."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Queue Time (milliseconds)"}),": This line graph indicates the time requests spend in the queue before being processed, highlighting potential bottlenecks in the system."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"NVIDIA Triton Server",src:t(3054).A+"",width:"1920",height:"1080"})}),"\n",(0,s.jsx)(n.p,{children:"To create a new Grafana dashboard to monitor these metrics, follow the steps below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'- Port-forward Grafana service:\nkubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n monitoring\n\n- Grafana Admin user\nadmin\n\n- Get secret name from Terraform output\nterraform output grafana_secret_name\n\n- Get admin user password\naws secretsmanager get-secret-value --secret-id <grafana_secret_name_output> --region $AWS_REGION --query "SecretString" --output text\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Login to Grafana:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Open your web browser and navigate to ",(0,s.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Login with the username ",(0,s.jsx)(n.code,{children:"admin"})," and the password retrieved from AWS Secrets Manager."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Import an Open Source Grafana Dashboard:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Once logged in, click on the "+" icon on the left sidebar and select "Import".'}),"\n",(0,s.jsxs)(n.li,{children:["Enter the following URL to import the dashboard JSON: ",(0,s.jsx)(n.a,{href:"https://github.com/triton-inference-server/server/blob/main/deploy/k8s-onprem/dashboard.json",children:"Triton Server Grafana Dashboard"})]}),"\n",(0,s.jsx)(n.li,{children:"Follow the prompts to complete the import process."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You should now see the metrics displayed on your new Grafana dashboard, allowing you to monitor the performance and health of your NVIDIA Triton Inference Server deployment."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"triton-grafana-dash2",src:t(780).A+"",width:"3836",height:"1938"})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Deploying and managing multiple large language models on Amazon EKS with NVIDIA Triton Inference Server and vLLM backend offers a powerful and scalable solution for modern AI applications. By following this blueprint, you have set up the necessary infrastructure, deployed the Triton server, and configured robust observability using the Kube Prometheus stack and Grafana."}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cleanup the EKS Cluster:"}),"\nThis script will cleanup the environment using ",(0,s.jsx)(n.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export AWS_DEAFULT_REGION="DEPLOYED_EKS_CLUSTER_REGION>"\ncd data-on-eks/ai-ml/nvidia-triton-server/ && chmod +x cleanup.sh\n./cleanup.sh\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},2450:(e,n,t)=>{t.d(n,{A:()=>p});var s=t(6540),i=t(5556),r=t.n(i),a=t(4164);const o="collapsibleContent_q3kw",l="header_QCEw",d="icon_PckA",c="content_qLC1",h="expanded_iGsi";var m=t(4848);function u(e){let{children:n,header:t}=e;const[i,r]=(0,s.useState)(!1);return(0,m.jsxs)("div",{className:o,children:[(0,m.jsxs)("div",{className:(0,a.A)(l,{[h]:i}),onClick:()=>{r(!i)},children:[t,(0,m.jsx)("span",{className:(0,a.A)(d,{[h]:i}),children:i?"\ud83d\udc47":"\ud83d\udc48"})]}),i&&(0,m.jsx)("div",{className:c,children:n})]})}u.propTypes={children:r().node.isRequired,header:r().node.isRequired};const p=u},6579:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/llma27b-hg-d0f19b8446707d67b4ea6d134a772895.png"},4639:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/mistral7b-hg-5c8f09faebcf4ed9c1c3c568559c05e2.png"},6288:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/triton-architecture-26f45e98081552c17f8381dbb7dd5f61.png"},780:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/triton-grafana-dash2-6bc3f332abbb3cd2dc6da59d2ecd46b7.png"},3627:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/triton-internals-ffbbc8d44314b0020343b41b63593ffa.png"},3054:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/triton-observability-c1fbf7456677ce1c8ed6f423f0cce09e.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);