"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["3522"],{47847:function(e,n,r){r.r(n),r.d(n,{frontMatter:()=>o,default:()=>u,toc:()=>c,metadata:()=>s,assets:()=>l,contentTitle:()=>a});var s=JSON.parse('{"id":"bestpractices/analytics/spark-best-practices","title":"Spark on EKS Best Practices","description":"This page aims to provide comprehensive best practices and guidelines for deploying, managing, and optimizing Apache Spark workloads on Amazon Elastic Kubernetes Service (EKS). This helps organizations to successfully run and scale their Spark Applications at scale in a containerised environment on Amazon EKS.","source":"@site/docs/bestpractices/analytics/spark-best-practices.md","sourceDirName":"bestpractices/analytics","slug":"/bestpractices/analytics/spark-best-practices","permalink":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/bestpractices/analytics/spark-best-practices.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Spark on EKS Best Practices"},"sidebar":"bestpractices","previous":{"title":"EMR on EKS Best Practices","permalink":"/data-on-eks/docs/bestpractices/analytics/emr-on-eks"},"next":{"title":"Networking","permalink":"/data-on-eks/docs/category/networking"}}'),i=r(85893),t=r(50065);r(47902),r(5525),r(96912);let o={sidebar_position:3,sidebar_label:"Spark on EKS Best Practices"},a="Spark on EKS Best Practices",l={},c=[{value:"EKS Networking",id:"eks-networking",level:2},{value:"VPC and Subnets Sizing",id:"vpc-and-subnets-sizing",level:3},{value:"VPC IP address exhaustion",id:"vpc-ip-address-exhaustion",level:4},{value:"Remediation for IP Address exhaustion",id:"remediation-for-ip-address-exhaustion",level:4},{value:"CoreDNS Recommendations",id:"coredns-recommendations",level:3},{value:"DNS Lookup Throttling",id:"dns-lookup-throttling",level:4},{value:"Remediation",id:"remediation",level:4},{value:"Reduce Inter AZ Traffic",id:"reduce-inter-az-traffic",level:3},{value:"Inter AZ Costs",id:"inter-az-costs",level:4},{value:"Remediation",id:"remediation-1",level:4},{value:"Karpenter Recommendations",id:"karpenter-recommendations",level:2},{value:"Driver Nodepool",id:"driver-nodepool",level:3},{value:"Executor Nodepool",id:"executor-nodepool",level:3},{value:"Configure Spot instances",id:"configure-spot-instances",level:4},{value:"Instance and Capacity type selection",id:"instance-and-capacity-type-selection",level:4},{value:"Consolidation Configuration",id:"consolidation-configuration",level:4},{value:"Handling interruptions during Karpenter Consolidation/Spot Termination",id:"handling-interruptions-during-karpenter-consolidationspot-termination",level:4},{value:"Advanced Scheduling Considerations",id:"advanced-scheduling-considerations",level:2},{value:"Default Kubernetes Scheduler behaviour.",id:"default-kubernetes-scheduler-behaviour",level:3},{value:"Custom Schedulers",id:"custom-schedulers",level:3},{value:"How will Yunikorn and Karpenter work together?",id:"how-will-yunikorn-and-karpenter-work-together",level:3},{value:"Storage Best Practices",id:"storage-best-practices",level:2},{value:"Node Storage",id:"node-storage",level:3},{value:"Reuse PVC",id:"reuse-pvc",level:3},{value:"External Shuffle services",id:"external-shuffle-services",level:3}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"spark-on-eks-best-practices",children:"Spark on EKS Best Practices"})}),"\n",(0,i.jsx)(n.p,{children:"This page aims to provide comprehensive best practices and guidelines for deploying, managing, and optimizing Apache Spark workloads on Amazon Elastic Kubernetes Service (EKS). This helps organizations to successfully run and scale their Spark Applications at scale in a containerised environment on Amazon EKS."}),"\n",(0,i.jsxs)(n.p,{children:["For deploying Spark on EKS you can leverage the ",(0,i.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn",children:"blueprints"}),", which readily incorporates most of the best practices. You can further customize this blueprint, to tweak the configurations to match your specific application requirements and environment constraints, as outlined in this guide."]}),"\n",(0,i.jsx)(n.h2,{id:"eks-networking",children:"EKS Networking"}),"\n",(0,i.jsx)(n.h3,{id:"vpc-and-subnets-sizing",children:"VPC and Subnets Sizing"}),"\n",(0,i.jsx)(n.h4,{id:"vpc-ip-address-exhaustion",children:"VPC IP address exhaustion"}),"\n",(0,i.jsx)(n.p,{children:"As EKS clusters scale up with additional Spark workloads, the number of pods managed by a cluster can easily grow into the thousands, each consuming an IP address. This creates challenges, since IP addresses within a VPC are limited, and it's not always feasible to recreate a larger VPC or extend the current VPC's CIDR blocks."}),"\n",(0,i.jsxs)(n.p,{children:["Worker nodes and pods both consume IP addresses. By default, VPC CNI has ",(0,i.jsx)(n.code,{children:"WARM_ENI_TARGET=1"})," means that ",(0,i.jsx)(n.code,{children:"ipamd"}),' should keep "a full ENI" of available IPs around in the ',(0,i.jsx)(n.code,{children:"ipamd"})," warm pool for the Pod IP assignment."]}),"\n",(0,i.jsx)(n.h4,{id:"remediation-for-ip-address-exhaustion",children:"Remediation for IP Address exhaustion"}),"\n",(0,i.jsx)(n.p,{children:"While IP exhaustion remediation methods exist for VPCs, they introduce additional operational complexity and have significant implications to consider. Hence, for new EKS clusters, it is recommended to over-provision the subnets you will use for Pod networking for growth."}),"\n",(0,i.jsx)(n.p,{children:"For addressing IP address exhaustion, consider adding secondary CIDR blocks to your VPC and creating new subnets from these additional address ranges, then deploying worker nodes in these expanded subnets."}),"\n",(0,i.jsxs)(n.p,{children:["If adding more subnets, is not an option, then you will have to work on optimising the IP address assignment by tweaking CNI Configuration Variables. Refer to ",(0,i.jsx)(n.a,{href:"/docs/bestpractices/networking#avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn",children:"configure MINIMUM_IP_TARGET"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"coredns-recommendations",children:"CoreDNS Recommendations"}),"\n",(0,i.jsx)(n.h4,{id:"dns-lookup-throttling",children:"DNS Lookup Throttling"}),"\n",(0,i.jsx)(n.p,{children:"Spark applications running on Kubernetes generate high volumes of DNS lookups when executors communicate with external services."}),"\n",(0,i.jsx)(n.p,{children:"This occurs because Kubernetes' DNS resolution model requires each pod to query the cluster's DNS service (kube-dns or CoreDNS) for every new connection, and during task executions Spark executors frequently create new connections for communicating with external services. By default, Kubernetes does not cache DNS results at the pod level, meaning each executor pod must perform a new DNS lookup even for previously resolved hostnames."}),"\n",(0,i.jsx)(n.p,{children:"This behavior is amplified in Spark applications due to their distributed nature, where multiple executor pods simultaneously attempt to resolve the same external service endpoints.This occurs during data ingestion, processing, and when connecting to external databases or shuffle services."}),"\n",(0,i.jsxs)(n.p,{children:["When DNS traffic exceeds 1024 packets per second for a CoreDNS replica, DNS requests will be throttled, resulting in ",(0,i.jsx)(n.code,{children:"unknownHostException"})," errors."]}),"\n",(0,i.jsx)(n.h4,{id:"remediation",children:"Remediation"}),"\n",(0,i.jsxs)(n.p,{children:["It is recommended to scale CoreDNS, as your workload scales. Refer to ",(0,i.jsx)(n.a,{href:"/docs/bestpractices/networking#scaling-coredns",children:"Scaling CoreDNS"})," for more details on implementation choices."]}),"\n",(0,i.jsxs)(n.p,{children:["It is also recommended to continuously monitor CoreDNS metrics. Refer to ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/best-practices/monitoring_eks_workloads_for_network_performance_issues.html#_monitoring_coredns_traffic_for_dns_throttling_issues",children:"EKS Networking Best Practices"})," for detailed information."]}),"\n",(0,i.jsx)(n.h3,{id:"reduce-inter-az-traffic",children:"Reduce Inter AZ Traffic"}),"\n",(0,i.jsx)(n.h4,{id:"inter-az-costs",children:"Inter AZ Costs"}),"\n",(0,i.jsx)(n.p,{children:"During the shuffle stage, Spark executors may need to exchange data between them. If the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front, which will be charged as Inter-AZ Traffic costs."}),"\n",(0,i.jsx)(n.h4,{id:"remediation-1",children:"Remediation"}),"\n",(0,i.jsx)(n.p,{children:"For Spark workloads, it is recommended to colocate executor pods and worker nodes in the same AZ. Colocating workloads in the same AZ serves two main purposes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Reduce inter-AZ traffic costs"}),"\n",(0,i.jsx)(n.li,{children:"Reduce network latency between executors/Pods"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Refer to ",(0,i.jsx)(n.a,{href:"/docs/bestpractices/networking#inter-az-network-optimization",children:"Inter AZ Network Optimization"})," for having pods co-locate on the same AZ."]}),"\n",(0,i.jsx)(n.h2,{id:"karpenter-recommendations",children:"Karpenter Recommendations"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/docs/",children:"Karpenter"})," enhances Spark on EKS deployments by providing rapid node provisioning capability that aligns with Spark's dynamic resource scaling needs. This automated scaling solution improves resource utilization and cost-efficiency by bringing in right-sized nodes as needed. This also allows Spark jobs to scale seamlessly without the need for pre-configured node groups or manual intervention, there by simplifying operational management."]}),"\n",(0,i.jsxs)(n.p,{children:["Here are the Karpenter recommendations for scaling compute nodes while running Spark workloads. For complete Karpenter configuration details, refer ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/docs/",children:"Karpenter documentation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Consider creating separate NodePools for driver and executor pods."}),"\n",(0,i.jsx)(n.h3,{id:"driver-nodepool",children:"Driver Nodepool"}),"\n",(0,i.jsx)(n.p,{children:"The Spark driver is a single pod and manages the entire lifecycle of the Spark application. Terminating Spark driver pod, effectively means terminating the entire Spark job."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Configure Driver Nodepool to always use ",(0,i.jsx)(n.code,{children:"on-demand"})," nodes only. When Spark driver pods run on spot instances, they are vulnerable to unexpected terminations due to spot instance reclamation, resulting in computation loss and interrupted processing that requires manual intervention to restart."]}),"\n",(0,i.jsxs)(n.li,{children:["Disable ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/docs/concepts/disruption/#consolidation",children:(0,i.jsx)(n.code,{children:"consolidation"})})," on Driver Nodepool."]}),"\n",(0,i.jsxs)(n.li,{children:["Use ",(0,i.jsx)(n.code,{children:"node selectors"})," or ",(0,i.jsx)(n.code,{children:"taints/tolerations"})," for placing driver pods on this designated Driver NodePool."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"executor-nodepool",children:"Executor Nodepool"}),"\n",(0,i.jsx)(n.h4,{id:"configure-spot-instances",children:"Configure Spot instances"}),"\n",(0,i.jsxs)(n.p,{children:["In the absence of ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/ec2/pricing/reserved-instances/",children:"Amazon EC2 Reserved Instances"})," or ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/savingsplans/",children:"Savings Plans"}),", consider using ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/ec2/spot/",children:"Amazon EC2 Spot Instances"})," for executors to reduce dataplane costs."]}),"\n",(0,i.jsxs)(n.p,{children:["When spot instances are interrupted, executors will be terminated and rescheduled on available nodes. For details on interruption behaviour and node termination management, refer to the ",(0,i.jsx)(n.code,{children:"Handling Interruptions"})," section."]}),"\n",(0,i.jsx)(n.h4,{id:"instance-and-capacity-type-selection",children:"Instance and Capacity type selection"}),"\n",(0,i.jsx)(n.p,{children:"Using multiple instance types in the node pool enables access to various spot instance pools, increasing capacity availability and optimizing for both price and capacity across the available instance options."}),"\n",(0,i.jsxs)(n.p,{children:["With ",(0,i.jsx)(n.code,{children:"Weighted Nodepools"}),", node selection can be optimized using weighted nodepools arranged in priority order. By assigning different weights to each nodepool, you can establish a selection hierarchy, such as: Spot (highest weight), followed by Graviton, AMD, and Intel (lowest weight)."]}),"\n",(0,i.jsx)(n.h4,{id:"consolidation-configuration",children:"Consolidation Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["While enabling ",(0,i.jsx)(n.code,{children:"consolidation"})," for Spark executor pods can lead to better cluster resource utilization, it's crucial to strike a balance with job performance. Frequent consolidation events can result in slower execution times for Spark jobs, as executors are forced to recompute the shuffle data and RDD blocks."]}),"\n",(0,i.jsx)(n.p,{children:"This impact is particularly noticeable in long-running Spark jobs. To mitigate this, it's essential to carefully tune the consolidation interval."}),"\n",(0,i.jsx)(n.p,{children:"Enable graceful executor pods shutdown:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"spark.executor.decommission.enabled=true"}),": Enables graceful decommissioning of executors, allowing them to complete their current tasks and transfer their cached data before shutting down. This is particularly useful when using spot instances for executors."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"spark.storage.decommission.enabled=true"}),": Enables the migration of cached RDD blocks from the decommissioning executor to other active executors before shutdown, preventing data loss and the need for recomputation."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["To explore other means to save intermediate data computed in Spark Executors, refer to ",(0,i.jsx)(n.a,{href:"#storage-best-practices",children:"Storage Best Practices"}),"."]}),"\n",(0,i.jsx)(n.h4,{id:"handling-interruptions-during-karpenter-consolidationspot-termination",children:"Handling interruptions during Karpenter Consolidation/Spot Termination"}),"\n",(0,i.jsx)(n.p,{children:"Perform controlled decommissioning instead of abruptly killing executors when nodes are scheduled for termination. To achieve this:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Configure appropriate TerminationGracePeriod values for Spark workloads."}),"\n",(0,i.jsx)(n.li,{children:"Implement executor-aware termination handling."}),"\n",(0,i.jsx)(n.li,{children:"Ensure shuffle data is saved before nodes are decommissioned."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Spark provides native configurations to control termination behavior:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Controlling executor interruptions"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configs"}),":"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"spark.executor.decommission.enabled"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"spark.executor.decommission.forceKillTimeout"}),"\nThese configurations are particularly useful in scenarios where executors might be terminated due to spot instance interruptions or Karpenter consolidation events. When enabled, executors will gracefully shutdown by stopping task acceptance and notifying the driver about their decommissioning state."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Controlling executor's BlockManager behavior"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configs"}),":"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"spark.storage.decommission.enabled"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"spark.storage.decommission.shuffleBlocks.enabled"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"spark.storage.decommission.rddBlocks.enabled"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"spark.storage.decommission.fallbackStorage.path"}),"\nThese settings enable the migration of shuffle and RDD blocks from decommissioning executors to other available executors or to a fallback storage location. This approach helps in dynamic environments by reducing the need to recompute shuffle data or RDD blocks, thereby improving job completion times and resource efficiency."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-scheduling-considerations",children:"Advanced Scheduling Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"default-kubernetes-scheduler-behaviour",children:"Default Kubernetes Scheduler behaviour."}),"\n",(0,i.jsxs)(n.p,{children:["Default Kubernetes scheduler uses ",(0,i.jsx)(n.code,{children:"least allocated"})," approach. This strategy aims to distribute pods evenly across cluster, which helps in maintaining availability and a balanced resource utilization across all nodes, rather than packing more pods in fewer nodes."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Most allocated"})," approach on the other hand, aims to favor nodes with most amount of allocated resources, which leads to packing more pods onto nodes that are already heavily allocated. This approach is favourable for Spark jobs, as it aims for high utilization on select nodes at pod scheduling time, leading to better consolidation of nodes. You will have to leverage a custom kube-scheduler with this option enabled, or leverage Custom Schedulers purpose built for more advanced orchestration."]}),"\n",(0,i.jsx)(n.h3,{id:"custom-schedulers",children:"Custom Schedulers"}),"\n",(0,i.jsx)(n.p,{children:"Custom schedulers enhance Kubernetes\u2019 native scheduling capabilities by providing advanced features tailored for batch and high-performance computing workloads. Custom schedulers enhance resource allocation by optimizing bin-packing and offering scheduling tailored to specific application needs. Here are popular custom schedulers for running Spark workloads on Kubernetes."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://yunikorn.apache.org/",children:"Apache Yunikorn"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://volcano.sh/en/",children:"Volcano"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Advantages of leveraging custom schedulers like Yunikorn."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Hierarchical queue system and configurable policies allowing for complex resource management."}),"\n",(0,i.jsx)(n.li,{children:"Gang scheduling, which ensures all related pods (like Spark executors) start together, preventing resource wastage."}),"\n",(0,i.jsx)(n.li,{children:"Resource fairness across different tenants and workloads."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"how-will-yunikorn-and-karpenter-work-together",children:"How will Yunikorn and Karpenter work together?"}),"\n",(0,i.jsx)(n.p,{children:"Karpenter and Yunikorn complement each other by handling different aspects of workload management in Kubernetes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Karpenter"})," focuses on node provisioning and scaling, determining when to add or remove nodes based on resource demands."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Yunikorn"})," brings application awareness to scheduling through advanced features like queue management, resource fairness, and gang scheduling."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In a typical workflow, Yunikorn first schedules pods based on application-aware policies and queue priorities. When these pods remain pending due to insufficient cluster resources, Karpenter detects these pending pods and provisions appropriate nodes to accommodate them. This integration ensures both efficient pod placement (Yunikorn) and optimal cluster scaling (Karpenter)."}),"\n",(0,i.jsx)(n.p,{children:"For Spark workloads, this combination is particularly effective: Yunikorn ensures executors are scheduled according to application SLAs and dependencies, while Karpenter ensures the right node types are available to meet those specific requirements."}),"\n",(0,i.jsx)(n.h2,{id:"storage-best-practices",children:"Storage Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"node-storage",children:"Node Storage"}),"\n",(0,i.jsx)(n.p,{children:"By default, the EBS root volumes of worker nodes are set to 20GB. Spark Executors use local storage for temporary data like shuffle data, intermediate results, and temporary files. This default storage of 20GB root volume attached to worker nodes can be limiting in both size and performance. Consider the following options to address your performance and storage size requirements:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Expand the root volume capacity to provide ample space for intermediate Spark data. You will have to arrive at optimal capacity based on average size of the dataset that each executor will be processing and complexity of Spark job."}),"\n",(0,i.jsx)(n.li,{children:"Configure high-performance storage with better I/O and latency."}),"\n",(0,i.jsx)(n.li,{children:"Mount additional volumes on worker nodes for temporary data storage."}),"\n",(0,i.jsx)(n.li,{children:"Leverage dynamically provisioned PVCs that can be attached directly to executor pods."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"reuse-pvc",children:"Reuse PVC"}),"\n",(0,i.jsx)(n.p,{children:"This option allows reusing PVCs associated with Spark executors even after the executors are terminated (either due to consolidation activity or preemption in case of Spot instances)."}),"\n",(0,i.jsx)(n.p,{children:"This allows for preserving the intermediate shuffle data and cached data on the PVC. When Spark requests new executor pod to replace the terminated one, the system attempts to reuse an existing PVC that belonged to terminated executor. This option can be enabled by the following configuration:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"spark.kubernetes.executor.reusePersistentVolume=true"})}),"\n",(0,i.jsx)(n.h3,{id:"external-shuffle-services",children:"External Shuffle services"}),"\n",(0,i.jsx)(n.p,{children:"Leverage external shuffle services like Apache Celeborn to decouple compute and storage, allowing Spark executors to write data to an external shuffle service instead of local disks. This reduces the risk of data loss and data re-computation due to executor termination or consolidation."}),"\n",(0,i.jsxs)(n.p,{children:["This also allows for better resource management, especially when ",(0,i.jsx)(n.code,{children:"Spark Dynamic Resource Allocation"})," is enabled. External shuffle service allows Spark to preserve shuffle data even after executors are removed during dynamic resource allocation, preventing the need for recomputation of shuffle data when new executors are added. This enables more efficient scale-down of resources when they're not needed."]}),"\n",(0,i.jsx)(n.p,{children:"Also consider the performance implications of external shuffle services. For smaller datasets or applications with low shuffle data volues, the overhead of setting up and managing external shuffle service might outweigh its benefits."}),"\n",(0,i.jsx)(n.p,{children:"External shuffle services is recommended when dealing with either shuffle data volumes exceeding 500GB to 1TB per job or long running Spark applications that run for several hours to multiple days."}),"\n",(0,i.jsxs)(n.p,{children:["Refer to this ",(0,i.jsx)(n.a,{href:"https://celeborn.apache.org/docs/latest/deploy_on_k8s/",children:"Celeborn Documentation"})," for deployment on Kubernetes and integration configuration with Apache Spark."]})]})}function u(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},5525:function(e,n,r){r.d(n,{Z:()=>t});var s=r(85893);r(67294);var i=r(67026);function t({children:e,hidden:n,className:r}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,i.Z)("tabItem_Ymn6",r),hidden:n,children:e})}},47902:function(e,n,r){r.d(n,{Z:()=>v});var s=r(85893),i=r(67294),t=r(67026),o=r(69599),a=r(16550),l=r(32e3),c=r(4520),d=r(38341),u=r(76009);function h(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){let{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}var m=r(7227);function f({className:e,block:n,selectedValue:r,selectValue:i,tabValues:a}){let l=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.o5)(),d=e=>{let n=e.currentTarget,s=a[l.indexOf(n)].value;s!==r&&(c(n),i(s))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{let r=l.indexOf(e.currentTarget)+1;n=l[r]??l[0];break}case"ArrowLeft":{let r=l.indexOf(e.currentTarget)-1;n=l[r]??l[l.length-1]}}n?.focus()};return(0,s.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.Z)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:i})=>(0,s.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{l.push(e)},onKeyDown:u,onClick:d,...i,className:(0,t.Z)("tabs__item","tabItem_LNqP",i?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function g({lazy:e,children:n,selectedValue:r}){let o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){let e=o.find(e=>e.props.value===r);return e?(0,i.cloneElement)(e,{className:(0,t.Z)("margin-top--md",e.props.className)}):null}return(0,s.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function x(e){let n=function(e){let{defaultValue:n,queryString:r=!1,groupId:s}=e,t=function(e){let{values:n,children:r}=e;return(0,i.useMemo)(()=>{let e=n??h(r).map(({props:{value:e,label:n,attributes:r,default:s}})=>({value:e,label:n,attributes:r,default:s})),s=(0,d.lx)(e,(e,n)=>e.value===n.value);if(s.length>0)throw Error(`Docusaurus error: Duplicate values "${s.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`);return e},[n,r])}(e),[o,m]=(0,i.useState)(()=>(function({defaultValue:e,tabValues:n}){if(0===n.length)throw Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}let r=n.find(e=>e.default)??n[0];if(!r)throw Error("Unexpected error: 0 tabValues");return r.value})({defaultValue:n,tabValues:t})),[f,g]=function({queryString:e=!1,groupId:n}){let r=(0,a.k6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c._X)(s),(0,i.useCallback)(e=>{if(!s)return;let n=new URLSearchParams(r.location.search);n.set(s,e),r.replace({...r.location,search:n.toString()})},[s,r])]}({queryString:r,groupId:s}),[x,v]=function({groupId:e}){let n=e?`docusaurus.tab.${e}`:null,[r,s]=(0,u.Nk)(n);return[r,(0,i.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),b=(()=>{let e=f??x;return p({value:e,tabValues:t})?e:null})();return(0,l.Z)(()=>{b&&m(b)},[b]),{selectedValue:o,selectValue:(0,i.useCallback)(e=>{if(!p({value:e,tabValues:t}))throw Error(`Can't select invalid tab value=${e}`);m(e),g(e),v(e)},[g,v,t]),tabValues:t}}(e);return(0,s.jsxs)("div",{className:(0,t.Z)("tabs-container","tabList__CuJ"),children:[(0,s.jsx)(f,{...n,...e}),(0,s.jsx)(g,{...n,...e})]})}function v(e){let n=(0,m.Z)();return(0,s.jsx)(x,{...e,children:h(e.children)},String(n))}},96912:function(e,n,r){r.d(n,{Z:()=>d});var s=r(85893),i=r(67294),t=r(45697),o=r.n(t),a=r(67026);let l="expanded_iGsi";function c({children:e,header:n}){let[r,t]=(0,i.useState)(!1);return(0,s.jsxs)("div",{className:"collapsibleContent_q3kw",children:[(0,s.jsxs)("div",{className:(0,a.Z)("header_QCEw",{[l]:r}),onClick:()=>{t(!r)},children:[n,(0,s.jsx)("span",{className:(0,a.Z)("icon_PckA",{[l]:r}),children:r?"\uD83D\uDC47":"\uD83D\uDC48"})]}),r&&(0,s.jsx)("div",{className:"content_qLC1",children:e})]})}c.propTypes={children:o().node.isRequired,header:o().node.isRequired};let d=c},50065:function(e,n,r){r.d(n,{Z:()=>a,a:()=>o});var s=r(67294);let i={},t=s.createContext(i);function o(e){let n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);