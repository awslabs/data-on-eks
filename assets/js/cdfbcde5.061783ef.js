"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[4965],{9618:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var t=s(4848),a=s(8453),r=s(2450);const i={title:"RayServe with vLLM",sidebar_position:1},o="Deploying LLMs with RayServe and vLLM",l={id:"gen-ai/inference/GPUs/vLLM-rayserve",title:"RayServe with vLLM",description:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue.",source:"@site/docs/gen-ai/inference/GPUs/vLLM-rayserve.md",sourceDirName:"gen-ai/inference/GPUs",slug:"/gen-ai/inference/GPUs/vLLM-rayserve",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/GPUs/vLLM-rayserve.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"RayServe with vLLM",sidebar_position:1},sidebar:"genai",previous:{title:"DeepSeek-R1 on EKS",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/ray-vllm-deepseek"},next:{title:"NVIDIA Triton Server with vLLM",permalink:"/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer"}},c={},d=[{value:"RayServe and vLLM Backend Integration",id:"rayserve-and-vllm-backend-integration",level:2},{value:"Mistralai/Mistral-7B-Instruct-v0.2",id:"mistralaimistral-7b-instruct-v02",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Deploying Mistral-7B-Instruct-v0.2 with RayServe and vLLM",id:"deploying-mistral-7b-instruct-v02-with-rayserve-and-vllm",level:2},{value:"Testing Mistral-7b Chat Model",id:"testing-mistral-7b-chat-model",level:2},{value:"Observability",id:"observability",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["The use of ",(0,t.jsx)(n.a,{href:"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",children:"Mistralai/Mistral-7B-Instruct-v0.2"})," models requires access through a Hugging Face account."]})}),"\n",(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"deploying-llms-with-rayserve-and-vllm",children:"Deploying LLMs with RayServe and vLLM"})}),"\n",(0,t.jsxs)(n.p,{children:["In this guide, we'll explore deploying large language models (LLMs) using ",(0,t.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/getting_started.html",children:"Ray Serve"})," with a ",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," backend on Amazon EKS (Elastic Kubernetes Service). We'll use the ",(0,t.jsx)(n.code,{children:"mistralai/Mistral-7B-Instruct-v0.2"})," model to demonstrate the pattern."]}),"\n",(0,t.jsx)(n.p,{children:"Using Ray Serve with a vLLM backend for LLM inference offers several compelling benefits, particularly in terms of scalability, efficiency, and ease of deployment. Ray Serve is designed to handle concurrent requests efficiently, making it ideal for real-time applications where multiple requests need to be processed simultaneously. It supports dynamic batching, which groups multiple requests together to optimize throughput and resource utilization, crucial for handling the high demands of LLM inference. Additionally, Ray Serve's ability to auto-scale based on incoming request load ensures that resources are used optimally, adapting to varying traffic without manual intervention. The integration with vLLM further enhances performance by leveraging vLLM's real-time batching capabilities, which are essential for efficient online inference. Moreover, Ray Serve's framework-agnostic nature allows it to be used alongside any Python framework, providing flexibility in model deployment and integration with existing systems. This combination of features makes Ray Serve with a vLLM backend a robust solution for deploying large language models in production environments, ensuring high performance, scalability, and ease of use."}),"\n",(0,t.jsx)(n.h2,{id:"rayserve-and-vllm-backend-integration",children:"RayServe and vLLM Backend Integration"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})}),": is a high-throughput and memory-efficient inference and serving engine for large language models (LLMs), specifically designed to optimize deployment and inference performance. A standout feature is PagedAttention, an innovative attention algorithm inspired by virtual memory paging in operating systems. PagedAttention efficiently manages the attention key and value tensors (KV cache) by storing them in non-contiguous memory spaces, which significantly reduces memory fragmentation and waste. Checkout the ",(0,t.jsx)(n.a,{href:"https://blog.vllm.ai/2023/06/20/vllm.html",children:"blog"})," for comparing vLLM with HuggingFace Transformers (HF) and HuggingFace Text Generation Inference (TGI)."]}),"\n",(0,t.jsx)(n.p,{children:"vLLM employs continuous batching of incoming requests, optimizing the use of computational resources and improving inference speed by grouping multiple requests together. This dynamic batching maximizes throughput and reduces latency. The engine also features optimized CUDA kernels for accelerated model execution on GPUs. Another key advantage is vLLM's efficient memory sharing during parallel sampling, where multiple output sequences are generated from a single prompt. This reduces memory usage by up to 55% and improves throughput by up to 2.2 times."}),"\n",(0,t.jsx)(n.h3,{id:"mistralaimistral-7b-instruct-v02",children:"Mistralai/Mistral-7B-Instruct-v0.2"}),"\n",(0,t.jsx)(n.p,{children:"In this guide, we are deploying the Mistral-7B-Instruct-v0.2 model with RayServe and vLLM. You can easily adapt these instructions to deploy any large language model, such as Llama2. Mistral-7B-Instruct-v0.2 is a state-of-the-art large language model designed to provide high-quality, instructive responses. Trained on a diverse dataset, it excels in understanding and generating human-like text across a variety of topics, making it suitable for applications requiring detailed explanations, complex queries, and natural language understanding."}),"\n",(0,t.jsxs)(r.A,{header:(0,t.jsx)(n.h2,{children:(0,t.jsx)(n.span,{children:"Deploying the Solution"})}),children:[(0,t.jsx)(n.p,{children:"We are utilizing Terraform Infrastructure as Code (IaC) templates to deploy an Amazon EKS cluster, and we dynamically scale GPU nodes using Karpenter when the model is deployed using RayServe YAML configurations."}),(0,t.jsx)(n.p,{children:"To get started with deploying mistralai/Mistral-7B-Instruct-v0.2 on Amazon EKS, this guide will cover the necessary prerequisites and walk you through the deployment process step by step. This process includes setting up the infrastructure, deploying the Ray cluster, and creating the client Python application that sends HTTP requests to the RayServe endpoint for inferencing."}),(0,t.jsx)(n.admonition,{type:"danger",children:(0,t.jsxs)(n.p,{children:["Important: Deploying on ",(0,t.jsx)(n.code,{children:"g5.8xlarge"})," instances can be expensive. Ensure you carefully monitor and manage your usage to avoid unexpected costs. Consider setting budget alerts and usage limits to keep track of your expenditures."]})}),(0,t.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,t.jsx)(n.p,{children:"Before we begin, ensure you have all the necessary prerequisites in place to make the deployment process smooth. Make sure you have installed the following tools on your machine:"}),(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["To simplify the demo process, we assume the use of an IAM role with administrative privileges due to the complexity of creating minimal IAM roles for each blueprint that may create various AWS services. However, for production deployments, it is strongly advised to create an IAM role with only the necessary permissions. Employing tools such as ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/iam/access-analyzer/",children:"IAM Access Analyzer"})," can assist in ensuring a least-privilege approach."]})}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,t.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,t.jsx)(n.p,{children:"Clone the repository"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Important Note:"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step1"}),": Ensure that you update the region in the ",(0,t.jsx)(n.code,{children:"variables.tf"})," file before deploying the blueprint.\nAdditionally, confirm that your local region setting matches the specified region to prevent any discrepancies."]}),(0,t.jsxs)(n.p,{children:["For example, set your ",(0,t.jsx)(n.code,{children:'export AWS_DEFAULT_REGION="<REGION>"'})," to the desired region:"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step2"}),": Run the installation script."]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/jark-stack/terraform && chmod +x install.sh\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,t.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,t.jsx)(n.p,{children:"Once the installation finishes, verify the Amazon EKS Cluster."}),(0,t.jsx)(n.p,{children:"Creates k8s config file to authenticate with EKS."}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 update-kubeconfig --name jark-stack\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"NAME                                           STATUS   ROLES    AGE    VERSION\nip-100-64-118-130.us-west-2.compute.internal   Ready    <none>   3h9m   v1.30.0-eks-036c24b\nip-100-64-127-174.us-west-2.compute.internal   Ready    <none>   9h     v1.30.0-eks-036c24b\nip-100-64-132-168.us-west-2.compute.internal   Ready    <none>   9h     v1.30.0-eks-036c24b\n"})}),(0,t.jsx)(n.p,{children:"Verify the Karpenter autosclaer Nodepools"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get nodepools\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"NAME                NODECLASS\ng5-gpu-karpenter    g5-gpu-karpenter\nx86-cpu-karpenter   x86-cpu-karpenter\n"})}),(0,t.jsx)(n.p,{children:"Verify the NVIDIA Device plugin"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n nvidia-device-plugin\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"NAME                                                              READY   STATUS    RESTARTS   AGE\nnvidia-device-plugin-gpu-feature-discovery-b4clk                  1/1     Running   0          3h13m\nnvidia-device-plugin-node-feature-discovery-master-568b49722ldt   1/1     Running   0          9h\nnvidia-device-plugin-node-feature-discovery-worker-clk9b          1/1     Running   0          3h13m\nnvidia-device-plugin-node-feature-discovery-worker-cwg28          1/1     Running   0          9h\nnvidia-device-plugin-node-feature-discovery-worker-ng52l          1/1     Running   0          9h\nnvidia-device-plugin-p56jj                                        1/1     Running   0          3h13m\n"})}),(0,t.jsx)(n.p,{children:"Verify Kuberay Operator which is used to create Ray Clusters"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n kuberay-operator\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-7894df98dc-447pm   1/1     Running   0          9h\n"})})]}),"\n",(0,t.jsx)(n.h2,{id:"deploying-mistral-7b-instruct-v02-with-rayserve-and-vllm",children:"Deploying Mistral-7B-Instruct-v0.2 with RayServe and vLLM"}),"\n",(0,t.jsxs)(n.p,{children:["Having deployed the EKS cluster with all the necessary components, we can now proceed with the steps to deploy ",(0,t.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," using ",(0,t.jsx)(n.code,{children:"RayServe"})," and ",(0,t.jsx)(n.code,{children:"vLLM"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step1:"})," As a prerequisite for this deployment, you must ensure that you have access to the model via your Hugging Face account:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"mistral7b-hg.png",src:s(4639).A+"",width:"2038",height:"1460"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step2:"})," Export Hugginface Hub Token"]}),"\n",(0,t.jsxs)(n.p,{children:["To deploy the ",(0,t.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," model with RayServe and vLLM backend, it's essential to configure your Hugging Face Hub token as an environment variable. This token is required for authentication and accessing the model. For guidance on how to create and manage your Hugging Face tokens, please visit ",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/hub/security-tokens",children:"Hugging Face Token Management"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Replace ",(0,t.jsx)(n.code,{children:"Your-Hugging-Face-Hub-Token-Value"})," with your actual Hugging Face Hub Token. This step ensures that your deployment has the necessary authentication to access the ",(0,t.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," model."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'export HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step3:"})," To deploy the RayService cluster, navigate to the directory containing the ray-service-vllm.yaml file and execute the deployment command. For more information about the RayService YAML configuration, you can refer to the file located at ",(0,t.jsx)(n.code,{children:"data-on-eks/gen-ai/inference/vllm-rayserve-gpu/ray-service-vllm.yaml"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Execute the following commands in your terminal. This will apply the RayService configuration and deploy the cluster on your EKS setup."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-gpu\n\nenvsubst < ray-service-vllm.yaml| kubectl apply -f -\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step4:"})," Verify the deployment by running the following commands"]}),"\n",(0,t.jsx)(n.p,{children:"To ensure that the deployment has been successfully completed, run the following commands:"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Deployment process may take up to ",(0,t.jsx)(n.strong,{children:"10 minutes"}),". The Head Pod is expected to be ready within 5 to 6 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface."]})}),"\n",(0,t.jsxs)(n.p,{children:["According to the RayServe configuration, you will have one Ray head pod running on an ",(0,t.jsx)(n.code,{children:"x86"})," instance and one worker pod running on a ",(0,t.jsx)(n.code,{children:"g5"})," GPU instance. You can modify the RayServe YAML file to run multiple replicas; however, be aware that each additional replica will require a separate GPU, potentially creating new instances."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pod -n rayserve-vllm\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"NAME                                           READY   STATUS    RESTARTS   AGE\nvllm-raycluster-nvtxg-head-g2cg8               1/1     Running   0          47m\nvllm-raycluster-nvtxg-worker-gpu-group-msl5p   1/1     Running   0          47m\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This deployment also configures a Mistral service with multiple ports. Port ",(0,t.jsx)(n.strong,{children:"8265"})," is designated for the Ray dashboard, and port ",(0,t.jsx)(n.strong,{children:"8000"})," is for the Mistral model endpoint."]}),"\n",(0,t.jsx)(n.p,{children:"Run the following command to verify the services:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n rayserve-vllm\n\nNAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE\nvllm             ClusterIP   172.20.208.16    <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   48m\nvllm-head-svc    ClusterIP   172.20.239.237   <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   37m\nvllm-serve-svc   ClusterIP   172.20.196.195   <none>        8000/TCP                                        37m\n"})}),"\n",(0,t.jsx)(n.p,{children:"To access the Ray dashboard, you can port-forward the relevant port to your local machine:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl -n rayserve-vllm port-forward svc/vllm 8265:8265\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can then access the web UI at ",(0,t.jsx)(n.a,{href:"http://localhost:8265",children:"http://localhost:8265"}),", which displays the deployment of jobs and actors within the Ray ecosystem."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"RayServe Deployment",src:s(5284).A+"",width:"2582",height:"1228"})}),"\n",(0,t.jsxs)(n.p,{children:["Once the deployment is complete, the Controller and Proxy status should be ",(0,t.jsx)(n.code,{children:"HEALTHY"})," and Application status should be ",(0,t.jsx)(n.code,{children:"RUNNING"})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"RayServe Deployment Logs",src:s(8224).A+"",width:"2532",height:"1418"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-mistral-7b-chat-model",children:"Testing Mistral-7b Chat Model"}),"\n",(0,t.jsxs)(n.p,{children:["Now it's time to test the Mistral-7B chat model. We'll use a Python client script to send prompts to the RayServe inference endpoint and verify the outputs generated by the model. The script reads prompts from a ",(0,t.jsx)(n.code,{children:"prompts.txt"})," file and writes the responses to a ",(0,t.jsx)(n.code,{children:"results.txt"})," file in the same location. It also logs the response times and token lengths for each response."]}),"\n",(0,t.jsxs)(n.p,{children:["First, execute a port forward to the ",(0,t.jsx)(n.code,{children:"vllm-serve-svc"})," Service using kubectl:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl -n rayserve-vllm port-forward svc/vllm-serve-svc 8000:8000\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"client.py"})," uses the HTTP POST method to send a list of prompts to the inference endpoint for text completion and Q&A, targeting the ",(0,t.jsx)(n.code,{children:"/vllm"})," model endpoint."]}),"\n",(0,t.jsxs)(n.p,{children:["You can test with your custom prompts by adding them to the ",(0,t.jsx)(n.code,{children:"prompts.txt"})," file."]}),"\n",(0,t.jsx)(n.p,{children:"To run the Python client application in a virtual environment, follow these steps:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-gpu\npython3 -m venv .venv\nsource .venv/bin/activate\npip install requests\npython3 client.py\n"})}),"\n",(0,t.jsx)(n.p,{children:"You will see an output something like below in the terminal:"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)("summary",{children:"Click to expand Python Client Terminal output"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"python3 client.py\nINFO:__main__:Warm-up successful\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nINFO:__main__:Response status: 200\nPrompt: [INST] Explain the concept of generative adversarial networks (GANs). [/INST]\nResponse Time: 20.72 seconds\nToken Length: 440\n================================================================================\nPrompt: [INST] How does a variational autoencoder (VAE) work? [/INST]\nResponse Time: 18.88 seconds\nToken Length: 397\n================================================================================\nPrompt: [INST] What are the applications of generative AI in healthcare? [/INST]\nResponse Time: 15.22 seconds\nToken Length: 323\n================================================================================\nPrompt: [INST] Describe the process of training a GAN. [/INST]\nResponse Time: 20.82 seconds\nToken Length: 437\n================================================================================\nPrompt: [INST] How can generative AI be used in creative arts? [/INST]\nResponse Time: 21.64 seconds\nToken Length: 454\n================================================================================\nPrompt: [INST] What is the difference between supervised and unsupervised learning in the context of generative AI? [/INST]\nResponse Time: 13.76 seconds\nToken Length: 310\n================================================================================\nPrompt: [INST] Explain the role of a discriminator in a GAN. [/INST]\nResponse Time: 11.96 seconds\nToken Length: 259\n================================================================================\nPrompt: [INST] How can generative AI improve natural language processing (NLP)? [/INST]\nResponse Time: 19.92 seconds\nToken Length: 393\n================================================================================\nPrompt: [INST] What are the ethical considerations of using generative AI? [/INST]\nResponse Time: 17.59 seconds\nToken Length: 361\n================================================================================\nPrompt: [INST] How is generative AI used in drug discovery? [/INST]\nResponse Time: 14.31 seconds\nToken Length: 311\n================================================================================\nPrompt: [INST] Describe the architecture of a Transformer model. [/INST]\nResponse Time: 26.96 seconds\nToken Length: 521\n================================================================================\nPrompt: [INST] How can generative AI be applied in the gaming industry? [/INST]\nResponse Time: 16.43 seconds\nToken Length: 348\n================================================================================\nPrompt: [INST] What is the purpose of latent space in generative models? [/INST]\nResponse Time: 11.55 seconds\nToken Length: 253\n================================================================================\nPrompt: [INST] How does text generation with GPT-3 work? [/INST]\nResponse Time: 12.64 seconds\nToken Length: 265\n================================================================================\nPrompt: [INST] What are the challenges of using generative AI in finance? [/INST]\nResponse Time: 18.21 seconds\nToken Length: 331\n================================================================================\nPrompt: [INST] Explain the concept of zero-shot learning in generative AI. [/INST]\nResponse Time: 14.92 seconds\nToken Length: 340\n================================================================================\nPrompt: [INST] How can generative AI be used for image synthesis? [/INST]\nResponse Time: 17.81 seconds\nToken Length: 352\n================================================================================\nPrompt: [INST] What are some real-world applications of deepfakes? [/INST]\nResponse Time: 14.39 seconds\nToken Length: 284\n================================================================================\nPrompt: [INST] How can generative AI contribute to personalized medicine? [/INST]\nResponse Time: 16.90 seconds\nToken Length: 338\n================================================================================\nPrompt: [INST] Describe the use of generative AI in autonomous vehicles. [/INST]\nResponse Time: 13.99 seconds\nToken Length: 299\n================================================================================\n"})})]}),"\n",(0,t.jsxs)(n.p,{children:["Verify the ",(0,t.jsx)(n.code,{children:"results.txt"})," file for actual responses for each prompt."]}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)("summary",{children:"Click to expand Mistral results partial output"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Prompt: [INST] Explain the theory of relativity.\nResponse: [INST] Explain the theory of relativity. [/INST] The theory of relativity, developed by Albert Einstein, is a fundamental theory in physics that describes the relationship between space and time, and how matter and energy interact within that framework. It is actually composed of two parts: the Special Theory of Relativity, published in 1905, and the General Theory of Relativity, published in 1915.\n\nThe Special Theory of Relativity is based on two postulates: the first one states that the laws of physics are the same in all inertial frames of reference (frames that are not accelerating); the second one asserts that the speed of light in a vacuum is the same for all observers, regardless of their motion or the source of the light.\n\nFrom these two postulates, several counter-intuitive consequences follow. For example, the length of an object contracts when it is in motion relative to an observer, and time dilation occurs, meaning that a moving clock appears to tick slower than a stationary one. These phenomena have been confirmed by numerous experiments.\n\nThe General Theory of Relativity is a theory of gravitation, which extended the Special Theory of Relativity by incorporating gravity into the fabric of spacetime. In this theory, mass causes a distortion or curvature in spacetime, which is felt as a gravitational force. This is in contrast to the Newtonian view of gravity as a force acting at a distance between two masses.\n\nOne of the most famous predictions of General Relativity is the bending of light by gravity, which was first observed during a solar eclipse in 1919. The theory has been extremely successful in explaining various phenomena, such as the precession of Mercury's orbit, the gravitational redshift of light, and the existence of black holes and gravitational waves.\n\nIn summary, the theory of relativity is a groundbreaking theory in physics that fundamentally changed our understanding of space, time, and matter. It has been incredibly successful in making accurate predictions about the natural world and has stood the test of time through numerous experiments and observations.\n--------------------------------------------------------------------------------\n"})})]}),"\n",(0,t.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,t.jsx)(n.p,{children:"As part of this blueprint, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability."}),"\n",(0,t.jsx)(n.p,{children:"First, let's verify the services deployed by the Kube Prometheus stack:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n kube-prometheus-stack\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see output similar to this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"kubectl get svc -n kube-prometheus-stack\nNAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nkube-prometheus-stack-grafana                    ClusterIP   172.20.252.10    <none>        80/TCP              11d\nkube-prometheus-stack-kube-state-metrics         ClusterIP   172.20.34.181    <none>        8080/TCP            11d\nkube-prometheus-stack-operator                   ClusterIP   172.20.186.93    <none>        443/TCP             11d\nkube-prometheus-stack-prometheus                 ClusterIP   172.20.147.64    <none>        9090/TCP,8080/TCP   11d\nkube-prometheus-stack-prometheus-node-exporter   ClusterIP   172.20.171.165   <none>        9100/TCP            11d\nprometheus-operated                              ClusterIP   None             <none>        9090/TCP            11d\n"})}),"\n",(0,t.jsx)(n.p,{children:"After verifying the Kube Prometheus stack services, we need to configure Prometheus to monitor our Ray cluster comprehensively.\nThis requires deploying both ServiceMonitor and PodMonitor resources:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ServiceMonitor"})," is used to collect metrics from the Ray head node, which has a Kubernetes Service exposing its metrics endpoint."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PodMonitor"})," is necessary because KubeRay operator does not create a Kubernetes service for the Ray worker Pods. Therefore, we cannot use a ServiceMonitor to scrape metrics from worker Pods and must use PodMonitors CRD instead."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/jark-stack/terraform/monitoring\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f serviceMonitor.yaml\nkubectl apply -f podMonitor.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Grafana and Prometheus Integration"})}),"\n",(0,t.jsx)(n.p,{children:"To integrate Grafana and Prometheus with the Ray Dashboard, we set specific environment variables in the Ray cluster configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"env:\n  - name: RAY_GRAFANA_HOST\n    value: http://kube-prometheus-stack-grafana.kube-prometheus-stack.svc:80\n  - name: RAY_PROMETHEUS_HOST\n    value: http://kube-prometheus-stack-prometheus.kube-prometheus-stack.svc:9090\n"})}),"\n",(0,t.jsx)(n.p,{children:"These environment variables are crucial for enabling the embedding of Grafana panels within the Ray Dashboard and for proper communication between Ray, Grafana, and Prometheus:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RAY_GRAFANA_HOST"})," defines the internal Kubernetes service URL for Grafana. The Ray head pod uses this for backend health checks and communication within the cluster."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RAY_PROMETHEUS_HOST"})," specifies the internal Kubernetes service URL for Prometheus, allowing Ray to query metrics when needed."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Access Prometheus Web UI"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Forward the port of Prometheus Web UI in the Prometheus server Pod.\nkubectl port-forward prometheus-kube-prometheus-stack-prometheus-0 -n kube-prometheus-stack 9090:9090\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Go to (YOUR_IP):9090/targets (e.g. 127.0.0.1:9090/targets). You should be able to see:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"podMonitor/kube-prometheus-stack/ray-workers-monitor/0 (1/1 up)"}),"\n",(0,t.jsx)(n.li,{children:"serviceMonitor/kube-prometheus-stack/ray-head-monitor/0 (2/2 up)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"RayServe Prometheus",src:s(354).A+"",width:"2870",height:"1508"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Access Grafana"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'- Port-forward Grafana service:\n\nkubectl port-forward deployment/kube-prometheus-stack-grafana -n kube-prometheus-stack 3000:3000\n\n# Check (YOUR_IP):3000/login for the Grafana login page (e.g. 127.0.0.1:3000/login).\n\n- Grafana Admin user\nadmin\n\n- Get secret name from Terraform output\nterraform output grafana_secret_name\n\n- Get admin user password\naws secretsmanager get-secret-value --secret-id <grafana_secret_name_output> --region $AWS_REGION --query "SecretString" --output text\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Note:"})," kubectl port-forward is not recommended for production use. Refer to ",(0,t.jsx)(n.a,{href:"https://grafana.com/tutorials/run-grafana-behind-a-proxy/",children:"this Grafana document"})," for exposing Grafana behind a reverse proxy."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Import Open Source Grafana Dashboards"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create new dashboard by importing JSON file via Dashboards menu."}),"\n",(0,t.jsx)(n.li,{children:"Click 'Dashboards' icon in left panel, 'New', 'Import', then 'Upload JSON file'."}),"\n",(0,t.jsxs)(n.li,{children:["Choose a JSON file.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Case 1:"})," If you are using Ray 2.24.0, you can use the sample config files in ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/ai-ml/jark-stack/terraform/monitoring/ray-dashboards",children:"GitHub repository"}),". The file names have a pattern of xxx_grafana_dashboard.json."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Case 2:"})," Otherwise, you should import the JSON files from ",(0,t.jsx)(n.code,{children:"/tmp/ray/session_latest/metrics/grafana/dashboards/"})," in the head Pod. You can use ",(0,t.jsx)(n.code,{children:"kubectl cp"})," to copy the files from the head Pod to your local machine."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Click \u201cImport\u201d."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"TODO: Note that importing the dashboard manually is not ideal. We should find a way to import the dashboard automatically.\n"})}),"\n",(0,t.jsx)(n.p,{children:"In the Grafana dashboard below, you can see several metrics:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scheduler Task State"})," displays the current number of tasks in a particular state."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Active Tasks by Name"})," shows the current number of (live) tasks with a particular name."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scheduler Actor State"})," illustrates the current number of actors in a particular state."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Active Actors by Name"})," presents the current number of (live) actors with a particular name."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"RayServe Grafana",src:s(6426).A+"",width:"2854",height:"1478"})}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Integrating Ray Serve with a vLLM backend offers numerous benefits for large language model (LLM) inference, particularly in terms of efficiency, scalability, and cost-effectiveness. Ray Serve's ability to handle concurrent requests and dynamically batch them ensures optimal GPU utilization, which is crucial for high-throughput LLM applications. The integration with vLLM enhances this further by enabling continuous batching, which significantly improves throughput and reduces latency compared to static batching. Overall, the combination of Ray Serve and vLLM provides a robust, scalable, and cost-efficient solution for deploying LLMs in production."}),"\n",(0,t.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,t.jsx)(n.p,{children:"Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed."}),"\n",(0,t.jsx)(n.p,{children:"Delete the RayCluster"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/vllm-rayserve-gpu\n\nkubectl delete -f ray-service-vllm.yaml\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/jark-stack/terraform/monitoring\n\nkubectl delete -f serviceMonitor.yaml\nkubectl delete -f podMonitor.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"Destroy the EKS Cluster and resources"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'export AWS_DEAFULT_REGION="DEPLOYED_EKS_CLUSTER_REGION>"\n\ncd data-on-eks/ai-ml/jark-stack/terraform/ && chmod +x cleanup.sh\n./cleanup.sh\n'})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},2450:(e,n,s)=>{s.d(n,{A:()=>m});var t=s(6540),a=s(5556),r=s.n(a),i=s(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=s(4848);function p(e){let{children:n,header:s}=e;const[a,r]=(0,t.useState)(!1);return(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("div",{className:(0,i.A)(l,{[h]:a}),onClick:()=>{r(!a)},children:[s,(0,u.jsx)("span",{className:(0,i.A)(c,{[h]:a}),children:a?"\ud83d\udc47":"\ud83d\udc48"})]}),a&&(0,u.jsx)("div",{className:d,children:n})]})}p.propTypes={children:r().node.isRequired,header:r().node.isRequired};const m=p},4639:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/mistral7b-hg-5c8f09faebcf4ed9c1c3c568559c05e2.png"},5284:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ray-dashboard-vllm-mistral-daad36acfb05ee2347312e4520427a03.png"},8224:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ray-deplo-logs-vllm-mistral-7a6139c7be415234f514833bc795794e.png"},6426:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ray-grafana-dashboard-c7c5d26237c24a281d3cafe9feb4004d.png"},354:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/ray-prometheus-38918dea8383254e5fff2542b666e317.png"},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var t=s(6540);const a={},r=t.createContext(a);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);