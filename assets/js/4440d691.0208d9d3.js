"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3959],{5647:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(4848),s=t(8453);const o={sidebar_position:2,sidebar_label:"Apache Pinot"},i="Deploying Apache Pinot (\ud83c\udf77) on EKS",r={id:"blueprints/distributed-databases/pinot",title:"Deploying Apache Pinot (\ud83c\udf77) on EKS",description:"Apache Pinot is real-time distributed OLAP datastore, purpose built for low-latency and high-throughput analytics. You can use pinot to ingest and immediately query data from streaming or batch data sources e.g. Apache Kafka, Amazon Kinesis Data Streams, Amazon S3, etc).",source:"@site/docs/blueprints/distributed-databases/pinot.md",sourceDirName:"blueprints/distributed-databases",slug:"/blueprints/distributed-databases/pinot",permalink:"/data-on-eks/docs/blueprints/distributed-databases/pinot",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/distributed-databases/pinot.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"Apache Pinot"},sidebar:"blueprints",previous:{title:"CloudNativePG PostgreSQL",permalink:"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres"},next:{title:"Trino on EKS",permalink:"/data-on-eks/docs/blueprints/distributed-databases/trino"}},l={},c=[{value:"Architecture",id:"architecture",level:2},{value:"Prerequisites \ud83d\udcdd",id:"prerequisites-",level:2},{value:"Deployment \u2699\ufe0f",id:"deployment-\ufe0f",level:2},{value:"Deploy the EKS Cluster with Apache Pinot",id:"deploy-the-eks-cluster-with-apache-pinot",level:3},{value:"Sample <code>terraform.tfvars</code>",id:"sample-terraformtfvars",level:4},{value:"Verify Deployment",id:"verify-deployment",level:3},{value:"Output",id:"output",level:4},{value:"Output",id:"output-1",level:4},{value:"Output",id:"output-2",level:4},{value:"Additional Deployment (Optional) \ud83c\udfc6",id:"additional-deployment-optional-",level:2},{value:"Deploy Apache Kafka for Streaming Data",id:"deploy-apache-kafka-for-streaming-data",level:3},{value:"Output",id:"output-3",level:4},{value:"Cleanup \ud83e\uddf9",id:"cleanup-",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"deploying-apache-pinot--on-eks",children:"Deploying Apache Pinot (\ud83c\udf77) on EKS"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://pinot.apache.org/",children:"Apache Pinot"})," is real-time distributed OLAP datastore, purpose built for low-latency and high-throughput analytics. You can use pinot to ingest and immediately query data from streaming or batch data sources e.g. Apache Kafka, Amazon Kinesis Data Streams, Amazon S3, etc)."]}),"\n",(0,a.jsx)(n.p,{children:"Apache Pinot includes the following characteristics:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ultra low-latency"})," analytics even at extremely high throughput."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Columnar data store"})," with several smart indexing and pre-aggregation techniques."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scaling up"})," and ",(0,a.jsx)(n.strong,{children:"out"})," with no upper bound."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consistent performance"})," based on the size of your cluster and an expected query per second (QPS) threshold."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["It's a perfect solution for user-facing real-time analytics and other analytical use cases, including internal dashboards, anomaly detection, and ad hoc data exploration. You can learn more about Apache Pinot and its components in its ",(0,a.jsx)(n.a,{href:"https://docs.pinot.apache.org/",children:"documentation"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"In this blueprint, we will deploy Apache Pinot on Kubernetes cluster managed by Elastic Kubernetes Service (EKS). Some of the benefits of deploying Apache Pinot on EKS cluster are"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Manage Apache Pinot Cluster using Kubernetes"}),"\n",(0,a.jsx)(n.li,{children:"Scale each layer independently"}),"\n",(0,a.jsx)(n.li,{children:"No single point of failure"}),"\n",(0,a.jsx)(n.li,{children:"Auto recovery"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Apache Pinot on EKS",src:t(6424).A+"",width:"1606",height:"808"})}),"\n",(0,a.jsxs)(n.p,{children:["In this setup we deploy all Apache Pinot components in private subnets across 3 availability zones. This allows for greater flexibility and resilience. Most pinot components can run on latest generation general purpose compute instances (",(0,a.jsx)(n.code,{children:"m7i"}),") except for server component which requires memory optimized instance types (",(0,a.jsx)(n.code,{children:"r7i"}),"). We also setup internal NLB to easily communicate with Controller and Broker components."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Note: All Apache Pinot components run on ",(0,a.jsx)(n.code,{children:"StatefulSet"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Note: This blueprint doesn't leverage ",(0,a.jsx)(n.a,{href:"https://docs.pinot.apache.org/basics/components/table/segment/deep-store",children:"DeepStore"})," currently and uses EBS volumes to store table segments on server."]}),"\n"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Note: Based on your use case, you will need to update the cluster size and configuration to better suite your use case. You can read more about Apache Pinot capacity planning ",(0,a.jsx)(n.a,{href:"https://startree.ai/blog/capacity-planning-in-apache-pinot-part-1",children:"here"})," and ",(0,a.jsx)(n.a,{href:"https://startree.ai/blog/capacity-planning-in-apache-pinot-part-2",children:"here"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites-",children:"Prerequisites \ud83d\udcdd"}),"\n",(0,a.jsx)(n.p,{children:"Ensure that you have following tools installed on your machine."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"deployment-\ufe0f",children:"Deployment \u2699\ufe0f"}),"\n",(0,a.jsx)(n.h3,{id:"deploy-the-eks-cluster-with-apache-pinot",children:"Deploy the EKS Cluster with Apache Pinot"}),"\n",(0,a.jsx)(n.p,{children:"First, clone the repository."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Navigate to apache pinot folder and create ",(0,a.jsx)(n.code,{children:"terraform.tfvars"})," to provide desired values for all the variables. This is also the time to update any other input variables or make any other changes to the terraform template."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/distributed-databases/pinot\ntouch terraform.tfvars\n"})}),"\n",(0,a.jsxs)(n.h4,{id:"sample-terraformtfvars",children:["Sample ",(0,a.jsx)(n.code,{children:"terraform.tfvars"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-terraform",children:'name                = "pinot-on-eks"\nregion              = "us-west-2"\neks_cluster_version = "1.25"\n...\n'})}),"\n",(0,a.jsx)(n.p,{children:"Once you have updated your variables, you can run the install script to deploy your pre-configured EKS cluster with Apache Pinot."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"./install.sh\n"})}),"\n",(0,a.jsx)(n.h3,{id:"verify-deployment",children:"Verify Deployment"}),"\n",(0,a.jsx)(n.p,{children:"Verify the Amazon EKS Cluster"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"aws eks describe-cluster --name pinot-on-eks\n"})}),"\n",(0,a.jsx)(n.p,{children:"Update local kubeconfig so we can access kubernetes cluster."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"aws eks update-kubeconfig --name pinot-on-eks --region us-west-2\n"})}),"\n",(0,a.jsx)(n.p,{children:"First, lets verify that we have worker nodes running in the cluster."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output",children:"Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"NAME                                         STATUS   ROLES    AGE   VERSION\nip-10-1-189-200.us-west-2.compute.internal   Ready    <none>   12d   v1.24.17-eks-43840fb\nip-10-1-46-117.us-west-2.compute.internal    Ready    <none>   12d   v1.24.17-eks-43840fb\nip-10-1-84-80.us-west-2.compute.internal     Ready    <none>   12d   v1.24.17-eks-43840fb\n"})}),"\n",(0,a.jsx)(n.p,{children:"Next, lets verify all the pods are running."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n pinot\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output-1",children:"Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"NAME                                                   READY   STATUS      RESTARTS   AGE\npinot-broker-0                                         1/1     Running     0          11d\npinot-broker-1                                         1/1     Running     0          11d\npinot-broker-2                                         1/1     Running     0          11d\npinot-controller-0                                     1/1     Running     0          11d\npinot-controller-1                                     1/1     Running     0          11d\npinot-controller-2                                     1/1     Running     0          11d\npinot-minion-stateless-86cf65f89-rlpwn                 1/1     Running     0          12d\npinot-minion-stateless-86cf65f89-tkbjf                 1/1     Running     0          12d\npinot-minion-stateless-86cf65f89-twp8n                 1/1     Running     0          12d\npinot-server-0                                         1/1     Running     0          11d\npinot-server-1                                         1/1     Running     0          11d\npinot-server-2                                         1/1     Running     0          11d\npinot-zookeeper-0                                      1/1     Running     0          12d\npinot-zookeeper-1                                      1/1     Running     0          12d\npinot-zookeeper-2                                      1/1     Running     0          12d\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We have also deployed ",(0,a.jsx)(n.code,{children:"prometheus"})," and ",(0,a.jsx)(n.code,{children:"grafana"})," under ",(0,a.jsx)(n.code,{children:"monitoring"})," namespace. So also make sure all the pods for ",(0,a.jsx)(n.code,{children:"monitoring"})," are also running."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n monitoring\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output-2",children:"Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"prometheus-grafana-85b4584dbf-4l72l                    3/3     Running   0          12d\nprometheus-kube-prometheus-operator-84dcddccfc-pv8nv   1/1     Running   0          12d\nprometheus-kube-state-metrics-57f6b6b4fd-txjtb         1/1     Running   0          12d\nprometheus-prometheus-kube-prometheus-prometheus-0     2/2     Running   0          4d3h\nprometheus-prometheus-node-exporter-4jh8q              1/1     Running   0          12d\nprometheus-prometheus-node-exporter-f5znb              1/1     Running   0          12d\nprometheus-prometheus-node-exporter-f9xrz              1/1     Running   0          12d\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now lets access Apache Pinot Console using the below command. Console consist of ",(0,a.jsx)(n.strong,{children:"Cluster Manager"}),", ",(0,a.jsx)(n.strong,{children:"Query Explorer"}),", ",(0,a.jsx)(n.strong,{children:"Zookeeper Browser"})," and ",(0,a.jsx)(n.strong,{children:"Swagger REST API Explorer"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/pinot-controller 9000:9000 -n pinot\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This will allow you to access Apache Pinot Console like the one shown below using ",(0,a.jsx)(n.code,{children:"http://localhost:9000"})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Apache Pinot Web Console",src:t(7960).A+"",width:"1627",height:"1098"})}),"\n",(0,a.jsxs)(n.p,{children:["Apache Pinot supports exporting metrics using Prometheus JMX exporter that is packaged within the Apache Pinot docker image. Lets ensure metrics from all Apache Pinot components are getting published to ",(0,a.jsx)(n.code,{children:"prometheus"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/prometheus-kube-prometheus-prometheus 9090:9090 -n monitoring\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Navigate to the prometheus UI at ",(0,a.jsx)(n.code,{children:"http://localhost:9090"}),", type ",(0,a.jsx)(n.code,{children:"pinot"})," in the search box and you should be able to see all the metrics."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Prometheus",src:t(1518).A+"",width:"1650",height:"675"})}),"\n",(0,a.jsx)(n.p,{children:"Next, Let's use Grafana to visualize the Apache Pinot metrics. In order to access Grafana, we need to get the grafana password from AWS Secrets Manager."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"aws secretsmanager get-secret-value --secret-id pinot-on-eks-grafana | jq '.SecretString' --raw-output\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now use the port-forwarding to access Grafana at port ",(0,a.jsx)(n.code,{children:"8080"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/prometheus-grafana 8080:80 -n monitoring\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Login to grafana dashboard using ",(0,a.jsx)(n.code,{children:"admin"})," and password retrieved in the previous step and then navigate to Dashboard and click New and then Import. Use the file ",(0,a.jsx)(n.code,{children:"pinot.json"})," under ",(0,a.jsx)(n.code,{children:"data-on-eks/distributed-database/pinot/dashboard"})," to create a pinot dashboard."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Grafana Dashboard for Pinot",src:t(9776).A+"",width:"1650",height:"1268"})}),"\n",(0,a.jsxs)(n.p,{children:["To learn more about the monitoring of Apache Pinot using Prometheus and Grafana use the ",(0,a.jsx)(n.a,{href:"https://docs.pinot.apache.org/operators/tutorials/monitor-pinot-using-prometheus-and-grafana",children:"official guide"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"additional-deployment-optional-",children:"Additional Deployment (Optional) \ud83c\udfc6"}),"\n",(0,a.jsx)(n.h3,{id:"deploy-apache-kafka-for-streaming-data",children:"Deploy Apache Kafka for Streaming Data"}),"\n",(0,a.jsxs)(n.p,{children:["Apache Pinot can ingest data from streaming data sources (real-time) as well as batch data sources (offline). In this example, we will leverage ",(0,a.jsx)(n.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"})," to push real-time data to a topic."]}),"\n",(0,a.jsx)(n.p,{children:"If you already have Apache Kafka running in your EKS cluster or you are leveraging Amazon Managed Streaming for Apache Kafka (MSK) you can skip this step. Otherwise, follow the steps below to install Kafka in your EKS cluster."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Note: Following deployment configure Kafka Brokers with PLAINTEXT listeners for simplified deployment. Modify the ",(0,a.jsx)(n.code,{children:"kafka-values.yaml"})," file for production deployment"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install -n pinot pinot-kafka bitnami/kafka --values ./helm/kafka-values.yaml\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output-3",children:"Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"NAME: pinot-kafka\nLAST DEPLOYED: Tue Oct 24 01:10:25 2023\nNAMESPACE: pinot\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: kafka\nCHART VERSION: 26.2.0\nAPP VERSION: 3.6.0\n\n** Please be patient while the chart is being deployed **\n\nKafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:\n\n    pinot-kafka.pinot.svc.cluster.local\n\nEach Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:\n\n    pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092\n    pinot-kafka-controller-1.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092\n    pinot-kafka-controller-2.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092\n\nTo create a pod that you can use as a Kafka client run the following commands:\n\n    kubectl run pinot-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.6.0-debian-11-r0 --namespace pinot --command -- sleep infinity\n    kubectl exec --tty -i pinot-kafka-client --namespace pinot -- bash\n\n    PRODUCER:\n        kafka-console-producer.sh \\\n            --broker-list pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092,pinot-kafka-controller-1.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092,pinot-kafka-controller-2.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 \\\n            --topic test\n\n    CONSUMER:\n        kafka-console-consumer.sh \\\n            --bootstrap-server pinot-kafka.pinot.svc.cluster.local:9092 \\\n            --topic test \\\n            --from-beginning\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Use the command mentioned above to create ",(0,a.jsx)(n.strong,{children:"Kafka Client"})," pod within your namespace."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl run pinot-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.6.0-debian-11-r0 --namespace pinot --command -- sleep infinity\n"})}),"\n",(0,a.jsx)(n.p,{children:"and then attach to the container shell"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl exec --tty -i pinot-kafka-client --namespace pinot -- bash\n"})}),"\n",(0,a.jsx)(n.p,{children:"Create Kafka topics using the below commands, which will then be used to publish messages."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kafka-topics.sh --bootstrap-server pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 --topic flights-realtime --create --partitions 1 --replication-factor 1\n\nkafka-topics.sh --bootstrap-server pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 --topic flights-realtime-avro --create --partitions 1 --replication-factor 1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["and then ",(0,a.jsx)(n.code,{children:"exit"})," from the container shell"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"exit\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Use provided ",(0,a.jsx)(n.code,{children:"example/pinot-realtime-quickstart.yml"})," to create tables and publish sample data to the above topics, which will then get ingested into tables."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f example/pinot-realtime-quickstart.yml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now, let's navigate back to ",(0,a.jsx)(n.strong,{children:"Query Console"})," and then click one of the tables. You should be able to see the newly created tables and data coming into tables."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/pinot-controller 9000:9000 -n pinot\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Pinot Example",src:t(5057).A+"",width:"1861",height:"1131"})}),"\n",(0,a.jsx)(n.h2,{id:"cleanup-",children:"Cleanup \ud83e\uddf9"}),"\n",(0,a.jsx)(n.p,{children:"To delete all the components provisioned as part of this blueprint, using the following command to destroy all the resources."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"./cleanup.sh\n"})}),"\n",(0,a.jsxs)(n.admonition,{type:"caution",children:[(0,a.jsx)(n.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"}),(0,a.jsx)(n.p,{children:"ex. Delete kafka-on-eks EBS volumes"})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},9776:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/grafana-1b023edced86ce564abe30bdc66e1058.png"},7960:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/pinot-console-d384a9acdde6014e42e7420a9acee927.png"},5057:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/pinot-example-ddd630a3c986e79b4ee18e7d3274e782.png"},6424:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/pinot-on-eks-a79291223d852e1628ff508b31041ed3.png"},1518:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/prometheus-dfd8da6b8f9f675e3b5aec86726bcefe.png"},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const s={},o=a.createContext(s);function i(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);