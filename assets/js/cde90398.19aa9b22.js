"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[1940],{2179:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var s=i(4848),t=i(8453),r=i(2450);const a={title:"Mistral-7B on Inferentia2",sidebar_position:2},o="Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio",l={id:"gen-ai/inference/Neuron/Mistral-7b-inf2",title:"Mistral-7B on Inferentia2",description:"The AI on EKS content is being migrated to a new repository.",source:"@site/docs/gen-ai/inference/Neuron/Mistral-7b-inf2.md",sourceDirName:"gen-ai/inference/Neuron",slug:"/gen-ai/inference/Neuron/Mistral-7b-inf2",permalink:"/data-on-eks/docs/gen-ai/inference/Neuron/Mistral-7b-inf2",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/Neuron/Mistral-7b-inf2.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Mistral-7B on Inferentia2",sidebar_position:2},sidebar:"genai",previous:{title:"Llama-3-8B with vLLM on Inferentia2",permalink:"/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2"},next:{title:"Llama-3-8B on Inferentia2",permalink:"/data-on-eks/docs/gen-ai/inference/Neuron/llama3-inf2"}},c={},d=[{value:"What is Mistral-7B-Instruct-v0.2 Model?",id:"what-is-mistral-7b-instruct-v02-model",level:3},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Deploying the Ray Cluster with Mistral 7B Model",id:"deploying-the-ray-cluster-with-mistral-7b-model",level:2},{value:"Deploy the Mistral-7B-Instruct-v0.2 Model",id:"deploy-the-mistral-7b-instruct-v02-model",level:3},{value:"Deploying the Gradio WebUI App",id:"deploying-the-gradio-webui-app",level:2},{value:"Steps to Deploy a Gradio App:",id:"steps-to-deploy-a-gradio-app",level:4},{value:"Invoke the WebUI",id:"invoke-the-webui",level:4},{value:"Interaction With Mistral Model",id:"interaction-with-mistral-model",level:4},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"AI on EKS"})," content ",(0,s.jsx)(n.strong,{children:"is being migrated"})," to a new repository.\n\ud83d\udd17 \ud83d\udc49 ",(0,s.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement",children:"Read the full migration announcement \xbb"})]})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsxs)(n.p,{children:["Note: Mistral-7B-Instruct-v0.2 is a gated model in ",(0,s.jsx)(n.a,{href:"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",children:"Huggingface"})," repository. In order to use this model, one needs to use a HuggingFace Token.\nTo generate a token in HuggingFace, log in using your HuggingFace account and click on ",(0,s.jsx)(n.code,{children:"Access Tokens"})," menu item on the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"Settings"})," page."]})}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"serving-mistral-7b-instruct-v02-using-inferentia2-ray-serve-gradio",children:"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio"})}),"\n",(0,s.jsxs)(n.p,{children:["This pattern outlines the deployment of the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",children:"Mistral-7B-Instruct-v0.2"})," model on Amazon EKS, utilizing ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/inf2/",children:"AWS Inferentia2"})," for enhanced text generation performance. ",(0,s.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/index.html",children:"Ray Serve"})," ensures efficient scaling of Ray Worker nodes, while ",(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," dynamically manages the provisioning of AWS Inferentia2 nodes. This setup optimizes for high-performance and cost-effective text generation applications in a scalable cloud environment."]}),"\n",(0,s.jsx)(n.p,{children:"Through this pattern, you will accomplish the following:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Create an ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"})," cluster with a Karpenter managed AWS Inferentia2 nodepool for dynamic provisioning of Nodes."]}),"\n",(0,s.jsxs)(n.li,{children:["Install ",(0,s.jsx)(n.a,{href:"https://github.com/ray-project/kuberay",children:"KubeRay Operator"})," and other core EKS add-ons using the ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/ai-ml/trainium-inferentia",children:"trainium-inferentia"})," Terraform blueprint."]}),"\n",(0,s.jsxs)(n.li,{children:["Deploy the ",(0,s.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," model with RayServe for efficient scaling."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-mistral-7b-instruct-v02-model",children:"What is Mistral-7B-Instruct-v0.2 Model?"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"mistralai/Mistral-7B-Instruct-v0.2"})," is an instruction-tuned version of the ",(0,s.jsx)(n.code,{children:"Mistral-7B-v0.2 base model"}),", which has been fine-tuned using publicly available conversation datasets. It is designed to follow instructions and complete tasks, making it suitable for applications such as chatbots, virtual assistants, and task-oriented dialogue systems. It is built on top of the ",(0,s.jsx)(n.code,{children:"Mistral-7B-v0.2"})," base model, which has 7.3 billion parameters and employs a state-of-the-art architecture including Grouped-Query Attention (GQA) for faster inference and a Byte-fallback BPE tokenizer for improved robustness."]}),"\n",(0,s.jsxs)(n.p,{children:["Please refer to the ",(0,s.jsx)(n.a,{href:"https://replicate.com/mistralai/mistral-7b-instruct-v0.2/readme",children:"Model Card"})," for more detail."]}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-solution",children:"Deploying the Solution"}),"\n",(0,s.jsxs)(n.p,{children:["Let's get ",(0,s.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," model up and running on Amazon EKS! In this section, we'll cover:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Ensuring all necessary tools are installed before you begin."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Infrastructure Setup"}),": Creating your your EKS cluster and setting the stage for deployment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deploying the Ray Cluster"}),": The core of your image generation pipeline, providing scalability and efficiency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Building the Gradio Web UI"}),": Creating a user-friendly interface for seamless interaction with the Mistral 7B model."]}),"\n"]}),"\n",(0,s.jsxs)(r.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Prerequisites"})}),children:[(0,s.jsx)(n.p,{children:"Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free.\nEnsure that you have installed the following tools on your machine."}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,s.jsx)(n.p,{children:"Clone the repository"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,s.jsxs)(n.p,{children:["Navigate into one of the example directories and run ",(0,s.jsx)(n.code,{children:"install.sh"})," script"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Important Note:"})," Ensure that you update the region in the ",(0,s.jsx)(n.code,{children:"variables.tf"})," file before deploying the blueprint.\nAdditionally, confirm that your local region setting matches the specified region to prevent any discrepancies.\nFor example, set your ",(0,s.jsx)(n.code,{children:'export AWS_DEFAULT_REGION="<REGION>"'})," to the desired region:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/trainium-inferentia/\n./install.sh\n"})}),(0,s.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,s.jsx)(n.p,{children:"Verify the Amazon EKS Cluster"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 describe-cluster --name trainium-inferentia\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Creates k8s config file to authenticate with EKS\naws eks --region us-west-2 update-kubeconfig --name trainium-inferentia\n\n# Output shows the EKS Managed Node group nodes\nkubectl get nodes\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-ray-cluster-with-mistral-7b-model",children:"Deploying the Ray Cluster with Mistral 7B Model"}),"\n",(0,s.jsxs)(n.p,{children:["Once the ",(0,s.jsx)(n.code,{children:"trainium-inferentia"})," EKS cluster is deployed, you can proceed to use ",(0,s.jsx)(n.code,{children:"kubectl"})," to deploy the ",(0,s.jsx)(n.code,{children:"ray-service-mistral.yaml"})," from ",(0,s.jsx)(n.code,{children:"/data-on-eks/gen-ai/inference/mistral-7b-rayserve-inf2/"})," path."]}),"\n",(0,s.jsxs)(n.p,{children:["In this step, we will deploy the Ray Serve cluster, which comprises one ",(0,s.jsx)(n.code,{children:"Head Pod"})," on ",(0,s.jsx)(n.code,{children:"x86 CPU"})," instances using Karpenter autoscaling, as well as ",(0,s.jsx)(n.code,{children:"Ray workers"})," on ",(0,s.jsx)(n.code,{children:"inf2.24xlarge"})," instances, autoscaled by ",(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ray_serve_mistral.py:"}),"\nThis script sets up a FastAPI application with two main components deployed using Ray Serve, which enables scalable model serving on AWS Neuron infrastructure(Inf2):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"mistral-7b Deployment"}),": This class initializes the Mistral 7B model using a scheduler and moves it to an Inf2 node for processing. The script leverages Transformers Neuron support for grouped-query attention (GQA) models for this Mistral model. The ",(0,s.jsx)(n.code,{children:"mistral-7b-instruct-v0.2"})," is a chat based model. The script also adds the required prefix for instructions by adding ",(0,s.jsx)(n.code,{children:"[INST]"})," and ",(0,s.jsx)(n.code,{children:"[/INST]"})," tokens surrounding the actual prompt."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"APIIngress"}),": This FastAPI endpoint acts as an interface to the Mistral 7B model. It exposes a GET method on the ",(0,s.jsx)(n.code,{children:"/infer"})," path that takes a text prompt. It responds to the prompt by replying with a text."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ray-service-mistral.yaml:"}),"\nThis RayServe deployment pattern sets up a scalable service for hosting the Mistral-7B-Instruct-v0.2 Model model on Amazon EKS with AWS Inferentia2 support. It creates a dedicated namespace and configures a RayService with autoscaling capabilities to efficiently manage resource utilization based on incoming traffic. The deployment ensures that the model, served under the RayService umbrella, can automatically adjust replicas, depending on demand, with each replica requiring 2 neuron cores. This pattern makes use of custom container images designed to maximize performance and minimizes startup delays by ensuring that heavy dependencies are preloaded."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"deploy-the-mistral-7b-instruct-v02-model",children:"Deploy the Mistral-7B-Instruct-v0.2 Model"}),"\n",(0,s.jsx)(n.p,{children:"Ensure the cluster is configured locally"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Deploy RayServe Cluster"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["To deploy the Mistral-7B-Instruct-v0.2 model, it's essential to configure your Hugging Face Hub token as an environment variable. This token is required for authentication and accessing the model. For guidance on how to create and manage your Hugging Face tokens, please visit ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/security-tokens",children:"Hugging Face Token Management"}),"."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# set the Hugging Face Hub Token as an environment variable. This variable will be substituted when applying the ray-service-mistral.yaml file\n\nexport HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n\ncd data-on-eks/gen-ai/inference/mistral-7b-rayserve-inf2\nenvsubst < ray-service-mistral.yaml| kubectl apply -f -\n'})}),"\n",(0,s.jsx)(n.p,{children:"Verify the deployment by running the following commands"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface."})}),"\n",(0,s.jsxs)(n.p,{children:["This deployment establishes a Ray head pod running on an ",(0,s.jsx)(n.code,{children:"x86"})," instance and a worker pod on ",(0,s.jsx)(n.code,{children:"inf2.24xl"})," instance as shown below."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n mistral\n\nNAME                                                      READY   STATUS\nservice-raycluster-68tvp-worker-inf2-worker-group-2kckv   1/1     Running\nmistral-service-raycluster-68tvp-head-dmfz5               2/2     Running\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This deployment also sets up a mistral service with multiple ports configured; port ",(0,s.jsx)(n.code,{children:"8265"})," is designated for the Ray dashboard and port ",(0,s.jsx)(n.code,{children:"8000"})," for the Mistral model endpoint."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n mistral\n\nNAME                        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)\nmistral-service             NodePort   172.20.118.238   <none>        10001:30998/TCP,8000:32437/TCP,52365:31487/TCP,8080:30351/TCP,6379:30392/TCP,8265:30904/TCP\nmistral-service-head-svc    NodePort   172.20.245.131   <none>        6379:31478/TCP,8265:31393/TCP,10001:32627/TCP,8000:31251/TCP,52365:31492/TCP,8080:31471/TCP\nmistral-service-serve-svc   NodePort   172.20.109.223   <none>        8000:31679/TCP\n"})}),"\n",(0,s.jsx)(n.p,{children:"For the Ray dashboard, you can port-forward these ports individually to access the web UI locally using localhost."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n mistral port-forward svc/mistral-service 8265:8265\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Access the web UI via ",(0,s.jsx)(n.code,{children:"http://localhost:8265"})," . This interface displays the deployment of jobs and actors within the Ray ecosystem."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RayServe Deployment In Progress",src:i(3768).A+"",width:"1329",height:"757"})}),"\n",(0,s.jsxs)(n.p,{children:["Once the deployment is complete, the Controller and Proxy status should be ",(0,s.jsx)(n.code,{children:"HEALTHY"})," and Application status should be ",(0,s.jsx)(n.code,{children:"RUNNING"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RayServe Deployment Completed",src:i(4341).A+"",width:"1329",height:"757"})}),"\n",(0,s.jsx)(n.p,{children:"You can monitor Serve deployment and the Ray Cluster deployment including resource utilization using the Ray Dashboard."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RayServe Cluster",src:i(4940).A+"",width:"2660",height:"1450"})}),"\n",(0,s.jsx)(n.h2,{id:"deploying-the-gradio-webui-app",children:"Deploying the Gradio WebUI App"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.gradio.app/",children:"Gradio"})," Web UI is used to interact with the Mistral7b inference service deployed on EKS Clusters using inf2 instances.\nThe Gradio UI communicates internally with the mistral service(",(0,s.jsx)(n.code,{children:"mistral-serve-svc.mistral.svc.cluster.local:8000"}),"), which is exposed on port ",(0,s.jsx)(n.code,{children:"8000"}),", using its service name and port."]}),"\n",(0,s.jsxs)(n.p,{children:["We have created a base Docker(",(0,s.jsx)(n.code,{children:"gen-ai/inference/gradio-ui/Dockerfile-gradio-base"}),") image for the Gradio app, which can be used with any model inference.\nThis image is published on ",(0,s.jsx)(n.a,{href:"https://gallery.ecr.aws/data-on-eks/gradio-web-app-base",children:"Public ECR"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"steps-to-deploy-a-gradio-app",children:"Steps to Deploy a Gradio App:"}),"\n",(0,s.jsxs)(n.p,{children:["The following YAML script (",(0,s.jsx)(n.code,{children:"gen-ai/inference/mistral-7b-rayserve-inf2/gradio-ui.yaml"}),") creates a dedicated namespace, deployment, service, and a ConfigMap where your model client script goes."]}),"\n",(0,s.jsx)(n.p,{children:"To deploy this, execute:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/mistral-7b-rayserve-inf2/\nkubectl apply -f gradio-ui.yaml\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Verification Steps:"}),"\nRun the following commands to verify the deployment, service, and ConfigMap:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get deployments -n gradio-mistral7b-inf2\n\nkubectl get services -n gradio-mistral7b-inf2\n\nkubectl get configmaps -n gradio-mistral7b-inf2\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Port-Forward the Service:"})}),"\n",(0,s.jsx)(n.p,{children:"Run the port-forward command so that you can access the Web UI locally:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward service/gradio-service 7860:7860 -n gradio-mistral7b-inf2\n"})}),"\n",(0,s.jsx)(n.h4,{id:"invoke-the-webui",children:"Invoke the WebUI"}),"\n",(0,s.jsx)(n.p,{children:"Open your web browser and access the Gradio WebUI by navigating to the following URL:"}),"\n",(0,s.jsxs)(n.p,{children:["Running on local URL:  ",(0,s.jsx)(n.a,{href:"http://localhost:7860",children:"http://localhost:7860"})]}),"\n",(0,s.jsx)(n.p,{children:"You should now be able to interact with the Gradio application from your local machine."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Gradio WebUI",src:i(5257).A+"",width:"2524",height:"1312"})}),"\n",(0,s.jsx)(n.h4,{id:"interaction-with-mistral-model",children:"Interaction With Mistral Model"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"Mistral-7B-Instruct-v0.2"})," Model can be used for purposes such as chat applications (Q&A, conversation), text generation, knowledge retrieval and others."]}),"\n",(0,s.jsx)(n.p,{children:"Below screenshots provide some examples of the model response based on different text prompts."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Gradio QA",src:i(8214).A+"",width:"2524",height:"1312"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Gradio Convo 1",src:i(2761).A+"",width:"2524",height:"1312"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Gradio Convo 2",src:i(4578).A+"",width:"2524",height:"1312"})}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step1:"})," Delete Gradio App and mistral Inference deployment"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/gen-ai/inference/mistral-7b-rayserve-inf2\nkubectl delete -f gradio-ui.yaml\nkubectl delete -f ray-service-mistral.yaml\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step2:"})," Cleanup the EKS Cluster\nThis script will cleanup the environment using ",(0,s.jsx)(n.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/trainium-inferentia/\n./cleanup.sh\n"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},2450:(e,n,i)=>{i.d(n,{A:()=>g});var s=i(6540),t=i(5556),r=i.n(t),a=i(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=i(4848);function u(e){let{children:n,header:i}=e;const[t,r]=(0,s.useState)(!1);return(0,p.jsxs)("div",{className:o,children:[(0,p.jsxs)("div",{className:(0,a.A)(l,{[h]:t}),onClick:()=>{r(!t)},children:[i,(0,p.jsx)("span",{className:(0,a.A)(c,{[h]:t}),children:t?"\ud83d\udc47":"\ud83d\udc48"})]}),t&&(0,p.jsx)("div",{className:d,children:n})]})}u.propTypes={children:r().node.isRequired,header:r().node.isRequired};const g=u},2761:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/mistral-conv-1-f0b03174f6db9faa610237bfce16bff9.png"},4578:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/mistral-conv-2-0ee154a5215a286f80bb37bb2c90d122.png"},5257:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/mistral-gradio-d91ad3fa28f7a1f904753fee4d4cd9b7.png"},8214:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/mistral-sample-prompt-1-77ebe43e4bed9c34e4359afb43c0e017.png"},4341:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ray-dashboard-deployed-mistral-inf2-4008cfe69e6cbfa994a42339bdebb864.png"},3768:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ray-dashboard-deploying-mistral-inf2-67ca2576deab67be936e1031e2e5d716.png"},4940:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ray-serve-inf2-mistral-cluster-3b63e4150bd3e9200d161ab9a2adaeca.png"},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);