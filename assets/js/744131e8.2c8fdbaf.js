"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[7740],{7920:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>l});var s=r(4848),t=r(8453);const a={title:"Spark Streaming from Kafka in EKS",sidebar_position:6},o=void 0,i={id:"blueprints/streaming-platforms/spark-streaming",title:"Spark Streaming from Kafka in EKS",description:"DEPRECATION NOTICE",source:"@site/docs/blueprints/streaming-platforms/spark-streaming.md",sourceDirName:"blueprints/streaming-platforms",slug:"/blueprints/streaming-platforms/spark-streaming",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/streaming-platforms/spark-streaming.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{title:"Spark Streaming from Kafka in EKS",sidebar_position:6},sidebar:"blueprints",previous:{title:"Apache NiFi on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/nifi"},next:{title:"Job Schedulers on EKS",permalink:"/data-on-eks/docs/category/job-schedulers-on-eks"}},c={},l=[{value:"Deploy the EKS Cluster with all the add-ons and infrastructure needed to test this example",id:"deploy-the-eks-cluster-with-all-the-add-ons-and-infrastructure-needed-to-test-this-example",level:2},{value:"Clone the repository",id:"clone-the-repository",level:3},{value:"Initialize Terraform",id:"initialize-terraform",level:3},{value:"Export Terraform Outputs",id:"export-terraform-outputs",level:3},{value:"Update kubeconfig",id:"update-kubeconfig",level:3},{value:"Configuring Producer",id:"configuring-producer",level:3},{value:"Configuring Consumer",id:"configuring-consumer",level:3},{value:"Deploy Producer and Consumer",id:"deploy-producer-and-consumer",level:3},{value:"Checking Producer to MSK",id:"checking-producer-to-msk",level:4},{value:"Checking Spark Streaming application with Spark Operator",id:"checking-spark-streaming-application-with-spark-operator",level:4},{value:"Verify Data Flow",id:"verify-data-flow",level:3},{value:"Using the <code>s3_automation</code> Script",id:"using-the-s3_automation-script",level:4},{value:"Tuning the Producer and Consumer for Better Performance",id:"tuning-the-producer-and-consumer-for-better-performance",level:2},{value:"Adjusting the Number of Producer Replicas",id:"adjusting-the-number-of-producer-replicas",level:3},{value:"Tuning Spark Executors for Better Ingestion Performance",id:"tuning-spark-executors-for-better-ingestion-performance",level:3},{value:"Verify and Monitor",id:"verify-and-monitor",level:3},{value:"Summary",id:"summary",level:3},{value:"Cleaning Up Producer and Consumer Resources",id:"cleaning-up-producer-and-consumer-resources",level:3},{value:"Restoring <code>.yaml</code> Files from <code>.bak</code>",id:"restoring-yaml-files-from-bak",level:3},{value:"Destroy the EKS Cluster and Resources",id:"destroy-the-eks-cluster-and-resources",level:3}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.admonition,{type:"danger",children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"DEPRECATION NOTICE"})}),(0,s.jsxs)(n.p,{children:["This blueprint will be deprecated and eventually removed from this GitHub repository on ",(0,s.jsx)(n.strong,{children:"October 27, 2024"}),". No bugs will be fixed, and no new features will be added. The decision to deprecate is based on the lack of demand and interest in this blueprint, as well as the difficulty in allocating resources to maintain a blueprint that is not actively used by any users or customers."]}),(0,s.jsxs)(n.p,{children:["If you are using this blueprint in production, please add yourself to the ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/ADOPTERS.md",children:"adopters.md"})," page and raise an issue in the repository. This will help us reconsider and possibly retain and continue to maintain the blueprint. Otherwise, you can make a local copy or use existing tags to access it."]})]}),"\n",(0,s.jsx)(n.p,{children:"This example showcases the usage of Spark Operator to create a producer and consumer stack using Kafka (Amazon MSK). The main idea is to show Spark Streaming working with Kafka, persisting data in Parquet format using Apache Iceberg."}),"\n",(0,s.jsx)(n.h2,{id:"deploy-the-eks-cluster-with-all-the-add-ons-and-infrastructure-needed-to-test-this-example",children:"Deploy the EKS Cluster with all the add-ons and infrastructure needed to test this example"}),"\n",(0,s.jsx)(n.h3,{id:"clone-the-repository",children:"Clone the repository"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),"\n",(0,s.jsx)(n.h3,{id:"initialize-terraform",children:"Initialize Terraform"}),"\n",(0,s.jsxs)(n.p,{children:["Navigate into the example directory and run the initialization script ",(0,s.jsx)(n.code,{children:"install.sh"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/streaming/spark-streaming/terraform/\n./install.sh\n"})}),"\n",(0,s.jsx)(n.h3,{id:"export-terraform-outputs",children:"Export Terraform Outputs"}),"\n",(0,s.jsxs)(n.p,{children:["After the Terraform script finishes, export the necessary variables to use them in the ",(0,s.jsx)(n.code,{children:"sed"})," commands."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export CLUSTER_NAME=$(terraform output -raw cluster_name)\nexport PRODUCER_ROLE_ARN=$(terraform output -raw producer_iam_role_arn)\nexport CONSUMER_ROLE_ARN=$(terraform output -raw consumer_iam_role_arn)\nexport MSK_BROKERS=$(terraform output -raw bootstrap_brokers)\nexport REGION=$(terraform output -raw s3_bucket_region_spark_history_server)\nexport ICEBERG_BUCKET=$(terraform output -raw s3_bucket_id_iceberg_bucket)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"update-kubeconfig",children:"Update kubeconfig"}),"\n",(0,s.jsx)(n.p,{children:"Update the kubeconfig to verify the deployment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region $REGION update-kubeconfig --name $CLUSTER_NAME\nkubectl get nodes\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuring-producer",children:"Configuring Producer"}),"\n",(0,s.jsxs)(n.p,{children:["In order to deploy the producer, update the ",(0,s.jsx)(n.code,{children:"examples/producer/00_deployment.yaml"})," manifest with the variables exported from Terraform."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Apply `sed` commands to replace placeholders in the producer manifest\nsed -i.bak -e "s|__MY_PRODUCER_ROLE_ARN__|$PRODUCER_ROLE_ARN|g" \\\n           -e "s|__MY_AWS_REGION__|$REGION|g" \\\n           -e "s|__MY_KAFKA_BROKERS__|$MSK_BROKERS|g" \\\n           ../examples/producer/00_deployment.yaml\n\n# Apply sed to delete topic manifest, this can be used to delete kafka topic and start the stack once again\nsed -i.bak -e "s|__MY_KAFKA_BROKERS__|$MSK_BROKERS|g" \\\n           ../examples/producer/01_delete_topic.yaml\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configuring-consumer",children:"Configuring Consumer"}),"\n",(0,s.jsxs)(n.p,{children:["In order to deploy the Spark consumer, update the ",(0,s.jsx)(n.code,{children:"examples/consumer/manifests/01_spark_application.yaml"})," manifests with the variables exported from Terraform."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Apply `sed` commands to replace placeholders in the consumer Spark application manifest\nsed -i.bak -e "s|__MY_BUCKET_NAME__|$ICEBERG_BUCKET|g" \\\n           -e "s|__MY_KAFKA_BROKERS_ADRESS__|$MSK_BROKERS|g" \\\n           ../examples/consumer/manifests/01_spark_application.yaml\n'})}),"\n",(0,s.jsx)(n.h3,{id:"deploy-producer-and-consumer",children:"Deploy Producer and Consumer"}),"\n",(0,s.jsx)(n.p,{children:"After configuring the producer and consumer manifests, deploy them using kubectl."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Producer\nkubectl apply -f ../examples/producer/00_deployment.yaml\n\n# Deploy Consumer\nkubectl apply -f ../examples/consumer/manifests/\n"})}),"\n",(0,s.jsx)(n.h4,{id:"checking-producer-to-msk",children:"Checking Producer to MSK"}),"\n",(0,s.jsx)(n.p,{children:"First, let's see the producer logs to verify data is being created and flowing into MSK:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs $(kubectl get pods -l app=producer -oname) -f\n"})}),"\n",(0,s.jsx)(n.h4,{id:"checking-spark-streaming-application-with-spark-operator",children:"Checking Spark Streaming application with Spark Operator"}),"\n",(0,s.jsxs)(n.p,{children:["For the consumer, we first need to get the ",(0,s.jsx)(n.code,{children:"SparkApplication"})," that generates the ",(0,s.jsx)(n.code,{children:"spark-submit"})," command to Spark Operator to create driver and executor pods based on the YAML configuration:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get SparkApplication -n spark-operator\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You should see the ",(0,s.jsx)(n.code,{children:"STATUS"})," equals ",(0,s.jsx)(n.code,{children:"RUNNING"}),", now let's verify the driver and executors pods:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n spark-operator\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should see an output like below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"NAME                                     READY   STATUS      RESTARTS   AGE\nkafkatoiceberg-1e9a438f4eeedfbb-exec-1   1/1     Running     0          7m15s\nkafkatoiceberg-1e9a438f4eeedfbb-exec-2   1/1     Running     0          7m14s\nkafkatoiceberg-1e9a438f4eeedfbb-exec-3   1/1     Running     0          7m14s\nspark-consumer-driver                    1/1     Running     0          9m\nspark-operator-9448b5c6d-d2ksp           1/1     Running     0          117m\nspark-operator-webhook-init-psm4x        0/1     Completed   0          117m\n"})}),"\n",(0,s.jsxs)(n.p,{children:["We have ",(0,s.jsx)(n.code,{children:"1 driver"})," and ",(0,s.jsx)(n.code,{children:"3 executors"})," pods. Now, let's check the driver logs:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs pod/spark-consumer-driver -n spark-operator\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You should see only ",(0,s.jsx)(n.code,{children:"INFO"})," logs indicating that the job is running."]}),"\n",(0,s.jsx)(n.h3,{id:"verify-data-flow",children:"Verify Data Flow"}),"\n",(0,s.jsxs)(n.p,{children:["After deploying both the producer and consumer, verify the data flow by checking the consumer application's output in the S3 bucket. You can run the ",(0,s.jsx)(n.code,{children:"s3_automation"})," script to get a live view of the data size in your S3 bucket."]}),"\n",(0,s.jsx)(n.p,{children:"Follow these steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:["Navigate to the ",(0,s.jsx)(n.code,{children:"s3_automation"})," directory"]}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ../examples/s3_automation/\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:["Run the ",(0,s.jsx)(n.code,{children:"s3_automation"})," script"]}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python app.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"This script will continuously monitor and display the total size of your S3 bucket, giving you a real-time view of data being ingested. You can choose to view the bucket size or delete specific directories as needed."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.h4,{id:"using-the-s3_automation-script",children:["Using the ",(0,s.jsx)(n.code,{children:"s3_automation"})," Script"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"s3_automation"})," script offers two primary functions:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Check Bucket Size"}),": Continuously monitor and display the total size of your S3 bucket."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Delete Directory"}),": Delete specific directories within your S3 bucket."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here's how to use these functions:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Check Bucket Size"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["When prompted, enter ",(0,s.jsx)(n.code,{children:"size"})," to get the current size of your bucket in megabytes (MB)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Delete Directory"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["When prompted, enter ",(0,s.jsx)(n.code,{children:"delete"})," and then provide the directory prefix you wish to delete (e.g., ",(0,s.jsx)(n.code,{children:"myfolder/"}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tuning-the-producer-and-consumer-for-better-performance",children:"Tuning the Producer and Consumer for Better Performance"}),"\n",(0,s.jsx)(n.p,{children:"After deploying the producer and consumer, you can further optimize the data ingestion and processing by adjusting the number of replicas for the producer and the executor configuration for the Spark application. Here are some suggestions to get you started:"}),"\n",(0,s.jsx)(n.h3,{id:"adjusting-the-number-of-producer-replicas",children:"Adjusting the Number of Producer Replicas"}),"\n",(0,s.jsx)(n.p,{children:"You can increase the number of replicas of the producer deployment to handle a higher rate of message production. By default, the producer deployment is configured with a single replica. Increasing this number allows more instances of the producer to run concurrently, increasing the overall throughput."}),"\n",(0,s.jsxs)(n.p,{children:["To change the number of replicas, update the ",(0,s.jsx)(n.code,{children:"replicas"})," field in ",(0,s.jsx)(n.code,{children:"examples/producer/00_deployment.yaml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\n  replicas: 200  # Increase this number to scale up the producer\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can also adjust the environment variables to control the rate and volume of messages produced:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'env:\n  - name: RATE_PER_SECOND\n    value: "200000"  # Increase this value to produce more messages per second\n  - name: NUM_OF_MESSAGES\n    value: "20000000"  # Increase this value to produce more messages in total\n'})}),"\n",(0,s.jsx)(n.p,{children:"Apply the updated deployment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ../examples/producer/00_deployment.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"tuning-spark-executors-for-better-ingestion-performance",children:"Tuning Spark Executors for Better Ingestion Performance"}),"\n",(0,s.jsx)(n.p,{children:"To handle the increased data volume efficiently, you can add more executors to the Spark application or increase the resources allocated to each executor. This will allow the consumer to process data faster and reduce ingestion time."}),"\n",(0,s.jsxs)(n.p,{children:["To adjust the Spark executor configuration, update ",(0,s.jsx)(n.code,{children:"examples/consumer/manifests/01_spark_application.yaml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'spec:\n  dynamicAllocation:\n    enabled: true\n    initialExecutors: 5\n    minExecutors: 5\n    maxExecutors: 50  # Increase this number to allow more executors\n  executor:\n    cores: 4  # Increase CPU allocation\n    memory: "8g"  # Increase memory allocation\n'})}),"\n",(0,s.jsx)(n.p,{children:"Apply the updated Spark application:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ../examples/consumer/manifests/01_spark_application.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"verify-and-monitor",children:"Verify and Monitor"}),"\n",(0,s.jsx)(n.p,{children:"After making these changes, monitor the logs and metrics to ensure the system is performing as expected. You can check the producer logs to verify data production and the consumer logs to verify data ingestion and processing."}),"\n",(0,s.jsx)(n.p,{children:"To check producer logs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs $(kubectl get pods -l app=producer -oname) -f\n"})}),"\n",(0,s.jsx)(n.p,{children:"To check consumer logs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs pod/spark-consumer-driver -n spark-operator\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Can use verify dataflow script again"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"By adjusting the number of producer replicas and tuning the Spark executor settings, you can optimize the performance of your data pipeline. This allows you to handle higher ingestion rates and process data more efficiently, ensuring that your Spark Streaming application can keep up with the increased data volume from Kafka."}),"\n",(0,s.jsx)(n.p,{children:"Feel free to experiment with these settings to find the optimal configuration for your workload. Happy streaming!"}),"\n",(0,s.jsx)(n.h3,{id:"cleaning-up-producer-and-consumer-resources",children:"Cleaning Up Producer and Consumer Resources"}),"\n",(0,s.jsx)(n.p,{children:"To clean up only the producer and consumer resources, use the following commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Clean up Producer resources\nkubectl delete -f ../examples/producer/00_deployment.yaml\n\n# Clean up Consumer resources\nkubectl delete -f ../examples/consumer/manifests/\n"})}),"\n",(0,s.jsxs)(n.h3,{id:"restoring-yaml-files-from-bak",children:["Restoring ",(0,s.jsx)(n.code,{children:".yaml"})," Files from ",(0,s.jsx)(n.code,{children:".bak"})]}),"\n",(0,s.jsxs)(n.p,{children:["If you need to reset the ",(0,s.jsx)(n.code,{children:".yaml"})," files to their original state with placeholders, move the ",(0,s.jsx)(n.code,{children:".bak"})," files back to ",(0,s.jsx)(n.code,{children:".yaml"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Restore Producer manifest\nmv ../examples/producer/00_deployment.yaml.bak ../examples/producer/00_deployment.yaml\n\n\n# Restore Consumer Spark application manifest\nmv ../examples/consumer/manifests/01_spark_application.yaml.bak ../examples/consumer/manifests/01_spark_application.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"destroy-the-eks-cluster-and-resources",children:"Destroy the EKS Cluster and Resources"}),"\n",(0,s.jsx)(n.p,{children:"To clean up the entire EKS cluster and associated resources:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/streaming/spark-streaming/terraform/\nterraform destroy\n"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var s=r(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);