"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3476],{9916:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var t=i(4848),a=i(8453),s=i(2450);const r={sidebar_position:1,sidebar_label:"Llama-3 with RayTrain on Trn1"},o="Llama3 fine-tuning on Trn1 with HuggingFace Optimum Neuron",l={id:"gen-ai/training/Neuron/Llama-LoRA-Finetuning",title:"Llama-LoRA-Finetuning",description:"The AI on EKS content is being migrated to a new repository.",source:"@site/docs/gen-ai/training/Neuron/Llama-LoRA-Finetuning.md",sourceDirName:"gen-ai/training/Neuron",slug:"/gen-ai/training/Neuron/Llama-LoRA-Finetuning",permalink:"/data-on-eks/docs/gen-ai/training/Neuron/Llama-LoRA-Finetuning",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/training/Neuron/Llama-LoRA-Finetuning.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,sidebar_label:"Llama-3 with RayTrain on Trn1"},sidebar:"genai",previous:{title:"BioNeMo on EKS",permalink:"/data-on-eks/docs/gen-ai/training/GPUs/bionemo"},next:{title:"Llama-2 with RayTrain on Trn1",permalink:"/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2"}},c={},d=[{value:"What is Llama-3?",id:"what-is-llama-3",level:3},{value:"AWS Trainium:",id:"aws-trainium",level:4},{value:"1. Deploying the Solution",id:"1-deploying-the-solution",level:2},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"2. Build the Docker Image (Optional Step)",id:"2-build-the-docker-image-optional-step",level:2},{value:"3. Launch the Llama training pod",id:"3-launch-the-llama-training-pod",level:2},{value:"4. Launch LoRA fine-tuning",id:"4-launch-lora-fine-tuning",level:2},{value:"Cleaning up",id:"cleaning-up",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"AI on EKS"})," content ",(0,t.jsx)(n.strong,{children:"is being migrated"})," to a new repository.\n\ud83d\udd17 \ud83d\udc49 ",(0,t.jsx)(n.a,{href:"https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement",children:"Read the full migration announcement \xbb"})]})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,t.jsx)(n.admonition,{type:"danger",children:(0,t.jsxs)(n.p,{children:["Note: Use of this Llama-3 model is governed by the Meta license.\nIn order to download the model weights and tokenizer, please visit the ",(0,t.jsx)(n.a,{href:"https://ai.meta.com/",children:"website"})," and accept the license before requesting access."]})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects."})}),"\n",(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"llama3-fine-tuning-on-trn1-with-huggingface-optimum-neuron",children:"Llama3 fine-tuning on Trn1 with HuggingFace Optimum Neuron"})}),"\n",(0,t.jsxs)(n.p,{children:["This comprehensive guide walks you through the steps for fine-tuning the ",(0,t.jsx)(n.code,{children:"Llama3-8B"})," language model using AWS Trainium (Trn1) EC2 instances. The fine-tuning process is facilitated by HuggingFace Optimum Neuron, a powerful library that simplifies the integration of Neuron into your training pipeline."]}),"\n",(0,t.jsx)(n.h3,{id:"what-is-llama-3",children:"What is Llama-3?"}),"\n",(0,t.jsx)(n.p,{children:"Llama-3 is a state-of-the-art large language model (LLM) designed for various natural language processing (NLP) tasks, including text generation, summarization, translation, question answering, and more. It's a powerful tool that can be fine-tuned for specific use cases."}),"\n",(0,t.jsx)(n.h4,{id:"aws-trainium",children:"AWS Trainium:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimized for Deep Learning"}),": AWS Trainium-based Trn1 instances are specifically designed for deep learning workloads. They offer high throughput and low latency, making them ideal for training large-scale models like Llama-3. Trainium chips provide significant performance improvements over traditional processors, accelerating training times."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Neuron SDK"}),": The AWS Neuron SDK is tailored to optimize your deep learning models for Trainium. It includes features like advanced compiler optimizations and support for mixed precision training, which can further accelerate your training workloads while maintaining accuracy."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"1-deploying-the-solution",children:"1. Deploying the Solution"}),"\n",(0,t.jsxs)(s.A,{header:(0,t.jsx)(n.h2,{children:(0,t.jsx)(n.span,{children:"Prerequisites"})}),children:[(0,t.jsx)(n.p,{children:"Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free.\nEnsure that you have installed the following tools on your EC2 instance."}),(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html",children:"EC2 Instance"})," \u2192 Ensure you have 100GB+ of storage for both options. This is crucial for creating a Docker image with x86 architecture and having the right amount of storage."]}),"\n"]}),(0,t.jsx)(n.p,{children:"If you are using a local Windows machine or Mac, ensure you have Docker installed locally with builder storage above 100GB and the image is created with x86 architecture."})]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"AWS CLI"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,t.jsxs)(n.p,{children:["To install all the pre-reqs on EC2, you can run this ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/blob/main/ai-ml/trainium-inferentia/examples/llama2/install-pre-requsites-for-ec2.sh",children:"script"})," which is compatible with Amazon Linux 2023."]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Clone the Data on EKS repository"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Navigate to the trainium-inferentia directory."})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/ai-ml/trainium-inferentia\n"})}),(0,t.jsx)(n.p,{children:"Let's run the below export commands to set environment variables."}),(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"NOTE:"})," Trainium instances are available in select regions, and the user can determine this list of regions using the commands outlined ",(0,t.jsx)(n.a,{href:"https://repost.aws/articles/ARmXIF-XS3RO27p0Pd1dVZXQ/what-regions-have-aws-inferentia-and-trainium-instances",children:"here"})," on re",":Post","."]})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Enable FSx for Lustre, which will mount pre-training data to all pods across multiple nodes\nexport TF_VAR_enable_fsx_for_lustre=true\n\n# Set the region according to your requirements. Check Trn1 instance availability in the specified region.\nexport TF_VAR_region=us-west-2\n\n# Note: This configuration will create two new Trn1 32xl instances. Ensure you validate the associated costs before proceeding.\nexport TF_VAR_trn1_32xl_min_size=1\nexport TF_VAR_trn1_32xl_desired_size=1\n"})}),(0,t.jsx)(n.p,{children:"Run the installation script to provision an EKS cluster with all the add-ons needed for the solution."}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,t.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,t.jsx)(n.p,{children:"Verify the Amazon EKS Cluster"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 describe-cluster --name trainium-inferentia\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Creates k8s config file to authenticate with EKS\naws eks --region us-west-2 update-kubeconfig --name trainium-inferentia\n\nkubectl get nodes # Output shows the EKS Managed Node group nodes\n"})})]}),"\n",(0,t.jsx)(n.h2,{id:"2-build-the-docker-image-optional-step",children:"2. Build the Docker Image (Optional Step)"}),"\n",(0,t.jsxs)(n.p,{children:["To simplify the blueprint deployment, we have already built the Docker image and made it available under the public ECR. If you want to customize the Docker image, you can update the ",(0,t.jsx)(n.code,{children:"Dockerfile"})," and follow the optional step to build the Docker image. Please note that you will also need to modify the YAML file, ",(0,t.jsx)(n.code,{children:"lora-finetune-pod.yaml"}),", with the newly created image using your own private ECR."]}),"\n",(0,t.jsx)(n.p,{children:"Execute the below commands after ensuring you are in the root folder of the data-on-eks repository."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd gen-ai/training/llama-lora-finetuning-trn1\n./build-container-image.sh\n"})}),"\n",(0,t.jsx)(n.p,{children:"After running this script, note the Docker image URL and tag that are produced.\nYou will need this information for the next step."}),"\n",(0,t.jsx)(n.h2,{id:"3-launch-the-llama-training-pod",children:"3. Launch the Llama training pod"}),"\n",(0,t.jsxs)(n.p,{children:["If you skip step 2, you don't need to modify the YAML file.\nYou can simply run the ",(0,t.jsx)(n.code,{children:"kubectl apply"})," command on the file, and it will use the public ECR image that we published."]}),"\n",(0,t.jsxs)(n.p,{children:["If you built a custom Docker image in ",(0,t.jsx)(n.strong,{children:"Step 2"}),", update the ",(0,t.jsx)(n.code,{children:"gen-ai/training/llama-lora-finetuning-trn1/lora-finetune-pod.yaml"})," file with the Docker image URL and tag obtained from the previous step."]}),"\n",(0,t.jsx)(n.p,{children:"Once you have updated the YAML file (if needed), run the following command to launch the pod in your EKS cluster:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f lora-finetune-pod.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Verify the Pod Status:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n\n"})}),"\n",(0,t.jsx)(n.h2,{id:"4-launch-lora-fine-tuning",children:"4. Launch LoRA fine-tuning"}),"\n",(0,t.jsx)(n.p,{children:"Once the pod is \u2018Running\u2019, connect to it using the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl exec -it lora-finetune-app -- /bin/bash\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Before running the launch script ",(0,t.jsx)(n.code,{children:"01__launch_training.sh"}),", you need to set an environment variable with your HuggingFace token. The access token is found under Settings \u2192 Access Tokens on the Hugging Face website."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export HF_TOKEN=<your-huggingface-token>\n\n./01__launch_training.sh\n"})}),"\n",(0,t.jsx)(n.p,{children:"Once the script is complete, you can verify the training progress by checking the logs of the training job."}),"\n",(0,t.jsxs)(n.p,{children:["Next, we need to consolidate the adapter shards and merge the model. For this we run the python script ",(0,t.jsx)(n.code,{children:"02__consolidate_adapter_shards_and_merge_model.py"})," by passing in the location of the checkpoint using the '-i' parameter and providing the location where you want to save the consolidated model using the '-o' parameter."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 ./02__consolidate_adapter_shards_and_merge_model.py -i /shared/finetuned_models/20250220_170215/checkpoint-250/ -o /shared/tuned_model/20250220_170215\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Once the script is complete, we can test the fine-tuned model by running the ",(0,t.jsx)(n.code,{children:"03__test_model.py"})," by passing in the location of the tuned model using the '-m' parameter."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./03__test_model.py -m /shared/tuned_model/20250220_170215\n"})}),"\n",(0,t.jsx)(n.p,{children:"You can exit from the interactive terminal of the pod once you are done testing the model."}),"\n",(0,t.jsx)(n.h3,{id:"cleaning-up",children:"Cleaning up"}),"\n",(0,t.jsx)(n.p,{children:"To remove the resources created using this solution, execute the below commands after ensuring you are in the root folder of the data-on-eks repository.:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Delete the Kubernetes Resources:\ncd gen-ai/training/llama-lora-finetuning-trn1\nkubectl delete -f lora-finetune-pod.yaml\n\n# Clean Up the EKS Cluster and Associated Resources:\ncd ../../../ai-ml/trainium-inferentia\n./cleanup.sh\n"})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},2450:(e,n,i)=>{i.d(n,{A:()=>g});var t=i(6540),a=i(5556),s=i.n(a),r=i(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=i(4848);function p(e){let{children:n,header:i}=e;const[a,s]=(0,t.useState)(!1);return(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("div",{className:(0,r.A)(l,{[h]:a}),onClick:()=>{s(!a)},children:[i,(0,u.jsx)("span",{className:(0,r.A)(c,{[h]:a}),children:a?"\ud83d\udc47":"\ud83d\udc48"})]}),a&&(0,u.jsx)("div",{className:d,children:n})]})}p.propTypes={children:s().node.isRequired,header:s().node.isRequired};const g=p},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);