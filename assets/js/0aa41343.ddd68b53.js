"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[5646],{6266:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var n=s(4848),r=s(8453),t=s(2450);const i={title:"Kafka on EKS",sidebar_position:4},o="Apache Kafka",l={id:"blueprints/streaming-platforms/kafka",title:"Kafka on EKS",description:"Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. This blueprint implements Kafka using KRaft (Kafka Raft) mode, a significant architectural improvement that eliminates the need for Zookeeper.",source:"@site/docs/blueprints/streaming-platforms/kafka.md",sourceDirName:"blueprints/streaming-platforms",slug:"/blueprints/streaming-platforms/kafka",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/kafka",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/streaming-platforms/kafka.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{title:"Kafka on EKS",sidebar_position:4},sidebar:"blueprints",previous:{title:"Flink Operator on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/flink"},next:{title:"Apache NiFi on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/nifi"}},c={},d=[{value:"Why and what&#39;s KRaft?",id:"why-and-whats-kraft",level:2},{value:"Strimzi for Apache Kafka",id:"strimzi-for-apache-kafka",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Amazon Managed Streaming for Apache Kafka (MSK)",id:"amazon-managed-streaming-for-apache-kafka-msk",level:3},{value:"Amazon Kinesis Data Streams (KDS)",id:"amazon-kinesis-data-streams-kds",level:3},{value:"Advantages to using EBS as persistent storage backend",id:"advantages-to-using-ebs-as-persistent-storage-backend",level:3},{value:"What EBS volumes should I use when self-managing Kafka on AWS?",id:"what-ebs-volumes-should-i-use-when-self-managing-kafka-on-aws",level:3},{value:"What about NVMe SSD Instance Storage for performance reasons?",id:"what-about-nvme-ssd-instance-storage-for-performance-reasons",level:3},{value:"Considerations and challenges with NVMe SSD Instance Storage",id:"considerations-and-challenges-with-nvme-ssd-instance-storage",level:4},{value:"So, when should you to consider to use Local Storage?",id:"so-when-should-you-to-consider-to-use-local-storage",level:4},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the deployment",id:"verify-the-deployment",level:2},{value:"Create kube config",id:"create-kube-config",level:3},{value:"Get nodes",id:"get-nodes",level:3},{value:"Deploy the Kafka cluster manifests",id:"deploy-the-kafka-cluster-manifests",level:2},{value:"Check nodes provisioned by Karpenter",id:"check-nodes-provisioned-by-karpenter",level:3},{value:"Verify Kafka Brokers and Controller",id:"verify-kafka-brokers-and-controller",level:3},{value:"Verify the running Kafka pods",id:"verify-the-running-kafka-pods",level:3},{value:"Create a kafka Topic",id:"create-a-kafka-topic",level:3},{value:"Execute sample Kafka Producer",id:"execute-sample-kafka-producer",level:3},{value:"Execute sample Kafka Consumer",id:"execute-sample-kafka-consumer",level:3},{value:"Kafka Producer and Consumer output",id:"kafka-producer-and-consumer-output",level:3},{value:"Login to Grafana",id:"login-to-grafana",level:3},{value:"Open Strimzi Kafka Dashboard",id:"open-strimzi-kafka-dashboard",level:3}];function h(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"apache-kafka",children:"Apache Kafka"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"})," is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. This blueprint implements Kafka using ",(0,n.jsx)(a.strong,{children:"KRaft (Kafka Raft) mode"}),", a significant architectural improvement that eliminates the need for Zookeeper."]}),"\n",(0,n.jsx)(a.h2,{id:"why-and-whats-kraft",children:"Why and what's KRaft?"}),"\n",(0,n.jsx)(a.p,{children:"KRaft mode simplifies Kafka deployments, enhances scalability, and improves overall system performance. By using a built-in consensus protocol, KRaft reduces operational complexity, potentially speeds up broker startup times, and allows for better handling of metadata operations. This architectural shift enables Kafka to manage larger clusters more efficiently, making it an attractive option for organizations looking to streamline their event streaming infrastructure and prepare for future scalability needs."}),"\n",(0,n.jsx)(a.h2,{id:"strimzi-for-apache-kafka",children:"Strimzi for Apache Kafka"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://strimzi.io/",children:"Strimzi"})," provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. Strimzi combines security and simple configuration to deploy and manage Kafka on Kubernetes using kubectl and/or GitOps based on the Operator Pattern."]}),"\n",(0,n.jsxs)(a.p,{children:["As of version ",(0,n.jsx)(a.code,{children:"0.32.0"}),", Strimzi provides full support for deploying Kafka clusters using KRaft, making it easier for organizations to leverage this new architecture. By using Strimzi, you can seamlessly deploy and manage Kafka clusters in KRaft mode on Kubernetes, taking advantage of its custom resource definitions (CRDs) and operators to handle the complexities of configuration and lifecycle management."]}),"\n",(0,n.jsx)(a.h2,{id:"architecture",children:"Architecture"}),"\n",(0,n.jsx)(a.admonition,{type:"info",children:(0,n.jsx)(a.p,{children:"Architecture diagram work in progress"})}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Managed Alternatives"})}),children:[(0,n.jsx)(a.h3,{id:"amazon-managed-streaming-for-apache-kafka-msk",children:"Amazon Managed Streaming for Apache Kafka (MSK)"}),(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/msk/",children:"Amazon Managed Streaming for Apache Kafka (Amazon MSK)"})," is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters. It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka. This means existing applications, tooling, and plugins from partners and the Apache Kafka community are supported. You can use Amazon MSK to create clusters that use any of the Apache Kafka versions listed under ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/msk/latest/developerguide/supported-kafka-versions.html",children:"Supported Apache Kafka versions"}),". Amazon MSK offers cluster-based or serverless deployment types."]}),(0,n.jsx)(a.h3,{id:"amazon-kinesis-data-streams-kds",children:"Amazon Kinesis Data Streams (KDS)"}),(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/kinesis/data-streams/",children:"Amazon Kinesis Data Streams (KDS)"})," allows users to collect and process large streams of data records in real time. You can create data-processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services. Kinesis Data Streams support your choice of stream processing framework including Kinesis Client Library (KCL), Apache Flink, and Apache Spark Streaming. It is serverless, and scales automatically."]})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Storage considerations when self-managing Kafka"})}),children:[(0,n.jsxs)(a.p,{children:["The most common resource bottlenecks for Kafka clusters are network throughput, storage throughput, and network throughput between brokers and the storage backend for brokers using network attached storage such as ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/ebs/",children:"Amazon Elastic Block Store (EBS)"}),"."]}),(0,n.jsx)(a.h3,{id:"advantages-to-using-ebs-as-persistent-storage-backend",children:"Advantages to using EBS as persistent storage backend"}),(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Improved flexibility and faster recovery:"})," Fault tolerance is commonly achieved via broker (server) replication within\nthe cluster and/or maintaining cross-AZ or region replicas. Since the lifecycle of EBS volumes is independent of\nKafka brokers, if a broker fails and needs to be replaced, the EBS volume attached to the failed broker can be reattached to a replacement broker. Most of the replicated data for the replacement broker is already available in the\nEBS volume, and does not need to be copied over the network from another broker. This avoids most of the\nreplication traffic required to bring the replacement broker up to speed with current operations."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Just in time scale up:"})," The characteristics of EBS volumes can be modified while they\u2019re in use. Broker storage can be\nautomatically scaled over time rather than provisioning storage for peak or adding additional brokers."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Optimized for frequently-accessed-throughput-intensive workloads:"})," Volume types such as st1 can be a good fit\nsince these volumes are offered at a relatively low cost, support a large 1 MiB I/O block size, max IOPS of\n500/volume, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB,\nand a maximum throughput of 500 MB/s per volume."]}),"\n"]}),(0,n.jsx)(a.h3,{id:"what-ebs-volumes-should-i-use-when-self-managing-kafka-on-aws",children:"What EBS volumes should I use when self-managing Kafka on AWS?"}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["General purpose SSD volume ",(0,n.jsx)(a.strong,{children:"gp3"})," with a balanced price and performance are widely used, and you can ",(0,n.jsx)(a.strong,{children:"independently"})," provision storage (up to 16TiB), IOPS (up to 16,000) and throughput (up to 1,000MiB/s)"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"st1"})," is a low-cost HDD option for frequently accessed and throughput intensive workloads with up to 500 IOPS and 500 MiB/s"]}),"\n",(0,n.jsxs)(a.li,{children:["For critical applications such as Zookeeper, provisioned IOPS volumes (",(0,n.jsx)(a.strong,{children:"io2 Block Express, io2"}),") provide higher durability"]}),"\n"]}),(0,n.jsx)(a.h3,{id:"what-about-nvme-ssd-instance-storage-for-performance-reasons",children:"What about NVMe SSD Instance Storage for performance reasons?"}),(0,n.jsx)(a.p,{children:"While EBS provides flexibility and ease of management, some high-performance use cases may benefit from using local NVMe SSD instance storage. This approach can offer significant performance improvements but comes with trade-offs in terms of data persistence and operational complexity."}),(0,n.jsx)(a.h4,{id:"considerations-and-challenges-with-nvme-ssd-instance-storage",children:"Considerations and challenges with NVMe SSD Instance Storage"}),(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Data Persistence:"})," Local storage is ephemeral. If an instance fails or is terminated, the data on that storage is lost. This requires careful consideration of your replication strategy and disaster recovery plans, especially if the cluster is big (hundreds of TBs of data)."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Cluster Upgrades:"})," Upgrading Kafka or EKS becomes more complex, as you need to ensure data is properly migrated or replicated before making changes to nodes with local storage."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Scaling Complexity:"})," Scaling the cluster may require data rebalancing, which can be more time-consuming and resource-intensive compared to using network-attached storage."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Instance Type Lock-in:"})," Your choice of instance types becomes more limited, as you need to select instances with appropriate local storage options."]}),"\n"]}),(0,n.jsx)(a.h4,{id:"so-when-should-you-to-consider-to-use-local-storage",children:"So, when should you to consider to use Local Storage?"}),(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"For extremely high-performance requirements where every millisecond of latency matters."}),"\n",(0,n.jsx)(a.li,{children:"When your use case can tolerate potential data loss on individual node failures, relying on Kafka's replication for data durability."}),"\n"]}),(0,n.jsx)(a.p,{children:"While local storage can offer performance benefits, it's important to carefully weigh these against the operational challenges, especially in a dynamic environment like EKS. For most use cases, we recommend starting with EBS storage for its flexibility and easier management, and only considering local storage for specific high-performance scenarios where the trade-offs are justified."})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Deploying the Solution"})}),children:[(0,n.jsxs)(a.p,{children:["In this ",(0,n.jsx)(a.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/streaming/kafka",children:"example"}),", you will provision the following resources to run Kafka Cluster on EKS."]}),(0,n.jsx)(a.p,{children:"This example deploys an EKS Cluster with Kafka into a new VPC."}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Creates a new sample VPC, 3 Private Subnets and 3 Public Subnets."}),"\n",(0,n.jsx)(a.li,{children:"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets."}),"\n",(0,n.jsx)(a.li,{children:"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with one managed node group."}),"\n",(0,n.jsx)(a.li,{children:"Deploys Metrics server, Karpenter, self-managed ebs-csi-driver, Strimzi Kafka Operator, Grafana Operator."}),"\n",(0,n.jsxs)(a.li,{children:["Strimzi Kafka Operator is a Kubernetes Operator for Apache Kafka deployed to ",(0,n.jsx)(a.code,{children:"strimzi-kafka-operator"})," namespace. The operator by default watches and handles ",(0,n.jsx)(a.code,{children:"kafka"})," in all namespaces."]}),"\n"]}),(0,n.jsx)(a.h3,{id:"prerequisites",children:"Prerequisites"}),(0,n.jsx)(a.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,n.jsx)(a.h3,{id:"deploy",children:"Deploy"}),(0,n.jsx)(a.p,{children:"Clone the repository:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,n.jsxs)(a.p,{children:["Navigate into one of the example directories and run ",(0,n.jsx)(a.code,{children:"install.sh"})," script:"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"cd data-on-eks/streaming/kafka\nchmod +x install.sh\n./install.sh\n"})}),(0,n.jsx)(a.admonition,{type:"info",children:(0,n.jsx)(a.p,{children:"This deployment may take between 20 to 30 mins."})}),(0,n.jsx)(a.h2,{id:"verify-the-deployment",children:"Verify the deployment"}),(0,n.jsx)(a.h3,{id:"create-kube-config",children:"Create kube config"}),(0,n.jsx)(a.p,{children:"Create kube config file."}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"aws eks --region $AWS_REGION update-kubeconfig --name kafka-on-eks\n"})}),(0,n.jsx)(a.h3,{id:"get-nodes",children:"Get nodes"}),(0,n.jsx)(a.p,{children:"Check if the deployment has created around 3 nodes for Core Node group:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,n.jsx)(a.p,{children:"You should see something similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME                                       STATUS   ROLES    AGE     VERSION\nip-10-1-0-193.eu-west-1.compute.internal   Ready    <none>   5h32m   v1.31.0-eks-a737599\nip-10-1-1-231.eu-west-1.compute.internal   Ready    <none>   5h32m   v1.31.0-eks-a737599\nip-10-1-2-20.eu-west-1.compute.internal    Ready    <none>   5h32m   v1.31.0-eks-a737599\n"})})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Create a Kafka cluster"})}),children:[(0,n.jsx)(a.h2,{id:"deploy-the-kafka-cluster-manifests",children:"Deploy the Kafka cluster manifests"}),(0,n.jsx)(a.p,{children:"Create a namespace dedicated to the Kafka cluster:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl create namespace kafka\n"})}),(0,n.jsx)(a.p,{children:"Deploy the Kafka cluster manifests:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl apply -f kafka-manifests/\n"})}),(0,n.jsx)(a.p,{children:"Deploy the Strimzi Kafka dashboards in Grafana:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl apply -f monitoring-manifests/\n"})}),(0,n.jsx)(a.h3,{id:"check-nodes-provisioned-by-karpenter",children:"Check nodes provisioned by Karpenter"}),(0,n.jsx)(a.p,{children:"Check if you now see around 9 nodes, 3 nodes for Core Node group and 6 for Kafka brokers across 3 AZs:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,n.jsx)(a.p,{children:"You should see something similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME                                       STATUS   ROLES    AGE     VERSION\nip-10-1-1-231.eu-west-1.compute.internal   Ready    <none>   5h32m   v1.31.0-eks-a737599\nip-10-1-2-20.eu-west-1.compute.internal    Ready    <none>   5h32m   v1.31.0-eks-a737599\nip-10-1-0-193.eu-west-1.compute.internal   Ready    <none>   5h32m   v1.31.0-eks-a737599\nip-10-1-0-151.eu-west-1.compute.internal   Ready    <none>   62m     v1.31.0-eks-5da6378\nip-10-1-0-175.eu-west-1.compute.internal   Ready    <none>   62m     v1.31.0-eks-5da6378\nip-10-1-1-104.eu-west-1.compute.internal   Ready    <none>   62m     v1.31.0-eks-5da6378\nip-10-1-1-106.eu-west-1.compute.internal   Ready    <none>   62m     v1.31.0-eks-5da6378\nip-10-1-2-4.eu-west-1.compute.internal     Ready    <none>   62m     v1.31.0-eks-5da6378\nip-10-1-2-56.eu-west-1.compute.internal    Ready    <none>   62m     v1.31.0-eks-5da6378\n"})}),(0,n.jsx)(a.h3,{id:"verify-kafka-brokers-and-controller",children:"Verify Kafka Brokers and Controller"}),(0,n.jsx)(a.p,{children:"Verify the Kafka Broker and Controller pods and the status created by the Strimzi Operator."}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get strimzipodsets.core.strimzi.io -n kafka\n"})}),(0,n.jsx)(a.p,{children:"You should see something similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME                 PODS   READY PODS   CURRENT PODS   AGE\ncluster-broker       3      3            3              64m\ncluster-controller   3      3            3              64m\n"})}),(0,n.jsx)(a.p,{children:"Let's confirm that you have created a Kafka cluster in KRaft mode:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get kafka.kafka.strimzi.io -n kafka\n"})}),(0,n.jsx)(a.p,{children:"You should see an output similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME      DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS   READY   METADATA STATE   WARNINGS\ncluster                                                  True    KRaft            True\n"})}),(0,n.jsx)(a.h3,{id:"verify-the-running-kafka-pods",children:"Verify the running Kafka pods"}),(0,n.jsx)(a.p,{children:"Let's confirm that the pods for the Kafka cluster are running:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get pods -n kafka\n"})}),(0,n.jsx)(a.p,{children:"You should see an output similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME                                      READY   STATUS    RESTARTS   AGE\ncluster-broker-0                          1/1     Running   0          24m\ncluster-broker-1                          1/1     Running   0          15m\ncluster-broker-2                          1/1     Running   0          8m31s\ncluster-controller-3                      1/1     Running   0          16m\ncluster-controller-4                      1/1     Running   0          7m8s\ncluster-controller-5                      1/1     Running   0          7m48s\ncluster-cruise-control-74f5977f48-l8pzp   1/1     Running   0          24m\ncluster-entity-operator-d46598d9c-xgwnh   2/2     Running   0          24m\ncluster-kafka-exporter-5ff5ff4675-2cz9m   1/1     Running   0          24m\n"})})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Create Kafka Topic and run Sample test"})}),children:[(0,n.jsx)(a.p,{children:"We will create one kafka topic and run sample producer script to produce new messages to the kafka topic."}),(0,n.jsx)(a.h3,{id:"create-a-kafka-topic",children:"Create a kafka Topic"}),(0,n.jsxs)(a.p,{children:["Run this command to create a new topic called ",(0,n.jsx)(a.code,{children:"test-topic"})," under ",(0,n.jsx)(a.code,{children:"kafka"})," namespace:"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl apply -f examples/kafka-topics.yaml\n"})}),(0,n.jsx)(a.p,{children:"Confirm that the topic has been created:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl get kafkatopic.kafka.strimzi.io -n kafka\n"})}),(0,n.jsx)(a.p,{children:"You should see an output similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"NAME         CLUSTER   PARTITIONS   REPLICATION FACTOR   READY\ntest-topic   cluster   12           3                    True\n"})}),(0,n.jsxs)(a.p,{children:["Verify the status of the ",(0,n.jsx)(a.code,{children:"test-topic"})," topic."]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'kubectl exec -it cluster-broker-0 -c kafka -n kafka -- /bin/bash -c "/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092"\n'})}),(0,n.jsx)(a.p,{children:"You should see an output similar to this:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"strimzi.cruisecontrol.metrics\nstrimzi.cruisecontrol.modeltrainingsamples\nstrimzi.cruisecontrol.partitionmetricsamples\ntest-topic\n"})}),(0,n.jsx)(a.h3,{id:"execute-sample-kafka-producer",children:"Execute sample Kafka Producer"}),(0,n.jsxs)(a.p,{children:["Open two terminals one for Kafka producer and one for Kafka Consumer. Execute the following command and press enter twice until you see the ",(0,n.jsx)(a.code,{children:">"})," prompt. Start typing some random content. This data will be written to the ",(0,n.jsx)(a.code,{children:"test-topic"}),"."]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --bootstrap-server cluster-kafka-bootstrap:9092 --topic test-topic\n"})}),(0,n.jsx)(a.h3,{id:"execute-sample-kafka-consumer",children:"Execute sample Kafka Consumer"}),(0,n.jsxs)(a.p,{children:["Now, you can verify the data written to ",(0,n.jsx)(a.code,{children:"test-topic"})," by running Kafka consumer pod in another terminal"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.43.0-kafka-3.8.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server cluster-kafka-bootstrap:9092 --topic test-topic --from-beginning\n"})}),(0,n.jsx)(a.h3,{id:"kafka-producer-and-consumer-output",children:"Kafka Producer and Consumer output"}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"img.png",src:s(5874).A+"",width:"1131",height:"440"})})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Grafana Dashboard for Kafka"})}),children:[(0,n.jsx)(a.h3,{id:"login-to-grafana",children:"Login to Grafana"}),(0,n.jsx)(a.p,{children:"Login to Grafana dashboard by running the following command."}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n kube-prometheus-stack\n"})}),(0,n.jsxs)(a.p,{children:["Open browser with local ",(0,n.jsx)(a.a,{href:"http://localhost:8080/",children:"Grafana Web UI"})]}),(0,n.jsxs)(a.p,{children:["Enter username as ",(0,n.jsx)(a.code,{children:"admin"})," and ",(0,n.jsx)(a.strong,{children:"password"})," can be extracted from AWS Secrets Manager with the below command."]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'aws secretsmanager get-secret-value --secret-id kafka-on-eks-grafana --region $AWS_REGION --query "SecretString" --output text\n'})}),(0,n.jsx)(a.h3,{id:"open-strimzi-kafka-dashboard",children:"Open Strimzi Kafka Dashboard"}),(0,n.jsxs)(a.p,{children:["Go to the ",(0,n.jsx)(a.code,{children:"Dashboards"})," page at ",(0,n.jsx)(a.code,{children:"http://localhost:8080/dashboards"}),", then click on ",(0,n.jsx)(a.code,{children:"General"}),", and then on ",(0,n.jsx)(a.code,{children:"Strimizi Kafka"}),"."]}),(0,n.jsx)(a.p,{children:"You should see the below builtin Kafka dashboards which was created during the deployment:"}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Kafka Brokers Dashboard",src:s(4982).A+"",width:"2876",height:"1570"})})]}),"\n",(0,n.jsxs)(t.A,{header:(0,n.jsx)(a.h2,{children:(0,n.jsx)(a.span,{children:"Cleanup"})}),children:[(0,n.jsx)(a.p,{children:"To clean up your environment, run the following commands and enter the same region you used when creating the EKS cluster:"}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"cd data-on-eks/streaming/kafka\nchmod +x cleanup.sh\n./cleanup.sh\n"})}),(0,n.jsx)(a.admonition,{type:"info",children:(0,n.jsx)(a.p,{children:"This may take between 20 to 30 mins."})})]})]})}function p(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},2450:(e,a,s)=>{s.d(a,{A:()=>m});var n=s(6540),r=s(5556),t=s.n(r),i=s(4164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=s(4848);function u(e){let{children:a,header:s}=e;const[r,t]=(0,n.useState)(!1);return(0,p.jsxs)("div",{className:o,children:[(0,p.jsxs)("div",{className:(0,i.A)(l,{[h]:r}),onClick:()=>{t(!r)},children:[s,(0,p.jsx)("span",{className:(0,i.A)(c,{[h]:r}),children:r?"\ud83d\udc47":"\ud83d\udc48"})]}),r&&(0,p.jsx)("div",{className:d,children:a})]})}u.propTypes={children:t().node.isRequired,header:t().node.isRequired};const m=u},4982:(e,a,s)=>{s.d(a,{A:()=>n});const n=s.p+"assets/images/kafka-brokers-c79c7eb2ebae38e6ea78fa02a5820805.png"},5874:(e,a,s)=>{s.d(a,{A:()=>n});const n=s.p+"assets/images/kafka-consumer-e78e78421501c1ecbcc75ff5f69001bf.png"},8453:(e,a,s)=>{s.d(a,{R:()=>i,x:()=>o});var n=s(6540);const r={},t=n.createContext(r);function i(e){const a=n.useContext(t);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(t.Provider,{value:a},e.children)}}}]);