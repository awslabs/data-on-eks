"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([["8291"],{34625:function(e,r,n){n.r(r),n.d(r,{frontMatter:()=>i,default:()=>p,toc:()=>c,metadata:()=>s,assets:()=>d,contentTitle:()=>l});var s=JSON.parse('{"id":"blueprints/amazon-emr-on-eks/emr-eks-spark-operator","title":"EMR Runtime with Spark Operator","description":"Introduction","source":"@site/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator.md","sourceDirName":"blueprints/amazon-emr-on-eks","slug":"/blueprints/amazon-emr-on-eks/emr-eks-spark-operator","permalink":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"EMR Runtime with Spark Operator"},"sidebar":"blueprints","previous":{"title":"EMR on EKS with Karpenter","permalink":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter"},"next":{"title":"EMR NVIDIA Spark-RAPIDS","permalink":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids"}}'),t=n(85893),a=n(50065),o=n(96912);let i={sidebar_position:3,sidebar_label:"EMR Runtime with Spark Operator"},l="EMR Runtime with Spark Operator",d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites:",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the resources",id:"verify-the-resources",level:2}];function h(e){let r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"emr-runtime-with-spark-operator",children:"EMR Runtime with Spark Operator"})}),"\n",(0,t.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(r.p,{children:"In this post, we will learn to deploy EKS with EMR Spark Operator and execute sample Spark job with EMR runtime."}),"\n",(0,t.jsxs)(r.p,{children:["In this ",(0,t.jsx)(r.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/emr-eks-karpenter",children:"example"}),", you will provision the following resources required to run Spark Applications using the Spark Operator and EMR runtime."]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Creates EKS Cluster Control plane with public endpoint (for demo purpose only)"}),"\n",(0,t.jsxs)(r.li,{children:["Two managed node groups","\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc."}),"\n",(0,t.jsx)(r.li,{children:"Spark Node group with single AZ for running Spark jobs"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(r.li,{children:["Creates one Data team (",(0,t.jsx)(r.code,{children:"emr-data-team-a"}),")","\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Creates new namespace for the team"}),"\n",(0,t.jsx)(r.li,{children:"New IAM role for the team execution role"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(r.li,{children:["IAM policy for ",(0,t.jsx)(r.code,{children:"emr-data-team-a"})]}),"\n",(0,t.jsx)(r.li,{children:"Spark History Server Live UI is configured for monitoring running Spark jobs through an NLB and NGINX ingress controller"}),"\n",(0,t.jsxs)(r.li,{children:["Deploys the following Kubernetes Add-ons","\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:["Managed Add-ons","\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(r.li,{children:["Self Managed Add-ons","\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, AWS for FluentBit, CloudWatchMetrics for EKS"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(o.Z,{header:(0,t.jsx)(r.h2,{children:(0,t.jsx)(r.span,{children:"EMR Spark Operator"})}),children:[(0,t.jsxs)(r.p,{children:["The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. To submit Spark Applications to Spark Operator and leverage the EMR Runtime we use the Helm Chart hosted in the EMR on EKS ECR repository. The charts are stored under the following path: ",(0,t.jsx)(r.code,{children:"ECR_URI/spark-operator"}),". The ECR repository can be obtained from this ",(0,t.jsx)(r.a,{href:"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-emr-runtime.html",children:"link"}),"."]}),(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"a SparkApplication controller that watches events of creation, updates, and deletion of SparkApplication objects and acts on the watch events,"}),"\n",(0,t.jsx)(r.li,{children:"a submission runner that runs spark-submit for submissions received from the controller,"}),"\n",(0,t.jsx)(r.li,{children:"a Spark pod monitor that watches for Spark pods and sends pod status updates to the controller,"}),"\n",(0,t.jsx)(r.li,{children:"a Mutating Admission Webhook that handles customizations for Spark driver and executor pods based on the annotations on the pods added by the controller"}),"\n"]})]}),"\n",(0,t.jsxs)(o.Z,{header:(0,t.jsx)(r.h2,{children:(0,t.jsx)(r.span,{children:"Deploying the Solution"})}),children:[(0,t.jsx)(r.h3,{id:"prerequisites",children:"Prerequisites:"}),(0,t.jsx)(r.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,t.jsx)(r.h3,{id:"deploy",children:"Deploy"}),(0,t.jsx)(r.p,{children:"Clone the repository"}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,t.jsxs)(r.p,{children:["Navigate to ",(0,t.jsx)(r.code,{children:"analytics/terraform/emr-eks-karpenter"})," and run ",(0,t.jsx)(r.code,{children:"terraform init"})]}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cd ./data-on-eks/analytics/terraform/emr-eks-karpenter\nterraform init\n"})}),(0,t.jsxs)(r.admonition,{type:"info",children:[(0,t.jsxs)(r.p,{children:["To deploy the EMR Spark Operator Add-on. You need to set the the below value to ",(0,t.jsx)(r.code,{children:"true"})," in ",(0,t.jsx)(r.code,{children:"variables.tf"})," file."]}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-hcl",children:'variable "enable_emr_spark_operator" {\n  description = "Enable the Spark Operator to submit jobs with EMR Runtime"\n  default     = true\n  type        = bool\n}\n'})})]}),(0,t.jsx)(r.p,{children:"Deploy the pattern"}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"terraform apply\n"})}),(0,t.jsxs)(r.p,{children:["Enter ",(0,t.jsx)(r.code,{children:"yes"})," to apply."]}),(0,t.jsx)(r.h2,{id:"verify-the-resources",children:"Verify the resources"}),(0,t.jsxs)(r.p,{children:["Let\u2019s verify the resources created by ",(0,t.jsx)(r.code,{children:"terraform apply"}),"."]}),(0,t.jsx)(r.p,{children:"Verify the Spark Operator and Amazon Managed service for Prometheus."}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"\nhelm list --namespace spark-operator -o yaml\n\naws amp list-workspaces --alias amp-ws-emr-eks-karpenter\n\n"})}),(0,t.jsxs)(r.p,{children:["Verify Namespace ",(0,t.jsx)(r.code,{children:"emr-data-team-a"})," and Pod status for ",(0,t.jsx)(r.code,{children:"Prometheus"}),", ",(0,t.jsx)(r.code,{children:"Vertical Pod Autoscaler"}),", ",(0,t.jsx)(r.code,{children:"Metrics Server"})," and ",(0,t.jsx)(r.code,{children:"Cluster Autoscaler"}),"."]}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"aws eks --region us-west-2 update-kubeconfig --name spark-operator-doeks # Creates k8s config file to authenticate with EKS Cluster\n\nkubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | grep emr-data-team # Output shows emr-data-team-a for data team\n\nkubectl get pods --namespace=vpa  # Output shows Vertical Pod Autoscaler pods\n\nkubectl get pods --namespace=kube-system | grep  metrics-server # Output shows Metric Server pod\n\nkubectl get pods --namespace=kube-system | grep  cluster-autoscaler # Output shows Cluster Autoscaler pod\n"})})]}),"\n",(0,t.jsxs)(o.Z,{header:(0,t.jsx)(r.h2,{children:(0,t.jsx)(r.span,{children:"Execute Sample Spark job with Karpenter"})}),children:[(0,t.jsx)(r.p,{children:"Navigate to example directory and submit the Spark job."}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/emr-spark-operator\nkubectl apply -f pyspark-pi-job.yaml\n"})}),(0,t.jsx)(r.p,{children:"Monitor the job status using the below command.\nYou should see the new nodes triggered by the karpenter and the YuniKorn will schedule one driver pod and 2 executor pods on this node."}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"kubectl get pods -n spark-team-a -w\n"})})]}),"\n",(0,t.jsxs)(o.Z,{header:(0,t.jsx)(r.h2,{children:(0,t.jsx)(r.span,{children:"Cleanup"})}),children:[(0,t.jsxs)(r.p,{children:["This script will cleanup the environment using ",(0,t.jsx)(r.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cd analytics/terraform/emr-eks-karpenter && chmod +x cleanup.sh\n./cleanup.sh\n"})})]}),"\n",(0,t.jsx)(r.admonition,{type:"caution",children:(0,t.jsx)(r.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"})})]})}function p(e={}){let{wrapper:r}={...(0,a.a)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},96912:function(e,r,n){n.d(r,{Z:()=>c});var s=n(85893),t=n(67294),a=n(45697),o=n.n(a),i=n(67026);let l="expanded_iGsi";function d({children:e,header:r}){let[n,a]=(0,t.useState)(!1);return(0,s.jsxs)("div",{className:"collapsibleContent_q3kw",children:[(0,s.jsxs)("div",{className:(0,i.Z)("header_QCEw",{[l]:n}),onClick:()=>{a(!n)},children:[r,(0,s.jsx)("span",{className:(0,i.Z)("icon_PckA",{[l]:n}),children:n?"\uD83D\uDC47":"\uD83D\uDC48"})]}),n&&(0,s.jsx)("div",{className:"content_qLC1",children:e})]})}d.propTypes={children:o().node.isRequired,header:o().node.isRequired};let c=d},50065:function(e,r,n){n.d(r,{Z:()=>i,a:()=>o});var s=n(67294);let t={},a=s.createContext(t);function o(e){let r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function i(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:r},e.children)}}}]);