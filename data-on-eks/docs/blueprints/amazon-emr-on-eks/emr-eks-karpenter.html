<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/amazon-emr-on-eks/emr-eks-karpenter" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.0"><title data-rh=true>EMR on EKS with [Karpenter](https://karpenter.sh/) | Data on EKS</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://awslabs.github.io/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter /><meta data-rh=true property=og:locale content=en /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docusaurus_version content=current /><meta data-rh=true name=docusaurus_tag content=docs-default-current /><meta data-rh=true name=docsearch:version content=current /><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current /><meta data-rh=true property=og:title content="EMR on EKS with [Karpenter](https://karpenter.sh/) | Data on EKS"/><meta data-rh=true name=description content=Introduction /><meta data-rh=true property=og:description content=Introduction /><link data-rh=true rel=icon href=/data-on-eks/img/header-icon.png /><link data-rh=true rel=canonical href=https://awslabs.github.io/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter /><link data-rh=true rel=alternate href=https://awslabs.github.io/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter hreflang=en /><link data-rh=true rel=alternate href=https://awslabs.github.io/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter hreflang=x-default /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://awslabs.github.io/data-on-eks/docs/category/amazon-emr-on-eks","name":"Amazon EMR on EKS","position":1},{"@type":"ListItem","item":"https://awslabs.github.io/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter","name":"EMR on EKS with Karpenter","position":2}]}</script><link rel=stylesheet href=/data-on-eks/assets/css/styles.1dd1ab63.css /><script src=/data-on-eks/assets/js/runtime~main.8ff5812d.js defer></script><script src=/data-on-eks/assets/js/main.133b39ee.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/data-on-eks/><div class=navbar__logo><img src=/data-on-eks/img/header-icon.png alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/data-on-eks/img/header-icon.png alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/data-on-eks/docs/blueprints/data-analytics>Blueprints</a><a class="navbar__item navbar__link" href=/data-on-eks/docs/bestpractices/intro>Best Practices</a><a class="navbar__item navbar__link" href=/data-on-eks/docs/benchmarks/emr-on-eks>Benchmarks</a><a class="navbar__item navbar__link" href=/data-on-eks/docs/resources/intro>Resources</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/awslabs/data-on-eks target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><div class=navbar__search><span aria-label="expand searchbar" role=button class=search-icon tabindex=0></span><input id=search_input_react type=search placeholder=Loading... aria-label=Search class="navbar__search-input search-bar" disabled/></div></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" href=/data-on-eks/docs/category/data-analytics-on-eks><span title="Data Analytics on EKS" class=categoryLinkLabel_W154>Data Analytics on EKS</span></a><button aria-label="Expand sidebar category 'Data Analytics on EKS'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href=/data-on-eks/docs/category/amazon-emr-on-eks><span title="Amazon EMR on EKS" class=categoryLinkLabel_W154>Amazon EMR on EKS</span></a><button aria-label="Collapse sidebar category 'Amazon EMR on EKS'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/blueprints/amazon-emr-on-eks><span title=Introduction class=linkLabel_WmDU>Introduction</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter><span title="EMR on EKS with Karpenter" class=linkLabel_WmDU>EMR on EKS with Karpenter</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator><span title="EMR Runtime with Spark Operator" class=linkLabel_WmDU>EMR Runtime with Spark Operator</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids><span title="EMR NVIDIA Spark-RAPIDS" class=linkLabel_WmDU>EMR NVIDIA Spark-RAPIDS</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability><span title="EMR on EKS Observability" class=linkLabel_WmDU>EMR on EKS Observability</span></a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" href=/data-on-eks/docs/category/streaming-platforms-on-eks><span title="Streaming Platforms on EKS" class=categoryLinkLabel_W154>Streaming Platforms on EKS</span></a><button aria-label="Expand sidebar category 'Streaming Platforms on EKS'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" href=/data-on-eks/docs/category/job-schedulers-on-eks><span title="Job Schedulers on EKS" class=categoryLinkLabel_W154>Job Schedulers on EKS</span></a><button aria-label="Expand sidebar category 'Job Schedulers on EKS'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_byQd menu__link menu__link--sublist" href=/data-on-eks/docs/category/distributed-databases-on-eks><span title="Distributed Databases on EKS" class=categoryLinkLabel_W154>Distributed Databases on EKS</span></a><button aria-label="Expand sidebar category 'Distributed Databases on EKS'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/data-on-eks/docs/blueprints/troubleshooting><span title=Troubleshooting class=linkLabel_WmDU>Troubleshooting</span></a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/data-on-eks/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/data-on-eks/docs/category/amazon-emr-on-eks><span>Amazon EMR on EKS</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>EMR on EKS with Karpenter</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>EMR on EKS with <a href=https://karpenter.sh/ target=_blank rel="noopener noreferrer">Karpenter</a></h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=introduction>Introduction<a href=#introduction class=hash-link aria-label="Direct link to Introduction" title="Direct link to Introduction" translate=no>â€‹</a></h2>
<p>In this <a href=https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/emr-eks-karpenter target=_blank rel="noopener noreferrer">pattern</a>, you will deploy an EMR on EKS cluster and use <a href=https://karpenter.sh/ target=_blank rel="noopener noreferrer">Karpenter</a> Nodepools for scaling Spark jobs.</p>
<p><strong>Architecture</strong>
<img decoding=async loading=lazy alt=emr-eks-karpenter src=/data-on-eks/assets/images/emr-eks-karpenter-aa9147ea1a119fb2da5f22404d4b2b83.png width=992 height=566 class=img_ev3q /></p>
<p>This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults if you are new to EMR on EKS and only customize if you have viable alternative option available for replacement.</p>
<p>In terms of infrastructure, here are the resources that are created by this pattern</p>
<ul>
<li>Creates an EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)</li>
<li>One managed node group<!-- -->
<ul>
<li>Core Node group with 3 instances spanning multi-AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc.</li>
</ul>
</li>
<li>Enables EMR on EKS<!-- -->
<ul>
<li>Creates two namespaces (<code>emr-data-team-a</code>, <code>emr-data-team-b</code>) for data teams</li>
<li>Creates Kubernetes role and role binding(<code>emr-containers</code> user) for both namespaces</li>
<li>IAM roles for both teams needed for job execution</li>
<li>Update <code>AWS_AUTH</code> config map with <code>emr-containers</code> user and <code>AWSServiceRoleForAmazonEMRContainers</code> role</li>
<li>Create a trust relationship between the job execution role and the identity of the EMR managed service account</li>
<li>Create EMR Virtual Cluster for <code>emr-data-team-a</code> & <code>emr-data-team-b</code> and IAM policies for both</li>
</ul>
</li>
</ul>
<p>You can see the list of add-ons available below.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as <code>core-node-group</code> as provided by this pattern.</div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"/></svg></span>danger</div><div class=admonitionContent_BuS1><p>We don't recommend removing critical add-ons (<code>Amazon VPC CNI</code>, <code>CoreDNS</code>, <code>Kube-proxy</code>).</div></div>
<table><thead><tr><th style=text-align:left>Add-on<th style=text-align:center>Enabled by default?<th style=text-align:left>Benefits<th style=text-align:left>Link<tbody><tr><td style=text-align:left>Amazon VPC CNI<td style=text-align:center>Yes<td style=text-align:left>VPC CNI is available as an <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html target=_blank rel="noopener noreferrer">EKS add-on</a> and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html target=_blank rel="noopener noreferrer">VPC CNI Documentation</a><tr><td style=text-align:left>CoreDNS<td style=text-align:center>Yes<td style=text-align:left>CoreDNS is available as an <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html target=_blank rel="noopener noreferrer">EKS add-on</a> and is responsible for resolving DNS queries for spark application and for Kubernetes cluster<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html target=_blank rel="noopener noreferrer">EKS CoreDNS Documentation</a><tr><td style=text-align:left>Kube-proxy<td style=text-align:center>Yes<td style=text-align:left>Kube-proxy is available as an <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html target=_blank rel="noopener noreferrer">EKS add-on</a> and it maintains network rules on your nodes and enables network communication to your spark application pods<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html target=_blank rel="noopener noreferrer">EKS kube-proxy Documentation</a><tr><td style=text-align:left>Amazon EBS CSI driver<td style=text-align:center>Yes<td style=text-align:left>EBS CSI driver is available as an <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html target=_blank rel="noopener noreferrer">EKS add-on</a> and it allows EKS clusters to manage the lifecycle of EBS volumes<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html target=_blank rel="noopener noreferrer">EBS CSI Driver Documentation</a><tr><td style=text-align:left>Karpenter<td style=text-align:center>Yes<td style=text-align:left>Karpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters<td style=text-align:left><a href=https://karpenter.sh/ target=_blank rel="noopener noreferrer">Karpenter Documentation</a><tr><td style=text-align:left>Cluster Autoscaler<td style=text-align:center>Yes<td style=text-align:left>Kubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as <code>core-node-group</code>) in the cluster<td style=text-align:left><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md target=_blank rel="noopener noreferrer">Cluster Autoscaler Documentation</a><tr><td style=text-align:left>Cluster proportional autoscaler<td style=text-align:center>Yes<td style=text-align:left>This is responsible for scaling CoreDNS pods in your Kubernetes cluster<td style=text-align:left><a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler target=_blank rel="noopener noreferrer">Cluster Proportional Autoscaler Documentation</a><tr><td style=text-align:left>Metrics server<td style=text-align:center>Yes<td style=text-align:left>Kubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html target=_blank rel="noopener noreferrer">EKS Metrics Server Documentation</a><tr><td style=text-align:left>Prometheus<td style=text-align:center>Yes<td style=text-align:left>Prometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost<td style=text-align:left><a href=https://prometheus.io/docs/introduction/overview/ target=_blank rel="noopener noreferrer">Prometheus Documentation</a><tr><td style=text-align:left>Amazon Managed Prometheus<td style=text-align:center>Yes<td style=text-align:left>This is responsible for storing and scaling of EKS cluster and spark application metrics<td style=text-align:left><a href=https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html target=_blank rel="noopener noreferrer">Amazon Managed Prometheus Documentation</a><tr><td style=text-align:left>Kubecost<td style=text-align:center>Yes<td style=text-align:left>Kubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/cost-monitoring.html target=_blank rel="noopener noreferrer">EKS Kubecost Documentation</a><tr><td style=text-align:left>CloudWatch metrics<td style=text-align:center>No<td style=text-align:left>CloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard<td style=text-align:left><a href=https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-metrics-EKS.html target=_blank rel="noopener noreferrer">CloudWatch Container Insights Documentation</a><tr><td style=text-align:left>AWS for Fluent-bit<td style=text-align:center>No<td style=text-align:left>This can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system<td style=text-align:left><a href=https://github.com/aws/aws-for-fluent-bit target=_blank rel="noopener noreferrer">AWS For Fluent-bit Documentation</a><tr><td style=text-align:left>FSx for Lustre CSI driver<td style=text-align:center>No<td style=text-align:left>This can be used for running Spark application using FSx for Lustre<td style=text-align:left><a href=https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html target=_blank rel="noopener noreferrer">FSx for Lustre CSI Driver Documentation</a></table>
<div class=collapsibleContent_q3kw><div class=header_QCEw><h3><span>Customizing Add-ons</span></h3><span class=icon_PckA>ðŸ‘ˆ</span></div></div>
<div class=collapsibleContent_q3kw><div class=header_QCEw><h2><span>Deploying the Solution</span></h2><span class=icon_PckA>ðŸ‘ˆ</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=run-sample-spark-job>Run Sample Spark job<a href=#run-sample-spark-job class=hash-link aria-label="Direct link to Run Sample Spark job" title="Direct link to Run Sample Spark job" translate=no>â€‹</a></h2>
<p>The pattern shows how to run spark jobs in a multi-tenant EKS cluster. The examples showcases two data teams using namespaces <code>emr-data-team-a</code> and <code>emr-data-team-b</code> mapped to their EMR virtual clusters. You can use different Karpenter Nodepools for each team so that they can submit jobs that are unique to their workload. Teams can also use different storage requirements to run their Spark jobs. For example, you can use compute optimized Nodepool that has <code>taints</code> and specify <code>tolerations</code> using pod templates so that you can run spark on compute optimized EC2 instances. In terms of storage, you can decide whether to use <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html target=_blank rel="noopener noreferrer">EC2 instance-store</a> or <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html target=_blank rel="noopener noreferrer">EBS</a> or <a href=https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html target=_blank rel="noopener noreferrer">FSx for lustre</a> volumes for data processing. The default storage that is used in these examples is EC2 instance store because of performance benefit</p>
<ul>
<li><code>spark-compute-optimized</code> Nodepool to run spark jobs on <code>c5d</code> instances.</li>
<li><code>spark-memory-optimized</code> Nodepool to run spark jobs on <code>r5d</code> instances.</li>
<li><code>spark-graviton-memory-optimized</code> Nodepool to run spark jobs on <code>r6gd</code> Graviton instances(<code>ARM64</code>).</li>
</ul>
<div class="theme-tabs-container tabs-container tabList__CuJ"><ul role=tablist aria-orientation=horizontal class=tabs><li role=tab tabindex=0 aria-selected=true class="tabs__item tabItem_LNqP tabs__item--active">spark-compute-optimized<li role=tab tabindex=-1 aria-selected=false class="tabs__item tabItem_LNqP">spark-memory-optimized<li role=tab tabindex=-1 aria-selected=false class="tabs__item tabItem_LNqP">spark-graviton-memory-optimized</ul><div class=margin-top--md><div role=tabpanel class=tabItem_Ymn6><p>In this tutorial, you will use Karpenter Nodepool that uses compute optimized instances. This template leverages the Karpenter AWSNodeTemplates.<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed=true><summary> To view Karpenter Nodepool for compute optimized instances, Click to toggle content!</summary><div><div class=collapsibleContent_i85q><p>Verify the Karpenter NodeClass and Nodepool code <a href=https://github.com/awslabs/data-on-eks/blob/35e09a8fbe64266778e0d86fe2eb805b8373e590/analytics/terraform/emr-eks-karpenter/addons.tf#L204 target=_blank rel="noopener noreferrer">here</a></div></div></details><p>To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding <code>tolerations</code> to your pod templates<p>For example,<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token key atrule" style=color:#00a4db>spec</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  </span><span class="token key atrule" style=color:#00a4db>tolerations</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token punctuation" style=color:#393A34>-</span><span class="token plain"> </span><span class="token key atrule" style=color:#00a4db>key</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"spark-compute-optimized"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>operator</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"Exists"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>effect</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"NoSchedule"</span><br/></span></code></pre></div></div><p><strong>Execute the sample PySpark Job to trigger compute optimized Karpenter Nodepool</strong><p>The following script requires four input parameters <code>virtual_cluster_id</code>, <code>job_execution_role_arn</code>, <code>cloudwatch_log_group_name</code> & <code>S3_Bucket</code> to store PySpark scripts, Pod templates and Input data. These values are auto populated by <code>execute_emr_eks_job.sh</code>.<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.</div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-compute-provisioner/</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./execute_emr_eks_job.sh</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span></code></pre></div></div><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name>: s3://example-bucket</span><br/></span></code></pre></div></div><p>Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.
Nodes will be drained with once the job is completed<p><strong>Verify the job execution</strong><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div></div><div role=tabpanel class=tabItem_Ymn6 hidden><p>In this tutorial, you will use Karpenter Nodepool that uses memory optimized instances. This template uses the AWS Node template with Userdata.<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed=true><summary> To view Karpenter Nodepool for memory optimized instances, Click to toggle content!</summary><div><div class=collapsibleContent_i85q><p>Verify the Karpenter NodeClass and Nodepool code <a href=https://github.com/awslabs/data-on-eks/blob/35e09a8fbe64266778e0d86fe2eb805b8373e590/analytics/terraform/emr-eks-karpenter/addons.tf#L204 target=_blank rel="noopener noreferrer">here</a></div></div></details><p>To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding <code>tolerations</code> to your pod templates<p>For example,<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token key atrule" style=color:#00a4db>spec</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  </span><span class="token key atrule" style=color:#00a4db>tolerations</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token punctuation" style=color:#393A34>-</span><span class="token plain"> </span><span class="token key atrule" style=color:#00a4db>key</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"spark-memory-optimized"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>operator</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"Exists"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>effect</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"NoSchedule"</span><br/></span></code></pre></div></div><p><strong>Execute the sample PySpark Job to trigger memory optimized Karpenter Nodepool</strong><p>The following script requires four input parameters <code>virtual_cluster_id</code>, <code>job_execution_role_arn</code>, <code>cloudwatch_log_group_name</code> & <code>S3_Bucket</code> to store PySpark scripts, Pod templates and Input data. You can get these values <code>terraform apply</code> output values or by running <code>terraform output</code>. For <code>S3_BUCKET</code>, Either create a new S3 bucket or use an existing S3 bucket.<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.</div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-memory-provisioner/</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./execute_emr_eks_job.sh</span><br/></span></code></pre></div></div><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name>: s3://example-bucket</span><br/></span></code></pre></div></div><p>Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.
Nodes will be drained with once the job is completed<p><strong>Verify the job execution</strong><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div></div><div role=tabpanel class=tabItem_Ymn6 hidden><p>In this tutorial, you will use Karpenter Nodepool that uses Graviton memory optimized instances. This template uses the AWS Node template with Userdata.<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed=true><summary> To view Karpenter Nodepool for Graviton memory optimized instances, Click to toggle content!</summary><div><div class=collapsibleContent_i85q><p>Verify the Karpenter NodeClass and Nodepool code <a href=https://github.com/awslabs/data-on-eks/blob/35e09a8fbe64266778e0d86fe2eb805b8373e590/analytics/terraform/emr-eks-karpenter/addons.tf#L204 target=_blank rel="noopener noreferrer">here</a></div></div></details><p>To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding <code>tolerations</code> to your pod templates<p>For example,<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token key atrule" style=color:#00a4db>spec</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  </span><span class="token key atrule" style=color:#00a4db>tolerations</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token punctuation" style=color:#393A34>-</span><span class="token plain"> </span><span class="token key atrule" style=color:#00a4db>key</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"spark-graviton-memory-optimized"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>operator</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"Exists"</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">      </span><span class="token key atrule" style=color:#00a4db>effect</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"NoSchedule"</span><br/></span></code></pre></div></div><p><strong>Execute the sample PySpark Job to trigger Graviton memory optimized Karpenter Nodepool</strong><p>The following script requires four input parameters <code>virtual_cluster_id</code>, <code>job_execution_role_arn</code>, <code>cloudwatch_log_group_name</code> & <code>S3_Bucket</code> to store PySpark scripts, Pod templates and Input data. You can get these values <code>terraform apply</code> output values or by running <code>terraform output</code>. For <code>S3_BUCKET</code>, Either create a new S3 bucket or use an existing S3 bucket.<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.</div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-graviton-memory-provisioner/</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./execute_emr_eks_job.sh</span><br/></span></code></pre></div></div><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name>: s3://example-bucket</span><br/></span></code></pre></div></div><p>Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.
Nodes will be drained with once the job is completed<p><strong>Verify the job execution</strong><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool>Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool<a href=#execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool class=hash-link aria-label="Direct link to Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool" title="Direct link to Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool" translate=no>â€‹</a></h3>
<p>This pattern uses EBS volumes for data processing and compute optimized Nodepool. You can modify the Nodepool by changing nodeselector in driver and executor pod templates. In order to change Nodepools, simply update your pod templates to desired Nodepool</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">  </span><span class="token key atrule" style=color:#00a4db>nodeSelector</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token key atrule" style=color:#00a4db>NodeGroupType</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"> </span><span class="token string" style=color:#e3116c>"SparkComputeOptimized"</span><br/></span></code></pre></div></div>
<p>You can also update <a href=https://aws.amazon.com/ec2/instance-types/#Compute_Optimized target=_blank rel="noopener noreferrer">EC2 instances</a> that doesn't include instance store volumes (for example c5.xlarge) and remove c5d's if needed for this exercise</p>
<p>We will create Storageclass that will be used by drivers and executors. We'll create static Persistent Volume Claim (PVC) for the driver pod but we'll use dynamically created ebs volumes for executors.</p>
<p>Create StorageClass and PVC using example provided</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">kubectl apply </span><span class="token parameter variable" style=color:#36acaa>-f</span><span class="token plain"> ebs-storageclass-pvc.yaml</span><br/></span></code></pre></div></div>
<p>Let's run the job</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./execute_emr_eks_job.sh</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the S3 Bucket </span><span class="token keyword" style=color:#00009f>for</span><span class="token plain"> storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://</span><span class="token operator" style=color:#393A34>&lt;</span><span class="token plain">bucket-name</span><span class="token operator" style=color:#393A34>></span><span class="token plain">: s3://example-bucket</span><br/></span></code></pre></div></div>
<p>You'll notice the PVC <code>spark-driver-pvc</code> will be used by driver pod but Spark will create multiple ebs volumes for executors mapped to Storageclass <code>emr-eks-karpenter-ebs-sc</code>. All dynamically created ebs volumes will be deleted once the job completes</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=running-sample-spark-job-using-fsx-for-lustre>Running Sample Spark job using FSx for Lustre<a href=#running-sample-spark-job-using-fsx-for-lustre class=hash-link aria-label="Direct link to Running Sample Spark job using FSx for Lustre" title="Direct link to Running Sample Spark job using FSx for Lustre" translate=no>â€‹</a></h3>
<p>Amazon FSx for Lustre is a fully managed shared storage option built on the worldâ€™s most popular high-performance file system. You can use FSx to store shuffle files and also to store intermediate data processing tasks in a data pipeline. You can read more about FSX for Lustre in <a href=https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html target=_blank rel="noopener noreferrer">documentation</a> and learn how to use this storage with EMR on EKS in our <a href=https://aws.github.io/aws-emr-containers-best-practices/storage/docs/spark/fsx-lustre/ target=_blank rel="noopener noreferrer">best practices guide</a></p>
<p>In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage. There are two ways to use FSx for Lustre</p>
<ul>
<li>using static FSx for Lustre volumes</li>
<li>using dynamically created FSx for Lustre volumes</li>
</ul>
<div class="theme-tabs-container tabs-container tabList__CuJ"><ul role=tablist aria-orientation=horizontal class=tabs><li role=tab tabindex=0 aria-selected=true class="tabs__item tabItem_LNqP tabs__item--active">fsx-static<li role=tab tabindex=-1 aria-selected=false class="tabs__item tabItem_LNqP">fsx-dynamic</ul><div class=margin-top--md><div role=tabpanel class=tabItem_Ymn6><p><strong>Execute Spark Job by using <code>FSx for Lustre</code> with statically provisioned volume and compute optimized Karpenter Nodepool.</strong><p>Fsx for Lustre Terraform module is disabled by default. Follow the <a href=#customizing-add-ons>customizing add-ons</a> steps before running Spark jobs.<p>Execute the Spark job using the below shell script.<p>This script requires input parameters which can be extracted from <code>terraform apply</code> output values.<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.</div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-static-pvc-shuffle-storage</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./fsx-static-spark.sh</span><br/></span></code></pre></div></div><p>Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.
Nodes will be drained with once the job is completed<p><strong>Verify the job execution events</strong><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div><p>This will show the mounted <code>/data</code> directory with FSx DNS name<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-ti</span><span class="token plain"> taxidata-exec-1 </span><span class="token parameter variable" style=color:#36acaa>-c</span><span class="token plain"> spark-kubernetes-executor </span><span class="token parameter variable" style=color:#36acaa>-n</span><span class="token plain"> emr-data-team-a -- </span><span class="token function" style=color:#d73a49>df</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-h</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-ti</span><span class="token plain"> taxidata-exec-1 </span><span class="token parameter variable" style=color:#36acaa>-c</span><span class="token plain"> spark-kubernetes-executor </span><span class="token parameter variable" style=color:#36acaa>-n</span><span class="token plain"> emr-data-team-a -- </span><span class="token function" style=color:#d73a49>ls</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-lah</span><span class="token plain"> /static</span><br/></span></code></pre></div></div></div><div role=tabpanel class=tabItem_Ymn6 hidden><p><strong>Execute Spark Job by using <code>FSx for Lustre</code> with dynamically provisioned volume and compute optimized Karpenter Nodepool.</strong><p>Fsx for Lustre Terraform module is disabled by default. Follow the <a href=#customizing-add-ons>customizing add-ons</a> steps before running Spark jobs.<p>Execute Spark Job by using <code>FSx for Lustre</code> as a Shuffle storage for Driver and Executor pods with dynamically provisioned FSx filesystem and Persistent volume.
Execute the Spark job using the below shell script.<p>This script requires input parameters which can be extracted from <code>terraform apply</code> output values.<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job.</div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-dynamic-pvc-shuffle-storage</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./fsx-dynamic-spark.sh</span><br/></span></code></pre></div></div><p>Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.
Nodes will be drained with once the job is completed<p><strong>Verify the job execution events</strong><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-ti</span><span class="token plain"> taxidata-exec-1 </span><span class="token parameter variable" style=color:#36acaa>-c</span><span class="token plain"> spark-kubernetes-executor </span><span class="token parameter variable" style=color:#36acaa>-n</span><span class="token plain"> emr-data-team-a -- </span><span class="token function" style=color:#d73a49>df</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-h</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-ti</span><span class="token plain"> taxidata-exec-1 </span><span class="token parameter variable" style=color:#36acaa>-c</span><span class="token plain"> spark-kubernetes-executor </span><span class="token parameter variable" style=color:#36acaa>-n</span><span class="token plain"> emr-data-team-a -- </span><span class="token function" style=color:#d73a49>ls</span><span class="token plain"> </span><span class="token parameter variable" style=color:#36acaa>-lah</span><span class="token plain"> /dynamic</span><br/></span></code></pre></div></div></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=running-sample-spark-job-using-apache-yunikorn-batch-scheduler>Running Sample Spark job using Apache YuniKorn Batch Scheduler<a href=#running-sample-spark-job-using-apache-yunikorn-batch-scheduler class=hash-link aria-label="Direct link to Running Sample Spark job using Apache YuniKorn Batch Scheduler" title="Direct link to Running Sample Spark job using Apache YuniKorn Batch Scheduler" translate=no>â€‹</a></h3>
<p>Apache YuniKorn is an open-source, universal resource scheduler for managing distributed big data processing workloads such as Spark, Flink, and Storm. It is designed to efficiently manage resources across multiple tenants in a shared, multi-tenant cluster environment.
Some of the key features of Apache YuniKorn include:</p>
<ul>
<li><strong>Flexibility</strong>: YuniKorn provides a flexible and scalable architecture that can handle a wide variety of workloads, from long-running services to batch jobs.</li>
<li><strong>Dynamic Resource Allocation</strong>: YuniKorn uses a dynamic resource allocation mechanism to allocate resources to workloads on an as-needed basis, which helps to minimize resource wastage and improve overall cluster utilization.</li>
<li><strong>Priority-based Scheduling</strong>: YuniKorn supports priority-based scheduling, which allows users to assign different levels of priority to their workloads based on business requirements.</li>
<li><strong>Multi-tenancy</strong>: YuniKorn supports multi-tenancy, which enables multiple users to share the same cluster while ensuring resource isolation and fairness.</li>
<li><strong>Pluggable Architecture</strong>: YuniKorn has a pluggable architecture that allows users to extend its functionality with custom scheduling policies and pluggable components.</li>
</ul>
<p>Apache YuniKorn is a powerful and versatile resource scheduler that can help organizations efficiently manage their big data workloads while ensuring high resource utilization and workload performance.</p>
<p><strong>Apache YuniKorn Architecture</strong>
<img decoding=async loading=lazy alt="Apache YuniKorn" src=/data-on-eks/assets/images/yunikorn-56ae296071d7f89d5e0de755c1d026ca.png width=3060 height=1683 class=img_ev3q /></p>
<p><strong>Apache YuniKorn Gang Scheduling with Karpenter</strong></p>
<p>Apache YuniKorn Scheduler add-on is disabled by default. Follow the steps to deploy the Apache YuniKorn add-on and execute the Spark job.</p>
<ol>
<li>Update the <code>analytics/terraform/emr-eks-karpenter/variables.tf</code> file with the following</li>
</ol>
<div class="language-terraform codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-terraform codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">variable "enable_yunikorn" {</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  default     = true</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  description = "Enable Apache YuniKorn Scheduler"</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">  type        = bool</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">}</span><br/></span></code></pre></div></div>
<ol start=2>
<li>Execute <code>terrafrom apply</code> again. This will deploy FSx for Lustre add-on and all the necessary resources.</li>
</ol>
<div class="language-terraform codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-terraform codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">terraform apply -auto-approve</span><br/></span></code></pre></div></div>
<p>This example demonstrates the <a href=https://yunikorn.apache.org/docs/user_guide/gang_scheduling/ target=_blank rel="noopener noreferrer">Apache YuniKorn Gang Scheduling</a> with Karpenter Autoscaler.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token builtin class-name">cd</span><span class="token plain"> analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-yunikorn-gangscheduling</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./execute_emr_eks_job.sh</span><br/></span></code></pre></div></div>
<p><strong>Verify the job execution</strong>
Apache YuniKorn Gang Scheduling will create pause pods for total number of executors requested.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">kubectl get pods </span><span class="token parameter variable" style=color:#36acaa>--namespace</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">emr-data-team-a </span><span class="token parameter variable" style=color:#36acaa>-w</span><br/></span></code></pre></div></div>
<p>Verify the driver and executor pods prefix with <code>tg-</code> indicates the pause pods.
These pods will be replaced with the actual Spark Driver and Executor pods once the Nodes are scaled and ready by the Karpenter.</p>
<p><img decoding=async loading=lazy alt=img.png src=/data-on-eks/assets/images/karpenter-yunikorn-gang-schedule-358958a77e7a238ef9edea38f09a7b9d.png width=2962 height=1932 class=img_ev3q /></p>
<div class=collapsibleContent_q3kw><div class=header_QCEw><h2><span>Delta Lake Table Format</span></h2><span class=icon_PckA>ðŸ‘ˆ</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=run-interactive-workload-with-managed-endpoint>Run Interactive Workload with Managed Endpoint<a href=#run-interactive-workload-with-managed-endpoint class=hash-link aria-label="Direct link to Run Interactive Workload with Managed Endpoint" title="Direct link to Run Interactive Workload with Managed Endpoint" translate=no>â€‹</a></h2>
<p>Managed endpoint is a gateway that provides connectivity from EMR Studio to EMR on EKS so that you can run interactive workloads. You can find out more information about it <a href=https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/how-it-works.html target=_blank rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=creating-a-managed-endpoint>Creating a managed endpoint<a href=#creating-a-managed-endpoint class=hash-link aria-label="Direct link to Creating a managed endpoint" title="Direct link to Creating a managed endpoint" translate=no>â€‹</a></h3>
<p>In this example, we will create a managed endpoint under one of the data teams.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Navigate to folder and execute script:</span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> analytics/terraform/emr-eks-karpenter/examples/managed-endpoints</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">./create-managed-endpoint.sh</span><br/></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Virtual Cluster Id: 4ucrncg6z4nd19vh1lidna2b3</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Provide your EMR on EKS team (emr-data-team-a or emr-data-team-b): emr-eks-data-team-a</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter your AWS Region: us-west-2</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter a name for your endpoint: emr-eks-team-a-endpoint</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Provide an S3 bucket location for logging (i.e. s3://my-bucket/logging/): s3://&lt;bucket-name>/logs</span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Enter the EMR Execution Role ARN (i.e. arn:aws:00000000000000000:role/EMR-Execution-Role): arn:aws:iam::181460066119:role/emr-eks-karpenter-emr-data-team-a</span><br/></span></code></pre></div></div>
<p>The script will provide the following:</p>
<ul>
<li>JSON configuration file for the Managed Endpoint</li>
<li>Configuration settings:<!-- -->
<ul>
<li>Default 8G Spark Driver</li>
<li>CloudWatch monitoring, with logs stored in the S3 bucket provided</li>
</ul>
</li>
<li>Proper endpoint creation with appropriate security group to allow using Karpenter</li>
<li>Outputs: Managed Endpoint ID and Load Balancer ARN.</li>
</ul>
<p>Once you have created a managed endpoint, you can follow the instructions <a href=https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-configure.html target=_blank rel="noopener noreferrer">here</a> to configure EMR Studio and associate the Managed endpoint to a workspace.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=cleanup-of-endpoint-resources>Cleanup of Endpoint resources<a href=#cleanup-of-endpoint-resources class=hash-link aria-label="Direct link to Cleanup of Endpoint resources" title="Direct link to Cleanup of Endpoint resources" translate=no>â€‹</a></h3>
<p>To delete the managed endpoint, simply run the following command:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">aws emr-containers delete-managed-endpoint </span><span class="token parameter variable" style=color:#36acaa>--id</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>&lt;</span><span class="token plain">Managed Endpoint ID</span><span class="token operator" style=color:#393A34>></span><span class="token plain"> --virtual-cluster-id </span><span class="token operator" style=color:#393A34>&lt;</span><span class="token plain">Virtual Cluster ID</span><span class="token operator" style=color:#393A34>></span><br/></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=cleanup>Cleanup<a href=#cleanup class=hash-link aria-label="Direct link to Cleanup" title="Direct link to Cleanup" translate=no>â€‹</a></h2>
<div class=collapsibleContent_q3kw><div class=header_QCEw><h2><span>Cleanup</span></h2><span class=icon_PckA>ðŸ‘ˆ</span></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_BuS1><p>To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment</div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col><a href=https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter.md target=_blank rel="noopener noreferrer" class=theme-edit-this-page><svg fill=currentColor height=20 width=20 viewBox="0 0 40 40" class=iconEdit_Z9Sw aria-hidden=true><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"/></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/data-on-eks/docs/blueprints/amazon-emr-on-eks><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>Introduction</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>EMR Runtime with Spark Operator</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#introduction class="table-of-contents__link toc-highlight">Introduction</a><ul><li><a href=#customizing-add-ons class="table-of-contents__link toc-highlight">Customizing Add-ons</a><li><a href=#prerequisites class="table-of-contents__link toc-highlight">Prerequisites:</a><li><a href=#verify-the-resources class="table-of-contents__link toc-highlight">Verify the resources</a></ul><li><a href=#run-sample-spark-job class="table-of-contents__link toc-highlight">Run Sample Spark job</a><ul><li><a href=#execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool class="table-of-contents__link toc-highlight">Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool</a><li><a href=#running-sample-spark-job-using-fsx-for-lustre class="table-of-contents__link toc-highlight">Running Sample Spark job using FSx for Lustre</a><li><a href=#running-sample-spark-job-using-apache-yunikorn-batch-scheduler class="table-of-contents__link toc-highlight">Running Sample Spark job using Apache YuniKorn Batch Scheduler</a><li><a href=#prerequisites-1 class="table-of-contents__link toc-highlight">Prerequisites:</a></ul><li><a href=#run-interactive-workload-with-managed-endpoint class="table-of-contents__link toc-highlight">Run Interactive Workload with Managed Endpoint</a><ul><li><a href=#creating-a-managed-endpoint class="table-of-contents__link toc-highlight">Creating a managed endpoint</a><li><a href=#cleanup-of-endpoint-resources class="table-of-contents__link toc-highlight">Cleanup of Endpoint resources</a></ul><li><a href=#cleanup class="table-of-contents__link toc-highlight">Cleanup</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Get Started</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/data-on-eks/docs/introduction/intro>Docs</a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Get Involved</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://github.com/awslabs/data-on-eks target=_blank rel="noopener noreferrer" class=footer__link-item>Github<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Built with â¤ï¸ at AWS  <br/> Â© 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "7fbc7ab02fae4767b1af2588eba0cdf2"}'></script></div></body>