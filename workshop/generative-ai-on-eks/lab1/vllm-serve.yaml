apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm-mistral-inf2
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfigV2: |
    applications:
      - name: vllm
        import_path: vllm_serve:deployment
        route_prefix: "/"
        runtime_env:
          env_vars:
            MODEL_ID: "/data/model/neuron-mistral7bv0.3"
            TENSOR_PARALLELISM: "2"
            BLOCK_SIZE: "10240"
            MAX_MODEL_LEN: "10240"
            MAX_NUM_SEQS: "4"
            PORT: "8000"
            NEURON_CC_FLAGS: "-O1"
            LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:"
            NEURON_CORES: "2"
            NEURON_COMPILE_CACHE_URL: "/data/model/neuron-mistral7bv0.3/neuron-compile-cache"
        deployments:
          - name: VLLMDeployment
            autoscaling_config:
              min_replicas: 1
              max_replicas: 1
              target_num_ongoing_requests_per_replica: 1
            ray_actor_options:
              num_cpus: 6
              resources: {"neuron_cores": 2}
              runtime_env:
                env_vars:
                  LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:"
  rayClusterConfig:
    rayVersion: '2.32.0'
    enableInTreeAutoscaling: true
    headGroupSpec:
      headService:
        metadata:
          name: vllm
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: "0"
      template:
        spec:
          containers:
          - name: head
            image: public.ecr.aws/data-on-eks/vllm-ray2.32.0-inf2-llama3:latest
            imagePullPolicy: IfNotPresent
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - name: vllm-script
              mountPath: /home/ray/vllm_serve.py
              subPath: vllm_serve.py
            - name: persistent-storage
              mountPath: "/data"
            resources:
              limits:
                cpu: "2"
                memory: "4G"
            env:
            - name: PORT
              value: "8000"
            - name: LD_LIBRARY_PATH
              value: "/home/ray/anaconda3/lib:"
          volumes:
          - name: ray-logs
            emptyDir: {}
          - name: vllm-script
            configMap:
              name: vllm-serve-script
          - name: persistent-storage
            persistentVolumeClaim:
              claimName: fsx-claim
          nodeSelector:
            instanceType: mixed-x86
            workload: rayhead
    workerGroupSpecs:
    - groupName: inf2-group
      replicas: 1
      minReplicas: 1
      maxReplicas: 1
      rayStartParams:
        # This setting is critical for inf2/Trn1 node autoscaling with RayServe
        resources: '"{\"neuron_cores\": 2}"'
        num-cpus: "6"
      template:
        spec:
          containers:
          - name: worker
            image: public.ecr.aws/data-on-eks/vllm-ray2.32.0-inf2-llama3:latest
            imagePullPolicy: IfNotPresent
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            resources:
              limits:
                memory: "28G"
                aws.amazon.com/neuron: "1"
            env:
            - name: PORT
              value: "8000"
            - name: LD_LIBRARY_PATH
              value: "/home/ray/anaconda3/lib:"
            - name: NEURON_RT_LOG_LEVEL
              value: "INFO"
            - name: RAY_memory_usage_threshold
              value: "0.99"
            volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - mountPath: /dev/shm
              name: dshm
            - name: vllm-script
              mountPath: /home/ray/vllm_serve.py
              subPath: vllm_serve.py
            - name: persistent-storage
              mountPath: "/data"
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          - name: ray-logs
            emptyDir: {}
          - name: vllm-script
            configMap:
              name: vllm-serve-script
            volumes:
          - name: persistent-storage
            persistentVolumeClaim:
              claimName: fsx-claim
          nodeSelector:
            instanceType: trainium-trn1
            provisionerType: Karpenter
          tolerations:
          - key: "aws.amazon.com/neuron"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "hub.jupyter.org/dedicated"
            operator: "Equal"
            value: "user"
            effect: "NoSchedule"
