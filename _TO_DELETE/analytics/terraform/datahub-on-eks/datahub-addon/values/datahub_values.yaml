# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true
  image:
    repository: acryldata/datahub-gms

datahub-frontend:
  enabled: true
  image:
    repository: acryldata/datahub-frontend-react
  # Set up ingress to expose react front-end
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal # Private Load Balancer can only be accessed within the VPC
      alb.ingress.kubernetes.io/target-type: instance
      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    hosts:
      - paths:
          - /*

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi

datahub-mae-consumer:
  image:
    repository: acryldata/datahub-mae-consumer

datahub-mce-consumer:
  image:
    repository: acryldata/datahub-mce-consumer

datahub-ingestion-cron:
  enabled: false
  image:
    repository: acryldata/datahub-ingestion

elasticsearchSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-elasticsearch-setup
    # tag: v0.8.44
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}
  extraEnvs:
      - name: USE_AWS_ELASTICSEARCH
        value: "true"

kafkaSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-kafka-setup
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

mysqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-mysql-setup
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

#postgresqlSetupJob:
#  enabled: false
#  image:
#    repository: acryldata/datahub-postgres-setup
#  podSecurityContext:
#    fsGroup: 1000
#  securityContext:
#    runAsUser: 1000
#  podAnnotations: {}


datahubUpgrade:
  enabled: true
  image:
    repository: acryldata/datahub-upgrade
  batchSize: 1000
  batchDelayMs: 100
  noCodeDataMigration:
    sqlDbType: "MYSQL"
    # sqlDbType: "POSTGRES"
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
  cleanupJob:
    # Add extra sidecar containers to job pod
    extraSidecars: []
      # - name: my-image-name
      #   image: my-image
      #   imagePullPolicy: Always
  restoreIndices:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    # Add extra sidecar containers to job pod
    extraSidecars: []
      # - name: my-image-name
      #   image: my-image
      #   imagePullPolicy: Always

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  image:
    repository: acryldata/datahub-upgrade
    # tag:
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/hook-delete-policy: before-hook-creation

# prometheus-kafka-exporter:
#   enabled: false
#   kafkaServer:
#   - prerequisites-kafka:9092  # <<release-name>>-kafka:9092
#   # Sarama logging
#   sarama:
#     logEnabled: true
#   prometheus:
#     serviceMonitor:
#       enabled: true
#       namespace: monitoring
#       interval: "30s"
#       # If serviceMonitor is enabled and you want prometheus to automatically register
#       # target using serviceMonitor, add additionalLabels with prometheus release name
#       # e.g. If you have deployed kube-prometheus-stack with release name kube-prometheus
#       # then additionalLabels will be
#       # additionalLabels:
#       #   release: kube-prometheus
#       additionalLabels: {}
#       targetLabels: []

global:
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false

  elasticsearch:
    host: ${es_endpoint}
    port: "443"
    useSSL: "true"
    auth:
      username: opensearch
      password:
        secretRef: elasticsearch-secrets
        secretKey: elasticsearch_password

  kafka:
    bootstrap:
      server: ${msk_bootstrap_brokers}
    zookeeper:
      server: ${msk_zookeeper_connect_string}
    ## For AWS MSK set this to a number larger than 1
    partitions: ${msk_partitions}
    replicationFactor: ${msk_partitions}
    schemaregistry:
      type: KAFKA
      url: "http://datahub-prerequisites-cp-schema-registry:8081"
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

  # neo4j:
  #   host: "prerequisites-neo4j-community:7474"
  #   uri: "bolt://prerequisites-neo4j-community"
  #   username: "neo4j"
  #   password:
  #     secretRef: neo4j-secrets
  #     secretKey: neo4j-password

  sql:
    datasource:
      host: ${datahub_rds_endpoint}
      hostForMysqlClient: ${datahub_rds_address}
      port: "3306"
      url: "jdbc:mysql://${datahub_rds_endpoint}/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.cj.jdbc.Driver"
      username: "admin"
      password:
        secretRef: mysql-secrets
        secretKey: mysql_root_password

      ## Use below for usage of PostgreSQL instead of MySQL
      # host: "prerequisites-postgresql:5432"
      # hostForpostgresqlClient: "prerequisites-postgresql"
      # port: "5432"
      # url: "jdbc:postgresql://prerequisites-postgresql:5432/datahub"
      # driver: "org.postgresql.Driver"
      # username: "postgres"
      # password:
      #   secretRef: postgresql-secrets
      #   secretKey: postgres-password

  datahub:
    version: v0.15.0.1

    systemUpdate:
      enabled: true

    gms:
      port: "8080"
    monitoring:
      enablePrometheus: true
    mae_consumer:
      port: "9091"
    appVersion: "1.0"

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.15.0.1"

    metadata_service_authentication:
      enabled: true
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false to use existing auth secrets
      provisionSecrets:
        enabled: false
        autoGenerate: false
        annotations:
          # added to force secret generation before system update
          helm.sh/hook: "pre-install,pre-upgrade"
          helm.sh/hook-weight: "-6"
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    secret: <secret value>
      #    signingKey: <signing key value>
      #    salt: <salt value>

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:
