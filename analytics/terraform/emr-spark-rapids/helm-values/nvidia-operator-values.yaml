# Default values for gpu-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

daemonsets:
  labels: {}
  annotations: {}
  priorityClassName: system-node-critical
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    - operator: "Exists" # Added this to ensure it can tolerate any custom Taints added to the GPU nodes

operator:
  repository: nvcr.io/nvidia
  priorityClassName: system-node-critical
  defaultRuntime: containerd
  image: gpu-operator
  cleanupCRD: false # This option doesn't do anything even if you change this to true. NVIDIA recommends to use the manual approach of upgrading the CRDs
  # upgrade CRD on chart upgrade, requires --disable-openapi-validation flag
  # to be passed during helm upgrade.
  upgradeCRD: false
  resources:
    limits:
      cpu: 500m
      memory: 350Mi
    requests:
      cpu: 200m
      memory: 100Mi

# https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/install-precompiled-signed-drivers.html
# Currently NVIDIA Operator takes more than 5 mins to make the node GPU ready with all the required drivers.
# With pre-compiled NVIDIA Drivers this process can be faster hence we are using the config values as driver.version: "515-signed"

driver:
  enabled: false  # Disabled as we are using latest EKS AMI 1.29 which comes with NVIDIA drivers pre-installed
  # repository: nvcr.io/nvidia
  # image: driver
  # # Commented this as latest Ubuntu AMIs are failing with this option enabled
  # # version: "515-signed" # supported DRIVER_BRANCH value currently are 470, 510 and 515 which will install latest drivers available on that branch for current running kernel version.
  # manager:
  #   image: k8s-driver-manager
  #   repository: nvcr.io/nvidia/cloud-native

# to ensure containers can properly access GPUs
toolkit:
  enabled: true

# to discover and advertise GPU resources to kubelet
devicePlugin:
  enabled: true

dcgm:
  enabled: false

# to monitor the GPU(s) on the node
dcgmExporter:
  enabled: true

gfd:
  enabled: true

migManager:
  enabled: false

nodeStatusExporter:
  enabled: false

gds:
  enabled: false

vgpuManager:
  enabled: false

vgpuDeviceManager:
  enabled: false

vfioManager:
  enabled: false

sandboxDevicePlugin:
  enabled: false

node-feature-discovery:
  enableNodeFeatureApi: true
  worker:
    tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Equal"
        value: ""
        effect: "NoSchedule"
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - operator: "Exists" # Added this to ensure it can tolerate any custom Taints added to the GPU nodes
