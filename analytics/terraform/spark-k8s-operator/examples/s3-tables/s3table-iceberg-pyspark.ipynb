{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e966091-0998-449f-9dcd-a2032fa39f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Logging configuration\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)s @ line %(lineno)d: %(message)s')\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "# Application-specific variables\n",
    "dt_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "AppName = \"EmployeeDataS3TableJob\"\n",
    "\n",
    "# Replace S3_BUCKET and ACCOUNT_NUMBER with your own values\n",
    "input_csv_path = \"s3a://<S3_BUCKET>/s3table-example/input/\"\n",
    "s3table_arn = \"arn:aws:s3tables:us-west-2:<ACCOUNT_NUMBER>:bucket/doeks-spark-s3-tables\"\n",
    "namespace = \"doeks_namespace\"\n",
    "table_name = \"employee_s3_table\"\n",
    "full_table_name = f\"s3tablesbucket.{namespace}.{table_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38e589b-b0ab-41b6-a339-07f5026a7262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-13 19:47:18,917] INFO @ line 22: Spark session initialized successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(f\"{AppName}_{dt_string}\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.s3tablesbucket\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.s3tablesbucket.catalog-impl\", \"software.amazon.s3tables.iceberg.S3TablesCatalog\")\n",
    "    .config(\"spark.sql.catalog.s3tablesbucket.warehouse\", s3table_arn)\n",
    "    .config('spark.hadoop.fs.s3.impl', \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.sql.defaultCatalog\", \"s3tablesbucket\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"1200000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.readahead.range\", \"256K\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.input.fadvise\", \"random\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider.mapping\", \"com.amazonaws.auth.WebIdentityTokenCredentialsProvider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "logger.info(\"Spark session initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c157786-d661-44e0-af6a-05b94a9e9b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-13 19:47:18,923] INFO @ line 6: Creating namespace: doeks_namespace\n",
      "[2025-01-13 19:47:21,093] INFO @ line 10: Reading employee data from input CSV: s3a://<S3_BUCKET>/s3table-example/input/\n",
      "[2025-01-13 19:47:24,560] INFO @ line 13: Previewing employee data schema\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "[2025-01-13 19:47:24,564] INFO @ line 16: Previewing first 10 records from the input data\n",
      "+---+-----------+------+--------+\n",
      "|id |name       |level |salary  |\n",
      "+---+-----------+------+--------+\n",
      "|1  |Employee_1 |Exec  |101000.0|\n",
      "|2  |Employee_2 |Exec  |149000.0|\n",
      "|3  |Employee_3 |Junior|86000.0 |\n",
      "|4  |Employee_4 |Exec  |147500.0|\n",
      "|5  |Employee_5 |Exec  |74000.0 |\n",
      "|6  |Employee_6 |Exec  |66500.0 |\n",
      "|7  |Employee_7 |Junior|69500.0 |\n",
      "|8  |Employee_8 |Exec  |116000.0|\n",
      "|9  |Employee_9 |Mid   |56000.0 |\n",
      "|10 |Employee_10|Exec  |186500.0|\n",
      "+---+-----------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[2025-01-13 19:47:24,799] INFO @ line 19: Source data count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespace = \"doeks_namespace\"\n",
    "table_name = \"employee_s3_table\"\n",
    "full_table_name = f\"s3tablesbucket.{namespace}.{table_name}\"\n",
    "\n",
    "# Step 1: Create namespace if not exists\n",
    "logger.info(f\"Creating namespace: {namespace}\")\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS s3tablesbucket.{namespace}\")\n",
    "\n",
    "# Step 2: Read input CSV data\n",
    "logger.info(f\"Reading employee data from input CSV: {input_csv_path}\")\n",
    "employee_df = spark.read.csv(input_csv_path, header=True, inferSchema=True)\n",
    "\n",
    "logger.info(\"Previewing employee data schema\")\n",
    "employee_df.printSchema()\n",
    "\n",
    "logger.info(\"Previewing first 10 records from the input data\")\n",
    "employee_df.show(10, truncate=False)\n",
    "\n",
    "logger.info(\"Source data count:\")\n",
    "employee_df.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe0e464-22f2-43e8-8cfe-36e108359a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-13 19:47:25,207] INFO @ line 2: Creating/Replacing and writing data to table: s3tablesbucket.doeks_namespace.employee_s3_table\n",
      "[2025-01-13 19:47:27,861] INFO @ line 8: Reading data back from Iceberg table: s3tablesbucket.doeks_namespace.employee_s3_table\n",
      "[2025-01-13 19:47:28,079] INFO @ line 11: Previewing first 10 records from the Iceberg table\n",
      "+---+-----------+------+--------+\n",
      "|id |name       |level |salary  |\n",
      "+---+-----------+------+--------+\n",
      "|1  |Employee_1 |Exec  |101000.0|\n",
      "|2  |Employee_2 |Exec  |149000.0|\n",
      "|3  |Employee_3 |Junior|86000.0 |\n",
      "|4  |Employee_4 |Exec  |147500.0|\n",
      "|5  |Employee_5 |Exec  |74000.0 |\n",
      "|6  |Employee_6 |Exec  |66500.0 |\n",
      "|7  |Employee_7 |Junior|69500.0 |\n",
      "|8  |Employee_8 |Exec  |116000.0|\n",
      "|9  |Employee_9 |Mid   |56000.0 |\n",
      "|10 |Employee_10|Exec  |186500.0|\n",
      "+---+-----------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[2025-01-13 19:47:28,794] INFO @ line 15: Total records in Iceberg table (DataFrame API):\n",
      "DataFrame count: 100\n",
      "[2025-01-13 19:47:29,153] INFO @ line 19: List the s3table snapshot versions:\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-01-13 19:45:...| 271131297078418895|     NULL|              false|\n",
      "|2025-01-13 19:47:...|1268450705309139006|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "[2025-01-13 19:47:29,349] INFO @ line 23: Stopping Spark Session\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create or replace table and write data in one operation\n",
    "logger.info(f\"Creating/Replacing and writing data to table: {full_table_name}\")\n",
    "(employee_df.writeTo(full_table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .createOrReplace())\n",
    "\n",
    "# Step 4: Read data back from the Iceberg table\n",
    "logger.info(f\"Reading data back from Iceberg table: {full_table_name}\")\n",
    "iceberg_data_df = spark.read.format(\"iceberg\").load(full_table_name)\n",
    "\n",
    "logger.info(\"Previewing first 10 records from the Iceberg table\")\n",
    "iceberg_data_df.show(10, truncate=False)\n",
    "\n",
    "# Count records using both DataFrame API and SQL\n",
    "logger.info(\"Total records in Iceberg table (DataFrame API):\")\n",
    "print(f\"DataFrame count: {iceberg_data_df.count()}\")\n",
    "\n",
    "# List the table snapshots\n",
    "logger.info(\"List the s3table snapshot versions:\")\n",
    "spark.sql(f\"SELECT * FROM {full_table_name}.history LIMIT 10\").show()\n",
    "\n",
    "# Stop Spark session\n",
    "logger.info(\"Stopping Spark Session\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ac2c9-96a0-420a-b270-8d720a0e094b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
