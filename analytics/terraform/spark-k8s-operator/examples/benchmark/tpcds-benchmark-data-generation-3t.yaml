# NOTE: This example requires the following prerequisites before executing the jobs
# 1. Ensure spark-team-a name space exists
# 2. replace <S3_BUCKET> with your bucket name

---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: tpcds-data-generation-3t
  namespace: spark-team-a
  labels:
    app: "tpcds-data-generation"
    applicationId: "tpcds-data-generation-3t"
    # Assign the job to a Yunikorn Queue via label.
    queue: root.prod
spec:
  type: Scala
  mode: cluster
  image: 463630279612.dkr.ecr.us-east-1.amazonaws.com/spark-tcpds:3.5.1-hadoop3.4.0-20240808
  imagePullPolicy: IfNotPresent
  sparkVersion: 3.5.1
  mainClass: com.amazonaws.eks.tpcds.DataGeneration
  mainApplicationFile: local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar
  arguments:
    # TPC-DS data location
    - "s3a://rotund-archive-463630279612-us-east-1/TPCDS-Data-3T"
    # Path to kit in the docker image
    - "/opt/tpcds-kit/tools"
    # Data Format
    - "parquet"
    # Scale factor (in GB)
    - "3000"
    # Generate data num partitions
    - "200"
    # Create the partitioned fact tables
    - "true"
    # Shuffle to get partitions coalesced into single files.
    - "true"
    # Logging set to WARN
    - "true"
  sparkConf:
    "spark.network.timeout": "2000s"
    "spark.executor.heartbeatInterval": "300s"
    "spark.kubernetes.memoryOverheadFactor": "0.3"
    "spark.sql.files.maxRecordsPerFile": "30000000"
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
    # S3 settings
    "spark.hadoop.fs.s3a.aws.credentials.provider": "software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"
    "spark.kubernetes.executor.podNamePrefix": "oss-data-gen"
    "spark.executor.defaultJavaOptions": "-verbose:gc -XX:+UseG1GC"
    "spark.driver.defaultJavaOptions": "-XX:+UseG1GC"
    # -----------------------------------------------------
    # This block is very critical when you get errors like
    #     Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred
    #     Caused by: java.net.SocketTimeoutException: timeout
    # spark.kubernetes.local.dirs.tmpfs: "true"
    spark.kubernetes.submission.connectionTimeout: "60000000"
    spark.kubernetes.submission.requestTimeout: "60000000"
    spark.kubernetes.driver.connectionTimeout: "60000000"
    spark.kubernetes.driver.requestTimeout: "60000000"
    # spark.kubernetes.allocation.batch.size: "20" # default 5 but adjust according to your cluster size
    # -----------------------------------------------------
    # Expose Spark metrics for Prometheus
    "spark.ui.prometheus.enabled": "true"
    "spark.executor.processTreeMetrics.enabled": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/scrape": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/path": "/metrics/executors/prometheus/"
    "spark.kubernetes.driver.annotation.prometheus.io/port": "4040"
    "spark.kubernetes.driver.service.annotation.prometheus.io/scrape": "true"
    "spark.kubernetes.driver.service.annotation.prometheus.io/path": "/metrics/driver/prometheus/"
    "spark.kubernetes.driver.service.annotation.prometheus.io/port": "4040"
    "spark.metrics.conf.*.sink.prometheusServlet.class": "org.apache.spark.metrics.sink.PrometheusServlet"
    "spark.metrics.conf.*.sink.prometheusServlet.path": "/metrics/driver/prometheus/"
    "spark.metrics.conf.master.sink.prometheusServlet.path": "/metrics/master/prometheus/"
    "spark.metrics.conf.applications.sink.prometheusServlet.path": "/metrics/applications/prometheus/"
  restartPolicy:
    type: Never
  driver:
    cores: 10
    # coreLimit: "10.1"
    memory: "10g"
    serviceAccount: spark-team-a
    # the c5d instances that Karpenter will launch will have the NVMe storage preformatted and available to the pod
    # we do not need to leverage a hostPath mount or volume to leverage that storage.
    # ephemeral-storage requests and limits can be used to manage the storage utilization
    nodeSelector:
      NodeGroupType: SparkBenchmarking
    tolerations:
      - key: "spark-benchmarking"
        operator: "Exists"
        effect: "NoSchedule"
  executor:
    cores: 11
    # coreLimit: "11.1"
    memory: "15g"
    instances: 26
    serviceAccount: spark-team-a
    # the c5d instances that Karpenter will launch will have the NVMe storage preformatted and available to the pod
    # we do not need to leverage a hostPath mount or volume to leverage that storage.
    # the data generation can utilize a large amount of storage
    nodeSelector:
      NodeGroupType: SparkBenchmarking
    tolerations:
      - key: "spark-benchmarking"
        operator: "Exists"
        effect: "NoSchedule"
