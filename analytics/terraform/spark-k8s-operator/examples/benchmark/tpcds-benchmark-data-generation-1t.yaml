# NOTE: This example requires the following prerequisites before executing the jobs
# 1. Ensure spark-team-a name space exists
# 2. replace `<S3_BUCKET>`  with your bucket name

apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: tpcds-data-generation-1tb
  namespace: spark-team-a
spec:
  # Temporarily commented out until the YuniKorn issue is resolved; falls back to the default Kubernetes scheduler
  # batchScheduler: yunikorn
  # batchSchedulerOptions:
  #   queue: root.default
  type: Scala
  mode: cluster
  image: public.ecr.aws/data-on-eks/spark3.5.3-scala2.12-java17-python3-ubuntu-tpcds:v2
  imagePullPolicy: IfNotPresent
  sparkVersion: 3.5.3
  mainClass: com.amazonaws.eks.tpcds.DataGeneration
  mainApplicationFile: local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar
  arguments:
  # TPC-DS data location
  - "s3a://<S3_BUCKET>/TPCDS-TEST-1TB"
  # Path to kit in the docker image
  - "/opt/tpcds-kit/tools"
  # Data Format
  - "parquet"
  # Scale factor (in GB) - S3 output size shows  309.4GB for 1000GB Input
  - "1000"
  # Generate data num partitions
  - "200"
  # Create the partitioned fact tables
  - "true"
  # Shuffle to get partitions coalesced into single files.
  - "true"
  # Logging set to WARN
  - "true"
  sparkConf:
    # Expose Spark metrics for Prometheus
    "spark.ui.prometheus.enabled": "true"
    "spark.executor.processTreeMetrics.enabled": "true"
    "spark.metrics.conf.*.sink.prometheusServlet.class": "org.apache.spark.metrics.sink.PrometheusServlet"
    "spark.metrics.conf.driver.sink.prometheusServlet.path": "/metrics/driver/prometheus/"
    "spark.metrics.conf.executor.sink.prometheusServlet.path": "/metrics/executors/prometheus/"

    # Spark Event logs
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://<S3_BUCKET>/spark-event-logs"
    "spark.eventLog.rolling.enabled": "true"
    "spark.eventLog.rolling.maxFileSize": "64m"

    "spark.executorEnv.JAVA_HOME": "/opt/java/openjdk"
    "spark.driverEnv.JAVA_HOME": "/opt/java/openjdk"
    "spark.network.timeout": "2000s"
    "spark.executor.heartbeatInterval": "300s"
    # Bug: memoryOverheadFactor is not calculated correctly with Spark Operator when used with YuniKorn Gang Scheduling. Just use memoryOverhead for driver and executor instead.
    # "spark.kubernetes.memoryOverheadFactor": "0.3"
    # AQE
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.localShuffleReader.enabled": "true"
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
    "spark.sql.adaptive.skewJoin.enabled": "true"
    "spark.sql.files.maxRecordsPerFile": "30000000"
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"

    # S3 Optimizations
    "spark.hadoop.fs.s3.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    # AWS SDK V1 is in maintenance mode hence commented and enabled AWS SDK V2 Class with S3 CRT support
    # "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
    # AWS SDK V2 https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/aws_sdk_upgrade.html
    "spark.hadoop.fs.s3a.aws.credentials.provider.mapping": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.fast.upload.buffer": "disk"
    "spark.hadoop.fs.s3a.buffer.dir": "/data1/s3a"
    "spark.hadoop.fs.s3a.multipart.size": "128M" # Good for large files
    "spark.hadoop.fs.s3a.multipart.threshold": "256M"
    "spark.hadoop.fs.s3a.threads.max": "50"
    "spark.hadoop.fs.s3a.connection.maximum": "200"
    "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2"
    "spark.kubernetes.executor.podNamePrefix": "oss-data-gen"
    "spark.sql.shuffle.partitions": "2000" # Adjust according to your job size
    # "spark.hadoop.fs.s3a.committer.staging.conflict-mode": "append"
    # Data writing and shuffle tuning
    "spark.shuffle.file.buffer": "1m"
    "spark.reducer.maxSizeInFlight": "128m"
    # Java options for driver and executor
    "spark.executor.defaultJavaOptions": "-verbose:gc -XX:+UseG1GC"
    "spark.driver.defaultJavaOptions": "-XX:+UseG1GC"
    # -----------------------------------------------------
    # This block is very critical when you get errors like
    #     Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred
    #     Caused by: java.net.SocketTimeoutException: timeout
    # Timeout settings for large data generation
    spark.kubernetes.submission.connectionTimeout: "60000000"
    spark.kubernetes.submission.requestTimeout: "60000000"
    spark.kubernetes.driver.connectionTimeout: "60000000"
    spark.kubernetes.driver.requestTimeout: "60000000"
  restartPolicy:
    type: Never
  driver:
    cores: 11
    # The maximum memory size of the container to the running executor is determined by the sum of
    #  spark.executor.memoryoverHead, spark.executor.memory, spark.memory.offHeap.size, spark.executor.pyspark.memory
    memory: "15g"
    memoryOverhead: "4g"
    serviceAccount: spark-team-a
    securityContext:
      runAsUser: 185
    volumeMounts:
    - name: spark-local-dir-1
      mountPath: /data1
    env:
    - name: JAVA_HOME
      value: "/opt/java/openjdk"
    initContainers:
    - name: volume-permission
      image: public.ecr.aws/docker/library/busybox
      command: ['sh', '-c', 'mkdir -p /data1; chown -R 185:185 /data1']
      volumeMounts:
      - name: spark-local-dir-1
        mountPath: /data1
    nodeSelector:
      NodeGroupType: spark_benchmark_ssd
  executor:
    cores: 11
    # The maximum memory size of the container to the running executor is determined by the sum of
    #  spark.executor.memoryoverHead, spark.executor.memory, spark.memory.offHeap.size, spark.executor.pyspark.memory
    memory: "15g"
    memoryOverhead: "4g"
    instances: 22
    serviceAccount: spark-team-a
    securityContext:
      runAsUser: 185
    volumeMounts:
    - name: spark-local-dir-1
      mountPath: /data1
    initContainers:
    - name: volume-permission
      image: public.ecr.aws/docker/library/busybox
      command: ['sh', '-c', 'mkdir -p /data1; chown -R 185:185 /data1']
      volumeMounts:
      - name: spark-local-dir-1
        mountPath: /data1
    env:
    - name: JAVA_HOME
      value: "/opt/java/openjdk"
    nodeSelector:
      NodeGroupType: spark_benchmark_ssd
  volumes:
  - name: spark-local-dir-1
    hostPath:
      path: "/mnt/k8s-disks/0"
      type: DirectoryOrCreate
