apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: spark-vertical-ebs-scale
  namespace: karpenter
spec:
  kubeletConfiguration:
    containerRuntime: containerd
#    podsPerCore: 2
#    maxPods: 20
  requirements:
    - key: "topology.kubernetes.io/zone"
      operator: In
      values: [${azs}b] #Update the correct region and zone
    - key: "karpenter.sh/capacity-type"
      operator: In
      values: ["spot", "on-demand"]
    - key: "karpenter.k8s.aws/instance-family"
      operator: In
      values: ["r4", "r4", "r5", "r5d", "r5n", "r5dn", "r5b", "m4", "m5", "m5n", "m5zn", "m5dn", "m5d", "c4", "c5", "c5n", "c5d"]
    - key: "kubernetes.io/arch"
      operator: In
      values: ["amd64"]
    - key: "karpenter.k8s.aws/instance-cpu"
      operator: Gt
      values: ["1"]
  providerRef: # optional, recommended to use instead of `provider`
    name: spark-vertical-ebs-scale
  labels:
    type: karpenter
    provisioner: spark-vertical-ebs-scale
  taints:
    - key: spark-vertical-ebs-scale
      value: 'true'
      effect: NoSchedule
  # If omitted, the feature is disabled, nodes will never scale down due to low utilization
  ttlSecondsAfterEmpty: 120

---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: spark-vertical-ebs-scale
  namespace: karpenter
spec:
  blockDeviceMappings:
    - deviceName: /dev/xvda # root volume
      ebs:
        volumeSize: 40Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true
  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
    httpTokens: required
  subnetSelector:
    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes
  securityGroupSelector:                      # required, when not using launchTemplate
    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes

  # Script for creating EBS Volume for Spark workloads proportional to number of cores available on the nodes
  userData: |
    MIME-Version: 1.0
    Content-Type: multipart/mixed; boundary="BOUNDARY"

    --BOUNDARY
    Content-Type: text/x-shellscript; charset="us-ascii"

    #!/bin/bash
    echo "Running a custom user data script"
    set -ex
    yum install mdadm -y

    IDX=1
    DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}')

    DISK_ARRAY=()

    for DEV in $DEVICES
    do
      DISK_ARRAY+=("/dev/$${DEV}")
    done

    DISK_COUNT=$${#DISK_ARRAY[@]}

    if [ $${DISK_COUNT} -eq 0 ]; then
      echo "No SSD disks available. Creating new EBS volume according to number of cores available in the node."
      yum install -y jq awscli
      TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 3600")

      # Get instance info
      INSTANCE_ID=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/instance-id)
      AVAILABILITY_ZONE=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/placement/availability-zone)
      REGION=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/[a-z]$//')

      # Get the number of cores available
      CORES=$(nproc --all)

      # Define volume size based on the number of cores and EBS volume size per core
      VOLUME_SIZE=$(expr $CORES \* 10) # 10GB per core. Change as desired

      # Create a volume
      VOLUME_ID=$(aws ec2 create-volume --availability-zone $AVAILABILITY_ZONE --size $VOLUME_SIZE --volume-type gp3 --region $REGION --output text --query 'VolumeId')

      # Check whether the volume is available
      while [ "$(aws ec2 describe-volumes --volume-ids $VOLUME_ID --region $REGION --query "Volumes[*].State" --output text)" != "available" ]; do
        echo "Waiting for volume to become available"
        sleep 5
      done

      # Attach the volume to the instance
      aws ec2 attach-volume --volume-id $VOLUME_ID --instance-id $INSTANCE_ID --device /dev/xvdb --region $REGION

      # Update the state to delete the volume when the node is terminated
      aws ec2 modify-instance-attribute --instance-id $INSTANCE_ID --block-device-mappings "[{\"DeviceName\": \"/dev/xvdb\",\"Ebs\":{\"DeleteOnTermination\":true}}]" --region $REGION

      # Wait for the volume to be attached
      while [ "$(aws ec2 describe-volumes --volume-ids $VOLUME_ID --region $REGION --query "Volumes[*].Attachments[*].State" --output text)" != "attached" ]; do
        echo "Waiting for volume to be attached"
        sleep 5
      done

      # Format the volume
      sudo mkfs -t ext4 /dev/xvdb # Improve this to get this value dynamically
      # Create a mount point
      sudo mkdir /mnt/k8s-disks # Change directory as you like
      # Mount the volume
      sudo mount /dev/xvdb /mnt/k8s-disks
      # To mount this EBS volume on every system reboot, you need to add an entry in /etc/fstab
      echo "/dev/xvdb /mnt/k8s-disks ext4 defaults,nofail 0 2" | sudo tee -a /etc/fstab

      # Adding permissions to the mount
      /usr/bin/chown -hR +999:+1000 /mnt/k8s-disks
    else
      if [ $${DISK_COUNT} -eq 1 ]; then
        TARGET_DEV=$${DISK_ARRAY[0]}
        mkfs.xfs $${TARGET_DEV}
      else
        mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]}
        mkfs.xfs /dev/md0
        TARGET_DEV=/dev/md0
      fi

      mkdir -p /mnt/k8s-disks
      echo $${TARGET_DEV} /mnt/k8s-disks xfs defaults,noatime 1 2 >> /etc/fstab
      mount -a
      /usr/bin/chown -hR +999:+1000 /mnt/k8s-disks
    fi

    --BOUNDARY--

  tags:
    InstanceType: "spark-vertical-ebs-scale"    # optional, add tags for your own use
