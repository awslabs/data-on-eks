apiVersion: v1
kind: Secret
metadata:
  name: token
stringData:
  token: HUGGING_FACE_HUB_TOKEN
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-app-code
data:
  app.py: |
    import multiprocessing
    import os
    import logging
    import time
    from fastapi import FastAPI
    from starlette.requests import Request
    from starlette.responses import StreamingResponse, JSONResponse
    from ray import serve
    from llama_cpp import Llama

    logger = logging.getLogger("ray.serve")
    app = FastAPI()

    @serve.deployment(
        name="LLamaCPPDeployment",
        ray_actor_options={"num_cpus": 29},
        autoscaling_config={
            "min_replicas": 10,
            "max_replicas": 10,
            "initial_replicas": 10,
            "upscale_delay_s": 5
        },
        max_ongoing_requests=100,
        graceful_shutdown_timeout_s=600
    )
    @serve.ingress(app)
    class LLamaCPPDeployment:
        def __init__(self, parallelism: str):
            os.environ["OMP_NUM_THREADS"] = parallelism
            self.model_id = os.getenv("MODEL_ID", default="SanctumAI/Llama-3.2-1B-Instruct-GGUF")
            self.filename = os.getenv("MODEL_FILENAME", default="*Q4_0.gguf")
            self.n_ctx = int(os.getenv("N_CTX"))
            self.n_threads = int(os.getenv("N_THREADS"))
            self.llm = Llama.from_pretrained(
                repo_id=self.model_id,
                filename=self.filename,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads
            )
            print("__init__ Complete")

        @app.post("/v1/chat/completions")
        async def call_llama(self, request: Request):
            try:
                body = await request.json()
                messages = body.get("messages", [])
                prompt = ""
                if messages:
                    for message in messages:
                        if message.get("role") == "user":
                            prompt = message.get("content", "")
                            break

                if not prompt:
                    return JSONResponse(
                        status_code=400,
                        content={"error": "prompt is required"}
                    )

                output = self.llm(
                    "Q: " + prompt + " A: ",
                    max_tokens=body.get("max_tokens", 32)
                )

                return JSONResponse(content={
                    "id": "cmpl-" + os.urandom(12).hex(),
                    "object": "text_completion",
                    "created": int(time.time()),
                    "model": self.model_id,
                    "choices": [{
                        "text": output["choices"][0]["text"],
                        "index": 0,
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": len(prompt.split()),
                        "completion_tokens": len(output["choices"][0]["text"].split()),
                        "total_tokens": len(prompt.split()) + len(output["choices"][0]["text"].split())
                    }
                })

            except Exception as e:
                logger.error(f"Error: {str(e)}")
                return JSONResponse(
                    status_code=500,
                    content={"error": str(e)}
                )

    host_cpu_count = multiprocessing.cpu_count()

    model = LLamaCPPDeployment.bind("host_cpu_count")
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: ray-service-llamacpp
spec:
  serveConfigV2: |
    applications:
    - name: llmcpp-arm
      route_prefix: /
      import_path: app:model
      deployments:
      - name: LLamaCPPDeployment
        ray_actor_options:
          num_cpus: 29
      runtime_env:
        pip: ["llama_cpp_python", "transformers"]
        env_vars:
          LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
          MODEL_ID: "bartowski/Llama-3.2-1B-Instruct-GGUF"
          MODEL_FILENAME: "Llama-3.2-1B-Instruct-Q4_0.gguf"
          N_CTX: "0"
          N_THREADS : "16"
          FORCE_CMAKE: "1"
          CMAKE_ARGS: "-DCMAKE_CXX_FLAGS='-mcpu=native -fopenmp' -DCMAKE_C_FLAGS='-mcpu=native -fopenmp'"
          CMAKE_CXX_COMPILER: "/usr/bin/g++"
          CMAKE_C_COMPILER: "/usr/bin/gcc"
          CXX: "/usr/bin/g++"
          CC: "/usr/bin/gcc"
          PYTHONPATH: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH"
  rayClusterConfig:
    rayVersion: '2.33.0'
    enableInTreeAutoscaling: true
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        metadata:
          labels:
            ray-control-plane: "true"
        spec:
          nodeSelector:
            kubernetes.io/arch: arm64
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                    - arm64
          restartPolicy: Always
          containers:
          - name: ray-head
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            volumeMounts:
              - name: app-code
                mountPath: /home/ray/app.py
                subPath: app.py
            resources:
              limits:
                cpu: "10"
                memory: "20Gi"
              requests:
                cpu: "10"
                memory: "20Gi"
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: token
                  key: token
            - name: RAY_enable_autoscaler_v2
              value: "1"
            - name: RAY_num_heartbeats_timeout
              value: "300"
          volumes:
              - name: app-code
                configMap:
                  name: llama-app-code
    workerGroupSpecs:
    - replicas: 10
      minReplicas: 10
      maxReplicas: 10
      groupName: cpu-group
      rayStartParams:
        num-cpus: "29"
      template:
        spec:
          nodeSelector:
            kubernetes.io/arch: arm64
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                    - arm64
          restartPolicy: Always
          containers:
          - name: llm
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            volumeMounts:
              - name: app-code
                mountPath: /home/ray/app.py
                subPath: app.py
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: token
                  key: token
            - name: CMAKE_ARGS
              value: "-DCMAKE_CXX_FLAGS=-fopenmp"
            - name: "CC"
              value: "/usr/bin/gcc"
            - name: "CXX"
              value: "/usr/bin/g++"
            - name: CMAKE_CXX_COMPILER
              value: "/usr/bin/g++"
            - name: CMAKE_C_COMPILER
              value: "/usr/bin/gcc"
            - name: PYTHONPATH
              value: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH"
            resources:
              limits:
                cpu: "30"
                memory: "55Gi"
              requests:
                cpu: "30"
                memory: "55Gi"
          volumes:
              - name: app-code
                configMap:
                  name: llama-app-code
