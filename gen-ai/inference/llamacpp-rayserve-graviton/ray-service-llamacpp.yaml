apiVersion: v1
kind: Secret
metadata:
  name: hf-token
  namespace: kuberay-system
stringData:
  hf-token: $HUGGING_FACE_HUB_TOKEN
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llama-3-8b-cpu-llama-arm
spec:
  serveConfigV2: |
    applications:
    - name: llmcpp-arm
      route_prefix: /
      import_path: llamacpp-serve:model
      deployments:
      - name: LLamaCPPDeployment
        ray_actor_options:
          num_cpus: 29
      runtime_env:
        working_dir: "https://github.com/ddynwzh1992/ray-llm/archive/refs/heads/main.zip"
        pip: ["llama_cpp_python", "transformers==4.46.0"]
        env_vars:
          LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
          MODEL_ID: "bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF"
          MODEL_FILENAME: "DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf"
          N_CTX: "0"
          N_THREADS : "32"
          FORCE_CMAKE: "1"
          CMAKE_ARGS: "-DCMAKE_CXX_FLAGS='-mcpu=native -fopenmp' -DCMAKE_C_FLAGS='-mcpu=native -fopenmp'"
          CMAKE_CXX_COMPILER: "/usr/bin/g++"  
          CMAKE_C_COMPILER: "/usr/bin/gcc"  
          CXX: "/usr/bin/g++"  
          CC: "/usr/bin/gcc" 
          PYTHONPATH: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH" 


  rayClusterConfig:
    rayVersion: '2.33.0'
    enableInTreeAutoscaling: true
    #rayVersion: 3.0.0.dev0
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        metadata:
          labels:
            ray-control-plane: "true"

        spec:
          nodeSelector:
            ray-control-plane: "false"
            model-inferencing: "cpu-arm"
          tolerations:
            - key: "model-inferencing"
              operator: "Equal"
              value: "cpu-arm"
              effect: "NoSchedule"

          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: model-inferencing
                        operator: In
                        values:
                          - "cpu-arm"            

          # nodeSelector:
          #   ray-control-plane: "true"

          # tolerations:
          #   - key: "ray-control-plane"
          #     operator: "Equal"
          #     value: "true"
          #     effect: "NoSchedule"

          # affinity:
          #   nodeAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       nodeSelectorTerms:
          #         - matchExpressions:
          #             - key: ray-control-plane
          #               operator: In
          #               values:
          #                 - "true"

          restartPolicy: Always
          containers:
          - name: ray-head
            # image: rayproject/ray:nightly-py311-cpu
            #image: 412381761882.dkr.ecr.us-west-2.amazonaws.com/kuberay-system/ray-graviton:latest
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            resources:
              limits:
                cpu: "30"
                memory: "55Gi"
              requests:
                cpu: "30"
                memory: "55Gi"
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: hf-token
            - name: RAY_enable_autoscaler_v2
              value: "1"
            - name: RAY_num_heartbeats_timeout
              value: "300"
            # - name: VLLM_ATTENTION_BACKEND
            #   value: "TORCH_SDPA"  

              # valueFrom:
              #   secretKeyRef:
              #     name: hf-secret
              #     key: hf_api_token
    workerGroupSpecs:
    - replicas: 10
      minReplicas: 10
      maxReplicas: 10
      groupName: cpu-group
      rayStartParams: 
        num-cpus: "29"
      template:
        spec:
          nodeSelector:
            ray-control-plane: "false"
            model-inferencing: "cpu-arm"
          tolerations:
            - key: "model-inferencing"
              operator: "Equal"
              value: "cpu-arm"
              effect: "NoSchedule"

          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: model-inferencing
                        operator: In
                        values:
                          - "cpu-arm"            


          restartPolicy: Always
          containers:
          - name: llm
            # image: rayproject/ray-ml:2.33.0.914af0-py311
            #image: 412381761882.dkr.ecr.us-west-2.amazonaws.com/kuberay-system/ray-graviton:latest
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            env:
            # - name: HUGGING_FACE_HUB_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: hf-secret
            #       key: hf_api_token
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: hf-token
            - name: CMAKE_ARGS
              value: "-DCMAKE_CXX_FLAGS=-fopenmp"  
            - name: "CC"
              value: "/usr/bin/gcc"   
            - name: "CXX"
              value: "/usr/bin/g++"   
            # - name: RAY_num_heartbeats_timeout
            #   value: "300"
            - name: CMAKE_CXX_COMPILER
              value: "/usr/bin/g++"  
            - name: CMAKE_C_COMPILER
              value: "/usr/bin/gcc" 
            - name: PYTHONPATH
              value: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH" 
            # - name: VLLM_ATTENTION_BACKEND
            #   value: "TORCH_SDPA"  


            # ports:
            # - containerPort: 8000
            #   name: serve

            resources:
              limits:
                cpu: "30"
                memory: "55Gi"
         
              requests:
                cpu: "30"
                memory: "55Gi"
                # nvidia.com/gpu: "2"

          # Please add the following taints to the GPU node.
            # tolerations:
            #   - key: "nvidia.com/gpu"
            #     operator: "Exists"
            #     effect: "NoSchedule"
