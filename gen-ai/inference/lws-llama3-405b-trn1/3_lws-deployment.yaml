---
apiVersion: v1
kind: Namespace
metadata:
  name: llama3_405b_ns
---

apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: llama3_405b_lws
  namespace: llama3_405b_ns
  annotations:
    leaderworkerset.sigs.k8s.io/exclusive-topology: eks.amazonaws.com/nodegroup
spec:
  leaderWorkerTemplate:
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
        - command:
          - /usr/local/bin/run.sh
          env:
          - name: AWS_DOEKS_ENDPOINT_NAME
            value: llama3-1-405b-instruct-v1
          - name: NEURON_RANK_ID
            value: "0"
          - name: NEURON_LOCAL_TP
            value: "32"
          - name: FI_EFA_USE_DEVICE_RDMA
            value: "1"
          - name: FI_PROVIDER
            value: efa
          - name: FI_EFA_FORK_SAFE
            value: "1"
          - name: AWS_MAX_ATTEMPTS
            value: "3"
          - name: CCOM_SOCKET_IFNAME
            value: eth0
          - name: WORKER_COUNT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          - name: AWS_DOEKS_REGION
            value: us-west-2
          - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
            value: "600"
          - name: AWS_DEFAULT_REGION
            value: us-west-2
          - name: AWS_DOEKS_MAX_BATCH_SIZE
            value: "1"
          # Move NEURON_COMPILE_CACHE_URL to FSx        
          - name: NEURON_COMPILE_CACHE_URL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/compiled_artifact
          - name: NEURON_MODEL_PATH
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_CONTEXT_LENGTH_ESTIMATE
            value: '[''1024'',''2048'',''4096'',''8192'',''16384'',''32768'',''65536'',''131072'']'
          - name: NEURON_CC_PIPELINE_FACTOR
            value: "4"
          - name: WORLD_SIZE
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          image: public.ecr.aws/c4b0z4q6/llama3.1_405b_neuron:latest
          imagePullPolicy: Always
          name: vllm-leader
          ports:
          - containerPort: 8989
            protocol: TCP
          - containerPort: 8080
            protocol: TCP
          - containerPort: 8990
            protocol: TCP
          resources:
            limits:
              aws.amazon.com/neuron: "16"
              vpc.amazonaws.com/efa: "16"
            requests:
              aws.amazon.com/neuron: "16"
              cpu: "110"
              memory: 350Gi
              vpc.amazonaws.com/efa: "16"
          volumeMounts:
          - mountPath: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct
            name: model-volume
        nodeSelector:
          instance-type: "trn1-32xl"
          # spine: 4-179-es-m1
        tolerations:
          - key: "aws.amazon.com/neuron"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        volumes:
        - hostPath:
            path: /mnt/k8s-disks/0
            type: DirectoryOrCreate
          name: model-volume
    restartPolicy: RecreateGroupOnPodRestart
    size: 4
    workerTemplate:
      metadata: {}
      spec:
        containers:
        - command:
          - /usr/local/bin/run.sh
          env:
          - name: AWS_DOEKS_ENDPOINT_NAME
            value: llama3-1-405b-instruct-v1
          - name: NEURON_RANK_ID
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['leaderworkerset.sigs.k8s.io/worker-index']
          - name: NEURON_LOCAL_TP
            value: "32"
          - name: FI_EFA_USE_DEVICE_RDMA
            value: "1"
          - name: FI_PROVIDER
            value: efa
          - name: FI_EFA_FORK_SAFE
            value: "1"
          - name: AWS_MAX_ATTEMPTS
            value: "3"
          - name: CCOM_SOCKET_IFNAME
            value: eth0
          - name: WORKER_COUNT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          - name: AWS_DOEKS_REGION
            value: us-west-2
          - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
            value: "600"
          - name: AWS_DEFAULT_REGION
            value: us-west-2
          - name: AWS_DOEKS_MAX_BATCH_SIZE
            value: "1"
          - name: NEURON_MODEL_PATH
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_COMPILE_CACHE_URL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/compiled_artifact
          - name: NEURON_CONTEXT_LENGTH_ESTIMATE
            value: '[''1024'',''2048'',''4096'',''8192'',''16384'',''32768'',''65536'',''131072'']'
          - name: NEURON_CC_PIPELINE_FACTOR
            value: "4"
          - name: WORLD_SIZE
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          image: public.ecr.aws/c4b0z4q6/llama3.1_405b_neuron:latest
          imagePullPolicy: Always
          name: vllm-worker
          ports:
          - containerPort: 8989
            protocol: TCP
          - containerPort: 8080
            protocol: TCP
          - containerPort: 8990
            protocol: TCP
          resources:
            limits:
              aws.amazon.com/neuron: "16"
              vpc.amazonaws.com/efa: "16"
            requests:
              aws.amazon.com/neuron: "16"
              cpu: "110"
              memory: 350Gi
              vpc.amazonaws.com/efa: "16"
          volumeMounts:
          - mountPath: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct
            name: model-volume
        nodeSelector:
          instance-type: "trn1-32xl"
          # spine: 4-179-es-m1
        tolerations:
          - key: "aws.amazon.com/neuron"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        volumes:
        - hostPath:
            path: /mnt/k8s-disks/0
            type: Directory
          name: model-volume
  replicas: 1
  rolloutStrategy:
    rollingUpdateConfiguration:
      maxSurge: 1
      maxUnavailable: 2
    type: RollingUpdate
  startupPolicy: LeaderCreated
