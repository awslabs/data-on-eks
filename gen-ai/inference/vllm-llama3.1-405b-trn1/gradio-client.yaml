apiVersion: v1
kind: ConfigMap
metadata:
  name: gradio-llama-script
data:
  app.py: |
    import gradio as gr
    import requests
    import json
    import time

    # Configuration
    MODEL_API_URL = "http://vllm-leader:8080/generate"  # Updated to use vllm-leader service
    DEFAULT_PROMPT = "Write a short story about a time traveler who accidentally ends up in the future and has to navigate the changes in technology and society."

    def generate_story(prompt, max_tokens, temperature, top_p):
        headers = {"Content-Type": "application/json"}
        data = {
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stream": False
        }

        try:
            start_time = time.time()
            response = requests.post(MODEL_API_URL, headers=headers, data=json.dumps(data), timeout=180)
            response.raise_for_status()
            end_time = time.time()

            result = response.json()
            generated_text = result["text"][0]

            # Calculate tokens per second
            input_tokens = len(prompt.split())
            output_tokens = len(generated_text.split())
            total_tokens = input_tokens + output_tokens
            time_taken = end_time - start_time
            tokens_per_second = total_tokens / time_taken

            performance_stats = f"\n\nPerformance Stats:\nTime taken: {time_taken:.2f} seconds\nTokens generated: {output_tokens}\nTokens per second: {tokens_per_second:.2f}"

            return generated_text + performance_stats
        except requests.exceptions.RequestException as e:
            return f"An error occurred: {str(e)}"

    # Gradio interface
    with gr.Blocks(title="Llama 3.1 405B Story Generator") as iface:
        gr.Markdown("# Llama 3.1 405B Story Generator")
        gr.Markdown("Generate creative stories using the Llama 3.1 405B model.")

        with gr.Row():
            with gr.Column(scale=2):
                prompt_input = gr.Textbox(
                    lines=5,
                    label="Enter your prompt",
                    placeholder="Type your story prompt here...",
                    value=DEFAULT_PROMPT
                )
            with gr.Column(scale=1):
                max_tokens_slider = gr.Slider(minimum=500, maximum=7692, value=500, step=50, label="Max Tokens")
                temperature_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label="Temperature")
                top_p_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.1, label="Top P")

        generate_button = gr.Button("Generate Story")
        output_text = gr.Textbox(lines=10, label="Generated Story")

        generate_button.click(
            generate_story,
            inputs=[prompt_input, max_tokens_slider, temperature_slider, top_p_slider],
            outputs=output_text
        )

    iface.launch(server_name="0.0.0.0", server_port=7860)

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gradio-llama-interface
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gradio-llama-interface
  template:
    metadata:
      labels:
        app: gradio-llama-interface
    spec:
      containers:
      - name: gradio-llama-interface
        image: python:3.9-slim
        ports:
        - containerPort: 7860
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install gradio requests
          python /app/app.py
        volumeMounts:
        - name: script-volume
          mountPath: /app
      volumes:
      - name: script-volume
        configMap:
          name: gradio-llama-script

---
apiVersion: v1
kind: Service
metadata:
  name: gradio-llama-interface
spec:
  selector:
    app: gradio-llama-interface
  ports:
  - protocol: TCP
    port: 7860
    targetPort: 7860
  type: ClusterIP
