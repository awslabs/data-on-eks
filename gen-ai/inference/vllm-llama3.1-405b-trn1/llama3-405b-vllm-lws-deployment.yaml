# Important Notice:
# Before deploying this configuration, please ensure the following:
# 1. **Region and Environment Variables**: Verify that the `AWS_DEFAULT_REGION` and other environment variables are correctly set for your deployment. This YAML is currently configured for the `us-west-2` region.
# 2. **Pre-requisite**: Check the README.md file for instructions.
# 3. **Custom Images**: This configuration uses a custom image hosted on ECR (`public.ecr.aws/data-on-eks/llama3.1_405b_neuron2.19-patch_vllm0.5.0-patch:latest`). Ensure that this image is accessible and meets your deployment needs.

---
apiVersion: v1
kind: Service
metadata:
  name: vllm-leader
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    leaderworkerset.sigs.k8s.io/name: llama3-405b-lws
    role: leader
  type: ClusterIP

---
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: llama3-405b-lws
  namespace: default
  annotations:
    leaderworkerset.sigs.k8s.io/exclusive-topology: eks.amazonaws.com/nodegroup
spec:
  leaderWorkerTemplate:
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        terminationGracePeriodSeconds: 90
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
        containers:
        - command:
          - /usr/local/bin/run.sh
          env:
          - name: SWAP_SPACE
            value: "2"
          - name: MODEL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_RANK_ID
            value: "0"
          - name: NEURON_CC_FLAGS
            value: "-O1"
          - name: NEURON_LOCAL_TP
            value: "32"
          - name: FI_EFA_USE_DEVICE_RDMA
            value: "1"
          - name: FI_PROVIDER
            value: efa
          - name: FI_EFA_FORK_SAFE
            value: "1"
          - name: AWS_MAX_ATTEMPTS
            value: "3"
          - name: CCOM_SOCKET_IFNAME
            value: eth0
          - name: WORKER_COUNT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
            value: "600"
          - name: AWS_DEFAULT_REGION
            value: us-west-2
          - name: NEURON_COMPILE_CACHE_URL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/compiled_artifact
          - name: NEURON_MODEL_PATH
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURONX_DUMP_TO
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_CONTEXT_LENGTH_ESTIMATE
            value: '[''1024'',''2048'',''4096'',''8192'',''16384'',''32768'',''65536'',''131072'']'
          - name: MAX_MODEL_LENGTH
            value: "8192"
          - name: BLOCK_SIZE
            value: "8192"
          - name: MAX_NUM_SEQ
            value: "2"
          - name: NEURON_CC_PIPELINE_FACTOR
            value: "4"
          - name: WORLD_SIZE
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          image: public.ecr.aws/data-on-eks/llama3.1_405b_neuron2.19-patch_vllm0.5.0-patch:latest
          imagePullPolicy: Always
          name: vllm-leader
          ports:
          - containerPort: 8989
            protocol: TCP
          - containerPort: 8080
            protocol: TCP
          - containerPort: 8990
            protocol: TCP
          resources:
            limits:
              aws.amazon.com/neuron: "16"
              vpc.amazonaws.com/efa: "8"
            requests:
              aws.amazon.com/neuron: "16"
              cpu: "110"
              memory: 350Gi
              vpc.amazonaws.com/efa: "8"
          volumeMounts:
          - mountPath: /mnt/k8s-disks/0
            name: model-volume
        nodeSelector:
          instance-type: "trn1-32xl"
        tolerations:
          - key: "aws.amazon.com/neuron"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        volumes:
        - hostPath:
            path: /mnt/k8s-disks/0
            type: DirectoryOrCreate
          name: model-volume
    restartPolicy: Default
    size: 4
    workerTemplate:
      metadata: {}
      spec:
        terminationGracePeriodSeconds: 90
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
        containers:
        - command:
          - /usr/local/bin/run.sh
          env:
          - name: MODEL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: SWAP_SPACE
            value: "2"
          - name: NEURON_RANK_ID
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['leaderworkerset.sigs.k8s.io/worker-index']
          - name: NEURON_LOCAL_TP
            value: "32"
          - name: NEURON_CC_FLAGS
            value: "-O1"
          - name: FI_EFA_USE_DEVICE_RDMA
            value: "1"
          - name: FI_PROVIDER
            value: efa
          - name: FI_EFA_FORK_SAFE
            value: "1"
          - name: AWS_MAX_ATTEMPTS
            value: "3"
          - name: CCOM_SOCKET_IFNAME
            value: eth0
          - name: WORKER_COUNT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
            value: "600"
          - name: AWS_DEFAULT_REGION
            value: us-west-2
          - name: NEURONX_DUMP_TO
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_MODEL_PATH
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/
          - name: NEURON_COMPILE_CACHE_URL
            value: /mnt/k8s-disks/0/checkpoints/llama-3.1-405b-instruct/compiled_artifact
          - name: NEURON_CONTEXT_LENGTH_ESTIMATE
            value: '[''1024'',''2048'',''4096'',''8192'',''16384'',''32768'',''65536'',''131072'']'
          - name: MAX_MODEL_LENGTH
            value: "8192"
          - name: BLOCK_SIZE
            value: "8192"
          - name: MAX_NUM_SEQ
            value: "2"
          - name: NEURON_CC_PIPELINE_FACTOR
            value: "4"
          - name: WORLD_SIZE
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
          image: public.ecr.aws/data-on-eks/llama3.1_405b_neuron2.19-patch_vllm0.5.0-patch:latest
          imagePullPolicy: Always
          name: vllm-worker
          ports:
          - containerPort: 8989
            protocol: TCP
          - containerPort: 8080
            protocol: TCP
          - containerPort: 8990
            protocol: TCP
          resources:
            limits:
              aws.amazon.com/neuron: "16"
              vpc.amazonaws.com/efa: "8"
            requests:
              aws.amazon.com/neuron: "16"
              cpu: "110"
              memory: 350Gi
              vpc.amazonaws.com/efa: "8"
          volumeMounts:
          - mountPath: /mnt/k8s-disks/0
            name: model-volume
        nodeSelector:
          instance-type: "trn1-32xl"
        tolerations:
          - key: "aws.amazon.com/neuron"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        volumes:
          - hostPath:
              path: /mnt/k8s-disks/0
              type: Directory
            name: model-volume
  replicas: 1
  rolloutStrategy:
    rollingUpdateConfiguration:
      maxSurge: 1
      maxUnavailable: 2
    type: RollingUpdate
  startupPolicy: LeaderCreated
