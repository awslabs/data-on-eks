#----------------------------------------------------------------------
# NOTE: For deployment instructions, refer to the DoEKS website.
#----------------------------------------------------------------------
---
apiVersion: v1
kind: Namespace
metadata:
  name: stablediffusion
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: stablediffusion
  namespace: stablediffusion
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
# Ray Serve can automatically scale deployment replicas up to 3 based on incoming traffic.
# Each replica in this example requires one GPU. For GPU types g5.xlarge to g5.16xlarge, a single node with one GPU can only run one replica.
# However, g5.12xlarge comes with 4 GPUs, allowing you to run all 4 replicas on a single node.
  serveConfigV2: |
    applications:
      - name: stable-diffusion-deployment
        import_path: "ray_serve_sd:entrypoint"
        route_prefix: "/"
        runtime_env:
          env_vars:
            MODEL_PATH: "./stable-diffusion-2"
            MODEL_ID: "stabilityai/stable-diffusion-2-1"
          pip:
            - transformers==4.25.1
            - accelerate==0.30.1
        deployments:
          - name: stable-diffusion-v2
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: 1
              max_replicas: 4
              look_back_period_s: 2
              downscale_delay_s: 600
              upscale_delay_s: 30
              target_ongoing_requests: 1
            graceful_shutdown_timeout_s: 5
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 3
              num_gpus: 1
  rayClusterConfig:
    rayVersion: '2.24.0'
    enableInTreeAutoscaling: true
    headGroupSpec:
      headService:
        metadata:
          name: stablediffusion
          namespace: stablediffusion
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            # Important Performance Note:
            # This image is large (20GB uncompressed, 10GB compressed) and may take up to 5 minutes to pull.
            # For faster inference scaling, consider building a custom image with only your workload's essential dependencies.
            # Smaller images lead to faster scaling, especially across multiple nodes.
            # Notice that we are using the same image for both the head and worker nodes. You might hit ModuleNotFoundError if you use a different image for head and worker nodes.
            # Preload Container Image into data volumes for faster new ray worker nodes
            - name: head
              image: public.ecr.aws/data-on-eks/ray-serve-gpu-stablediffusion:2.33.0-py311-gpu
              imagePullPolicy: Always # Ensure the image is always pulled when updated
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /serve_app/stable-diffusion-2
                  name: cache-dir
              resources:
                limits:
                  cpu: 2
                  memory: 16Gi
                requests:
                  cpu: 2
                  memory: 16Gi
          nodeSelector:
            NodeGroupType: x86-cpu-karpenter
            type: karpenter
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: cache-dir
              persistentVolumeClaim:
                claimName: s3-claim
    workerGroupSpecs:
      - groupName: gpu
        # With g5.2xlarge instance, Ray can scale up to 4 nodes, with one pod per node and 1 GPU per pod.
        minReplicas: 1
        maxReplicas: 4
        rayStartParams: {}
        template:
          spec:
            containers:
            # Important Performance Note:
            # This image is large (20GB uncompressed, 10GB compressed) and may take up to 5 minutes to pull.
            # For faster inference scaling, consider building a custom image with only your workload's essential dependencies.
            # Smaller images lead to faster scaling, especially across multiple nodes.
            # Notice that we are using the same image for both the head and worker nodes. You might hit ModuleNotFoundError if you use a different image for head and worker nodes.
              - name: worker
                image: public.ecr.aws/data-on-eks/ray-serve-gpu-stablediffusion:2.33.0-py311-gpu
                imagePullPolicy: Always # Ensure the image is always pulled when updated
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "ray stop"]
                resources:
                  limits:
                    cpu: "3"
                    memory: "14Gi"
                    nvidia.com/gpu: 1
                  requests:
                    cpu: "3"
                    memory: "14Gi"
                    nvidia.com/gpu: 1
                volumeMounts:
                  - mountPath: /serve_app/stable-diffusion-2
                    name: cache-dir
            nodeSelector:
              NodeGroupType: g5-gpu-karpenter
              type: karpenter
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
            volumes:
              - name: cache-dir
                persistentVolumeClaim:
                  claimName: s3-claim
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: stablediffusion
  namespace: stablediffusion
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      # Ray Dashboard: you can access the dashboard using the NLB DNS name with port 8265 and the path "/dashboard". However, the NLB is currently configured as private, preventing access from outside the VPC.
      # To access the dashboard:
      # Option 1: Use the `kubectl port-forward` command.
      # Option 2: Deploy a public-facing NLB by modifying the configuration file "ai-ml/jark-stack/terraform/helm-values/ingress-nginx-values.yaml".
      - path: /dashboard/(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: stablediffusion
            port:
              number: 8265
      # Ray Serve
      - path: /serve/(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: stablediffusion
            port:
              number: 8000
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: s3-pv-model
spec:
  capacity:
    storage: 1200Gi # ignored, required
  accessModes:
    - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany
  mountOptions:
    - allow-overwrite
    - allow-delete
    - region us-west-2
    - prefix stable-diffusion-2/
  csi:
    driver: s3.csi.aws.com # required
    volumeHandle: s3-csi-driver-volume
    volumeAttributes:
      bucketName: <ENTER_S3_BUCKET_NAME> # Replace bucketName with S3 bucket name created from `terraform output`
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-claim
  namespace: stablediffusion
spec:
  accessModes:
    - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany
  storageClassName: "" # required for static provisioning
  resources:
    requests:
      storage: 1200Gi # ignored, required
  volumeName: s3-pv-model
