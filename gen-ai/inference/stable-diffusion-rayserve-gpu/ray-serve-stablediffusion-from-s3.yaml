apiVersion: ray.io/v1
kind: RayService
metadata:
  name: stablediffusion
  namespace: stablediffusion
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
# Ray Serve can automatically scale deployment replicas up to 3 based on incoming traffic.
# Each replica in this example requires one GPU. For GPU types g5.xlarge to g5.16xlarge, a single node with one GPU can only run one replica.
# However, g5.12xlarge comes with 4 GPUs, allowing you to run all 4 replicas on a single node.
  serveConfigV2: |
    applications:
      - name: stable-diffusion-deployment
        import_path: "ray_serve_sd:entrypoint"
        route_prefix: "/"
        runtime_env:
          env_vars:
            MODEL_PATH: "./stable-diffusion-2"
            MODEL_ID: "stabilityai/stable-diffusion-2-1"
          pip:
            - transformers==4.25.1
            - accelerate==0.30.1
        deployments:
          - name: stable-diffusion-v2
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: 1
              max_replicas: 4
              look_back_period_s: 2
              downscale_delay_s: 600
              upscale_delay_s: 30
              target_ongoing_requests: 1
            graceful_shutdown_timeout_s: 5
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 3
              num_gpus: 1
  rayClusterConfig:
    rayVersion: '2.24.0'
    enableInTreeAutoscaling: true
    headGroupSpec:
      headService:
        metadata:
          name: stablediffusion
          namespace: stablediffusion
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            # Important Performance Note:
            # This image is large (20GB uncompressed, 10GB compressed) and may take up to 5 minutes to pull.
            # For faster inference scaling, consider building a custom image with only your workload's essential dependencies.
            # Smaller images lead to faster scaling, especially across multiple nodes.
            # Notice that we are using the same image for both the head and worker nodes. You might hit ModuleNotFoundError if you use a different image for head and worker nodes.
            # Preload Container Image into data volumes for faster new ray worker nodes
            - name: head
              image: public.ecr.aws/data-on-eks/ray-serve-gpu-stablediffusion:2.33.0-py311-gpu
              imagePullPolicy: Always # Ensure the image is always pulled when updated
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /serve_app/stable-diffusion-2
                  name: cache-dir
                - mountPath: /serve_app
                  name: serve-script
              resources:
                limits:
                  cpu: 2
                  memory: 16Gi
                requests:
                  cpu: 2
                  memory: 16Gi
          nodeSelector:
            NodeGroupType: x86-cpu-karpenter
            type: karpenter
          volumes:
            - name: serve-script
              configMap:
                name: stable-diffusion-script
                defaultMode: 0777
            - name: ray-logs
              emptyDir: {}
            - name: cache-dir
              persistentVolumeClaim:
                claimName: s3-model-storage-claim
    workerGroupSpecs:
      - groupName: gpu
        # With g5.2xlarge instance, Ray can scale up to 4 nodes, with one pod per node and 1 GPU per pod.
        minReplicas: 1
        maxReplicas: 4
        rayStartParams: {}
        template:
          spec:
            containers:
            # Important Performance Note:
            # This image is large (20GB uncompressed, 10GB compressed) and may take up to 5 minutes to pull.
            # For faster inference scaling, consider building a custom image with only your workload's essential dependencies.
            # Smaller images lead to faster scaling, especially across multiple nodes.
            # Notice that we are using the same image for both the head and worker nodes. You might hit ModuleNotFoundError if you use a different image for head and worker nodes.
              - name: worker
                image: public.ecr.aws/data-on-eks/ray-serve-gpu-stablediffusion:2.33.0-py311-gpu
                imagePullPolicy: Always # Ensure the image is always pulled when updated
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "ray stop"]
                resources:
                  limits:
                    cpu: "3"
                    memory: "14Gi"
                    nvidia.com/gpu: 1
                  requests:
                    cpu: "3"
                    memory: "14Gi"
                    nvidia.com/gpu: 1
                volumeMounts:
                  - mountPath: /serve_app/stable-diffusion-2
                    name: cache-dir
                  - mountPath: /serve_app
                    name: serve-script
            nodeSelector:
              NodeGroupType: g5-gpu-karpenter
              type: karpenter
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
            volumes:
            - name: serve-script
              configMap:
                name: stable-diffusion-script
                defaultMode: 0777
            - name: cache-dir
              persistentVolumeClaim:
                claimName: s3-model-storage-claim
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: stablediffusion
  namespace: stablediffusion
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      # Ray Dashboard: you can access the dashboard using the NLB DNS name with port 8265 and the path "/dashboard". However, the NLB is currently configured as private, preventing access from outside the VPC.
      # To access the dashboard:
      # Option 1: Use the `kubectl port-forward` command.
      # Option 2: Deploy a public-facing NLB by modifying the configuration file "ai-ml/jark-stack/terraform/helm-values/ingress-nginx-values.yaml".
      - path: /dashboard/(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: stablediffusion
            port:
              number: 8265
      # Ray Serve
      - path: /serve/(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: stablediffusion
            port:
              number: 8000
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: stable-diffusion-script
  namespace: stablediffusion
data:
  ray_serve_sd.py: |
    from io import BytesIO
    from fastapi import FastAPI
    from fastapi.responses import Response
    import torch
    import os
    import ray
    from ray import serve

    app = FastAPI()

    @serve.deployment(num_replicas=1)
    @serve.ingress(app)
    class APIIngress:
        def __init__(self, diffusion_model_handle) -> None:
            self.handle = diffusion_model_handle

        @app.get(
            "/imagine",
            responses={200: {"content": {"image/png": {}}}},
            response_class=Response,
        )
        async def generate(self, prompt: str, img_size: int = 768):
            assert len(prompt), "prompt parameter cannot be empty"

            image = await self.handle.generate.remote(prompt, img_size=img_size)
            file_stream = BytesIO()
            image.save(file_stream, "PNG")
            return Response(content=file_stream.getvalue(), media_type="image/png")


    @serve.deployment(
        name="stable-diffusion-v2",
        ray_actor_options={"num_gpus": 1},
        autoscaling_config={"min_replicas": 1, "max_replicas": 2},
    )
    class StableDiffusionV2:
        def __init__(self):
            from diffusers import EulerDiscreteScheduler, StableDiffusionPipeline

            model_id = os.getenv("MODEL_ID")
            model_path = os.getenv("MODEL_PATH")
            if model_path is not None:
                model_id = model_path

            self.pipe = StableDiffusionPipeline.from_pretrained(
                model_id, torch_dtype=torch.float16
            )
            self.pipe = self.pipe.to("cuda")

        def generate(self, prompt: str, img_size: int = 768):
            assert len(prompt), "prompt parameter cannot be empty"

            image = self.pipe(
                prompt,
                height=img_size,
                width=img_size,
                negative_prompt="",
                num_inference_steps=50,
                guidance_scale=7.0,
            ).images[0]
            return image

    entrypoint = APIIngress.bind(StableDiffusionV2.bind())
