# Spark on EKS - Karpenter Overrides for Dev Environment

# Override base Karpenter configuration for Spark workloads
karpenter:
  controller:
    # Dev environment - single replica
    replicas: 1
    
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi

# Spark-specific NodePool configurations
sparkNodePools:
  # Compute-optimized for Spark drivers and small jobs
  - name: spark-compute-optimized
    metadata:
      labels:
        workload-type: compute-intensive
        blueprint: spark-on-eks
    spec:
      template:
        spec:
          requirements:
            - key: "karpenter.k8s.aws/instance-family"
              operator: In
              values: ["c5", "c6i", "c7i", "m5", "m6i", "m7i"]
            - key: "karpenter.k8s.aws/instance-size"
              operator: In
              values: ["large", "xlarge", "2xlarge", "4xlarge"]
            - key: "karpenter.sh/capacity-type"
              operator: In
              values: ["spot", "on-demand"]
          
          # Spark-specific startup taints
          startupTaints:
            - key: "spark.apache.org/node-type"
              value: "compute-optimized"
              effect: NoSchedule
          
          # Dev environment - shorter expiry
          expireAfter: 15m
      
      # Dev limits - smaller scale
      limits:
        cpu: 500
        memory: 1000Gi
      
      # Conservative disruption in dev
      disruption:
        consolidationPolicy: WhenEmpty
        consolidateAfter: 60s

  # Memory-optimized for large Spark jobs
  - name: spark-memory-optimized
    metadata:
      labels:
        workload-type: memory-intensive
        blueprint: spark-on-eks
    spec:
      template:
        spec:
          requirements:
            - key: "karpenter.k8s.aws/instance-family"
              operator: In
              values: ["r5", "r6i", "r7i", "x1e"]
            - key: "karpenter.k8s.aws/instance-size"
              operator: In
              values: ["xlarge", "2xlarge", "4xlarge", "8xlarge"]
          
          startupTaints:
            - key: "spark.apache.org/node-type"
              value: "memory-optimized"  
              effect: NoSchedule
          
          expireAfter: 30m
      
      limits:
        cpu: 800
        memory: 2000Gi
      
      disruption:
        consolidationPolicy: WhenEmpty
        consolidateAfter: 120s

# Spark-specific EC2NodeClasses
sparkEC2NodeClasses:
  - name: spark-compute-optimized
    spec:
      # Instance store for Spark shuffle
      instanceStorePolicy: RAID0
      
      # Spark-optimized user data
      userData: |
        #!/bin/bash
        /etc/eks/bootstrap.sh {{ .Values.global.clusterName }} \
          --container-runtime containerd \
          --kubelet-extra-args '--node-labels=node-type=spark-compute-optimized,workload-class=compute-intensive'
        
        # Spark optimizations
        echo 'vm.swappiness=1' >> /etc/sysctl.conf
        echo 'net.core.somaxconn=65536' >> /etc/sysctl.conf
        sysctl -p
        
        # Configure instance store for Spark
        if [[ -b /dev/nvme1n1 ]]; then
          mkfs.xfs -f /dev/nvme1n1
          mkdir -p /mnt/spark-local
          mount /dev/nvme1n1 /mnt/spark-local
          chmod 755 /mnt/spark-local
        fi
      
      tags:
        Blueprint: spark-on-eks
        Environment: dev
        WorkloadType: compute-intensive

  - name: spark-memory-optimized
    spec:
      instanceStorePolicy: RAID0
      
      # Enhanced block device mappings for memory workloads
      blockDeviceMappings:
        - deviceName: /dev/xvda
          ebs:
            volumeSize: 150Gi
            volumeType: gp3
            iops: 5000
            throughput: 400
            encrypted: true
            deleteOnTermination: true
      
      # Memory-optimized kubelet settings
      kubelet:
        maxPods: 110
        systemReserved:
          cpu: 200m
          memory: 2Gi
        kubeReserved:
          cpu: 300m
          memory: 2Gi
        evictionHard:
          memory.available: 1Gi
          nodefs.available: 10%
      
      userData: |
        #!/bin/bash
        /etc/eks/bootstrap.sh {{ .Values.global.clusterName }} \
          --container-runtime containerd \
          --kubelet-extra-args '--node-labels=node-type=spark-memory-optimized,workload-class=memory-intensive'
        
        # Memory-specific optimizations
        echo 'vm.swappiness=1' >> /etc/sysctl.conf
        echo 'vm.dirty_ratio=80' >> /etc/sysctl.conf
        echo 'vm.overcommit_memory=1' >> /etc/sysctl.conf
        sysctl -p
        
        # Instance store configuration
        if [[ -b /dev/nvme1n1 ]]; then
          mkfs.xfs -f -K -d agcount=32 /dev/nvme1n1
          mkdir -p /mnt/spark-local
          mount -o noatime,allocsize=16m /dev/nvme1n1 /mnt/spark-local
          chmod 755 /mnt/spark-local
        fi
      
      tags:
        Blueprint: spark-on-eks
        Environment: dev
        WorkloadType: memory-intensive