# Spark on EKS - Spark Operator Overrides for Dev Environment
# Override base spark-operator config from infra/argocd/data/spark-operator/values.yaml

# Override base spark-operator configuration
spark-operator:
  # Dev controller settings (single replica vs base's production config)
  controller:
    replicas: 1
    
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  webhook:
    replicas: 1  # Dev uses single replica
    
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

  # Use global namespaces from base configuration
  spark:
    jobNamespaces: 
      - default
      - spark-jobs
      - "{{ .Values.global.environment }}-spark"  # Dynamic namespace

# Spark job defaults (can be used in SparkApplication CRDs)
sparkDefaults:
  # Basic resource settings
  driver:
    cores: 1
    memory: "1g"
  
  executor:
    cores: 1
    memory: "2g"
    instances: 2
  
  # Event logging with dynamic S3 bucket
  eventLog:
    enabled: true
    dir: "s3a://{{ .Values.global.s3BucketName }}/spark-event-logs/"
  
  # Labels for tracking
  labels:
    environment: "{{ .Values.global.environment }}"
    blueprint: "spark-on-eks"
    region: "{{ .Values.global.region }}"

# Monitoring configuration
monitoring:
  enabled: true
  
  metrics:
    enabled: true
    port: 8080

# Logging
logging:
  level: INFO
