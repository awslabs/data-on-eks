# AWS Load Balancer Controller - Spark on EKS Configuration

aws-load-balancer-controller:
  # Cluster name from global values
  clusterName: "{{ .Values.global.clusterName }}"

  # Service Account with Pod Identity
  serviceAccount:
    create: true
    name: aws-load-balancer-controller
    annotations:
      eks.amazonaws.com/pod-identity-association: "aws-load-balancer-controller"

  # Enhanced resources for production
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # High availability
  replicaCount: 2

  # Node placement
  nodeSelector:
    kubernetes.io/arch: "amd64"
    type: karpenter

  # Tolerations for system workloads
  tolerations:
    - key: "node-role.kubernetes.io/system"
      operator: "Equal"
      effect: "NoSchedule"

  # Anti-affinity for HA
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: aws-load-balancer-controller
            topologyKey: kubernetes.io/hostname

  # Enhanced configuration for Spark workloads
  enableServiceMutatorWebhook: false
  enableEndpointSlices: true
  enableBackendSecurityGroup: true
  disableIngressClassAnnotation: false
  ingressClass: alb
  enableShield: true
  enableWAFv2: false
  logLevel: info

  # Pod annotations for monitoring
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"

  # Security hardening
  securityContext:
    runAsNonRoot: true
    runAsUser: 65534
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

# Override global values
global:
  region: "us-west-2"
