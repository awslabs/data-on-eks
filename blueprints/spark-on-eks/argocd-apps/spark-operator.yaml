apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: spark-operator
  namespace: argocd
  labels:
    blueprint: spark-on-eks
    category: analytics
    component: spark-operator
  annotations:
    argocd.argoproj.io/sync-wave: "3"  # Deploy after Karpenter NodePools
spec:
  project: data-platform
  source:
    repoURL: https://googlecloudplatform.github.io/spark-on-k8s-operator
    chart: spark-operator
    targetRevision: "2.2.0"  # Latest stable version
    helm:
      releaseName: spark-operator
      valueFiles: []
      values: |
        # Spark Operator Configuration
        controller:
          replicas: 2  # HA setup
          workers: 10
          batchScheduler:
            enable: false  # Using Karpenter instead
          
          # Resource management
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 1Gi
          
          # Node placement
          nodeSelector:
            node-type: spark-compute-optimized
          
          tolerations:
            - key: "spark.apache.org/node-type"
              operator: Equal
              value: "compute-optimized"
              effect: NoSchedule
          
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
        
        # Webhook configuration
        webhook:
          enabled: true
          replicas: 2  # HA webhook
          port: 8443
          
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          
          # Admission control
          namespaceSelector:
            matchLabels:
              spark-operator/enabled: "true"
          
          # Security
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
        
        # Spark configuration
        spark:
          jobNamespaces:
            - default
            - spark-team-a
            - spark-team-b
            - spark-team-c
            - spark-examples
          
          # Service account management
          serviceAccount:
            create: false  # Managed by Pod Identity
            name: spark-operator
          
          # RBAC
          rbac:
            create: true
            createClusterRole: true
            createRole: true
        
        # Monitoring and observability
        prometheus:
          metrics:
            enable: true
            port: 8080
            portName: metrics
            endpoint: /metrics
            prefix: ""
          
          podMonitor:
            create: true
            labels:
              app.kubernetes.io/name: spark-operator
              app.kubernetes.io/component: metrics
            jobLabel: spark-operator-podmonitor
            podMetricsEndpoint:
              scheme: http
              interval: 30s
              path: /metrics
              honorLabels: true
        
        # Image configuration
        image:
          repository: ghcr.io/googlecloudplatform/spark-operator
          tag: "v2.2.0-v1.9.0-3.5.1"  # Spark 3.5.1, K8s 1.9.0
          pullPolicy: IfNotPresent
        
        # Pod disruption budget
        podDisruptionBudget:
          enabled: true
          minAvailable: 1
        
        # Additional labels and annotations
        commonLabels:
          blueprint: spark-on-eks
          managed-by: argocd
          version: "2.2.0"
        
        commonAnnotations:
          blueprint.data-on-eks.io/name: "spark-on-eks"
          blueprint.data-on-eks.io/version: "2.0"
        
        # Network policies
        networkPolicy:
          enabled: true
          ingress:
            - from:
              - namespaceSelector:
                  matchLabels:
                    name: karpenter
              - namespaceSelector:
                  matchLabels:
                    spark-operator/enabled: "true"
          egress:
            - to: []  # Allow all egress
        
        # Logging configuration
        logLevel: INFO
        
        # Spark defaults
        sparkJobNamespace: default
        
        # Leader election
        leaderElection:
          lockName: spark-operator-lock
          lockNamespace: spark-operator
        
        # Resync period
        resyncInterval: 30s
        
        # Spark application TTL
        sparkApplicationTTLSeconds: 86400  # 24 hours
  
  destination:
    server: https://kubernetes.default.svc
    namespace: spark-operator
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
      - ApplyOutOfSyncOnly=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # Health checks
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jqPathExpressions:
        - '.spec.template.spec.containers[]?.resources'
  
  # Application health
  healthChecks:
    - group: apps
      kind: Deployment
      check: |
        health_status = {}
        if obj.status ~= nil then
          if obj.status.readyReplicas ~= nil and obj.status.readyReplicas > 0 then
            health_status.status = "Healthy"
            health_status.message = "Deployment has ready replicas"
            return health_status
          end
        end
        health_status.status = "Progressing"
        health_status.message = "Deployment is not ready"
        return health_status