---
sidebar_position: 4
sidebar_label: S3 Tables ä¸ EKS
hide_table_of_contents: true
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CollapsibleContent from '../../../../../../src/components/CollapsibleContent';

import TaxiTripExecute from './_taxi_trip_exec.md'
import ReplaceS3BucketPlaceholders from './_replace_s3_bucket_placeholders.mdx';

import CodeBlock from '@theme/CodeBlock';

# S3 Tables ä¸ Amazon EKS

![s3tables](../../../../../../docs/blueprints/data-analytics/img/s3tables.png)

## ä»€ä¹ˆæ˜¯ S3 Tablesï¼Ÿ

Amazon S3 Tables æ˜¯ä¸€ä¸ªå®Œå…¨æ‰˜ç®¡çš„è¡¨æ ¼æ•°æ®å­˜å‚¨ï¼Œä¸“ä¸ºä¼˜åŒ–æ€§èƒ½ã€ç®€åŒ–å®‰å…¨æ€§å¹¶ä¸ºå¤§è§„æ¨¡åˆ†æå·¥ä½œè´Ÿè½½æä¾›ç»æµé«˜æ•ˆçš„å­˜å‚¨è€Œæ„å»ºã€‚å®ƒç›´æ¥ä¸ Amazon EMRã€Amazon Athenaã€Amazon Redshiftã€AWS Glue å’Œ AWS Lake Formation ç­‰æœåŠ¡é›†æˆï¼Œä¸ºè¿è¡Œåˆ†æå’Œæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½æä¾›æ— ç¼ä½“éªŒã€‚

## ä¸ºä»€ä¹ˆåœ¨ Amazon EKS ä¸Šè¿è¡Œ S3 Tablesï¼Ÿ

å¯¹äºå·²ç»é‡‡ç”¨ Amazon EKS è¿è¡Œ Spark å·¥ä½œè´Ÿè½½å¹¶ä½¿ç”¨ Iceberg ç­‰è¡¨æ ¼å¼çš„ç”¨æˆ·ï¼Œåˆ©ç”¨ S3 Tables åœ¨æ€§èƒ½ã€æˆæœ¬æ•ˆç‡å’Œå®‰å…¨æ§åˆ¶æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚è¿™ç§é›†æˆå…è®¸ç»„ç»‡å°† Kubernetes åŸç”ŸåŠŸèƒ½ä¸ S3 Tables çš„èƒ½åŠ›ç›¸ç»“åˆï¼Œå¯èƒ½åœ¨å…¶ç°æœ‰ç¯å¢ƒä¸­æ”¹å–„æŸ¥è¯¢æ€§èƒ½å’Œèµ„æºæ‰©å±•ã€‚é€šè¿‡éµå¾ªæœ¬æ–‡æ¡£ä¸­è¯¦è¿°çš„æ­¥éª¤ï¼Œç”¨æˆ·å¯ä»¥å°† S3 Tables æ— ç¼é›†æˆåˆ°ä»–ä»¬çš„ EKS è®¾ç½®ä¸­ï¼Œä¸ºåˆ†æå·¥ä½œè´Ÿè½½æä¾›çµæ´»ä¸”äº’è¡¥çš„è§£å†³æ–¹æ¡ˆã€‚

## S3 Tables ä¸ Iceberg è¡¨æ ¼å¼çš„åŒºåˆ«

è™½ç„¶ S3 Tables ä½¿ç”¨ Apache Iceberg ä½œä¸ºåº•å±‚å®ç°ï¼Œä½†å®ƒä»¬æä¾›äº†ä¸“ä¸º AWS å®¢æˆ·è®¾è®¡çš„å¢å¼ºåŠŸèƒ½ï¼š

- **ğŸ› ï¸ è‡ªåŠ¨å‹ç¼©**ï¼šS3 Tables å®ç°è‡ªåŠ¨å‹ç¼©ï¼Œé€šè¿‡åœ¨åå°æ™ºèƒ½åœ°å°†è¾ƒå°çš„æ–‡ä»¶åˆå¹¶ä¸ºæ›´å¤§ã€æ›´é«˜æ•ˆçš„æ–‡ä»¶æ¥ä¼˜åŒ–æ•°æ®å­˜å‚¨ã€‚æ­¤è¿‡ç¨‹é™ä½å­˜å‚¨æˆæœ¬ï¼Œæé«˜æŸ¥è¯¢é€Ÿåº¦ï¼Œå¹¶æŒç»­è¿è¡Œè€Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚

- ğŸ”„ **è¡¨ç»´æŠ¤**ï¼šå®ƒæä¾›å…³é”®çš„ç»´æŠ¤ä»»åŠ¡ï¼Œå¦‚å¿«ç…§ç®¡ç†å’Œæœªå¼•ç”¨æ–‡ä»¶åˆ é™¤ã€‚è¿™ç§æŒç»­ä¼˜åŒ–ç¡®ä¿è¡¨ä¿æŒé«˜æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ï¼Œå‡å°‘è¿è¥å¼€é”€ï¼Œè®©å›¢é˜Ÿä¸“æ³¨äºæ•°æ®æ´å¯Ÿã€‚

- â„ï¸ **Apache Iceberg æ”¯æŒ**ï¼šæä¾›å¯¹ Apache Iceberg çš„å†…ç½®æ”¯æŒï¼Œç®€åŒ–å¤§è§„æ¨¡æ•°æ®æ¹–ç®¡ç†ï¼ŒåŒæ—¶æé«˜æŸ¥è¯¢æ€§èƒ½å¹¶é™ä½æˆæœ¬ã€‚å¦‚æœæ‚¨å¸Œæœ›ä½“éªŒä»¥ä¸‹ç»“æœï¼Œè¯·è€ƒè™‘ä¸ºæ‚¨çš„æ•°æ®æ¹–ä½¿ç”¨ S3 Tablesã€‚

- ğŸ”’ **ç®€åŒ–å®‰å…¨æ€§**ï¼šS3 Tables å°†æ‚¨çš„è¡¨è§†ä¸º AWS èµ„æºï¼Œåœ¨è¡¨çº§åˆ«å¯ç”¨ç»†ç²’åº¦çš„ AWS Identity and Access Management (IAM) æƒé™ã€‚è¿™ç®€åŒ–äº†æ•°æ®æ²»ç†ï¼Œå¢å¼ºäº†å®‰å…¨æ€§ï¼Œå¹¶ä½¿è®¿é—®æ§åˆ¶ä¸æ‚¨ç†Ÿæ‚‰çš„ AWS æœåŠ¡æ›´åŠ ç›´è§‚å’Œå¯ç®¡ç†ã€‚

- âš¡ **å¢å¼ºæ€§èƒ½**ï¼šAmazon S3 Tables å¼•å…¥äº†ä¸€ç§æ–°å‹å­˜å‚¨æ¡¶ï¼Œä¸“ä¸ºå­˜å‚¨ Apache Iceberg è¡¨è€Œæ„å»ºã€‚ä¸åœ¨é€šç”¨ S3 å­˜å‚¨æ¡¶ä¸­å­˜å‚¨ Iceberg è¡¨ç›¸æ¯”ï¼Œè¡¨å­˜å‚¨æ¡¶æä¾›é«˜è¾¾ 3 å€çš„æŸ¥è¯¢æ€§èƒ½å’Œé«˜è¾¾ 10 å€çš„æ¯ç§’äº‹åŠ¡æ•°ã€‚è¿™ç§æ€§èƒ½å¢å¼ºæ”¯æŒé«˜é¢‘æ›´æ–°ã€å®æ—¶æ‘„å–å’Œæ›´è‹›åˆ»çš„å·¥ä½œè´Ÿè½½ï¼Œç¡®ä¿éšç€æ•°æ®é‡å¢é•¿çš„å¯æ‰©å±•æ€§å’Œå“åº”æ€§ã€‚

- ğŸ› ï¸ **ä¸ AWS æœåŠ¡é›†æˆ**ï¼šS3 Tables ä¸ AWS åˆ†ææœåŠ¡ï¼ˆå¦‚ Athenaã€Redshiftã€EMR å’Œ Glueï¼‰ç´§å¯†é›†æˆï¼Œä¸ºåˆ†æå·¥ä½œè´Ÿè½½æä¾›åŸç”Ÿæ”¯æŒã€‚

<CollapsibleContent header={<h2><span>éƒ¨ç½²è§£å†³æ–¹æ¡ˆ</span></h2>}>

åœ¨è¿™ä¸ª[ç¤ºä¾‹](https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator)ä¸­ï¼Œæ‚¨å°†é…ç½®ä½¿ç”¨å¼€æº Spark Operator å’Œ Apache YuniKorn è¿è¡Œ Spark ä½œä¸šæ‰€éœ€çš„ä»¥ä¸‹èµ„æºã€‚

æ­¤ç¤ºä¾‹å°†è¿è¡Œ Spark K8s Operator çš„ EKS é›†ç¾¤éƒ¨ç½²åˆ°æ–°çš„ VPC ä¸­ã€‚

- åˆ›å»ºæ–°çš„ç¤ºä¾‹ VPCã€2 ä¸ªç§æœ‰å­ç½‘ã€2 ä¸ªå…¬æœ‰å­ç½‘ï¼Œä»¥åŠ RFC6598 ç©ºé—´ï¼ˆ100.64.0.0/10ï¼‰ä¸­ç”¨äº EKS Pod çš„ 2 ä¸ªå­ç½‘ã€‚
- ä¸ºå…¬æœ‰å­ç½‘åˆ›å»ºäº’è”ç½‘ç½‘å…³ï¼Œä¸ºç§æœ‰å­ç½‘åˆ›å»º NAT ç½‘å…³
- åˆ›å»ºå…·æœ‰å…¬å…±ç«¯ç‚¹çš„ EKS é›†ç¾¤æ§åˆ¶å¹³é¢ï¼ˆä»…ç”¨äºæ¼”ç¤ºç›®çš„ï¼‰ï¼ŒåŒ…å«ç”¨äºåŸºå‡†æµ‹è¯•å’Œæ ¸å¿ƒæœåŠ¡çš„æ‰˜ç®¡èŠ‚ç‚¹ç»„ï¼Œä»¥åŠç”¨äº Spark å·¥ä½œè´Ÿè½½çš„ Karpenter NodePoolã€‚
- éƒ¨ç½² Metrics serverã€Spark-operatorã€Apache Yunikornã€Karpenterã€Grafana å’Œ Prometheus serverã€‚

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²åœ¨è®¡ç®—æœºä¸Šå®‰è£…äº†ä»¥ä¸‹å·¥å…·ã€‚

1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### éƒ¨ç½²

å…‹éš†å­˜å‚¨åº“ã€‚

```bash
git clone https://github.com/awslabs/data-on-eks.git
cd data-on-eks
export DOEKS_HOME=$(pwd)
```

å¦‚æœ DOEKS_HOME è¢«å–æ¶ˆè®¾ç½®ï¼Œæ‚¨å¯ä»¥å§‹ç»ˆä» data-on-eks ç›®å½•ä½¿ç”¨ `export DATA_ON_EKS=$(pwd)` æ‰‹åŠ¨è®¾ç½®å®ƒã€‚

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•ä¹‹ä¸€å¹¶è¿è¡Œ `install.sh` è„šæœ¬ã€‚

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator
chmod +x install.sh
./install.sh
```

ç°åœ¨åˆ›å»ºä¸€ä¸ª `S3_BUCKET` å˜é‡ï¼Œè¯¥å˜é‡ä¿å­˜å®‰è£…æœŸé—´åˆ›å»ºçš„å­˜å‚¨æ¡¶çš„åç§°ã€‚æ­¤å­˜å‚¨æ¡¶å°†åœ¨åç»­ç¤ºä¾‹ä¸­ç”¨äºå­˜å‚¨è¾“å‡ºæ•°æ®ã€‚å¦‚æœ S3_BUCKET è¢«å–æ¶ˆè®¾ç½®ï¼Œæ‚¨å¯ä»¥å†æ¬¡è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)
echo $S3_BUCKET
```

</CollapsibleContent>

## æ‰§è¡Œç¤ºä¾‹ Spark ä½œä¸š

### æ­¥éª¤ 1ï¼šåˆ›å»ºä¸ S3 Tables å…¼å®¹çš„ Apache Spark Docker é•œåƒ

åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸ S3 Tables é€šä¿¡æ‰€éœ€ jar æ–‡ä»¶çš„ Docker é•œåƒã€‚

- æŸ¥çœ‹ç¤ºä¾‹ [Dockerfile](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/Dockerfile-S3Table)
- æ³¨æ„ç”¨äº S3 Tables äº¤äº’çš„[å…³é”® jar æ–‡ä»¶](https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/examples/s3-tables/Dockerfile-S3Table#L43C1-L48C1)ï¼ŒåŒ…æ‹¬ Icebergã€AWS SDK bundle å’Œç”¨äº Iceberg è¿è¡Œæ—¶çš„ S3 Tables Catalog
- æ ¹æ®æ‚¨çš„ç¯å¢ƒéœ€è¦è‡ªå®šä¹‰ Dockerfile
- æ„å»º Docker é•œåƒå¹¶å°†é•œåƒæ¨é€åˆ°æ‚¨é¦–é€‰çš„å®¹å™¨æ³¨å†Œè¡¨

æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ª docker é•œåƒå¹¶å‘å¸ƒåœ¨ ECR ä¸­ï¼Œä»…ç”¨äºæ¼”ç¤ºç›®çš„ã€‚

### æ­¥éª¤ 2ï¼šä¸ºä½œä¸šåˆ›å»ºæµ‹è¯•æ•°æ®

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•ï¼Œä½¿ç”¨æ­¤[shell](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/input-data-gen.sh)è„šæœ¬ä¸º Spark ä½œä¸šè¾“å…¥ç”Ÿæˆç¤ºä¾‹å‘˜å·¥æ•°æ®ã€‚

```sh
cd analytics/terraform/spark-k8s-operator/examples/s3-tables
./input-data-gen.sh
```

æ­¤è„šæœ¬å°†åœ¨æ‚¨çš„å½“å‰ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªåä¸º `employee_data.csv` çš„æ–‡ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒç”Ÿæˆ 100 æ¡è®°å½•ã€‚

æ³¨æ„ï¼šå¦‚æœæ‚¨éœ€è¦è°ƒæ•´è®°å½•æ•°é‡ï¼Œå¯ä»¥ä¿®æ”¹ input-data-gen.sh è„šæœ¬ã€‚æŸ¥æ‰¾ç”Ÿæˆæ•°æ®çš„å¾ªç¯å¹¶æ ¹æ®éœ€è¦æ›´æ”¹è¿­ä»£è®¡æ•°ã€‚

### æ­¥éª¤ 3ï¼šå°†æµ‹è¯•è¾“å…¥æ•°æ®ä¸Šä¼ åˆ° Amazon S3 å­˜å‚¨æ¡¶

å°† `<YOUR_S3_BUCKET>` æ›¿æ¢ä¸ºæ‚¨çš„è“å›¾åˆ›å»ºçš„ S3 å­˜å‚¨æ¡¶åç§°å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
aws s3 cp employee_data.csv s3://<S3_BUCKET>/s3table-example/input/
```

æ­¤å‘½ä»¤å°† CSV æ–‡ä»¶ä¸Šä¼ åˆ°æ‚¨çš„ S3 å­˜å‚¨æ¡¶ã€‚Spark ä½œä¸šç¨åå°†å¼•ç”¨æ­¤è·¯å¾„æ¥è¯»å–è¾“å…¥æ•°æ®ã€‚åœ¨æ‰§è¡Œå‘½ä»¤ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å…·æœ‰å†™å…¥æ­¤å­˜å‚¨æ¡¶çš„å¿…è¦æƒé™ã€‚

### æ­¥éª¤ 4ï¼šå°† PySpark è„šæœ¬ä¸Šä¼ åˆ° S3 å­˜å‚¨æ¡¶

ä»¥ä¸‹è„šæœ¬æ˜¯ [Spark ä½œä¸š](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.py)çš„ç‰‡æ®µï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸ S3 Tables é…åˆå·¥ä½œæ‰€éœ€çš„ Spark é…ç½®ã€‚

```python
def main(args):
    if len(args) != 3:
        logger.error("Usage: spark-etl [input-csv-path] [s3table-arn]")
        sys.exit(1)

    # Input parameters
    input_csv_path = args[1]    # Path to the input CSV file
    s3table_arn = args[2]       # s3table arn

    # Initialize Spark session
    logger.info("Initializing Spark Session")
    spark = (SparkSession
             .builder
             .appName(f"{AppName}_{dt_string}")
             .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
             .config("spark.sql.catalog.s3tablesbucket", "org.apache.iceberg.spark.SparkCatalog")
             .config("spark.sql.catalog.s3tablesbucket.catalog-impl", "software.amazon.s3tables.iceberg.S3TablesCatalog")
             .config("spark.sql.catalog.s3tablesbucket.warehouse", s3table_arn)
             .config('spark.hadoop.fs.s3.impl', "org.apache.hadoop.fs.s3a.S3AFileSystem")
             .config("spark.sql.defaultCatalog", "s3tablesbucket")
             .getOrCreate())

    spark.sparkContext.setLogLevel("INFO")
    logger.info("Spark session initialized successfully")

    namespace = "doeks_namespace"
    table_name = "employee_s3_table"
    full_table_name = f"s3tablesbucket.{namespace}.{table_name}"

...

```

å°† `S3_BUCKET` æ›¿æ¢ä¸ºæ‚¨çš„è“å›¾åˆ›å»ºçš„ S3 å­˜å‚¨æ¡¶åç§°ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†ç¤ºä¾‹ [Spark ä½œä¸š](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.py)ä¸Šä¼ åˆ° S3 å­˜å‚¨æ¡¶ã€‚

```bash
aws s3 cp s3table-iceberg-pyspark.py s3://<S3_BUCKET>/s3table-example/scripts/
```

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•å¹¶æäº¤ Spark ä½œä¸šã€‚

### æ­¥éª¤ 5ï¼šåˆ›å»º Amazon S3 è¡¨å­˜å‚¨æ¡¶

è¿™æ˜¯ä¸»è¦æ­¥éª¤ï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ªç”¨äº S3 Tables çš„ S3 è¡¨å­˜å‚¨æ¡¶ï¼Œæ‚¨çš„ PySpark ä½œä¸šç¨åå°†è®¿é—®å®ƒã€‚

å°† `<S3TABLE_BUCKET_NAME>` æ›¿æ¢ä¸ºæ‚¨æ‰€éœ€çš„å­˜å‚¨æ¡¶åç§°ã€‚å°† `<REGION>` æ›¿æ¢ä¸ºæ‚¨çš„ AWS åŒºåŸŸã€‚

```bash
aws s3tables create-table-bucket \
    --region "<REGION>" \
    --name "<S3TABLE_BUCKET_NAME>"
```

è®°ä¸‹æ­¤å‘½ä»¤ç”Ÿæˆçš„ S3TABLE BUCKET ARNã€‚ä» AWS æ§åˆ¶å°éªŒè¯ S3 è¡¨å­˜å‚¨æ¡¶ ARNã€‚

![alt text](../../../../../../docs/blueprints/data-analytics/img/s3table_bucket.png)

### æ­¥éª¤ 6ï¼šæ›´æ–° Spark Operator YAML æ–‡ä»¶

æŒ‰å¦‚ä¸‹æ–¹å¼æ›´æ–° Spark Operator YAML æ–‡ä»¶ï¼š

- åœ¨æ‚¨é¦–é€‰çš„æ–‡æœ¬ç¼–è¾‘å™¨ä¸­æ‰“å¼€ [s3table-spark-operator.yaml](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-spark-operator.yaml) æ–‡ä»¶ã€‚
- å°† `<S3_BUCKET>` æ›¿æ¢ä¸ºæ­¤è“å›¾åˆ›å»ºçš„ S3 å­˜å‚¨æ¡¶ï¼ˆæ£€æŸ¥ Terraform è¾“å‡ºï¼‰ã€‚S3 å­˜å‚¨æ¡¶æ˜¯æ‚¨åœ¨ä¸Šè¿°æ­¥éª¤ä¸­å¤åˆ¶æµ‹è¯•æ•°æ®å’Œç¤ºä¾‹ spark ä½œä¸šçš„åœ°æ–¹ã€‚
- å°† `<S3TABLE_BUCKET_ARN>` æ›¿æ¢ä¸ºæ‚¨åœ¨ä¸Šä¸€æ­¥ä¸­æ•è·çš„ S3 è¡¨å­˜å‚¨æ¡¶ ARNã€‚

æ‚¨å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ° Spark Operator ä½œä¸šé…ç½®çš„ç‰‡æ®µã€‚

```yaml
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: "s3table-example"
  namespace: spark-team-a
  labels:
    app: "s3table-example"
    applicationId: "s3table-example-nvme"
spec:
  type: Python
  sparkVersion: "3.5.3"
  mode: cluster
  # CAUTION: Unsupported test image
  # This image is created solely for testing and reference purposes.
  # Before use, please:
  # 1. Review the Dockerfile used to create this image
  # 2. Create your own image that meets your organization's security requirements
  image: "public.ecr.aws/data-on-eks/spark:3.5.3-scala2.12-java17-python3-ubuntu-s3table0.1.3-iceberg1.6.1"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "s3a://<S3_BUCKET>/s3table-example/scripts/s3table-iceberg-pyspark.py"
  arguments:
    - "s3a://<S3_BUCKET>/s3table-example/input/"
    - "<S3TABLE_BUCKET_ARN>"
  sparkConf:
    "spark.app.name": "s3table-example"
    "spark.kubernetes.driver.pod.name": "s3table-example"
    "spark.kubernetes.executor.podNamePrefix": "s3table-example"
    "spark.local.dir": "/data"
    "spark.speculation": "false"
    "spark.network.timeout": "2400"
    "spark.hadoop.fs.s3a.connection.timeout": "1200000"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.maximum": "200"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.fs.s3a.readahead.range": "256K"
    "spark.hadoop.fs.s3a.input.fadvise": "random"
    "spark.hadoop.fs.s3a.aws.credentials.provider.mapping": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"  # AWS SDK V2 https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/aws_sdk_upgrade.html
...

```

### æ­¥éª¤ 7ï¼šæ‰§è¡Œ Spark ä½œä¸š

å°†æ›´æ–°çš„ YAML é…ç½®æ–‡ä»¶åº”ç”¨åˆ°æ‚¨çš„ Kubernetes é›†ç¾¤ä»¥æäº¤å’Œæ‰§è¡Œ Spark ä½œä¸šï¼š

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/s3-tables
kubectl apply -f s3table-spark-operator.yaml
```

è¿™å°†åœ¨ EKS é›†ç¾¤ä¸Šè°ƒåº¦ Spark ä½œä¸šã€‚Spark Operator å¤„ç†å°†ä½œä¸šæäº¤åˆ° Kubernetes API Serverã€‚

Kubernetes å°†è°ƒåº¦ Spark é©±åŠ¨ç¨‹åºå’Œæ‰§è¡Œå™¨ Pod åœ¨å•ç‹¬çš„å·¥ä½œèŠ‚ç‚¹ä¸Šè¿è¡Œã€‚å¦‚æœéœ€è¦ï¼ŒKarpenter å°†æ ¹æ® Terraform è„šæœ¬ä¸­çš„ nodepool é…ç½®è‡ªåŠ¨é…ç½®æ–°èŠ‚ç‚¹ã€‚

ç›‘æ§ Spark é©±åŠ¨ç¨‹åº Pod çš„æ—¥å¿—ä»¥è·Ÿè¸ªä½œä¸šè¿›åº¦ã€‚Pod é»˜è®¤ä½¿ç”¨ `c5d` å®ä¾‹ï¼Œä½†å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹ YAML å’Œ Karpenter nodepool ä»¥ä½¿ç”¨ä¸åŒçš„ EC2 å®ä¾‹ç±»å‹ã€‚

### æ­¥éª¤ 8ï¼šéªŒè¯ Spark é©±åŠ¨ç¨‹åºæ—¥å¿—çš„è¾“å‡º

åˆ—å‡ºåœ¨ spark-team-a å‘½åç©ºé—´ä¸‹è¿è¡Œçš„ Podï¼š

```bash
kubectl get pods -n spark-team-a
```

éªŒè¯é©±åŠ¨ç¨‹åºæ—¥å¿—ä»¥æŸ¥çœ‹ Spark ä½œä¸šçš„å®Œæ•´è¾“å‡ºã€‚è¯¥ä½œä¸šä» S3 å­˜å‚¨æ¡¶è¯»å– CSV æ•°æ®ï¼Œå¹¶ä½¿ç”¨ Iceberg æ ¼å¼å°†å…¶å†™å› S3 Tables å­˜å‚¨æ¡¶ã€‚å®ƒè¿˜è®¡ç®—å¤„ç†çš„è®°å½•æ•°å¹¶æ˜¾ç¤ºå‰ 10 æ¡è®°å½•ï¼š

```bash
kubectl logs <spark-driver-pod-name> -n spark-team-a
```

å½“ä½œä¸šæˆåŠŸå®Œæˆæ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ° Spark é©±åŠ¨ç¨‹åº Pod è½¬æ¢ä¸º `Succeeded` çŠ¶æ€ï¼Œæ—¥å¿—åº”æ˜¾ç¤ºå¦‚ä¸‹è¾“å‡ºã€‚

```text
...
[2025-01-07 22:07:44,185] INFO @ line 59: Previewing employee data schema
root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- level: string (nullable = true)
 |-- salary: double (nullable = true)

....

25/01/07 22:07:44 INFO CodeGenerator: Code generated in 10.594982 ms
+---+-----------+------+--------+
|id |name       |level |salary  |
+---+-----------+------+--------+
|1  |Employee_1 |Mid   |134000.0|
|2  |Employee_2 |Senior|162500.0|
|3  |Employee_3 |Senior|174500.0|
|4  |Employee_4 |Exec  |69500.0 |
|5  |Employee_5 |Senior|54500.0 |
|6  |Employee_6 |Mid   |164000.0|
|7  |Employee_7 |Junior|119000.0|
|8  |Employee_8 |Senior|54500.0 |
|9  |Employee_9 |Senior|57500.0 |
|10 |Employee_10|Mid   |152000.0|
+---+-----------+------+--------+
only showing top 10 rows

....
```

ä½œä¸šæˆåŠŸåï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªæ–°çš„è¡¨å’Œå‘½åç©ºé—´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![alt text](../../../../../../docs/blueprints/data-analytics/img/s3tables-2.png)

ä»¥ä¸‹å‘½ä»¤å°†æ˜¾ç¤º S3 Tables çš„å…¶ä»–ä¿¡æ¯ã€‚

### ä½¿ç”¨ S3Table API éªŒè¯ S3Table

ä½¿ç”¨ S3Table API ç¡®è®¤è¡¨å·²æˆåŠŸåˆ›å»ºã€‚åªéœ€æ›¿æ¢ `<ACCOUNT_ID>` å¹¶è¿è¡Œå‘½ä»¤ã€‚

```bash
aws s3tables get-table --table-bucket-arn arn:aws:s3tables:<REGION>:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table \
--namespace doeks_namespace \
--name employee_s3_table
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºï¼š

```text
è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºã€‚

{
    "name": "employee_s3_table",
    "type": "customer",
    "tableARN": "arn:aws:s3tables:us-west-2:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf",
    "namespace": [
        "doeks_namespace"
    ],
    "versionToken": "aafc39ddd462690d2a0c",
    "metadataLocation": "s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3/metadata/00004-62cc4be3-59b5-4647-a78d-1cdf69ec5ed8.metadata.json",
    "warehouseLocation": "s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3",
    "createdAt": "2025-01-07T22:14:48.689581+00:00",
    "createdBy": "<ACCOUNT_ID>",
    "modifiedAt": "2025-01-09T00:06:09.222917+00:00",
    "ownerAccountId": "<ACCOUNT_ID>",
    "format": "ICEBERG"
}
```

### ç›‘æ§è¡¨ç»´æŠ¤ä½œä¸šçŠ¶æ€ï¼š

```bash
aws s3tables get-table-maintenance-job-status --table-bucket-arn arn:aws:s3tables:us-west-2:"\<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table --namespace doeks_namespace --name employee_s3_table
```

æ­¤å‘½ä»¤æä¾›æœ‰å…³ Iceberg å‹ç¼©ã€å¿«ç…§ç®¡ç†å’Œæœªå¼•ç”¨æ–‡ä»¶åˆ é™¤è¿‡ç¨‹çš„ä¿¡æ¯ã€‚

```json
{
    "tableARN": "arn:aws:s3tables:us-west-2:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf",
    "status": {
        "icebergCompaction": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T01:18:08.857000+00:00"
        },
        "icebergSnapshotManagement": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T22:17:08.811000+00:00"
        },
        "icebergUnreferencedFileRemoval": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T22:17:10.377000+00:00"
        }
    }
}

```

:::info

è¦åœ¨ EKS ä¸Šä½¿ç”¨ S3 Tablesï¼Œéœ€è¦èŠ‚ç‚¹çº§ç­–ç•¥å’Œ Pod çº§ç­–ç•¥ã€‚

	1.	**èŠ‚ç‚¹çº§ç­–ç•¥**ï¼šè¿™äº›æ·»åŠ åˆ° Karpenter Node IAM è§’è‰²ã€‚ä½œä¸ºå‚è€ƒï¼Œæ‚¨å¯ä»¥åœ¨ [addons.tf](https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/addons.tf#L649C1-L687C5) æ–‡ä»¶ä¸­æŸ¥çœ‹æƒé™é…ç½®ã€‚

	2.	**Pod çº§ç­–ç•¥**ï¼šè¿™äº›å¯¹äºåˆ›å»ºå‘½åç©ºé—´ã€ç®¡ç†è¡¨ä»¥åŠè¯»å–/å†™å…¥è¡¨æ•°æ®æ˜¯å¿…éœ€çš„ã€‚https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/main.tf#L98C1-L156C2 é€šè¿‡ IAM Roles for Service Accounts (IRSA) ä¸º `spark-team-a` å‘½åç©ºé—´æˆäºˆã€‚è¿™ç¡®ä¿ Spark ä½œä¸š Pod å…·æœ‰å¯¹ S3 Tables æ‰§è¡Œæ“ä½œæ‰€éœ€çš„è®¿é—®æƒé™ã€‚

é€šè¿‡é€‚å½“é…ç½®è¿™äº›æƒé™ï¼Œæ‚¨å¯ä»¥ç¡®ä¿ Spark ä½œä¸šçš„æ— ç¼æ‰§è¡Œå’Œå¯¹èµ„æºçš„å®‰å…¨è®¿é—®ã€‚

è¯·æ³¨æ„ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥æ ¹æ®æ‚¨çš„å®‰å…¨è¦æ±‚è¿›ä¸€æ­¥è°ƒæ•´å¹¶ä½¿å…¶æ›´åŠ ç»†ç²’åº¦ã€‚

:::

<CollapsibleContent header={<h2><span>åœ¨ JupyterLab ä¸­ä½¿ç”¨ S3 Tables</span></h2>}>

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ JupyterLab äº¤äº’å¼åœ°ä½¿ç”¨ S3 Tablesï¼Œæ­¤è“å›¾å…è®¸æ‚¨å°† JupyterLab å•ç”¨æˆ·å®ä¾‹éƒ¨ç½²åˆ°æ‚¨çš„é›†ç¾¤ã€‚

> :warning: æ­¤å¤„æä¾›çš„ JupyterHub é…ç½®ä»…ç”¨äºæµ‹è¯•ç›®çš„ã€‚
>
> :warning: è¯·æŸ¥çœ‹é…ç½®å¹¶è¿›è¡Œå¿…è¦çš„æ›´æ”¹ä»¥æ»¡è¶³æ‚¨çš„å®‰å…¨æ ‡å‡†ã€‚

### æ›´æ–° Terraform å˜é‡å¹¶åº”ç”¨

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator

echo 'enable_jupyterhub = true' >> spark-operator.tfvars
terraform apply -var-file spark-operator.tfvars
```

### ç¡®ä¿æ‚¨çš„ S3 å­˜å‚¨æ¡¶ä¸­æœ‰æµ‹è¯•æ•°æ®å¯ç”¨

```bash
cd analytics/terraform/spark-k8s-operator/examples/s3-tables
./input-data-gen.sh
aws s3 cp employee_data.csv s3://${S3_BUCKET}/s3table-example/input/
```

### è®¿é—® JupyterHub UI å¹¶é…ç½® JupyterLab æœåŠ¡å™¨

1. å°†ä»£ç†æœåŠ¡ç«¯å£è½¬å‘åˆ°æ‚¨çš„æœ¬åœ°æœºå™¨ã€‚

    ```bash
    kubectl port-forward svc/proxy-public 8888:80 -n jupyterhub
    ```

1. è½¬åˆ° [`http://localhost:8888`](http://localhost:8888)ã€‚è¾“å…¥ä»»ä½•ç”¨æˆ·åï¼Œå°†å¯†ç å­—æ®µç•™ç©ºï¼Œç„¶åå•å‡»"ç™»å½•"ã€‚

   ![sign in](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-signin.png)

1. å•å‡»å¼€å§‹ã€‚å¦‚æœæ‚¨æƒ³è‡ªå·±è‡ªå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ä¸Šæ¸¸ PySpark NoteBook é•œåƒã€‚

    ![select](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-select.png)

1. ä»[ç¤ºä¾‹ Jupyter Notebook](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.ipynb) å¤åˆ¶ç¤ºä¾‹ä½œä¸ºèµ·ç‚¹ï¼Œä»¥äº¤äº’å¼æµ‹è¯• S3 Tables åŠŸèƒ½ã€‚

    **ç¡®ä¿åœ¨ç¬”è®°æœ¬ä¸­æ›´æ–° `S3_BUCKET` å’Œ `s3table_arn` å€¼**

    ![notebook](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-notebook.png)

</CollapsibleContent>

<CollapsibleContent header={<h2><span>æ¸…ç†</span></h2>}>

:::caution
ä¸ºé¿å…å¯¹æ‚¨çš„ AWS è´¦æˆ·äº§ç”Ÿä¸å¿…è¦çš„è´¹ç”¨ï¼Œè¯·åˆ é™¤åœ¨æ­¤éƒ¨ç½²æœŸé—´åˆ›å»ºçš„æ‰€æœ‰ AWS èµ„æº
:::

## åˆ é™¤ S3 è¡¨

```bash
aws s3tables delete-table \
  --namespace doeks_namespace \
  --table-bucket-arn ${S3TABLE_ARN} \
  --name employee_s3_table
```

## åˆ é™¤å‘½åç©ºé—´

```bash
aws s3tables delete-namespace \
  --namespace doeks_namespace \
  --table-bucket-arn ${S3TABLE_ARN}
```

## åˆ é™¤å­˜å‚¨æ¡¶è¡¨

```bash
aws s3tables delete-table-bucket \
  --region "<REGION>" \
  --table-bucket-arn ${S3TABLE_ARN}
```

## åˆ é™¤ Jupyter Notebook æœåŠ¡å™¨

å¦‚æœæ‚¨åˆ›å»ºäº† Jupyter notebook æœåŠ¡å™¨

```bash
kubectl delete pods -n jupyterhub -l component=singleuser-server
```

## åˆ é™¤ EKS é›†ç¾¤

æ­¤è„šæœ¬å°†ä½¿ç”¨ `-target` é€‰é¡¹æ¸…ç†ç¯å¢ƒï¼Œä»¥ç¡®ä¿æ‰€æœ‰èµ„æºæŒ‰æ­£ç¡®é¡ºåºåˆ é™¤ã€‚

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator && chmod +x cleanup.sh
./cleanup.sh
```

</CollapsibleContent>
