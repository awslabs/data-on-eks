---
sidebar_position: 4
sidebar_label: EKSä¸Šçš„S3è¡¨
hide_table_of_contents: true
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CollapsibleContent from '../../../../../../src/components/CollapsibleContent';

import TaxiTripExecute from '../../../../../../docs/blueprints/data-analytics/_taxi_trip_exec.md'
import ReplaceS3BucketPlaceholders from '../../../../../../docs/blueprints/data-analytics/_replace_s3_bucket_placeholders.mdx';

import CodeBlock from '@theme/CodeBlock';

# Amazon EKSä¸Šçš„S3è¡¨

![s3tables](../../../../../../docs/blueprints/data-analytics/img/s3tables.png)


## ä»€ä¹ˆæ˜¯S3è¡¨ï¼Ÿ

Amazon S3è¡¨æ˜¯ä¸€ä¸ªå®Œå…¨æ‰˜ç®¡çš„è¡¨æ ¼æ•°æ®å­˜å‚¨ï¼Œä¸“ä¸ºä¼˜åŒ–æ€§èƒ½ã€ç®€åŒ–å®‰å…¨æ€§å’Œä¸ºå¤§è§„æ¨¡åˆ†æå·¥ä½œè´Ÿè½½æä¾›æˆæœ¬æ•ˆç›Šé«˜çš„å­˜å‚¨è€Œæ„å»ºã€‚å®ƒç›´æ¥ä¸Amazon EMRã€Amazon Athenaã€Amazon Redshiftã€AWS Glueå’ŒAWS Lake Formationç­‰æœåŠ¡é›†æˆï¼Œä¸ºè¿è¡Œåˆ†æå’Œæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½æä¾›æ— ç¼ä½“éªŒã€‚

## ä¸ºä»€ä¹ˆåœ¨Amazon EKSä¸Šè¿è¡ŒS3è¡¨ï¼Ÿ

å¯¹äºå·²ç»é‡‡ç”¨Amazon EKSè¿›è¡ŒSparkå·¥ä½œè´Ÿè½½å¹¶ä½¿ç”¨Icebergç­‰è¡¨æ ¼æ ¼å¼çš„ç”¨æˆ·ï¼Œåˆ©ç”¨S3è¡¨å¯ä»¥åœ¨æ€§èƒ½ã€æˆæœ¬æ•ˆç›Šå’Œå®‰å…¨æ§åˆ¶æ–¹é¢æä¾›ä¼˜åŠ¿ã€‚è¿™ç§é›†æˆå…è®¸ç»„ç»‡å°†KubernetesåŸç”ŸåŠŸèƒ½ä¸S3è¡¨çš„åŠŸèƒ½ç›¸ç»“åˆï¼Œå¯èƒ½åœ¨å…¶ç°æœ‰ç¯å¢ƒä¸­æ”¹å–„æŸ¥è¯¢æ€§èƒ½å’Œèµ„æºæ‰©å±•ã€‚é€šè¿‡éµå¾ªæœ¬æ–‡æ¡£ä¸­è¯¦è¿°çš„æ­¥éª¤ï¼Œç”¨æˆ·å¯ä»¥å°†S3è¡¨æ— ç¼é›†æˆåˆ°ä»–ä»¬çš„EKSè®¾ç½®ä¸­ï¼Œä¸ºåˆ†æå·¥ä½œè´Ÿè½½æä¾›çµæ´»ä¸”äº’è¡¥çš„è§£å†³æ–¹æ¡ˆã€‚

## S3è¡¨ä¸Icebergè¡¨æ ¼å¼æœ‰ä½•ä¸åŒ

è™½ç„¶S3è¡¨ä½¿ç”¨Apache Icebergä½œä¸ºåº•å±‚å®ç°ï¼Œä½†å®ƒä»¬æä¾›äº†ä¸“ä¸ºAWSå®¢æˆ·è®¾è®¡çš„å¢å¼ºåŠŸèƒ½ï¼š

- **ğŸ› ï¸ è‡ªåŠ¨å‹ç¼©**ï¼šS3è¡¨å®ç°äº†è‡ªåŠ¨å‹ç¼©ï¼Œé€šè¿‡å°†è¾ƒå°çš„æ–‡ä»¶æ™ºèƒ½åœ°ç»„åˆæˆæ›´å¤§ã€æ›´é«˜æ•ˆçš„æ–‡ä»¶ï¼Œåœ¨åå°ä¼˜åŒ–æ•°æ®å­˜å‚¨ã€‚è¿™ä¸ªè¿‡ç¨‹é™ä½äº†å­˜å‚¨æˆæœ¬ï¼Œæé«˜äº†æŸ¥è¯¢é€Ÿåº¦ï¼Œå¹¶ä¸”æ— éœ€æ‰‹åŠ¨å¹²é¢„å³å¯æŒç»­è¿è¡Œã€‚

- ğŸ”„ **è¡¨ç»´æŠ¤**ï¼šå®ƒæä¾›äº†å…³é”®çš„ç»´æŠ¤ä»»åŠ¡ï¼Œå¦‚å¿«ç…§ç®¡ç†å’Œæœªå¼•ç”¨æ–‡ä»¶çš„ç§»é™¤ã€‚è¿™ç§æŒç»­ä¼˜åŒ–ç¡®ä¿è¡¨ä¿æŒé«˜æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ï¼Œå‡å°‘äº†è¿è¥å¼€é”€ï¼Œä½¿å›¢é˜Ÿèƒ½å¤Ÿä¸“æ³¨äºæ•°æ®æ´å¯Ÿã€‚

- â„ï¸ **Apache Icebergæ”¯æŒ**ï¼šå†…ç½®æ”¯æŒApache Icebergï¼Œç®€åŒ–äº†å¤§è§„æ¨¡æ•°æ®æ¹–çš„ç®¡ç†ï¼ŒåŒæ—¶æé«˜äº†æŸ¥è¯¢æ€§èƒ½å¹¶é™ä½äº†æˆæœ¬ã€‚å¦‚æœæ‚¨æƒ³ä½“éªŒä»¥ä¸‹ç»“æœï¼Œè¯·è€ƒè™‘ä½¿ç”¨S3è¡¨ä½œä¸ºæ‚¨çš„æ•°æ®æ¹–ã€‚

- ğŸ”’ **ç®€åŒ–å®‰å…¨æ€§**ï¼šS3è¡¨å°†æ‚¨çš„è¡¨è§†ä¸ºAWSèµ„æºï¼Œåœ¨è¡¨çº§åˆ«å¯ç”¨ç»†ç²’åº¦çš„AWSèº«ä»½å’Œè®¿é—®ç®¡ç†(IAM)æƒé™ã€‚è¿™ç®€åŒ–äº†æ•°æ®æ²»ç†ï¼Œå¢å¼ºäº†å®‰å…¨æ€§ï¼Œå¹¶ä½¿è®¿é—®æ§åˆ¶ä¸æ‚¨ç†Ÿæ‚‰çš„AWSæœåŠ¡æ›´åŠ ç›´è§‚å’Œå¯ç®¡ç†ã€‚

- âš¡ **å¢å¼ºæ€§èƒ½**ï¼šAmazon S3è¡¨å¼•å…¥äº†ä¸€ç§æ–°å‹å­˜å‚¨æ¡¶ï¼Œä¸“ä¸ºå­˜å‚¨Apache Icebergè¡¨è€Œæ„å»ºã€‚ä¸åœ¨é€šç”¨S3å­˜å‚¨æ¡¶ä¸­å­˜å‚¨Icebergè¡¨ç›¸æ¯”ï¼Œè¡¨å­˜å‚¨æ¡¶æä¾›é«˜è¾¾3å€çš„æŸ¥è¯¢æ€§èƒ½å’Œé«˜è¾¾10å€çš„æ¯ç§’äº‹åŠ¡æ•°ã€‚è¿™ç§æ€§èƒ½å¢å¼ºæ”¯æŒé«˜é¢‘æ›´æ–°ã€å®æ—¶æ‘„å–å’Œæ›´é«˜è¦æ±‚çš„å·¥ä½œè´Ÿè½½ï¼Œç¡®ä¿éšç€æ•°æ®é‡å¢é•¿çš„å¯æ‰©å±•æ€§å’Œå“åº”æ€§ã€‚

- ğŸ› ï¸ **ä¸AWSæœåŠ¡é›†æˆ**ï¼šS3è¡¨ä¸AWSåˆ†ææœåŠ¡ï¼ˆå¦‚Athenaã€Redshiftã€EMRå’ŒGlueï¼‰ç´§å¯†é›†æˆï¼Œä¸ºåˆ†æå·¥ä½œè´Ÿè½½æä¾›åŸç”Ÿæ”¯æŒã€‚


<CollapsibleContent header={<h2><span>éƒ¨ç½²è§£å†³æ–¹æ¡ˆ</span></h2>}>

åœ¨è¿™ä¸ª[ç¤ºä¾‹](https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator)ä¸­ï¼Œæ‚¨å°†é…ç½®è¿è¡Œå¸¦æœ‰å¼€æºSpark Operatorå’ŒApache YuniKornçš„Sparkä½œä¸šæ‰€éœ€çš„ä»¥ä¸‹èµ„æºã€‚

æ­¤ç¤ºä¾‹å°†Spark K8s Operatoréƒ¨ç½²åˆ°æ–°çš„VPCä¸­çš„EKSé›†ç¾¤ã€‚

- åˆ›å»ºä¸€ä¸ªæ–°çš„ç¤ºä¾‹VPCï¼Œ2ä¸ªç§æœ‰å­ç½‘ï¼Œ2ä¸ªå…¬å…±å­ç½‘ï¼Œä»¥åŠRFC6598ç©ºé—´(100.64.0.0/10)ä¸­çš„2ä¸ªå­ç½‘ç”¨äºEKS Podã€‚
- ä¸ºå…¬å…±å­ç½‘åˆ›å»ºäº’è”ç½‘ç½‘å…³ï¼Œä¸ºç§æœ‰å­ç½‘åˆ›å»ºNATç½‘å…³
- åˆ›å»ºå¸¦æœ‰å…¬å…±ç«¯ç‚¹çš„EKSé›†ç¾¤æ§åˆ¶å¹³é¢ï¼ˆä»…ç”¨äºæ¼”ç¤ºç›®çš„ï¼‰ï¼Œå¸¦æœ‰ç”¨äºåŸºå‡†æµ‹è¯•å’Œæ ¸å¿ƒæœåŠ¡çš„æ‰˜ç®¡èŠ‚ç‚¹ç»„ï¼Œä»¥åŠç”¨äºSparkå·¥ä½œè´Ÿè½½çš„Karpenter NodePoolsã€‚
- éƒ¨ç½²Metrics serverã€Spark-operatorã€Apache Yunikornã€Karpenterã€Grafanaå’ŒPrometheusæœåŠ¡å™¨ã€‚

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²åœ¨è®¡ç®—æœºä¸Šå®‰è£…äº†ä»¥ä¸‹å·¥å…·ã€‚

1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### éƒ¨ç½²

å…‹éš†ä»“åº“ã€‚

```bash
git clone https://github.com/awslabs/data-on-eks.git
cd data-on-eks
export DOEKS_HOME=$(pwd)
```

å¦‚æœDOEKS_HOMEå˜é‡è¢«å–æ¶ˆè®¾ç½®ï¼Œæ‚¨å¯ä»¥éšæ—¶ä»data-on-eksç›®å½•ä½¿ç”¨`export
DATA_ON_EKS=$(pwd)`æ‰‹åŠ¨è®¾ç½®å®ƒã€‚

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•ä¹‹ä¸€å¹¶è¿è¡Œ`install.sh`è„šæœ¬ã€‚

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator
chmod +x install.sh
./install.sh
```

ç°åœ¨åˆ›å»ºä¸€ä¸ª`S3_BUCKET`å˜é‡ï¼Œä¿å­˜å®‰è£…æœŸé—´åˆ›å»ºçš„å­˜å‚¨æ¡¶åç§°ã€‚æ­¤å­˜å‚¨æ¡¶å°†åœ¨åç»­ç¤ºä¾‹ä¸­ç”¨äºå­˜å‚¨è¾“å‡ºæ•°æ®ã€‚å¦‚æœS3_BUCKETå˜é‡è¢«å–æ¶ˆè®¾ç½®ï¼Œæ‚¨å¯ä»¥å†æ¬¡è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)
echo $S3_BUCKET
```

</CollapsibleContent>
## æ‰§è¡Œç¤ºä¾‹Sparkä½œä¸š

### æ­¥éª¤1ï¼šåˆ›å»ºä¸S3è¡¨å…¼å®¹çš„Apache Spark Dockeré•œåƒ

åˆ›å»ºä¸€ä¸ªåŒ…å«S3è¡¨é€šä¿¡æ‰€éœ€jaråŒ…çš„Dockeré•œåƒã€‚

- æŸ¥çœ‹ç¤ºä¾‹[Dockerfile](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/Dockerfile-S3Table)
- æ³¨æ„S3è¡¨äº¤äº’çš„[å…³é”®jaræ–‡ä»¶](https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/examples/s3-tables/Dockerfile-S3Table#L43C1-L48C1)ï¼ŒåŒ…æ‹¬Icebergã€AWS SDK bundleå’Œç”¨äºIcebergè¿è¡Œæ—¶çš„S3è¡¨ç›®å½•
- æ ¹æ®æ‚¨çš„ç¯å¢ƒéœ€è¦è‡ªå®šä¹‰Dockerfile
- æ„å»ºDockeré•œåƒå¹¶å°†é•œåƒæ¨é€åˆ°æ‚¨é¦–é€‰çš„å®¹å™¨æ³¨å†Œè¡¨

æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªDockeré•œåƒå¹¶å‘å¸ƒåœ¨ECRä¸­ï¼Œä»…ç”¨äºæ¼”ç¤ºç›®çš„ã€‚

### æ­¥éª¤2ï¼šä¸ºä½œä¸šåˆ›å»ºæµ‹è¯•æ•°æ®

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•ï¼Œä½¿ç”¨è¿™ä¸ª[shell](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/input-data-gen.sh)è„šæœ¬ä¸ºSparkä½œä¸šè¾“å…¥ç”Ÿæˆç¤ºä¾‹å‘˜å·¥æ•°æ®ã€‚

```sh
cd analytics/terraform/spark-k8s-operator/examples/s3-tables
./input-data-gen.sh
```

æ­¤è„šæœ¬å°†åœ¨æ‚¨å½“å‰ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªåä¸º`employee_data.csv`çš„æ–‡ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒç”Ÿæˆ100æ¡è®°å½•ã€‚

æ³¨æ„ï¼šå¦‚æœæ‚¨éœ€è¦è°ƒæ•´è®°å½•æ•°é‡ï¼Œå¯ä»¥ä¿®æ”¹input-data-gen.shè„šæœ¬ã€‚æŸ¥æ‰¾ç”Ÿæˆæ•°æ®çš„å¾ªç¯å¹¶æ ¹æ®éœ€è¦æ›´æ”¹è¿­ä»£è®¡æ•°ã€‚

### æ­¥éª¤3ï¼šå°†æµ‹è¯•è¾“å…¥æ•°æ®ä¸Šä¼ åˆ°Amazon S3å­˜å‚¨æ¡¶

å°†`<YOUR_S3_BUCKET>`æ›¿æ¢ä¸ºæ‚¨çš„è“å›¾åˆ›å»ºçš„S3å­˜å‚¨æ¡¶åç§°ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
aws s3 cp employee_data.csv s3://<S3_BUCKET>/s3table-example/input/
```

æ­¤å‘½ä»¤å°†CSVæ–‡ä»¶ä¸Šä¼ åˆ°æ‚¨çš„S3å­˜å‚¨æ¡¶ã€‚Sparkä½œä¸šç¨åå°†å¼•ç”¨æ­¤è·¯å¾„æ¥è¯»å–è¾“å…¥æ•°æ®ã€‚åœ¨æ‰§è¡Œå‘½ä»¤ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰å¿…è¦çš„æƒé™å†™å…¥æ­¤å­˜å‚¨æ¡¶ã€‚

### æ­¥éª¤4ï¼šå°†PySparkè„šæœ¬ä¸Šä¼ åˆ°S3å­˜å‚¨æ¡¶

ä»¥ä¸‹è„šæœ¬æ˜¯[Sparkä½œä¸š](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.py)çš„ç‰‡æ®µï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä½¿ç”¨S3è¡¨æ‰€éœ€çš„Sparké…ç½®ã€‚

```python
def main(args):
    if len(args) != 3:
        logger.error("Usage: spark-etl [input-csv-path] [s3table-arn]")
        sys.exit(1)

    # Input parameters
    input_csv_path = args[1]    # Path to the input CSV file
    s3table_arn = args[2]       # s3table arn

    # Initialize Spark session
    logger.info("Initializing Spark Session")
    spark = (SparkSession
             .builder
             .appName(f"{AppName}_{dt_string}")
             .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
             .config("spark.sql.catalog.s3tablesbucket", "org.apache.iceberg.spark.SparkCatalog")
             .config("spark.sql.catalog.s3tablesbucket.catalog-impl", "software.amazon.s3tables.iceberg.S3TablesCatalog")
             .config("spark.sql.catalog.s3tablesbucket.warehouse", s3table_arn)
             .config('spark.hadoop.fs.s3.impl', "org.apache.hadoop.fs.s3a.S3AFileSystem")
             .config("spark.sql.defaultCatalog", "s3tablesbucket")
             .getOrCreate())

    spark.sparkContext.setLogLevel("INFO")
    logger.info("Spark session initialized successfully")

    namespace = "doeks_namespace"
    table_name = "employee_s3_table"
    full_table_name = f"s3tablesbucket.{namespace}.{table_name}"

...

```

å°†`S3_BUCKET`æ›¿æ¢ä¸ºæ‚¨çš„è“å›¾åˆ›å»ºçš„S3å­˜å‚¨æ¡¶åç§°ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†ç¤ºä¾‹[Sparkä½œä¸š](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.py)ä¸Šä¼ åˆ°S3å­˜å‚¨æ¡¶ã€‚

```bash
aws s3 cp s3table-iceberg-pyspark.py s3://<S3_BUCKET>/s3table-example/scripts/
```

å¯¼èˆªåˆ°ç¤ºä¾‹ç›®å½•å¹¶æäº¤Sparkä½œä¸šã€‚

### æ­¥éª¤5ï¼šåˆ›å»ºAmazon S3è¡¨å­˜å‚¨æ¡¶

è¿™æ˜¯ä¸»è¦æ­¥éª¤ï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ªS3è¡¨å­˜å‚¨æ¡¶ï¼Œç”¨äºS3è¡¨ï¼Œæ‚¨çš„PySparkä½œä¸šç¨åå°†è®¿é—®å®ƒã€‚

å°†`<S3TABLE_BUCKET_NAME>`æ›¿æ¢ä¸ºæ‚¨æƒ³è¦çš„å­˜å‚¨æ¡¶åç§°ã€‚å°†`<REGION>`æ›¿æ¢ä¸ºæ‚¨çš„AWSåŒºåŸŸã€‚


```bash
aws s3tables create-table-bucket \
    --region "<REGION>" \
    --name "<S3TABLE_BUCKET_NAME>"
```

è®°ä¸‹æ­¤å‘½ä»¤ç”Ÿæˆçš„S3TABLE BUCKET ARNã€‚ä»AWSæ§åˆ¶å°éªŒè¯S3è¡¨å­˜å‚¨æ¡¶ARNã€‚

![æ›¿ä»£æ–‡æœ¬](../../../../../../docs/blueprints/data-analytics/img/s3table_bucket.png)

### æ­¥éª¤6ï¼šæ›´æ–°Spark Operator YAMLæ–‡ä»¶

å¦‚ä¸‹æ›´æ–°Spark Operator YAMLæ–‡ä»¶ï¼š

- åœ¨æ‚¨é¦–é€‰çš„æ–‡æœ¬ç¼–è¾‘å™¨ä¸­æ‰“å¼€[s3table-spark-operator.yaml](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-spark-operator.yaml)æ–‡ä»¶ã€‚
- å°†`<S3_BUCKET>`æ›¿æ¢ä¸ºæ­¤è“å›¾åˆ›å»ºçš„S3å­˜å‚¨æ¡¶(æŸ¥çœ‹Terraformè¾“å‡º)ã€‚S3å­˜å‚¨æ¡¶æ˜¯æ‚¨åœ¨ä¸Šè¿°æ­¥éª¤ä¸­å¤åˆ¶æµ‹è¯•æ•°æ®å’Œç¤ºä¾‹sparkä½œä¸šçš„åœ°æ–¹ã€‚
- å°†`<S3TABLE_BUCKET_ARN>`æ›¿æ¢ä¸ºæ‚¨åœ¨ä¸Šä¸€æ­¥ä¸­æ•è·çš„S3è¡¨å­˜å‚¨æ¡¶ARNã€‚

æ‚¨å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°Spark Operatorä½œä¸šé…ç½®çš„ç‰‡æ®µã€‚


```yaml
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: "s3table-example"
  namespace: spark-team-a
  labels:
    app: "s3table-example"
    applicationId: "s3table-example-nvme"
spec:
  type: Python
  sparkVersion: "3.5.3"
  mode: cluster
  # CAUTION: Unsupported test image
  # This image is created solely for testing and reference purposes.
  # Before use, please:
  # 1. Review the Dockerfile used to create this image
  # 2. Create your own image that meets your organization's security requirements
  image: "public.ecr.aws/data-on-eks/spark:3.5.3-scala2.12-java17-python3-ubuntu-s3table0.1.3-iceberg1.6.1"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "s3a://<S3_BUCKET>/s3table-example/scripts/s3table-iceberg-pyspark.py"
  arguments:
    - "s3a://<S3_BUCKET>/s3table-example/input/"
    - "<S3TABLE_BUCKET_ARN>"
  sparkConf:
    "spark.app.name": "s3table-example"
    "spark.kubernetes.driver.pod.name": "s3table-example"
    "spark.kubernetes.executor.podNamePrefix": "s3table-example"
    "spark.local.dir": "/data"
    "spark.speculation": "false"
    "spark.network.timeout": "2400"
    "spark.hadoop.fs.s3a.connection.timeout": "1200000"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.maximum": "200"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.fs.s3a.readahead.range": "256K"
    "spark.hadoop.fs.s3a.input.fadvise": "random"
    "spark.hadoop.fs.s3a.aws.credentials.provider.mapping": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider"  # AWS SDK V2 https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/aws_sdk_upgrade.html
...

```
### æ­¥éª¤7ï¼šæ‰§è¡ŒSparkä½œä¸š

å°†æ›´æ–°åçš„YAMLé…ç½®æ–‡ä»¶åº”ç”¨åˆ°æ‚¨çš„Kubernetesé›†ç¾¤ï¼Œä»¥æäº¤å’Œæ‰§è¡ŒSparkä½œä¸šï¼š

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/s3-tables
kubectl apply -f s3table-spark-operator.yaml
```

è¿™å°†åœ¨EKSé›†ç¾¤ä¸Šè°ƒåº¦Sparkä½œä¸šã€‚Spark Operatorè´Ÿè´£å°†ä½œä¸šæäº¤åˆ°Kubernetes APIæœåŠ¡å™¨ã€‚

Kuberneteså°†è°ƒåº¦Sparké©±åŠ¨ç¨‹åºå’Œæ‰§è¡Œå™¨podåœ¨ä¸åŒçš„å·¥ä½œèŠ‚ç‚¹ä¸Šè¿è¡Œã€‚å¦‚æœéœ€è¦ï¼ŒKarpenterå°†æ ¹æ®Terraformè„šæœ¬ä¸­çš„nodepoolé…ç½®è‡ªåŠ¨é…ç½®æ–°èŠ‚ç‚¹ã€‚

ç›‘æ§Sparké©±åŠ¨ç¨‹åºpodçš„æ—¥å¿—ä»¥è·Ÿè¸ªä½œä¸šè¿›åº¦ã€‚podé»˜è®¤ä½¿ç”¨`c5d`å®ä¾‹ï¼Œä½†å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹YAMLå’ŒKarpenter nodepoolä»¥ä½¿ç”¨ä¸åŒçš„EC2å®ä¾‹ç±»å‹ã€‚

### æ­¥éª¤8ï¼šéªŒè¯Sparké©±åŠ¨ç¨‹åºæ—¥å¿—çš„è¾“å‡º

åˆ—å‡ºåœ¨spark-team-aå‘½åç©ºé—´ä¸‹è¿è¡Œçš„podï¼š

```bash
kubectl get pods -n spark-team-a
```

éªŒè¯é©±åŠ¨ç¨‹åºæ—¥å¿—ä»¥æŸ¥çœ‹Sparkä½œä¸šçš„å®Œæ•´è¾“å‡ºã€‚è¯¥ä½œä¸šä»S3å­˜å‚¨æ¡¶è¯»å–CSVæ•°æ®ï¼Œå¹¶ä½¿ç”¨Icebergæ ¼å¼å°†å…¶å†™å›S3è¡¨å­˜å‚¨æ¡¶ã€‚å®ƒè¿˜è®¡ç®—å¤„ç†çš„è®°å½•æ•°å¹¶æ˜¾ç¤ºå‰10æ¡è®°å½•ï¼š

```bash
kubectl logs <spark-driver-pod-name> -n spark-team-a
```

å½“ä½œä¸šæˆåŠŸå®Œæˆæ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°Sparké©±åŠ¨ç¨‹åºpodè½¬æ¢ä¸º`Succeeded`çŠ¶æ€ï¼Œæ—¥å¿—åº”è¯¥æ˜¾ç¤ºå¦‚ä¸‹è¾“å‡ºã€‚

```text
...
[2025-01-07 22:07:44,185] INFO @ line 59: Previewing employee data schema
root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- level: string (nullable = true)
 |-- salary: double (nullable = true)

....

25/01/07 22:07:44 INFO CodeGenerator: Code generated in 10.594982 ms
+---+-----------+------+--------+
|id |name       |level |salary  |
+---+-----------+------+--------+
|1  |Employee_1 |Mid   |134000.0|
|2  |Employee_2 |Senior|162500.0|
|3  |Employee_3 |Senior|174500.0|
|4  |Employee_4 |Exec  |69500.0 |
|5  |Employee_5 |Senior|54500.0 |
|6  |Employee_6 |Mid   |164000.0|
|7  |Employee_7 |Junior|119000.0|
|8  |Employee_8 |Senior|54500.0 |
|9  |Employee_9 |Senior|57500.0 |
|10 |Employee_10|Mid   |152000.0|
+---+-----------+------+--------+
only showing top 10 rows

....
```

ä½œä¸šæˆåŠŸåï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªæ–°çš„è¡¨å’Œå‘½åç©ºé—´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![æ›¿ä»£æ–‡æœ¬](../../../../../../docs/blueprints/data-analytics/img/s3tables-2.png)


ä»¥ä¸‹å‘½ä»¤å°†æ˜¾ç¤ºS3è¡¨çš„å…¶ä»–ä¿¡æ¯ã€‚

### ä½¿ç”¨S3Table APIéªŒè¯S3è¡¨

ä½¿ç”¨S3Table APIç¡®è®¤è¡¨å·²æˆåŠŸåˆ›å»ºã€‚åªéœ€æ›¿æ¢`<ACCOUNT_ID>`å¹¶è¿è¡Œå‘½ä»¤ã€‚

```bash
aws s3tables get-table --table-bucket-arn arn:aws:s3tables:<REGION>:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table \
--namespace doeks_namespace \
--name employee_s3_table
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºï¼š

```text
è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºã€‚

{
    "name": "employee_s3_table",
    "type": "customer",
    "tableARN": "arn:aws:s3tables:us-west-2:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf",
    "namespace": [
        "doeks_namespace"
    ],
    "versionToken": "aafc39ddd462690d2a0c",
    "metadataLocation": "s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3/metadata/00004-62cc4be3-59b5-4647-a78d-1cdf69ec5ed8.metadata.json",
    "warehouseLocation": "s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3",
    "createdAt": "2025-01-07T22:14:48.689581+00:00",
    "createdBy": "<ACCOUNT_ID>",
    "modifiedAt": "2025-01-09T00:06:09.222917+00:00",
    "ownerAccountId": "<ACCOUNT_ID>",
    "format": "ICEBERG"
}
```

### ç›‘æ§è¡¨ç»´æŠ¤ä½œä¸šçŠ¶æ€ï¼š

```bash
aws s3tables get-table-maintenance-job-status --table-bucket-arn arn:aws:s3tables:us-west-2:"\<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table --namespace doeks_namespace --name employee_s3_table
```

æ­¤å‘½ä»¤æä¾›æœ‰å…³Icebergå‹ç¼©ã€å¿«ç…§ç®¡ç†å’Œæœªå¼•ç”¨æ–‡ä»¶ç§»é™¤è¿‡ç¨‹çš„ä¿¡æ¯ã€‚

```json
{
    "tableARN": "arn:aws:s3tables:us-west-2:<ACCOUNT_ID>:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf",
    "status": {
        "icebergCompaction": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T01:18:08.857000+00:00"
        },
        "icebergSnapshotManagement": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T22:17:08.811000+00:00"
        },
        "icebergUnreferencedFileRemoval": {
            "status": "Successful",
            "lastRunTimestamp": "2025-01-08T22:17:10.377000+00:00"
        }
    }
}

```


:::info

è¦åœ¨EKSä¸Šä½¿ç”¨S3è¡¨ï¼Œéœ€è¦èŠ‚ç‚¹çº§ç­–ç•¥å’ŒPodçº§ç­–ç•¥ã€‚

	1.	**èŠ‚ç‚¹çº§ç­–ç•¥**ï¼šè¿™äº›æ·»åŠ åˆ°KarpenterèŠ‚ç‚¹IAMè§’è‰²ã€‚ä½œä¸ºå‚è€ƒï¼Œæ‚¨å¯ä»¥åœ¨[addons.tf](https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/addons.tf#L649C1-L687C5)æ–‡ä»¶ä¸­æŸ¥çœ‹æƒé™é…ç½®ã€‚

	2.	**Podçº§ç­–ç•¥**ï¼šè¿™äº›å¯¹äºåˆ›å»ºå‘½åç©ºé—´ã€ç®¡ç†è¡¨ä»¥åŠè¯»å–/å†™å…¥æ•°æ®åˆ°è¡¨æ˜¯å¿…è¦çš„ã€‚https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/main.tf#L98C1-L156C2 é€šè¿‡`spark-team-a`å‘½åç©ºé—´çš„æœåŠ¡è´¦æˆ·IAMè§’è‰²(IRSA)æˆäºˆã€‚è¿™ç¡®ä¿Sparkä½œä¸špodå…·æœ‰æ‰§è¡ŒS3è¡¨æ“ä½œæ‰€éœ€çš„è®¿é—®æƒé™ã€‚

é€šè¿‡é€‚å½“é…ç½®è¿™äº›æƒé™ï¼Œæ‚¨å¯ä»¥ç¡®ä¿Sparkä½œä¸šçš„æ— ç¼æ‰§è¡Œå’Œå¯¹èµ„æºçš„å®‰å…¨è®¿é—®ã€‚

è¯·æ³¨æ„ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥æ ¹æ®æ‚¨çš„å®‰å…¨è¦æ±‚è¿›ä¸€æ­¥è°ƒæ•´å¹¶ä½¿å…¶æ›´åŠ ç²¾ç»†ã€‚

:::


<CollapsibleContent header={<h2><span>ä½¿ç”¨JupyterLabçš„S3è¡¨</span></h2>}>

å¦‚æœæ‚¨æƒ³ä½¿ç”¨JupyterLabä¸S3è¡¨äº¤äº’å¼å·¥ä½œï¼Œæ­¤è“å›¾å…è®¸æ‚¨å°†JupyterLabå•ç”¨æˆ·å®ä¾‹éƒ¨ç½²åˆ°æ‚¨çš„é›†ç¾¤ã€‚

> :warning: æ­¤å¤„æä¾›çš„JupyterHubé…ç½®ä»…ç”¨äºæµ‹è¯•ç›®çš„ã€‚
>
> :warning: æŸ¥çœ‹é…ç½®å¹¶è¿›è¡Œå¿…è¦çš„æ›´æ”¹ä»¥æ»¡è¶³æ‚¨çš„å®‰å…¨æ ‡å‡†ã€‚

### æ›´æ–°Terraformå˜é‡å¹¶åº”ç”¨

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator

echo 'enable_jupyterhub = true' >> spark-operator.tfvars
terraform apply -var-file spark-operator.tfvars
```

### ç¡®ä¿æ‚¨çš„S3å­˜å‚¨æ¡¶ä¸­æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®

```bash
cd analytics/terraform/spark-k8s-operator/examples/s3-tables
./input-data-gen.sh
aws s3 cp employee_data.csv s3://${S3_BUCKET}/s3table-example/input/
```

### è®¿é—®JupyterHub UIå¹¶é…ç½®JupyterLabæœåŠ¡å™¨

1. å°†ä»£ç†æœåŠ¡ç«¯å£è½¬å‘åˆ°æ‚¨çš„æœ¬åœ°æœºå™¨ã€‚

    ```bash
    kubectl port-forward svc/proxy-public 8888:80 -n jupyterhub
    ```

1. å‰å¾€[`http://localhost:8888`](http://localhost:8888)ã€‚è¾“å…¥ä»»ä½•ç”¨æˆ·åï¼Œå°†å¯†ç å­—æ®µç•™ç©ºï¼Œç„¶åç‚¹å‡»"ç™»å½•"ã€‚

   ![ç™»å½•](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-signin.png)

1. ç‚¹å‡»å¼€å§‹ã€‚å¦‚æœæ‚¨æƒ³è‡ªå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ä¸Šæ¸¸PySparkç¬”è®°æœ¬é•œåƒã€‚

    ![é€‰æ‹©](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-select.png)

1. ä»[ç¤ºä¾‹Jupyterç¬”è®°æœ¬](https://github.com/awslabs/data-on-eks/blob/main/analytics/terraform/spark-k8s-operator/examples/s3-tables/s3table-iceberg-pyspark.ipynb)å¤åˆ¶ç¤ºä¾‹ä½œä¸ºèµ·ç‚¹ï¼Œä»¥äº¤äº’å¼æµ‹è¯•S3è¡¨åŠŸèƒ½ã€‚

    **ç¡®ä¿åœ¨ç¬”è®°æœ¬ä¸­æ›´æ–°`S3_BUCKET`å’Œ`s3table_arn`å€¼**

    ![ç¬”è®°æœ¬](../../../../../../docs/blueprints/data-analytics/img/s3tables-jupyter-notebook.png)

</CollapsibleContent>
<CollapsibleContent header={<h2><span>æ¸…ç†</span></h2>}>

:::caution
ä¸ºé¿å…å¯¹æ‚¨çš„AWSè´¦æˆ·äº§ç”Ÿä¸å¿…è¦çš„è´¹ç”¨ï¼Œè¯·åˆ é™¤åœ¨æ­¤éƒ¨ç½²æœŸé—´åˆ›å»ºçš„æ‰€æœ‰AWSèµ„æº
:::

## åˆ é™¤S3è¡¨

```bash
aws s3tables delete-table \
  --namespace doeks_namespace \
  --table-bucket-arn ${S3TABLE_ARN} \
  --name employee_s3_table
```

## åˆ é™¤å‘½åç©ºé—´

```bash
aws s3tables delete-namespace \
  --namespace doeks_namespace \
  --table-bucket-arn ${S3TABLE_ARN}
```

## åˆ é™¤å­˜å‚¨æ¡¶è¡¨

```bash
aws s3tables delete-table-bucket \
  --region "<REGION>" \
  --table-bucket-arn ${S3TABLE_ARN}
```

## åˆ é™¤Jupyterç¬”è®°æœ¬æœåŠ¡å™¨

å¦‚æœæ‚¨åˆ›å»ºäº†Jupyterç¬”è®°æœ¬æœåŠ¡å™¨

```bash
kubectl delete pods -n jupyterhub -l component=singleuser-server
```


## åˆ é™¤EKSé›†ç¾¤

æ­¤è„šæœ¬å°†ä½¿ç”¨`-target`é€‰é¡¹æ¸…ç†ç¯å¢ƒï¼Œä»¥ç¡®ä¿æ‰€æœ‰èµ„æºæŒ‰æ­£ç¡®é¡ºåºåˆ é™¤ã€‚

```bash
cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator && chmod +x cleanup.sh
./cleanup.sh
```

</CollapsibleContent>
