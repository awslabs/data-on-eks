---
title: Inferentia2ä¸Šçš„Llama-3-8Bä¸vLLM
sidebar_position: 1
description: ä½¿ç”¨Rayå’ŒvLLMåœ¨AWS Inferentia2ä¸Šéƒ¨ç½²Meta-Llama-3-8B-Instructæ¨¡å‹ä»¥è·å¾—ä¼˜åŒ–çš„æ¨ç†æ€§èƒ½ã€‚
---
import CollapsibleContent from '../../../../../../../src/components/CollapsibleContent';

:::caution

**EKSä¸Šçš„AI**å†…å®¹**æ­£åœ¨è¿ç§»**åˆ°ä¸€ä¸ªæ–°çš„ä»“åº“ã€‚
ğŸ”— ğŸ‘‰ [é˜…è¯»å®Œæ•´çš„è¿ç§»å…¬å‘Š Â»](https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement)

:::

:::warning
åœ¨EKSä¸Šéƒ¨ç½²MLæ¨¡å‹éœ€è¦è®¿é—®GPUæˆ–Neuronå®ä¾‹ã€‚å¦‚æœæ‚¨çš„éƒ¨ç½²ä¸èµ·ä½œç”¨ï¼Œé€šå¸¸æ˜¯å› ä¸ºç¼ºå°‘å¯¹è¿™äº›èµ„æºçš„è®¿é—®ã€‚æ­¤å¤–ï¼Œä¸€äº›éƒ¨ç½²æ¨¡å¼ä¾èµ–äºKarpenterè‡ªåŠ¨æ‰©å±•å’Œé™æ€èŠ‚ç‚¹ç»„ï¼›å¦‚æœèŠ‚ç‚¹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥Karpenteræˆ–èŠ‚ç‚¹ç»„çš„æ—¥å¿—ä»¥è§£å†³é—®é¢˜ã€‚
:::


:::danger

æ³¨æ„ï¼šä½¿ç”¨æ­¤Llama-3 Instructæ¨¡å‹å—Metaè®¸å¯è¯çš„çº¦æŸã€‚
ä¸ºäº†ä¸‹è½½æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ï¼Œè¯·è®¿é—®[ç½‘ç«™](https://huggingface.co/meta-llama/Meta-Llama-3-8B)å¹¶åœ¨è¯·æ±‚è®¿é—®å‰æ¥å—è®¸å¯è¯ã€‚

:::

:::info

æˆ‘ä»¬æ­£åœ¨ç§¯æå¢å¼ºæ­¤è“å›¾ï¼Œä»¥çº³å…¥å¯è§‚æµ‹æ€§ã€æ—¥å¿—è®°å½•å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æ”¹è¿›ã€‚
:::


# åœ¨AWS Neuronä¸Šä½¿ç”¨RayServeå’ŒvLLMéƒ¨ç½²LLM

æ¬¢è¿é˜…è¯»è¿™ä»½å…³äºä½¿ç”¨[Ray Serve](https://docs.ray.io/en/latest/serve/index.html)å’ŒAWS Neuronåœ¨Amazon Elastic Kubernetes Service (EKS)ä¸Šéƒ¨ç½²LLMçš„ç»¼åˆæŒ‡å—ã€‚

### ä»€ä¹ˆæ˜¯AWS Neuronï¼Ÿ

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†åˆ©ç”¨[AWS Neuron](https://aws.amazon.com/machine-learning/neuron/)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„SDKï¼Œå¯ä¼˜åŒ–AWS Inferentiaå’ŒTrainiumåŠ é€Ÿå™¨ä¸Šçš„æ·±åº¦å­¦ä¹ æ€§èƒ½ã€‚Neuronä¸PyTorchå’ŒTensorFlowç­‰æ¡†æ¶æ— ç¼é›†æˆï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„å·¥å…·åŒ…ï¼Œç”¨äºåœ¨ä¸“é—¨çš„EC2å®ä¾‹ï¼ˆå¦‚Inf1ã€Inf2ã€Trn1å’ŒTrn1nï¼‰ä¸Šå¼€å‘ã€åˆ†æå’Œéƒ¨ç½²é«˜æ€§èƒ½æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

### ä»€ä¹ˆæ˜¯vLLMï¼Ÿ

[vLLM](https://docs.vllm.ai/en/latest/)æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„LLMæ¨ç†å’ŒæœåŠ¡åº“ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ååé‡å¹¶æœ€å°åŒ–å»¶è¿Ÿã€‚å…¶æ ¸å¿ƒæ˜¯[PagedAttention](https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html)ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œæ˜¾è‘—æé«˜äº†å†…å­˜æ•ˆç‡ï¼Œå…è®¸GPUèµ„æºçš„æœ€ä½³åˆ©ç”¨ã€‚è¿™ä¸ªå¼€æºè§£å†³æ–¹æ¡ˆé€šè¿‡å…¶Python APIå’ŒOpenAIå…¼å®¹æœåŠ¡å™¨æä¾›æ— ç¼é›†æˆï¼Œä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„æ•ˆç‡åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å’Œæ‰©å±•Llama 3ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

### ä»€ä¹ˆæ˜¯RayServeï¼Ÿ

Ray Serveæ˜¯ä¸€ä¸ªå»ºç«‹åœ¨Rayä¹‹ä¸Šçš„å¯æ‰©å±•æ¨¡å‹æœåŠ¡åº“ï¼Œæ—¨åœ¨éƒ¨ç½²å…·æœ‰æ¡†æ¶æ— å…³éƒ¨ç½²ã€æ¨¡å‹ç»„åˆå’Œå†…ç½®æ‰©å±•ç­‰åŠŸèƒ½çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’ŒAIåº”ç”¨ç¨‹åºã€‚æ‚¨è¿˜å°†é‡åˆ°RayServiceï¼Œè¿™æ˜¯KubeRayé¡¹ç›®çš„ä¸€éƒ¨åˆ†çš„Kubernetesè‡ªå®šä¹‰èµ„æºï¼Œç”¨äºåœ¨Kubernetesé›†ç¾¤ä¸Šéƒ¨ç½²å’Œç®¡ç†Ray Serveåº”ç”¨ç¨‹åºã€‚

### ä»€ä¹ˆæ˜¯Llama-3-8B Instructï¼Ÿ

Metaå¼€å‘å¹¶å‘å¸ƒäº†Meta Llama 3ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´çš„ç”Ÿæˆæ–‡æœ¬æ¨¡å‹ï¼Œè§„æ¨¡ä¸º8Bå’Œ70Bã€‚Llama 3æŒ‡ä»¤è°ƒæ•´æ¨¡å‹é’ˆå¯¹å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¸¸è§è¡Œä¸šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºè®¸å¤šå¯ç”¨çš„å¼€æºèŠå¤©æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨å¼€å‘è¿™äº›æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éå¸¸æ³¨é‡ä¼˜åŒ–æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ã€‚

æœ‰å…³Llama3è§„æ¨¡å’Œæ¨¡å‹æ¶æ„çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)æ‰¾åˆ°ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹©AWSåŠ é€Ÿå™¨ï¼Ÿ

**å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§**

éƒ¨ç½²åƒLlama-3è¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹(`LLM`)çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯åˆé€‚ç¡¬ä»¶çš„å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§ã€‚ç”±äºéœ€æ±‚é«˜ï¼Œä¼ ç»Ÿçš„`GPU`å®ä¾‹ç»å¸¸é¢ä¸´ç¨€ç¼ºæ€§ï¼Œè¿™ä½¿å¾—æœ‰æ•ˆé…ç½®å’Œæ‰©å±•èµ„æºå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œ`Trn1/Inf2`å®ä¾‹ï¼Œå¦‚`trn1.32xlarge`ã€`trn1n.32xlarge`ã€`inf2.24xlarge`å’Œ`inf2.48xlarge`ï¼Œæ˜¯ä¸“ä¸ºç”Ÿæˆå¼AIæ¨¡å‹ï¼ˆåŒ…æ‹¬LLMï¼‰çš„é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ (DL)è®­ç»ƒå’Œæ¨ç†è€Œæ„å»ºçš„ã€‚å®ƒä»¬æä¾›äº†å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§ï¼Œç¡®ä¿æ‚¨å¯ä»¥æ ¹æ®éœ€è¦éƒ¨ç½²å’Œæ‰©å±•`Llama-3`æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°èµ„æºç“¶é¢ˆæˆ–å»¶è¿Ÿã€‚
**æˆæœ¬ä¼˜åŒ–**

åœ¨ä¼ ç»ŸGPUå®ä¾‹ä¸Šè¿è¡ŒLLMå¯èƒ½æˆæœ¬é«˜æ˜‚ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°GPUçš„ç¨€ç¼ºæ€§å’Œå…¶ç«äº‰æ€§å®šä»·ã€‚**Trn1/Inf2**å®ä¾‹æä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡æä¾›é’ˆå¯¹AIå’Œæœºå™¨å­¦ä¹ ä»»åŠ¡ä¼˜åŒ–çš„ä¸“ç”¨ç¡¬ä»¶ï¼ŒTrn1/Inf2å®ä¾‹ä½¿æ‚¨èƒ½å¤Ÿä»¥è¾ƒä½çš„æˆæœ¬å®ç°é¡¶çº§æ€§èƒ½ã€‚è¿™ç§æˆæœ¬ä¼˜åŒ–ä½¿æ‚¨èƒ½å¤Ÿé«˜æ•ˆåˆ†é…é¢„ç®—ï¼Œä½¿LLMéƒ¨ç½²å˜å¾—å¯è®¿é—®å’Œå¯æŒç»­ã€‚

**æ€§èƒ½æå‡**

è™½ç„¶Llama-3å¯ä»¥åœ¨GPUä¸Šå®ç°é«˜æ€§èƒ½æ¨ç†ï¼Œä½†NeuronåŠ é€Ÿå™¨å°†æ€§èƒ½æå‡åˆ°äº†æ–°çš„æ°´å¹³ã€‚NeuronåŠ é€Ÿå™¨æ˜¯ä¸“ä¸ºæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„ï¼Œæä¾›ç¡¬ä»¶åŠ é€Ÿï¼Œæ˜¾è‘—å¢å¼ºäº†Llama-3çš„æ¨ç†é€Ÿåº¦ã€‚è¿™è½¬åŒ–ä¸ºåœ¨Trn1/Inf2å®ä¾‹ä¸Šéƒ¨ç½²Llama-3æ—¶æ›´å¿«çš„å“åº”æ—¶é—´å’Œæ”¹è¿›çš„ç”¨æˆ·ä½“éªŒã€‚

## è§£å†³æ–¹æ¡ˆæ¶æ„

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†Llama-3æ¨¡å‹ã€[Ray Serve](https://docs.ray.io/en/latest/serve/index.html)å’ŒAmazon EKSä¸Šçš„[Inferentia2](https://aws.amazon.com/ec2/instance-types/inf2/)ã€‚

![Llama-3-inf2](../../../../../../../docs/gen-ai/inference/img/ray-vllm-inf2.png)

## éƒ¨ç½²è§£å†³æ–¹æ¡ˆ

è¦å¼€å§‹åœ¨[Amazon EKS](https://aws.amazon.com/eks/)ä¸Šéƒ¨ç½²`Llama-3-8B-instruct`ï¼Œæˆ‘ä»¬å°†æ¶µç›–å¿…è¦çš„å…ˆå†³æ¡ä»¶ï¼Œå¹¶ä¸€æ­¥æ­¥å¼•å¯¼æ‚¨å®Œæˆéƒ¨ç½²è¿‡ç¨‹ã€‚

è¿™åŒ…æ‹¬ä½¿ç”¨AWS Inferentiaå®ä¾‹è®¾ç½®åŸºç¡€è®¾æ–½å’Œéƒ¨ç½²**Rayé›†ç¾¤**ã€‚

<CollapsibleContent header={<h2><span>å…ˆå†³æ¡ä»¶</span></h2>}>
åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»å‡†å¤‡å¥½æ‰€æœ‰å…ˆå†³æ¡ä»¶ï¼Œä»¥ä½¿éƒ¨ç½²è¿‡ç¨‹é¡ºåˆ©æ— å¿§ã€‚
ç¡®ä¿æ‚¨å·²åœ¨è®¡ç®—æœºä¸Šå®‰è£…äº†ä»¥ä¸‹å·¥å…·ã€‚

1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### éƒ¨ç½²

å…‹éš†ä»“åº“ï¼š

```bash
git clone https://github.com/awslabs/data-on-eks.git
```

å¯¼èˆªåˆ°ä»¥ä¸‹ç›®å½•å¹¶è¿è¡Œ`install.sh`è„šæœ¬ï¼š

**é‡è¦æç¤ºï¼š** ç¡®ä¿åœ¨éƒ¨ç½²è“å›¾ä¹‹å‰æ›´æ–°`variables.tf`æ–‡ä»¶ä¸­çš„åŒºåŸŸã€‚
æ­¤å¤–ï¼Œç¡®è®¤æ‚¨çš„æœ¬åœ°åŒºåŸŸè®¾ç½®ä¸æŒ‡å®šåŒºåŸŸåŒ¹é…ï¼Œä»¥é˜²æ­¢ä»»ä½•å·®å¼‚ã€‚
ä¾‹å¦‚ï¼Œå°†æ‚¨çš„`export AWS_DEFAULT_REGION="<REGION>"`è®¾ç½®ä¸ºæ‰€éœ€åŒºåŸŸã€‚

```bash
cd data-on-eks/ai-ml/trainium-inferentia/
./install.sh
```

### éªŒè¯èµ„æº

éªŒè¯Amazon EKSé›†ç¾¤

```bash
aws eks --region us-west-2 describe-cluster --name trainium-inferentia
```

```bash
# åˆ›å»ºk8sé…ç½®æ–‡ä»¶ä»¥ä¸EKSè¿›è¡Œèº«ä»½éªŒè¯
aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia

kubectl get nodes # è¾“å‡ºæ˜¾ç¤ºEKSæ‰˜ç®¡èŠ‚ç‚¹ç»„èŠ‚ç‚¹
```
éªŒè¯Karpenterè‡ªåŠ¨ç¼©æ”¾å™¨èŠ‚ç‚¹æ± 

```bash
kubectl get nodepools
```

```text
NAME              NODECLASS
default           default
inferentia-inf2   inferentia-inf2
trainium-trn1     trainium-trn1
```

### éªŒè¯Neuronæ’ä»¶

Neuronè®¾å¤‡æ’ä»¶å°†Neuronæ ¸å¿ƒå’Œè®¾å¤‡ä½œä¸ºèµ„æºå…¬å¼€ç»™kubernetesã€‚éªŒè¯è“å›¾å®‰è£…çš„æ’ä»¶çŠ¶æ€ã€‚

```bash
kubectl get ds neuron-device-plugin --namespace kube-system
```
```bash
NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
neuron-device-plugin   1         1         1       1            1           <none>          15d
```

### éªŒè¯Neuronè°ƒåº¦å™¨

Neuronè°ƒåº¦å™¨æ‰©å±•å¯¹äºè°ƒåº¦éœ€è¦å¤šä¸ªNeuronæ ¸å¿ƒæˆ–è®¾å¤‡èµ„æºçš„podæ˜¯å¿…éœ€çš„ã€‚éªŒè¯è“å›¾å®‰è£…çš„è°ƒåº¦å™¨çŠ¶æ€ã€‚

```bash
kubectl get pods -n kube-system | grep my-scheduler
```
```text
my-scheduler-c6fc957d9-hzrf7  1/1     Running   0  2d1h
```
</CollapsibleContent>
## ä½¿ç”¨Llama3æ¨¡å‹éƒ¨ç½²Rayé›†ç¾¤

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨KubeRayæ“ä½œç¬¦ï¼Œå®ƒé€šè¿‡è‡ªå®šä¹‰èµ„æºå®šä¹‰æ‰©å±•Kubernetesï¼Œç”¨äºRayç‰¹å®šçš„æ„é€ ï¼Œå¦‚RayClusterã€RayJobå’ŒRayServiceã€‚æ“ä½œç¬¦ç›‘è§†ä¸è¿™äº›èµ„æºç›¸å…³çš„ç”¨æˆ·äº‹ä»¶ï¼Œè‡ªåŠ¨åˆ›å»ºå¿…è¦çš„Kuberneteså·¥ä»¶ä»¥å½¢æˆRayé›†ç¾¤ï¼Œå¹¶æŒç»­ç›‘æ§é›†ç¾¤çŠ¶æ€ä»¥ç¡®ä¿æ‰€éœ€é…ç½®ä¸å®é™…çŠ¶æ€åŒ¹é…ã€‚å®ƒå¤„ç†ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬è®¾ç½®ã€å·¥ä½œèŠ‚ç‚¹ç»„çš„åŠ¨æ€æ‰©å±•å’Œæ‹†å¸ï¼ŒæŠ½è±¡å‡ºåœ¨Kubernetesä¸Šç®¡ç†Rayåº”ç”¨ç¨‹åºçš„å¤æ‚æ€§ã€‚

æ¯ä¸ªRayé›†ç¾¤ç”±ä¸€ä¸ªå¤´èŠ‚ç‚¹podå’Œä¸€ç³»åˆ—å·¥ä½œèŠ‚ç‚¹podç»„æˆï¼Œå…·æœ‰å¯é€‰çš„è‡ªåŠ¨æ‰©å±•æ”¯æŒï¼Œæ ¹æ®å·¥ä½œè´Ÿè½½éœ€æ±‚è°ƒæ•´é›†ç¾¤å¤§å°ã€‚KubeRayæ”¯æŒå¼‚æ„è®¡ç®—èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬GPUï¼‰å’Œåœ¨åŒä¸€Kubernetesé›†ç¾¤ä¸­è¿è¡Œå…·æœ‰ä¸åŒRayç‰ˆæœ¬çš„å¤šä¸ªRayé›†ç¾¤ã€‚æ­¤å¤–ï¼ŒKubeRayå¯ä»¥ä¸AWS InferentiaåŠ é€Ÿå™¨é›†æˆï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama 3ï¼‰èƒ½å¤Ÿåœ¨ä¸“é—¨çš„ç¡¬ä»¶ä¸Šé«˜æ•ˆéƒ¨ç½²ï¼Œå¯èƒ½æé«˜æœºå™¨å­¦ä¹ æ¨ç†ä»»åŠ¡çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚

åœ¨éƒ¨ç½²äº†å…·æœ‰æ‰€æœ‰å¿…è¦ç»„ä»¶çš„EKSé›†ç¾¤åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç»§ç»­ä½¿ç”¨`RayServe`å’Œ`vLLM`åœ¨AWSåŠ é€Ÿå™¨ä¸Šéƒ¨ç½²`NousResearch/Meta-Llama-3-8B-Instruct`çš„æ­¥éª¤ã€‚

**æ­¥éª¤1ï¼š** è¦éƒ¨ç½²RayServiceé›†ç¾¤ï¼Œå¯¼èˆªåˆ°åŒ…å«`vllm-rayserve-deployment.yaml`æ–‡ä»¶çš„ç›®å½•ï¼Œå¹¶åœ¨ç»ˆç«¯ä¸­æ‰§è¡Œ`kubectl apply`å‘½ä»¤ã€‚
è¿™å°†åº”ç”¨RayServiceé…ç½®å¹¶åœ¨æ‚¨çš„EKSè®¾ç½®ä¸Šéƒ¨ç½²é›†ç¾¤ã€‚

```bash
cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2

kubectl apply -f vllm-rayserve-deployment.yaml
```
**å¯é€‰é…ç½®**

é»˜è®¤æƒ…å†µä¸‹ï¼Œå°†é…ç½®ä¸€ä¸ª`inf2.8xlarge`å®ä¾‹ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨`inf2.48xlarge`ï¼Œè¯·ä¿®æ”¹æ–‡ä»¶`vllm-rayserve-deployment.yaml`ï¼Œæ›´æ”¹`worker`å®¹å™¨ä¸‹çš„`resources`éƒ¨åˆ†ã€‚

```bash
limits:
    cpu: "30"
    memory: "110G"
    aws.amazon.com/neuron: "1"
requests:
    cpu: "30"
    memory: "110G"
    aws.amazon.com/neuron: "1"
```
æ›´æ”¹ä¸ºä»¥ä¸‹å†…å®¹ï¼š

```bash
limits:
    cpu: "90"
    memory: "360G"
    aws.amazon.com/neuron: "12"
requests:
    cpu: "90"
    memory: "360G"
    aws.amazon.com/neuron: "12"
```

**æ­¥éª¤2ï¼š** é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯éƒ¨ç½²

è¦ç¡®ä¿éƒ¨ç½²å·²æˆåŠŸå®Œæˆï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

:::info

éƒ¨ç½²è¿‡ç¨‹å¯èƒ½éœ€è¦é•¿è¾¾**10åˆ†é’Ÿ**ã€‚å¤´éƒ¨Podé¢„è®¡åœ¨5åˆ°6åˆ†é’Ÿå†…å‡†å¤‡å°±ç»ªï¼Œè€ŒRay Serveå·¥ä½œèŠ‚ç‚¹podå¯èƒ½éœ€è¦é•¿è¾¾10åˆ†é’Ÿç”¨äºé•œåƒæ£€ç´¢å’Œä»Huggingfaceéƒ¨ç½²æ¨¡å‹ã€‚

:::

æ ¹æ®RayServeé…ç½®ï¼Œæ‚¨å°†æœ‰ä¸€ä¸ªåœ¨`x86`å®ä¾‹ä¸Šè¿è¡Œçš„Rayå¤´éƒ¨podå’Œä¸€ä¸ªåœ¨`inf2`å®ä¾‹ä¸Šè¿è¡Œçš„å·¥ä½œèŠ‚ç‚¹podã€‚æ‚¨å¯ä»¥ä¿®æ”¹RayServe YAMLæ–‡ä»¶ä»¥è¿è¡Œå¤šä¸ªå‰¯æœ¬ï¼›ä½†æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œæ¯ä¸ªé¢å¤–çš„å‰¯æœ¬å¯èƒ½ä¼šåˆ›å»ºæ–°çš„å®ä¾‹ã€‚

```bash
kubectl get pods -n vllm
```

```text
NAME                                                      READY   STATUS    RESTARTS   AGE
lm-llama3-inf2-raycluster-ksh7w-worker-inf2-group-dcs5n   1/1     Running   0          2d4h
vllm-llama3-inf2-raycluster-ksh7w-head-4ck8f              2/2     Running   0          2d4h
```

æ­¤éƒ¨ç½²è¿˜é…ç½®äº†ä¸€ä¸ªå…·æœ‰å¤šä¸ªç«¯å£çš„æœåŠ¡ã€‚ç«¯å£**8265**æŒ‡å®šç”¨äºRayä»ªè¡¨æ¿ï¼Œç«¯å£**8000**ç”¨äºvLLMæ¨ç†æœåŠ¡å™¨ç«¯ç‚¹ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯æœåŠ¡ï¼š

```bash
kubectl get svc -n vllm

NAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE
vllm                         ClusterIP   172.20.23.54    <none>        8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP   2d4h
vllm-llama3-inf2-head-svc    ClusterIP   172.20.18.130   <none>        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   2d4h
vllm-llama3-inf2-serve-svc   ClusterIP   172.20.153.10   <none>        8000/TCP                                        2d4h
```
è¦è®¿é—®Rayä»ªè¡¨æ¿ï¼Œæ‚¨å¯ä»¥å°†ç›¸å…³ç«¯å£è½¬å‘åˆ°æœ¬åœ°è®¡ç®—æœºï¼š

```bash
kubectl -n vllm port-forward svc/vllm 8265:8265
```

ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨[http://localhost:8265](http://localhost:8265)è®¿é—®Web UIï¼Œå®ƒæ˜¾ç¤ºäº†Rayç”Ÿæ€ç³»ç»Ÿä¸­ä½œä¸šå’Œè§’è‰²çš„éƒ¨ç½²æƒ…å†µã€‚

![RayServeéƒ¨ç½²](../../../../../../../docs/gen-ai/inference/img/ray-dashboard-vllm-llama3-inf2.png)

ä¸€æ—¦éƒ¨ç½²å®Œæˆï¼Œæ§åˆ¶å™¨å’Œä»£ç†çŠ¶æ€åº”ä¸º`HEALTHY`ï¼Œåº”ç”¨ç¨‹åºçŠ¶æ€åº”ä¸º`RUNNING`

![RayServeéƒ¨ç½²æ—¥å¿—](../../../../../../../docs/gen-ai/inference/img/ray-logs-vllm-llama3-inf2.png)

### æµ‹è¯•Llama3æ¨¡å‹

ç°åœ¨æ˜¯æ—¶å€™æµ‹è¯•`Meta-Llama-3-8B-Instruct`èŠå¤©æ¨¡å‹äº†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Pythonå®¢æˆ·ç«¯è„šæœ¬å‘RayServeæ¨ç†ç«¯ç‚¹å‘é€æç¤ºï¼Œå¹¶éªŒè¯æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºã€‚

é¦–å…ˆï¼Œä½¿ç”¨kubectlæ‰§è¡Œç«¯å£è½¬å‘åˆ°`vllm-llama3-inf2-serve-svc`æœåŠ¡ï¼š

```bash
kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000
```

`openai-client.py`ä½¿ç”¨HTTP POSTæ–¹æ³•å‘æ¨ç†ç«¯ç‚¹å‘é€æç¤ºåˆ—è¡¨ï¼Œç”¨äºæ–‡æœ¬å®Œæˆå’Œé—®ç­”ï¼Œç›®æ ‡æ˜¯vllmæœåŠ¡å™¨ã€‚

è¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡ŒPythonå®¢æˆ·ç«¯åº”ç”¨ç¨‹åºï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

```bash
cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2
python3 -m venv .venv
source .venv/bin/activate
pip3 install openai
python3 openai-client.py
```

æ‚¨å°†åœ¨ç»ˆç«¯ä¸­çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

<details>
<summary>ç‚¹å‡»å±•å¼€Pythonå®¢æˆ·ç«¯ç»ˆç«¯è¾“å‡º</summary>

```text
Example 1 - Simple chat completion:
Handling connection for 8000
The capital of India is New Delhi.


Example 2 - Chat completion with different parameters:
The twin suns of Tatooine set slowly in the horizon, casting a warm orange glow over the bustling spaceport of Anchorhead. Amidst the hustle and bustle, a young farm boy named Anakin Skywalker sat atop a dusty speeder, his eyes fixed on the horizon as he dreamed of adventure beyond the desert planet.

As the suns dipped below the dunes, Anakin's uncle, Owen Lars, called out to him from the doorway of their humble moisture farm. "Anakin, it's time to head back! Your aunt and I have prepared a special dinner in your honor."

But Anakin was torn. He had received a strange message from an unknown sender, hinting at a great destiny waiting for him. Against his uncle's warnings, Anakin decided to investigate further, sneaking away into the night to follow the mysterious clues.

As he rode his speeder through the desert, the darkness seemed to grow thicker, and the silence was broken only by the distant


Example 3 - Streaming chat completion:
I'd be happy to help you with that. Here we go:

1...

(Pause)

2...

(Pause)

3...

(Pause)

4...

(Pause)

5...

(Pause)

6...

(Pause)

7...

(Pause)

8...

(Pause)

9...

(Pause)

10!

Let me know if you have any other requests!
```
</details>

## å¯è§‚æµ‹æ€§

### ä½¿ç”¨AWS CloudWatchå’ŒNeuron Monitorè¿›è¡Œå¯è§‚æµ‹æ€§

æ­¤è“å›¾éƒ¨ç½²äº†CloudWatchå¯è§‚æµ‹æ€§ä»£ç†ä½œä¸ºæ‰˜ç®¡é™„åŠ ç»„ä»¶ï¼Œä¸ºå®¹å™¨åŒ–å·¥ä½œè´Ÿè½½æä¾›å…¨é¢ç›‘æ§ã€‚å®ƒåŒ…æ‹¬å®¹å™¨æ´å¯Ÿï¼Œç”¨äºè·Ÿè¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚CPUå’Œå†…å­˜åˆ©ç”¨ç‡ã€‚æ­¤å¤–ï¼Œè¯¥é™„åŠ ç»„ä»¶åˆ©ç”¨[Neuron Monitoræ’ä»¶](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide)æ¥æ•è·å’ŒæŠ¥å‘ŠNeuronç‰¹å®šæŒ‡æ ‡ã€‚

æ‰€æœ‰æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å®¹å™¨æ´å¯Ÿå’ŒNeuronæŒ‡æ ‡ï¼Œå¦‚Neuronæ ¸å¿ƒåˆ©ç”¨ç‡ã€NeuronCoreå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œéƒ½å‘é€åˆ°Amazon CloudWatchï¼Œæ‚¨å¯ä»¥åœ¨é‚£é‡Œå®æ—¶ç›‘æ§å’Œåˆ†æå®ƒä»¬ã€‚éƒ¨ç½²å®Œæˆåï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿç›´æ¥ä»CloudWatchæ§åˆ¶å°è®¿é—®è¿™äº›æŒ‡æ ‡ï¼Œä½¿æ‚¨èƒ½å¤Ÿæœ‰æ•ˆåœ°ç®¡ç†å’Œä¼˜åŒ–å·¥ä½œè´Ÿè½½ã€‚

![CloudWatch-neuron-monitor](../../../../../../../docs/gen-ai/inference/img/neuron-monitor-cwci.png)
## Open WebUIéƒ¨ç½²


:::info

[Open WebUI](https://github.com/open-webui/open-webui)ä»…ä¸ä½¿ç”¨OpenAI APIæœåŠ¡å™¨å’ŒOllamaå·¥ä½œçš„æ¨¡å‹å…¼å®¹ã€‚

:::

**1. éƒ¨ç½²WebUI**

é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²[Open WebUI](https://github.com/open-webui/open-webui)ï¼š

```sh
kubectl apply -f openai-webui-deployment.yaml
```

**2. ç«¯å£è½¬å‘ä»¥è®¿é—®WebUI**

**æ³¨æ„** å¦‚æœæ‚¨å·²ç»åœ¨è¿è¡Œç«¯å£è½¬å‘ä»¥ä½¿ç”¨pythonå®¢æˆ·ç«¯æµ‹è¯•æ¨ç†ï¼Œè¯·æŒ‰`ctrl+c`ä¸­æ–­è¯¥æ“ä½œã€‚

ä½¿ç”¨kubectlç«¯å£è½¬å‘åœ¨æœ¬åœ°è®¿é—®WebUIï¼š

```sh
kubectl port-forward svc/open-webui 8081:80 -n openai-webui
```

**3. è®¿é—®WebUI**

æ‰“å¼€æ‚¨çš„æµè§ˆå™¨å¹¶è®¿é—®http://localhost:8081

**4. æ³¨å†Œ**

ä½¿ç”¨æ‚¨çš„å§“åã€ç”µå­é‚®ä»¶å’Œè™šæ‹Ÿå¯†ç æ³¨å†Œã€‚

**5. å¼€å§‹æ–°çš„èŠå¤©**

ç‚¹å‡»æ–°å»ºèŠå¤©å¹¶ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©æ¨¡å‹ï¼Œå¦‚ä¸‹é¢çš„æˆªå›¾æ‰€ç¤ºï¼š

![alt text](../../../../../../../docs/gen-ai/inference/img/openweb-ui-ray-vllm-inf2-1.png)

**6. è¾“å…¥æµ‹è¯•æç¤º**

è¾“å…¥æ‚¨çš„æç¤ºï¼Œæ‚¨å°†çœ‹åˆ°æµå¼ç»“æœï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![alt text](../../../../../../../docs/gen-ai/inference/img/openweb-ui-ray-vllm-inf2-2.png)

## ä½¿ç”¨LLMPerfå·¥å…·è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•

[LLMPerf](https://github.com/ray-project/llmperf/blob/main/README.md)æ˜¯ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œæ—¨åœ¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

LLMPerfå·¥å…·é€šè¿‡ç«¯å£8000ä½¿ç”¨ä¸Šé¢è®¾ç½®çš„ç«¯å£è½¬å‘è¿æ¥åˆ°vllmæœåŠ¡ï¼Œä½¿ç”¨å‘½ä»¤`kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000`ã€‚

åœ¨æ‚¨çš„ç»ˆç«¯ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

å…‹éš†LLMPerfä»“åº“ï¼š

```bash
git clone https://github.com/ray-project/llmperf.git
cd llmperf
pip install -e .
pip install pandas
pip install ray
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»º`vllm_benchmark.sh`æ–‡ä»¶ï¼š

```bash
cat << 'EOF' > vllm_benchmark.sh
#!/bin/bash
model=${1:-NousResearch/Meta-Llama-3-8B-Instruct}
vu=${2:-1}
export OPENAI_API_KEY=EMPTY
export OPENAI_API_BASE="http://localhost:8000/v1"
export TOKENIZERS_PARALLELISM=true
#if you have more vllm servers, append the below line to the above
#;http://localhost:8001/v1;http://localhost:8002/v1"
max_requests=$(expr ${vu} \* 8 )
date_str=$(date '+%Y-%m-%d-%H-%M-%S')
python ./token_benchmark_ray.py \
       --model ${model} \
       --mean-input-tokens 512 \
       --stddev-input-tokens 20 \
       --mean-output-tokens 245 \
       --stddev-output-tokens 20 \
       --max-num-completed-requests ${max_requests} \
       --timeout 7200 \
       --num-concurrent-requests ${vu} \
       --results-dir "vllm_bench_results/${date_str}" \
       --llm-api openai \
       --additional-sampling-params '{}'
EOF
```

`--mean-input-tokens`ï¼šæŒ‡å®šè¾“å…¥æç¤ºä¸­çš„å¹³å‡ä»¤ç‰Œæ•°

`--stddev-input-tokens`ï¼šæŒ‡å®šè¾“å…¥ä»¤ç‰Œé•¿åº¦çš„å˜å¼‚æ€§ï¼Œä»¥åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•ç¯å¢ƒ

`--mean-output-tokens`ï¼šæŒ‡å®šæ¨¡å‹è¾“å‡ºä¸­é¢„æœŸçš„å¹³å‡ä»¤ç‰Œæ•°ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„å“åº”é•¿åº¦

`--stddev-output-tokens`ï¼šæŒ‡å®šè¾“å‡ºä»¤ç‰Œé•¿åº¦çš„å˜å¼‚æ€§ï¼Œå¼•å…¥å“åº”å¤§å°çš„å¤šæ ·æ€§

`--max-num-completed-requests`ï¼šè®¾ç½®è¦å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°

`--num-concurrent-requests`ï¼šæŒ‡å®šåŒæ—¶è¯·æ±‚çš„æ•°é‡ï¼Œä»¥æ¨¡æ‹Ÿå¹¶è¡Œå·¥ä½œè´Ÿè½½
ä¸‹é¢çš„å‘½ä»¤æ‰§è¡ŒåŸºå‡†æµ‹è¯•è„šæœ¬ï¼ŒæŒ‡å®šæ¨¡å‹ä¸º`NousResearch/Meta-Llama-3-8B-Instruct`ï¼Œå¹¶å°†è™šæ‹Ÿç”¨æˆ·æ•°è®¾ç½®ä¸º2ã€‚è¿™å¯¼è‡´åŸºå‡†æµ‹è¯•ä½¿ç”¨2ä¸ªå¹¶å‘è¯·æ±‚æµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ï¼Œè®¡ç®—è¦å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ä¸º16ã€‚

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2
```

æ‚¨åº”è¯¥çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```bash
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-03 09:54:45,976	INFO worker.py:1783 -- Started a local Ray instance.
  0%|                                                                                                                                                                                                                                                    | 0/16 [00:00<?, ?it/s]Handling connection for 8000
Handling connection for 8000
 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                                              | 2/16 [00:17<02:00,  8.58s/it]Handling connection for 8000
Handling connection for 8000
 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                                                 | 4/16 [00:33<01:38,  8.20s/it]Handling connection for 8000
Handling connection for 8000
 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                   | 6/16 [00:47<01:17,  7.75s/it]Handling connection for 8000
Handling connection for 8000
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                      | 8/16 [01:00<00:58,  7.36s/it]Handling connection for 8000
Handling connection for 8000
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                        | 10/16 [01:15<00:43,  7.31s/it]Handling connection for 8000
Handling connection for 8000
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 12/16 [01:29<00:28,  7.20s/it]Handling connection for 8000
Handling connection for 8000
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 14/16 [01:45<00:15,  7.52s/it]Handling connection for 8000
Handling connection for 8000
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [02:01<00:00,  7.58s/it]
\Results for token benchmark for NousResearch/Meta-Llama-3-8B-Instruct queried with the openai api.

inter_token_latency_s
    p25 = 0.051964785839225695
    p50 = 0.053331799814278796
    p75 = 0.05520852723583741
    p90 = 0.05562424625711179
    p95 = 0.05629651696856784
    p99 = 0.057518213120178636
    mean = 0.053548951905597324
    min = 0.0499955879607504
    max = 0.05782363715808134
    stddev = 0.002070751885022901
ttft_s
    p25 = 1.5284210312238429
    p50 = 1.7579061459982768
    p75 = 1.8209733433031943
    p90 = 1.842437624989543
    p95 = 1.852818323241081
    p99 = 1.8528624982456676
    mean = 1.5821313202395686
    min = 0.928935999982059
    max = 1.8528735419968143
    stddev = 0.37523908630204694
end_to_end_latency_s
    p25 = 13.74749460403109
    p50 = 14.441407957987394
    p75 = 15.53337344751344
    p90 = 16.104882833489683
    p95 = 16.366086292022374
    p99 = 16.395070491998922
    mean = 14.528114874927269
    min = 10.75658329098951
    max = 16.40231654199306
    stddev = 1.4182672949824733
request_output_throughput_token_per_s
    p25 = 18.111220396798153
    p50 = 18.703139371912407
    p75 = 19.243016652511997
    p90 = 19.37836414194298
    p95 = 19.571455249271224
    p99 = 19.915057038539217
    mean = 18.682678715983627
    min = 17.198769813363445
    max = 20.000957485856215
    stddev = 0.725563381521316
number_input_tokens
    p25 = 502.5
    p50 = 509.5
    p75 = 516.5
    p90 = 546.5
    p95 = 569.25
    p99 = 574.65
    mean = 515.25
    min = 485
    max = 576
    stddev = 24.054105678657024
number_output_tokens
    p25 = 259.75
    p50 = 279.5
    p75 = 291.75
    p90 = 297.0
    p95 = 300.5
    p99 = 301.7
    mean = 271.625
    min = 185
    max = 302
    stddev = 29.257192847799555
Number Of Errored Requests: 0
Overall Output Throughput: 35.827933968528434
Number Of Completed Requests: 16
Completed Requests Per Minute: 7.914131755588426
```

æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨å¤šä¸ªå¹¶å‘è¯·æ±‚ç”ŸæˆåŸºå‡†æµ‹è¯•ç»“æœï¼Œä»¥äº†è§£æ€§èƒ½å¦‚ä½•éšç€å¹¶å‘è¯·æ±‚æ•°é‡çš„å¢åŠ è€Œå˜åŒ–ï¼š

```
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 4
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 8
./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 16
.
.
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•æŒ‡æ ‡

æ‚¨å¯ä»¥åœ¨`llmperf`ç›®å½•ä¸‹çš„`vllm_bench_results`ç›®å½•ä¸­æ‰¾åˆ°åŸºå‡†æµ‹è¯•è„šæœ¬çš„ç»“æœã€‚ç»“æœå­˜å‚¨åœ¨éµå¾ªæ—¥æœŸæ—¶é—´å‘½åçº¦å®šçš„æ–‡ä»¶å¤¹ä¸­ã€‚æ¯æ¬¡æ‰§è¡ŒåŸºå‡†æµ‹è¯•è„šæœ¬æ—¶éƒ½ä¼šåˆ›å»ºæ–°æ–‡ä»¶å¤¹ã€‚

æ‚¨ä¼šå‘ç°ï¼ŒåŸºå‡†æµ‹è¯•è„šæœ¬çš„æ¯æ¬¡æ‰§è¡Œçš„ç»“æœåŒ…å«ä»¥ä¸‹æ ¼å¼çš„2ä¸ªæ–‡ä»¶ï¼š

`NousResearch-Meta-Llama-3-8B-Instruct_512_245_summary_32.json` - åŒ…å«æ‰€æœ‰è¯·æ±‚/å“åº”å¯¹çš„æ€§èƒ½æŒ‡æ ‡æ‘˜è¦ã€‚

`NousResearch-Meta-Llama-3-8B-Instruct_512_245_individual_responses.json` - åŒ…å«æ¯ä¸ªè¯·æ±‚/å“åº”å¯¹çš„æ€§èƒ½æŒ‡æ ‡ã€‚

è¿™äº›æ–‡ä»¶ä¸­çš„æ¯ä¸€ä¸ªéƒ½åŒ…å«ä»¥ä¸‹æ€§èƒ½åŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼š

```results_inter_token_latency_s_*```ï¼šä¹Ÿç§°ä¸ºä»¤ç‰Œç”Ÿæˆå»¶è¿Ÿ(TPOT)ã€‚ä»¤ç‰Œé—´å»¶è¿ŸæŒ‡çš„æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨è§£ç æˆ–ç”Ÿæˆé˜¶æ®µç”Ÿæˆè¿ç»­è¾“å‡ºä»¤ç‰Œä¹‹é—´çš„å¹³å‡ç»è¿‡æ—¶é—´

```results_ttft_s_*```ï¼šç”Ÿæˆç¬¬ä¸€ä¸ªä»¤ç‰Œæ‰€éœ€çš„æ—¶é—´(TTFT)

```results_end_to_end_s_*```ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿ - ä»ç”¨æˆ·æäº¤è¾“å…¥æç¤ºåˆ°LLMç”Ÿæˆå®Œæ•´è¾“å‡ºå“åº”çš„æ€»æ—¶é—´

```results_request_output_throughput_token_per_s_*```ï¼šå¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨æ‰€æœ‰ç”¨æˆ·è¯·æ±‚æˆ–æŸ¥è¯¢ä¸­æ¯ç§’ç”Ÿæˆçš„è¾“å‡ºä»¤ç‰Œæ•°

```results_number_input_tokens_*```ï¼šè¯·æ±‚ä¸­çš„è¾“å…¥ä»¤ç‰Œæ•°ï¼ˆè¾“å…¥é•¿åº¦ï¼‰

```results_number_output_tokens_*```ï¼šè¯·æ±‚ä¸­çš„è¾“å‡ºä»¤ç‰Œæ•°ï¼ˆè¾“å‡ºé•¿åº¦ï¼‰
## ç»“è®º

æ€»ä¹‹ï¼Œåœ¨éƒ¨ç½²å’Œæ‰©å±•Llama-3æ–¹é¢ï¼ŒAWS Trn1/Inf2å®ä¾‹æä¾›äº†ä»¤äººä¿¡æœçš„ä¼˜åŠ¿ã€‚
å®ƒä»¬æä¾›äº†è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹æ‰€éœ€çš„å¯æ‰©å±•æ€§ã€æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½æå‡ï¼ŒåŒæ—¶å…‹æœäº†ä¸GPUç¨€ç¼ºæ€§ç›¸å…³çš„æŒ‘æˆ˜ã€‚æ— è®ºæ‚¨æ˜¯æ„å»ºèŠå¤©æœºå™¨äººã€è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ç¨‹åºè¿˜æ˜¯ä»»ä½•å…¶ä»–ç”±LLMé©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼ŒTrn1/Inf2å®ä¾‹éƒ½ä½¿æ‚¨èƒ½å¤Ÿåœ¨AWSäº‘ä¸Šå……åˆ†å‘æŒ¥Llama-3çš„æ½œåŠ›ã€‚

## æ¸…ç†

æœ€åï¼Œæˆ‘ä»¬å°†æä¾›åœ¨ä¸å†éœ€è¦èµ„æºæ—¶æ¸…ç†å’Œå–æ¶ˆé…ç½®èµ„æºçš„è¯´æ˜ã€‚

åˆ é™¤RayCluster

```bash
cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2

kubectl delete -f vllm-rayserve-deployment.yaml
```

é”€æ¯EKSé›†ç¾¤å’Œèµ„æº

```bash
cd data-on-eks/ai-ml/trainium-inferentia/

./cleanup.sh
```
