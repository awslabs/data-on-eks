---
title: NVIDIA Triton Server on vLLM
sidebar_position: 2
---
import CollapsibleContent from '../../../../../../../src/components/CollapsibleContent';

:::caution

**AI on EKS**å†…å®¹**æ­£åœ¨è¿ç§»**åˆ°ä¸€ä¸ªæ–°çš„ä»“åº“ã€‚
ğŸ”— ğŸ‘‰ [é˜…è¯»å®Œæ•´çš„è¿ç§»å…¬å‘Š Â»](https://awslabs.github.io/data-on-eks/docs/migration/migration-announcement)

:::

:::warning
åœ¨EKSä¸Šéƒ¨ç½²MLæ¨¡å‹éœ€è¦è®¿é—®GPUæˆ–Neuronå®ä¾‹ã€‚å¦‚æœæ‚¨çš„éƒ¨ç½²ä¸èµ·ä½œç”¨ï¼Œé€šå¸¸æ˜¯å› ä¸ºç¼ºå°‘å¯¹è¿™äº›èµ„æºçš„è®¿é—®ã€‚æ­¤å¤–ï¼Œä¸€äº›éƒ¨ç½²æ¨¡å¼ä¾èµ–äºKarpenterè‡ªåŠ¨æ‰©å±•å’Œé™æ€èŠ‚ç‚¹ç»„ï¼›å¦‚æœèŠ‚ç‚¹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥Karpenteræˆ–èŠ‚ç‚¹ç»„çš„æ—¥å¿—ä»¥è§£å†³é—®é¢˜ã€‚
:::

:::caution

ä½¿ç”¨[Meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Meta-Llama-3-8B)å’Œ[Mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)æ¨¡å‹éœ€è¦é€šè¿‡Hugging Faceè´¦æˆ·è®¿é—®ã€‚

:::

# ä½¿ç”¨NVIDIA TritonæœåŠ¡å™¨å’ŒvLLMéƒ¨ç½²å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹
åœ¨è¿™ä¸ªæ¨¡å¼ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦‚ä½•ä½¿ç”¨[Tritonæ¨ç†æœåŠ¡å™¨](https://github.com/triton-inference-server/server)å’Œ[vLLM](https://github.com/vllm-project/vllm)åç«¯/å¼•æ“éƒ¨ç½²å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹(LLM)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªç‰¹å®šæ¨¡å‹æ¼”ç¤ºè¿™ä¸ªè¿‡ç¨‹ï¼š`mistralai/Mistral-7B-Instruct-v0.2`å’Œ`meta-llama/Llama-2-7b-chat-hf`ã€‚è¿™äº›æ¨¡å‹å°†æ‰˜ç®¡åœ¨é…å¤‡**4ä¸ªGPU**çš„**g5.24xlarge**å¤šGPUå®ä¾‹ä¸Šï¼Œæ¯ä¸ªæ¨¡å‹æœ€å¤šä½¿ç”¨ä¸€ä¸ªGPUã€‚

NVIDIA Tritonæ¨ç†æœåŠ¡å™¨ä¸vLLMåç«¯ç»“åˆæ—¶ï¼Œä¸ºéƒ¨ç½²å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹(LLM)æä¾›äº†å¼ºå¤§çš„æ¡†æ¶ã€‚ç”¨æˆ·åº”ç”¨ç¨‹åºé€šè¿‡REST APIæˆ–gRPCä¸æ¨ç†æœåŠ¡äº¤äº’ï¼Œè¿™ç”±NGINXå’Œç½‘ç»œè´Ÿè½½å‡è¡¡å™¨(NLB)ç®¡ç†ï¼Œä»¥æœ‰æ•ˆåˆ†é…ä¼ å…¥è¯·æ±‚åˆ°Triton K8sæœåŠ¡ã€‚Triton K8sæœåŠ¡æ˜¯æˆ‘ä»¬éƒ¨ç½²çš„æ ¸å¿ƒï¼ŒTritonæœåŠ¡å™¨åœ¨è¿™é‡Œå¤„ç†æ¨ç†è¯·æ±‚ã€‚å¯¹äºæ­¤éƒ¨ç½²ï¼Œæˆ‘ä»¬ä½¿ç”¨g5.24xlargeå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹é…å¤‡4ä¸ªGPUï¼Œè¿è¡Œå¤šä¸ªæ¨¡å‹ï¼Œå¦‚Llama2-7bå’ŒMistral7bã€‚æ°´å¹³Podè‡ªåŠ¨ç¼©æ”¾å™¨(HPA)ç›‘æ§è‡ªå®šä¹‰æŒ‡æ ‡å¹¶æ ¹æ®éœ€æ±‚åŠ¨æ€æ‰©å±•Triton podï¼Œç¡®ä¿é«˜æ•ˆå¤„ç†ä¸åŒè´Ÿè½½ã€‚Prometheuså’ŒGrafanaç”¨äºæ”¶é›†å’Œå¯è§†åŒ–æŒ‡æ ‡ï¼Œæä¾›æ€§èƒ½æ´å¯Ÿå¹¶å¸®åŠ©è‡ªåŠ¨æ‰©å±•å†³ç­–ã€‚

![NVIDIA TritonæœåŠ¡å™¨](../../../../../../../docs/gen-ai/inference/img/triton-architecture.png)

## é¢„æœŸç»“æœ

å½“æ‚¨æŒ‰ç…§æè¿°éƒ¨ç½²æ‰€æœ‰å†…å®¹æ—¶ï¼Œæ‚¨å¯ä»¥é¢„æœŸæ¨ç†è¯·æ±‚çš„å¿«é€Ÿå“åº”æ—¶é—´ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨`Llama-2-7b-chat-hf`å’Œ`Mistral-7B-Instruct-v0.2`æ¨¡å‹è¿è¡Œ`triton-client.py`è„šæœ¬çš„ç¤ºä¾‹è¾“å‡ºï¼š


<details>
<summary>ç‚¹å‡»å±•å¼€æ¯”è¾ƒç»“æœ</summary>

| **è¿è¡Œ1: Llama2** | **è¿è¡Œ2: Mistral7b** |
|-------------------|----------------------|
| python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt | python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt |
| Loading inputs from `prompts.txt`... | Loading inputs from `prompts.txt`... |
| Model llama2 - Request 11: 0.00 ms | Model mistral7b - Request 3: 0.00 ms |
| Model llama2 - Request 15: 0.02 ms | Model mistral7b - Request 14: 0.00 ms |
| Model llama2 - Request 3: 0.00 ms | Model mistral7b - Request 11: 0.00 ms |
| Model llama2 - Request 8: 0.01 ms | Model mistral7b - Request 15: 0.00 ms |
| Model llama2 - Request 0: 0.01 ms | Model mistral7b - Request 5: 0.00 ms |
| Model llama2 - Request 9: 0.01 ms | Model mistral7b - Request 0: 0.01 ms |
| Model llama2 - Request 14: 0.01 ms | Model mistral7b - Request 7: 0.01 ms |
| Model llama2 - Request 16: 0.00 ms | Model mistral7b - Request 13: 0.00 ms |
| Model llama2 - Request 19: 0.02 ms | Model mistral7b - Request 9: 0.00 ms |
| Model llama2 - Request 4: 0.02 ms | Model mistral7b - Request 16: 0.01 ms |
| Model llama2 - Request 10: 0.02 ms | Model mistral7b - Request 18: 0.01 ms |
| Model llama2 - Request 6: 0.01 ms | Model mistral7b - Request 4: 0.01 ms |
| Model llama2 - Request 1: 0.02 ms | Model mistral7b - Request 8: 0.01 ms |
| Model llama2 - Request 7: 0.02 ms | Model mistral7b - Request 1: 0.01 ms |
| Model llama2 - Request 18: 0.01 ms | Model mistral7b - Request 6: 0.00 ms |
| Model llama2 - Request 12: 0.01 ms | Model mistral7b - Request 12: 0.00 ms |
| Model llama2 - Request 2: 0.01 ms | Model mistral7b - Request 17: 0.00 ms |
| Model llama2 - Request 17: 0.02 ms | Model mistral7b - Request 2: 0.01 ms |
| Model llama2 - Request 13: 0.01 ms | Model mistral7b - Request 19: 0.01 ms |
| Model llama2 - Request 5: 0.02 ms | Model mistral7b - Request 10: 0.02 ms |
| Storing results into `llama2_results.txt`... | Storing results into `mistral_results.txt`... |
| Total time for all requests: 0.00 seconds (0.18 milliseconds) | Total time for all requests: 0.00 seconds (0.11 milliseconds) |
| PASS: vLLM example | PASS: vLLM example |


</details>
# TritonæœåŠ¡å™¨å†…éƒ¨ç»“æ„å’Œåç«¯é›†æˆ

NVIDIA Tritonæ¨ç†æœåŠ¡å™¨ä¸“ä¸ºåœ¨å„ç§æ¨¡å‹ç±»å‹å’Œéƒ¨ç½²åœºæ™¯ä¸­å®ç°é«˜æ€§èƒ½æ¨ç†è€Œè®¾è®¡ã€‚Tritonçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå®ƒæ”¯æŒå„ç§åç«¯ï¼Œè¿™äº›åç«¯æä¾›äº†æœ‰æ•ˆå¤„ç†ä¸åŒç±»å‹æ¨¡å‹å’Œå·¥ä½œè´Ÿè½½æ‰€éœ€çš„çµæ´»æ€§å’Œèƒ½åŠ›ã€‚

ä¸€æ—¦è¯·æ±‚åˆ°è¾¾Triton K8sæœåŠ¡ï¼Œå®ƒå°±ä¼šè¢«TritonæœåŠ¡å™¨å¤„ç†ã€‚æœåŠ¡å™¨æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†ï¼Œå…è®¸å°†å¤šä¸ªæ¨ç†è¯·æ±‚åˆ†ç»„åœ¨ä¸€èµ·ä»¥ä¼˜åŒ–å¤„ç†ã€‚è¿™åœ¨é«˜ååé‡éœ€æ±‚çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºå‡å°‘å»¶è¿Ÿå¹¶æé«˜æ•´ä½“æ€§èƒ½ã€‚

ç„¶åï¼Œè¯·æ±‚ç”±è°ƒåº¦é˜Ÿåˆ—ç®¡ç†ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å‹çš„æ¨ç†è¯·æ±‚ä»¥æœ‰åºçš„æ–¹å¼å¤„ç†ã€‚TritonæœåŠ¡å™¨æ”¯æŒé€‰æ‹©æ€§å’Œè®¡ç®—æ¨¡å‹åŠ è½½ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æ ¹æ®å½“å‰å·¥ä½œè´Ÿè½½å’Œèµ„æºå¯ç”¨æ€§åŠ¨æ€åŠ è½½æ¨¡å‹ã€‚è¿™ä¸ªç‰¹æ€§å¯¹äºåœ¨å¤šæ¨¡å‹éƒ¨ç½²ä¸­é«˜æ•ˆç®¡ç†èµ„æºè‡³å…³é‡è¦ã€‚

Tritonæ¨ç†èƒ½åŠ›çš„æ”¯æŸ±æ˜¯å…¶å„ç§åç«¯ï¼ŒåŒ…æ‹¬TensorRT-LLMå’ŒvLLMï¼š

**[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)**ï¼šTensorRT-LLMåç«¯ä¼˜åŒ–äº†NVIDIA GPUä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ã€‚åˆ©ç”¨TensorRTçš„é«˜æ€§èƒ½åŠŸèƒ½ï¼Œå®ƒåŠ é€Ÿäº†æ¨ç†ï¼Œæä¾›ä½å»¶è¿Ÿå’Œé«˜ååé‡æ€§èƒ½ã€‚TensorRTç‰¹åˆ«é€‚åˆéœ€è¦å¯†é›†è®¡ç®—èµ„æºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºå®æ—¶AIåº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚

**[vLLM](https://github.com/vllm-project/vllm)**ï¼švLLMåç«¯ä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†å„ç§LLMå·¥ä½œè´Ÿè½½ã€‚å®ƒæä¾›äº†ä¸ºå¤§å‹æ¨¡å‹é‡èº«å®šåˆ¶çš„é«˜æ•ˆå†…å­˜ç®¡ç†å’Œæ‰§è¡Œç®¡é“ã€‚è¿™ä¸ªåç«¯ç¡®ä¿å†…å­˜èµ„æºå¾—åˆ°æœ€ä½³ä½¿ç”¨ï¼Œå…è®¸éƒ¨ç½²éå¸¸å¤§çš„æ¨¡å‹è€Œä¸ä¼šé‡åˆ°å†…å­˜ç“¶é¢ˆã€‚vLLMå¯¹äºéœ€è¦åŒæ—¶æœåŠ¡å¤šä¸ªå¤§å‹æ¨¡å‹çš„åº”ç”¨è‡³å…³é‡è¦ï¼Œæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚


![NVIDIA TritonæœåŠ¡å™¨](../../../../../../../docs/gen-ai/inference/img/triton-internals.png)

### Mistralai/Mistral-7B-Instruct-v0.2
Mistralai/Mistral-7B-Instruct-v0.2æ˜¯ä¸€ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æä¾›é«˜è´¨é‡ã€æœ‰æŒ‡å¯¼æ€§çš„å“åº”ã€‚å®ƒåœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ“…é•¿ç†è§£å’Œç”Ÿæˆå„ç§ä¸»é¢˜çš„ç±»äººæ–‡æœ¬ã€‚å…¶åŠŸèƒ½ä½¿å…¶é€‚ç”¨äºéœ€è¦è¯¦ç»†è§£é‡Šã€å¤æ‚æŸ¥è¯¢å’Œè‡ªç„¶è¯­è¨€ç†è§£çš„åº”ç”¨ã€‚

### Meta-llama/Llama-2-7b-chat-hf
Meta-llama/Llama-2-7b-chat-hfæ˜¯ç”±Metaå¼€å‘çš„å…ˆè¿›å¯¹è¯AIæ¨¡å‹ã€‚å®ƒé’ˆå¯¹èŠå¤©åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæä¾›è¿è´¯ä¸”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”ã€‚å‡­å€Ÿå…¶åœ¨å¹¿æ³›å¯¹è¯æ•°æ®é›†ä¸Šçš„å¼ºå¤§è®­ç»ƒï¼Œè¯¥æ¨¡å‹æ“…é•¿ç»´æŒå¼•äººå…¥èƒœä¸”åŠ¨æ€çš„å¯¹è¯ï¼Œä½¿å…¶æˆä¸ºå®¢æˆ·æœåŠ¡æœºå™¨äººã€äº¤äº’å¼ä»£ç†å’Œå…¶ä»–åŸºäºèŠå¤©çš„åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚

## éƒ¨ç½²è§£å†³æ–¹æ¡ˆ
è¦å¼€å§‹åœ¨[Amazon EKS](https://aws.amazon.com/eks/)ä¸Šéƒ¨ç½²`mistralai/Mistral-7B-Instruct-v0.2`å’Œ`meta-llama/Llama-2-7b-chat-hf`ï¼Œæˆ‘ä»¬å°†æ¶µç›–å¿…è¦çš„å…ˆå†³æ¡ä»¶ï¼Œå¹¶ä¸€æ­¥æ­¥å¼•å¯¼æ‚¨å®Œæˆéƒ¨ç½²è¿‡ç¨‹ã€‚æ­¤è¿‡ç¨‹åŒ…æ‹¬è®¾ç½®åŸºç¡€è®¾æ–½ã€éƒ¨ç½²NVIDIA Tritonæ¨ç†æœåŠ¡å™¨ï¼Œä»¥åŠåˆ›å»ºå‘TritonæœåŠ¡å™¨å‘é€gRPCè¯·æ±‚è¿›è¡Œæ¨ç†çš„Tritonå®¢æˆ·ç«¯Pythonåº”ç”¨ç¨‹åºã€‚

:::danger

é‡è¦æç¤ºï¼šåœ¨é…å¤‡å¤šä¸ªGPUçš„`g5.24xlarge`å®ä¾‹ä¸Šéƒ¨ç½²å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚ç¡®ä¿æ‚¨ä»”ç»†ç›‘æ§å’Œç®¡ç†æ‚¨çš„ä½¿ç”¨æƒ…å†µï¼Œä»¥é¿å…æ„å¤–æˆæœ¬ã€‚è€ƒè™‘è®¾ç½®é¢„ç®—è­¦æŠ¥å’Œä½¿ç”¨é™åˆ¶æ¥è·Ÿè¸ªæ‚¨çš„æ”¯å‡ºã€‚

:::

<CollapsibleContent header={<h2><span>å…ˆå†³æ¡ä»¶</span></h2>}>
åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»å‡†å¤‡å¥½æ‰€æœ‰å¿…è¦çš„å…ˆå†³æ¡ä»¶ï¼Œä»¥ä½¿éƒ¨ç½²è¿‡ç¨‹é¡ºåˆ©è¿›è¡Œã€‚ç¡®ä¿æ‚¨å·²åœ¨è®¡ç®—æœºä¸Šå®‰è£…äº†ä»¥ä¸‹å·¥å…·ï¼š

1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)
3. [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### éƒ¨ç½²

å…‹éš†ä»“åº“

```bash
git clone https://github.com/awslabs/data-on-eks.git
```

å¯¼èˆªåˆ°å…¶ä¸­ä¸€ä¸ªç¤ºä¾‹ç›®å½•å¹¶è¿è¡Œ`install.sh`è„šæœ¬

**é‡è¦æç¤ºï¼š**

**æ­¥éª¤1**ï¼šç¡®ä¿åœ¨éƒ¨ç½²è“å›¾ä¹‹å‰æ›´æ–°`variables.tf`æ–‡ä»¶ä¸­çš„åŒºåŸŸã€‚
æ­¤å¤–ï¼Œç¡®è®¤æ‚¨çš„æœ¬åœ°åŒºåŸŸè®¾ç½®ä¸æŒ‡å®šåŒºåŸŸåŒ¹é…ï¼Œä»¥é˜²æ­¢ä»»ä½•å·®å¼‚ã€‚

ä¾‹å¦‚ï¼Œå°†æ‚¨çš„`export AWS_DEFAULT_REGION="<REGION>"`è®¾ç½®ä¸ºæ‰€éœ€åŒºåŸŸï¼š

**æ­¥éª¤2**ï¼šè¦ç»§ç»­ï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨Huggingfaceè´¦æˆ·è®¿é—®è¿™ä¸¤ä¸ªæ¨¡å‹ï¼š

![mistral7b-hg.png](../../../../../../../docs/gen-ai/inference/img/mistral7b-hg.png)

![llma27b-hg.png](../../../../../../../docs/gen-ai/inference/img/llma27b-hg.png)

**æ­¥éª¤3**ï¼šæ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ‚¨çš„Huggingfaceè´¦æˆ·ä»¤ç‰Œè®¾ç½®ç¯å¢ƒå˜é‡TF_VAR_huggingface_tokenï¼š
  `export TF_VAR_huggingface_token=<your Huggingface token>`ã€‚

**æ­¥éª¤4**ï¼šè¿è¡Œå®‰è£…è„šæœ¬ã€‚

```bash
cd data-on-eks/ai-ml/nvidia-triton-server/ && chmod +x install.sh
./install.sh
```

### éªŒè¯èµ„æº

**æ­¥éª¤5**ï¼šå®‰è£…å®Œæˆåï¼ŒéªŒè¯Amazon EKSé›†ç¾¤

```bash
# åˆ›å»ºk8sé…ç½®æ–‡ä»¶ä»¥ä¸EKSè¿›è¡Œèº«ä»½éªŒè¯
aws eks --region us-west-2 update-kubeconfig --name nvidia-triton-server

kubectl get nodes # è¾“å‡ºæ˜¾ç¤ºEKSå·¥ä½œèŠ‚ç‚¹
```

æ‚¨åº”è¯¥çœ‹åˆ°æ­¤å®‰è£…éƒ¨ç½²çš„ä¸‰ä¸ªèŠ‚ç‚¹ï¼šä¸¤ä¸ª`m5.xlarge`å’Œä¸€ä¸ª`g5.24xlarge`ã€‚

```text
ip-100-64-190-174.us-west-2.compute.internal   Ready    <none>   11d     v1.29.3-eks-ae9a62a
ip-100-64-59-224.us-west-2.compute.internal    Ready    <none>   8m26s   v1.29.3-eks-ae9a62a
ip-100-64-59-227.us-west-2.compute.internal    Ready    <none>   11d     v1.29.3-eks-ae9a62a
```

</CollapsibleContent>


### ä½¿ç”¨vLLMåç«¯çš„NVIDIA TritonæœåŠ¡å™¨

æ­¤è“å›¾ä½¿ç”¨[Triton helmå›¾è¡¨](https://github.com/aws-ia/terraform-aws-eks-data-addons/tree/main/helm-charts/nvidia-triton-server)åœ¨Amazon EKSä¸Šå®‰è£…å’Œé…ç½®TritonæœåŠ¡å™¨ã€‚éƒ¨ç½²ä½¿ç”¨è“å›¾ä¸­çš„ä»¥ä¸‹Terraformä»£ç è¿›è¡Œé…ç½®ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€éƒ¨ç½²ä»£ç </summary>
```hcl
module "triton_server_vllm" {
  depends_on = [module.eks_blueprints_addons.kube_prometheus_stack]
  source     = "aws-ia/eks-data-addons/aws"
  version    = "~> 1.32.0" # ensure to update this to the latest/desired version

  oidc_provider_arn = module.eks.oidc_provider_arn

  enable_nvidia_triton_server = false

  nvidia_triton_server_helm_config = {
    version   = "1.0.0"
    timeout   = 120
    wait      = false
    namespace = kubernetes_namespace_v1.triton.metadata[0].name
    values = [
      <<-EOT
      replicaCount: 1
      image:
        repository: nvcr.io/nvidia/tritonserver
        tag: "24.06-vllm-python-py3"
      serviceAccount:
        create: false
        name: ${kubernetes_service_account_v1.triton.metadata[0].name}
      modelRepositoryPath: s3://${module.s3_bucket.s3_bucket_id}/model_repository
      environment:
        - name: model_name
          value: ${local.default_model_name}
        - name: "LD_PRELOAD"
          value: ""
        - name: "TRANSFORMERS_CACHE"
          value: "/home/triton-server/.cache"
        - name: "shm-size"
          value: "5g"
        - name: "NCCL_IGNORE_DISABLED_P2P"
          value: "1"
        - name: tensor_parallel_size
          value: "1"
        - name: gpu_memory_utilization
          value: "0.9"
        - name: dtype
          value: "auto"
      secretEnvironment:
        - name: "HUGGING_FACE_TOKEN"
          secretName: ${kubernetes_secret_v1.huggingface_token.metadata[0].name}
          key: "HF_TOKEN"
      resources:
        limits:
          cpu: 6
          memory: 25Gi
          nvidia.com/gpu: 4
        requests:
          cpu: 6
          memory: 25Gi
          nvidia.com/gpu: 4
      nodeSelector:
        NodeGroupType: g5-gpu-karpenter
        type: karpenter

      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      EOT
    ]
  }
}

```
</details>


**æ³¨æ„ï¼š** ç”¨äºTritonæœåŠ¡å™¨çš„å®¹å™¨é•œåƒæ˜¯`nvcr.io/nvidia/tritonserver:24.02-vllm-python-py3`ï¼Œå¹¶å¯ç”¨äº†vLLMåç«¯ã€‚æ‚¨å¯ä»¥åœ¨[NGCç›®å½•](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)ä¸­é€‰æ‹©é€‚å½“çš„æ ‡ç­¾ã€‚

**æ¨¡å‹ä»“åº“**ï¼š
Tritonæ¨ç†æœåŠ¡å™¨ä»æœåŠ¡å™¨å¯åŠ¨æ—¶æŒ‡å®šçš„ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹ä»“åº“æä¾›æ¨¡å‹ã€‚Tritonå¯ä»¥è®¿é—®æœ¬åœ°å¯è®¿é—®çš„æ–‡ä»¶è·¯å¾„å’Œäº‘å­˜å‚¨ä½ç½®ï¼ˆå¦‚Amazon S3ï¼‰ä¸­çš„æ¨¡å‹ã€‚

ç»„æˆæ¨¡å‹ä»“åº“çš„ç›®å½•å’Œæ–‡ä»¶å¿…é¡»éµå¾ªæ‰€éœ€çš„å¸ƒå±€ã€‚ä»“åº“å¸ƒå±€åº”æŒ‰å¦‚ä¸‹ç»“æ„ï¼š
<details>
<summary>ç‚¹å‡»å±•å¼€æ¨¡å‹ç›®å½•å±‚æ¬¡ç»“æ„</summary>
```text
<model-repository-path>/
  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>

  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>
    ...


-------------
ç¤ºä¾‹:
-------------
model-repository/
  mistral-7b/
    config.pbtxt
    1/
      model.py
  llama-2/
    config.pbtxt
    1/
      model.py
```
</details>


å¯¹äºå¯ç”¨vLLMçš„Tritonæ¨¡å‹ï¼Œmodel_repositoryå¯ä»¥åœ¨`gen-ai/inference/vllm-nvidia-triton-server-gpu/model_repository`ä½ç½®æ‰¾åˆ°ã€‚åœ¨éƒ¨ç½²æœŸé—´ï¼Œè“å›¾åˆ›å»ºä¸€ä¸ªS3å­˜å‚¨æ¡¶å¹¶å°†æœ¬åœ°`model_repository`å†…å®¹åŒæ­¥åˆ°S3å­˜å‚¨æ¡¶ã€‚

**model.py**ï¼šæ­¤è„šæœ¬ä½¿ç”¨vLLMåº“ä½œä¸ºTritonåç«¯æ¡†æ¶ï¼Œé€šè¿‡åŠ è½½æ¨¡å‹é…ç½®å’Œé…ç½®vLLMå¼•æ“æ¥åˆå§‹åŒ–`TritonPythonModel`ç±»ã€‚ä½¿ç”¨`huggingface_hub`åº“çš„ç™»å½•å‡½æ•°å»ºç«‹å¯¹hugging faceä»“åº“çš„è®¿é—®ä»¥è·å–æ¨¡å‹è®¿é—®æƒé™ã€‚ç„¶åå®ƒå¯åŠ¨ä¸€ä¸ªasyncioäº‹ä»¶å¾ªç¯æ¥å¼‚æ­¥å¤„ç†æ¥æ”¶åˆ°çš„è¯·æ±‚ã€‚è¯¥è„šæœ¬æœ‰å‡ ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†æ¨ç†è¯·æ±‚ï¼Œå‘vLLMåç«¯å‘å‡ºè¯·æ±‚å¹¶è¿”å›å“åº”ã€‚

**config.pbtxt**ï¼šè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†ä»¥ä¸‹å‚æ•°ï¼š

- åç§° - æ¨¡å‹çš„åç§°å¿…é¡»ä¸åŒ…å«æ¨¡å‹çš„æ¨¡å‹ä»“åº“ç›®å½•çš„`name`åŒ¹é…ã€‚
- max_batch_size - `max_batch_size`å€¼è¡¨ç¤ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§æ‰¹å¤„ç†å¤§å°ï¼Œç”¨äºTritonå¯ä»¥åˆ©ç”¨çš„æ‰¹å¤„ç†ç±»å‹
- è¾“å…¥å’Œè¾“å‡º - æ¯ä¸ªæ¨¡å‹è¾“å…¥å’Œè¾“å‡ºå¿…é¡»æŒ‡å®šåç§°ã€æ•°æ®ç±»å‹å’Œå½¢çŠ¶ã€‚è¾“å…¥å½¢çŠ¶è¡¨ç¤ºæ¨¡å‹é¢„æœŸçš„è¾“å…¥å¼ é‡çš„å½¢çŠ¶ï¼Œä»¥åŠTritonåœ¨æ¨ç†è¯·æ±‚ä¸­çš„å½¢çŠ¶ã€‚è¾“å‡ºå½¢çŠ¶è¡¨ç¤ºæ¨¡å‹äº§ç”Ÿçš„è¾“å‡ºå¼ é‡çš„å½¢çŠ¶ï¼Œä»¥åŠTritonåœ¨å“åº”æ¨ç†è¯·æ±‚æ—¶è¿”å›çš„å½¢çŠ¶ã€‚è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ç”±`max_batch_size`å’Œ`input dims`æˆ–`output dims`æŒ‡å®šçš„ç»´åº¦ç»„åˆæŒ‡å®šã€‚

### éªŒè¯éƒ¨ç½²

è¦éªŒè¯Tritonæ¨ç†æœåŠ¡å™¨å·²æˆåŠŸéƒ¨ç½²ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
kubectl get all -n triton-vllm
```

ä¸‹é¢çš„è¾“å‡ºæ˜¾ç¤ºæœ‰ä¸€ä¸ªpodè¿è¡ŒTritonæœåŠ¡å™¨ï¼Œè¯¥æœåŠ¡å™¨æ‰˜ç®¡ä¸¤ä¸ªæ¨¡å‹ã€‚
æœ‰ä¸€ä¸ªæœåŠ¡ç”¨äºä¸æ¨¡å‹äº¤äº’ï¼Œä»¥åŠä¸€ä¸ªç”¨äºTritonæœåŠ¡å™¨çš„ReplicaSetã€‚
éƒ¨ç½²å°†æ ¹æ®è‡ªå®šä¹‰æŒ‡æ ‡å’ŒHPAå¯¹è±¡è¿›è¡Œæ°´å¹³æ‰©å±•ã€‚

```text
NAME                                                               READY   STATUS    RESTARTS   AGE
pod/nvidia-triton-server-triton-inference-server-c49bd559d-szlpf   1/1     Running   0          13m

NAME                                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/nvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   <none>        8000/TCP,8001/TCP,8002/TCP   13m
service/nvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    <none>        8080/TCP                     13m

NAME                                                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nvidia-triton-server-triton-inference-server   1/1     1            1           13m

NAME                                                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/nvidia-triton-server-triton-inference-server-c49bd559d   1         1         1       13m

NAME                                                                               REFERENCE                                                 TARGETS                        MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/nvidia-triton-server-triton-inference-server   Deployment/nvidia-triton-server-triton-inference-server   <unknown>/80%, <unknown>/80%   1         5         1          13m

```

æ­¤è¾“å‡ºè¡¨æ˜TritonæœåŠ¡å™¨podæ­£åœ¨è¿è¡Œï¼ŒæœåŠ¡å·²æ­£ç¡®è®¾ç½®ï¼Œå¹¶ä¸”éƒ¨ç½²æŒ‰é¢„æœŸè¿è¡Œã€‚æ°´å¹³Podè‡ªåŠ¨ç¼©æ”¾å™¨ä¹Ÿå¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œç¡®ä¿podæ•°é‡æ ¹æ®æŒ‡å®šçš„æŒ‡æ ‡è¿›è¡Œæ‰©å±•ã€‚

### æµ‹è¯•Llama-2-7bèŠå¤©å’ŒMistral-7bèŠå¤©æ¨¡å‹
æ˜¯æ—¶å€™æµ‹è¯•Llama-2-7bèŠå¤©å’ŒMistral-7bèŠå¤©æ¨¡å‹äº†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„æç¤ºè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä»¥éªŒè¯ä¸¤ä¸ªæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºã€‚

é¦–å…ˆï¼Œä½¿ç”¨kubectlæ‰§è¡Œç«¯å£è½¬å‘åˆ°Triton-inference-serveræœåŠ¡ï¼š

```bash
kubectl -n triton-vllm port-forward svc/nvidia-triton-server-triton-inference-server 8001:8001
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ç›¸åŒçš„æç¤ºä¸ºæ¯ä¸ªæ¨¡å‹è¿è¡ŒTritonå®¢æˆ·ç«¯ï¼š

```bash
cd data-on-eks/gen-ai/inference/vllm-nvidia-triton-server-gpu/triton-client
python3 -m venv .venv
source .venv/bin/activate
pip install tritonclient[all]
python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt
```

æ‚¨å°†çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```text
python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt
Loading inputs from `prompts.txt`...
Model mistral7b - Request 3: 0.00 ms
Model mistral7b - Request 14: 0.00 ms
Model mistral7b - Request 11: 0.00 ms
Model mistral7b - Request 15: 0.00 ms
Model mistral7b - Request 5: 0.00 ms
Model mistral7b - Request 0: 0.01 ms
Model mistral7b - Request 7: 0.01 ms
Model mistral7b - Request 13: 0.00 ms
Model mistral7b - Request 9: 0.00 ms
Model mistral7b - Request 16: 0.01 ms
Model mistral7b - Request 18: 0.01 ms
Model mistral7b - Request 4: 0.01 ms
Model mistral7b - Request 8: 0.01 ms
Model mistral7b - Request 1: 0.01 ms
Model mistral7b - Request 6: 0.00 ms
Model mistral7b - Request 12: 0.00 ms
Model mistral7b - Request 17: 0.00 ms
Model mistral7b - Request 2: 0.01 ms
Model mistral7b - Request 19: 0.01 ms
Model mistral7b - Request 10: 0.02 ms
Storing results into `mistral_results.txt`...
Total time for all requests: 0.00 seconds (0.11 milliseconds)
PASS: vLLM example
```

`mistral_results.txt`çš„è¾“å‡ºåº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
<details>
<summary>ç‚¹å‡»å±•å¼€Mistralç»“æœéƒ¨åˆ†è¾“å‡º</summary>
```text
<s>[INST]<<SYS>>
Keep short answers of no more than 100 sentences.
<</SYS>>

What are the key differences between traditional machine learning models and very large language models (vLLM)?
[/INST] Traditional machine learning models (MLMs) are trained on specific datasets and features to learn patterns and make predictions based on that data. They require labeled data for training and are limited by the size and diversity of the training data. MLMs can be effective for solving structured problems, such as image recognition or speech recognition.

Very Large Language Models (vLLMs), on the other hand, are trained on vast amounts of text data using deep learning techniques. They learn to generate human-like text based on the input they receive. vLLMs can understand and generate text in a more contextually aware and nuanced way than MLMs. They can also perform a wider range of tasks, such as text summarization, translation, and question answering. However, vLLMs can be more computationally expensive and require large amounts of data and power to train. They also have the potential to generate inaccurate or biased responses if not properly managed.

=========

<s>[INST]<<SYS>>
Keep short answers of no more than 100 sentences.
<</SYS>>

Can you explain how TensorRT optimizes LLM inference on NVIDIA hardware?
[/INST] TensorRT is a deep learning inference optimization tool from NVIDIA. It utilizes dynamic and static analysis to optimize deep learning models for inference on NVIDIA GPUs. For Maximum Likelihood Modeling (LLM) inference, TensorRT applies the following optimizations:

1. Model Optimization: TensorRT converts the LLM model into an optimized format, such as INT8 or FP16, which reduces memory usage and increases inference speed.

2. Engine Generation: TensorRT generates a custom engine for the optimized model, which includes kernel optimizations for specific NVIDIA GPUs.

3. Memory Optimization: TensorRT minimizes memory usage by using data layout optimizations, memory pooling, and other techniques.

4. Execution Optimization: TensorRT optimizes the execution of the engine on the GPU by scheduling and managing thread execution, reducing latency and increasing throughput.

5. I/O Optimization: TensorRT optimizes input and output data transfer between the host and the GPU, reducing the time spent on data transfer and increasing overall inference speed.

6. Dynamic Batching: TensorRT dynamically batches input data to maximize GPU utilization and reduce latency.

7. Multi-Streaming: TensorRT supports multi-streaming, allowing multiple inference requests to be processed concurrently, increasing overall throughput.

8. Profiling and Monitoring: TensorRT provides profiling and monitoring tools to help developers identify performance bottlenecks and optimize their models further.

Overall, TensorRT optimizes LLM inference on NVIDIA hardware by applying a combination of model, engine, memory, execution, I/O, dynamic batching, multi-streaming, and profiling optimizations.
```
</details>


ç°åœ¨ï¼Œå°è¯•ä½¿ç”¨ç›¸åŒçš„æç¤ºåœ¨Llama-2-7b-chatæ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ï¼Œå¹¶è§‚å¯Ÿåä¸º`llama2_results.txt`çš„æ–°æ–‡ä»¶ä¸‹çš„è¾“å‡ºã€‚

```bash
python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt
```

è¾“å‡ºåº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```text
python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt
Loading inputs from `prompts.txt`...
Model llama2 - Request 11: 0.00 ms
Model llama2 - Request 15: 0.02 ms
Model llama2 - Request 3: 0.00 ms
Model llama2 - Request 8: 0.03 ms
Model llama2 - Request 5: 0.02 ms
Model llama2 - Request 0: 0.00 ms
Model llama2 - Request 14: 0.00 ms
Model llama2 - Request 16: 0.01 ms
Model llama2 - Request 19: 0.02 ms
Model llama2 - Request 4: 0.01 ms
Model llama2 - Request 1: 0.01 ms
Model llama2 - Request 10: 0.01 ms
Model llama2 - Request 9: 0.01 ms
Model llama2 - Request 7: 0.01 ms
Model llama2 - Request 18: 0.01 ms
Model llama2 - Request 12: 0.00 ms
Model llama2 - Request 2: 0.00 ms
Model llama2 - Request 6: 0.00 ms
Model llama2 - Request 17: 0.01 ms
Model llama2 - Request 13: 0.01 ms
Storing results into `llama2_results.txt`...
Total time for all requests: 0.00 seconds (0.18 milliseconds)
PASS: vLLM example
```

## å¯è§‚æµ‹æ€§

### ä½¿ç”¨AWS CloudWatchå’ŒNeuron Monitorè¿›è¡Œå¯è§‚æµ‹æ€§

æ­¤è“å›¾éƒ¨ç½²äº†CloudWatchå¯è§‚æµ‹æ€§ä»£ç†ä½œä¸ºæ‰˜ç®¡é™„åŠ ç»„ä»¶ï¼Œä¸ºå®¹å™¨åŒ–å·¥ä½œè´Ÿè½½æä¾›å…¨é¢ç›‘æ§ã€‚å®ƒåŒ…æ‹¬å®¹å™¨æ´å¯Ÿï¼Œç”¨äºè·Ÿè¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚CPUå’Œå†…å­˜åˆ©ç”¨ç‡ã€‚æ­¤å¤–ï¼Œè¯¥è“å›¾ä½¿ç”¨NVIDIAçš„DCGMæ’ä»¶é›†æˆäº†GPUæŒ‡æ ‡ï¼Œè¿™å¯¹äºç›‘æ§é«˜æ€§èƒ½GPUå·¥ä½œè´Ÿè½½è‡³å…³é‡è¦ã€‚å¯¹äºåœ¨AWS Inferentiaæˆ–Trainiumä¸Šè¿è¡Œçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ·»åŠ äº†[Neuron Monitoræ’ä»¶](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide)æ¥æ•è·å’ŒæŠ¥å‘ŠNeuronç‰¹å®šæŒ‡æ ‡ã€‚

æ‰€æœ‰æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å®¹å™¨æ´å¯Ÿã€GPUæ€§èƒ½å’ŒNeuronæŒ‡æ ‡ï¼Œéƒ½å‘é€åˆ°Amazon CloudWatchï¼Œæ‚¨å¯ä»¥åœ¨é‚£é‡Œå®æ—¶ç›‘æ§å’Œåˆ†æå®ƒä»¬ã€‚éƒ¨ç½²å®Œæˆåï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿç›´æ¥ä»CloudWatchæ§åˆ¶å°è®¿é—®è¿™äº›æŒ‡æ ‡ï¼Œä½¿æ‚¨èƒ½å¤Ÿæœ‰æ•ˆåœ°ç®¡ç†å’Œä¼˜åŒ–å·¥ä½œè´Ÿè½½ã€‚

é™¤äº†éƒ¨ç½²CloudWatch EKSé™„åŠ ç»„ä»¶å¤–ï¼Œæˆ‘ä»¬è¿˜éƒ¨ç½²äº†Kube Prometheuså †æ ˆï¼Œå®ƒæä¾›PrometheusæœåŠ¡å™¨å’ŒGrafanaéƒ¨ç½²ï¼Œç”¨äºç›‘æ§å’Œå¯è§‚æµ‹æ€§ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬éªŒè¯Kube Prometheuså †æ ˆéƒ¨ç½²çš„æœåŠ¡ï¼š

```bash
kubectl get svc -n monitoring
```

æ‚¨åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š

```text
kubectl get svc -n monitoring
NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
kube-prometheus-stack-grafana                    ClusterIP   172.20.252.10    <none>        80/TCP              11d
kube-prometheus-stack-kube-state-metrics         ClusterIP   172.20.34.181    <none>        8080/TCP            11d
kube-prometheus-stack-operator                   ClusterIP   172.20.186.93    <none>        443/TCP             11d
kube-prometheus-stack-prometheus                 ClusterIP   172.20.147.64    <none>        9090/TCP,8080/TCP   11d
kube-prometheus-stack-prometheus-node-exporter   ClusterIP   172.20.171.165   <none>        9100/TCP            11d
prometheus-operated                              ClusterIP   None             <none>        9090/TCP            11d
```

è¦å…¬å¼€NVIDIA TritonæœåŠ¡å™¨æŒ‡æ ‡ï¼Œæˆ‘ä»¬åœ¨ç«¯å£`8080`ä¸Šéƒ¨ç½²äº†ä¸€ä¸ªæŒ‡æ ‡æœåŠ¡(`nvidia-triton-server-triton-inference-server-metrics`)ã€‚é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡ŒéªŒè¯

```bash
kubectl get svc -n triton-vllm
```

è¾“å‡ºåº”è¯¥æ˜¯ï¼š

```text
kubectl get svc -n triton-vllm
NAME                                                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   <none>        8000/TCP,8001/TCP,8002/TCP   34m
nvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    <none>        8080/TCP                     34m
```
è¿™ç¡®è®¤äº†NVIDIA TritonæœåŠ¡å™¨æŒ‡æ ‡æ­£åœ¨è¢«PrometheusæœåŠ¡å™¨æŠ“å–ã€‚æ‚¨å¯ä»¥ä½¿ç”¨Grafanaä»ªè¡¨æ¿å¯è§†åŒ–è¿™äº›æŒ‡æ ‡ã€‚

åœ¨ä¸‹é¢çš„Grafanaä»ªè¡¨æ¿ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å‡ ä¸ªé‡è¦æŒ‡æ ‡ï¼š

- **å¹³å‡GPUåŠŸç‡ä½¿ç”¨**ï¼šè¿™ä¸ªä»ªè¡¨æ˜¾ç¤ºäº†GPUçš„å½“å‰åŠŸç‡ä½¿ç”¨æƒ…å†µï¼Œè¿™å¯¹äºç›‘æ§æ¨ç†ä»»åŠ¡çš„æ•ˆç‡å’Œæ€§èƒ½è‡³å…³é‡è¦ã€‚
- **è®¡ç®—æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰**ï¼šè¿™ä¸ªæ¡å½¢å›¾æ˜¾ç¤ºäº†è®¡ç®—æ¨ç†è¯·æ±‚æ‰€éœ€çš„æ—¶é—´ï¼Œæœ‰åŠ©äºè¯†åˆ«ä»»ä½•å»¶è¿Ÿé—®é¢˜ã€‚
- **ç´¯è®¡æ¨ç†è¯·æ±‚**ï¼šè¿™ä¸ªå›¾è¡¨æ˜¾ç¤ºäº†éšæ—¶é—´å¤„ç†çš„æ¨ç†è¯·æ±‚æ€»æ•°ï¼Œæä¾›äº†å·¥ä½œè´Ÿè½½å’Œæ€§èƒ½è¶‹åŠ¿çš„æ´å¯Ÿã€‚
- **é˜Ÿåˆ—æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰**ï¼šè¿™ä¸ªæŠ˜çº¿å›¾è¡¨ç¤ºè¯·æ±‚åœ¨è¢«å¤„ç†å‰åœ¨é˜Ÿåˆ—ä¸­èŠ±è´¹çš„æ—¶é—´ï¼Œçªå‡ºäº†ç³»ç»Ÿä¸­æ½œåœ¨çš„ç“¶é¢ˆã€‚

![NVIDIA TritonæœåŠ¡å™¨](../../../../../../../docs/gen-ai/inference/img/triton-observability.png)

è¦åˆ›å»ºæ–°çš„Grafanaä»ªè¡¨æ¿æ¥ç›‘æ§è¿™äº›æŒ‡æ ‡ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

```bash
- ç«¯å£è½¬å‘GrafanaæœåŠ¡ï¼š
kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n monitoring

- Grafanaç®¡ç†å‘˜ç”¨æˆ·
admin

- ä»Terraformè¾“å‡ºè·å–å¯†é’¥åç§°
terraform output grafana_secret_name

- è·å–ç®¡ç†å‘˜ç”¨æˆ·å¯†ç 
aws secretsmanager get-secret-value --secret-id <grafana_secret_name_output> --region $AWS_REGION --query "SecretString" --output text
```

**ç™»å½•Grafanaï¼š**

- æ‰“å¼€æ‚¨çš„ç½‘ç»œæµè§ˆå™¨å¹¶å¯¼èˆªè‡³[http://localhost:8080](http://localhost:8080)ã€‚
- ä½¿ç”¨ç”¨æˆ·å`admin`å’Œä»AWS Secrets Manageræ£€ç´¢çš„å¯†ç ç™»å½•ã€‚

**å¯¼å…¥å¼€æºGrafanaä»ªè¡¨æ¿ï¼š**
- ç™»å½•åï¼Œç‚¹å‡»å·¦ä¾§è¾¹æ ä¸Šçš„"+"å›¾æ ‡ï¼Œé€‰æ‹©"å¯¼å…¥"ã€‚
- è¾“å…¥ä»¥ä¸‹URLä»¥å¯¼å…¥ä»ªè¡¨æ¿JSONï¼š[TritonæœåŠ¡å™¨Grafanaä»ªè¡¨æ¿](https://github.com/triton-inference-server/server/blob/main/deploy/k8s-onprem/dashboard.json)
- æŒ‰ç…§æç¤ºå®Œæˆå¯¼å…¥è¿‡ç¨‹ã€‚

æ‚¨ç°åœ¨åº”è¯¥å¯ä»¥åœ¨æ–°çš„Grafanaä»ªè¡¨æ¿ä¸Šçœ‹åˆ°æ˜¾ç¤ºçš„æŒ‡æ ‡ï¼Œä½¿æ‚¨èƒ½å¤Ÿç›‘æ§NVIDIA Tritonæ¨ç†æœåŠ¡å™¨éƒ¨ç½²çš„æ€§èƒ½å’Œå¥åº·çŠ¶å†µã€‚

![triton-grafana-dash2](../../../../../../../docs/gen-ai/inference/img/triton-grafana-dash2.png)


## ç»“è®º
åœ¨Amazon EKSä¸Šä½¿ç”¨NVIDIA Tritonæ¨ç†æœåŠ¡å™¨å’ŒvLLMåç«¯éƒ¨ç½²å’Œç®¡ç†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸ºç°ä»£AIåº”ç”¨æä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡éµå¾ªæ­¤è“å›¾ï¼Œæ‚¨å·²ç»è®¾ç½®äº†å¿…è¦çš„åŸºç¡€è®¾æ–½ï¼Œéƒ¨ç½²äº†TritonæœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨Kube Prometheuså †æ ˆå’ŒGrafanaé…ç½®äº†å¼ºå¤§çš„å¯è§‚æµ‹æ€§ã€‚

## æ¸…ç†

æœ€åï¼Œæˆ‘ä»¬å°†æä¾›åœ¨ä¸å†éœ€è¦èµ„æºæ—¶æ¸…ç†å’Œå–æ¶ˆé…ç½®èµ„æºçš„è¯´æ˜ã€‚

**æ¸…ç†EKSé›†ç¾¤ï¼š**
æ­¤è„šæœ¬å°†ä½¿ç”¨`-target`é€‰é¡¹æ¸…ç†ç¯å¢ƒï¼Œä»¥ç¡®ä¿æ‰€æœ‰èµ„æºæŒ‰æ­£ç¡®é¡ºåºåˆ é™¤ã€‚

```bash
export AWS_DEAFULT_REGION="DEPLOYED_EKS_CLUSTER_REGION>"
cd data-on-eks/ai-ml/nvidia-triton-server/ && chmod +x cleanup.sh
./cleanup.sh
```
