---
title: Ray Dataë¥¼ ì‚¬ìš©í•œ Spark ë¡œê·¸ ì²˜ë¦¬
sidebar_position: 2
---

# Ray Dataì™€ Icebergë¡œ Spark ë¡œê·¸ ì²˜ë¦¬

ì´ ê°€ì´ë“œëŠ” Ray Dataë¥¼ ì‚¬ìš©í•˜ì—¬ S3ì—ì„œ Apache Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ë¥¼ ì²˜ë¦¬í•˜ê³  AWS Glue ì¹´íƒˆë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Apache Iceberg í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ê°œìš”

ì´ ì˜ˆì œëŠ” ë‹¤ìŒì„ ìœ„í•œ í”„ë¡œë•ì…˜ ì¤€ë¹„ íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
- S3ì—ì„œ ëŒ€ìš©ëŸ‰ Spark ì´ë²¤íŠ¸ ë¡œê·¸ ì½ê¸°
- Ray Dataë¡œ ë³‘ë ¬ ë¡œê·¸ ì²˜ë¦¬
- Apache Iceberg í…Œì´ë¸”ì— ê²°ê³¼ ì“°ê¸°
- Iceberg ì¹´íƒˆë¡œê·¸ë¡œ AWS Glue ì‚¬ìš©
- ìë™ ë…¸ë“œ í™•ì¥ì„ ìœ„í•œ Karpenter í™œìš©

## ì•„í‚¤í…ì²˜

```mermaid
graph LR
    A[S3 Spark ë¡œê·¸] --> B[Ray Data ì‘ì—…]
    B --> C[íŒŒì‹± ë° ë³€í™˜]
    C --> D[Iceberg í…Œì´ë¸”]
    D --> E[AWS Glue Catalog]
    F[Karpenter] -.->|í”„ë¡œë¹„ì €ë‹| B
    G[Pod Identity] -.->|IAM ì ‘ê·¼| B
```

## ì „ì œ ì¡°ê±´

### 1. Ray on EKS ì¸í”„ë¼ ë°°í¬

ë¨¼ì € KubeRay Operatorê°€ í¬í•¨ëœ Ray on EKS ìŠ¤íƒì„ ë°°í¬í•©ë‹ˆë‹¤:

**[ì¸í”„ë¼ ë°°í¬ ê°€ì´ë“œ](infra)** ë¥¼ ë”°ë¼:
- Karpenter ì˜¤í† ìŠ¤ì¼€ì¼ë§ì´ í¬í•¨ëœ EKS í´ëŸ¬ìŠ¤í„° í”„ë¡œë¹„ì €ë‹
- KubeRay Operator ë°°í¬ (`enable_raydata = true` ì„¤ì •)
- AWS ì ‘ê·¼ì„ ìœ„í•œ Pod Identity êµ¬ì„±
- Prometheus ë° Grafanaë¡œ ëª¨ë‹ˆí„°ë§ ì„¤ì •

**ë°°í¬ í™•ì¸:**
```bash
kubectl get pods -n kuberay-operator
kubectl get pods -n raydata
```

### 2. Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ìƒì„±

ì²˜ë¦¬í•  ë¡œê·¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ Spark ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. Spark OperatorëŠ” Ray on EKS ë°°í¬ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

:::info
Spark ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Fluent Bitë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ JSON ë¡œê·¸ë¥¼ S3ì— ì”ë‹ˆë‹¤. ê° ë¡œê·¸ ë¼ì¸ì€ ë¡œê·¸ ë©”íƒ€ë°ì´í„°ì™€ ë©”ì‹œì§€ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” JSON ê°ì²´ì…ë‹ˆë‹¤.
:::

### Spark ë¡œê·¸ ìƒì„±

ì²˜ë¦¬í•  Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ë¥¼ ìƒì„±í•˜ë ¤ë©´ ìƒ˜í”Œ Spark ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:

**ì˜µì…˜ 1: ê°„ë‹¨í•œ PVC ìŠ¤í† ë¦¬ì§€ ì˜ˆì œ**

Spark on EKS ì˜ˆì œë¡œ ì´ë™í•˜ì—¬ PVC ìŠ¤í† ë¦¬ì§€ë¡œ ì‘ì—… ì‹¤í–‰:

```bash
cd data-stacks/spark-on-eks/examples/ebs-pvc-storage
```

[PVC ìŠ¤í† ë¦¬ì§€ ì˜ˆì œ ê°€ì´ë“œ](/data-on-eks/docs/datastacks/processing/spark-on-eks/ebs-pvc-storage) ë¥¼ ë”°ë¼:
1. Spark ë°ì´í„°ìš© ì˜êµ¬ ë³¼ë¥¨ ìƒì„±
2. ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ìƒì„±í•˜ëŠ” ìƒ˜í”Œ Spark ì‘ì—… ì‹¤í–‰
3. ë¡œê·¸ê°€ S3ì— ì‘ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸

**ì˜µì…˜ 2: ê¸°íƒ€ Spark ì˜ˆì œ**

`data-stacks/spark-on-eks/examples/` ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  Spark ì˜ˆì œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `taxi-trip` - NYC íƒì‹œ ë°ì´í„° ì²˜ë¦¬
- `benchmark` - TPC-DS ë²¤ì¹˜ë§ˆí¬
- `gluten-velox` - ì„±ëŠ¥ ìµœì í™”ëœ Spark

ê° ì‘ì—…ì€ ìë™ìœ¼ë¡œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ë‹¤ìŒ ìœ„ì¹˜ì— ì”ë‹ˆë‹¤:
```
s3://<bucket-name>/<cluster-name>/spark-application-logs/spark-team-a/
```

**S3ì—ì„œ ë¡œê·¸ í™•ì¸**

Spark ì‘ì—…ì„ ì‹¤í–‰í•œ í›„ Fluent Bitê°€ ë¡œê·¸ë¥¼ S3ë¡œ ì „ì†¡í–ˆëŠ”ì§€ í™•ì¸:

```bash
# Terraform ì¶œë ¥ì—ì„œ ë²„í‚· ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
cd data-stacks/ray-on-eks/terraform/_local
BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server)

# Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ë‚˜ì—´ (Fluent Bitì˜ JSON í˜•ì‹)
aws s3 ls s3://${BUCKET}/spark-on-eks/spark-application-logs/spark-team-a/ --recursive
```

**ì˜ˆìƒ ì¶œë ¥** - spark-app-selectorë³„ë¡œ êµ¬ì„±ëœ JSON ë¡œê·¸ íŒŒì¼:
```
spark-on-eks/spark-application-logs/spark-team-a/spark-1b3b78e281664cefb74ca64a1ed5a2d9/taxi-trip-ebs-pvc/taxi-trip-ebs-pvc_230728_veVAzLFN.json
spark-on-eks/spark-application-logs/spark-team-a/spark-1b3b78e281664cefb74ca64a1ed5a2d9/taxi-trip-ebs-pvc-exec-1/taxi-trip-ebs-pvc-exec-1_230915_8kTnRfkj.json
spark-on-eks/spark-application-logs/spark-team-a/spark-1b3b78e281664cefb74ca64a1ed5a2d9/taxi-trip-ebs-pvc-exec-2/taxi-trip-ebs-pvc-exec-2_230916_j5NsRfkj.json
```

ê° JSON íŒŒì¼ì—ëŠ” Spark Pod(Driver ë° Executor)ì˜ êµ¬ì¡°í™”ëœ ë¡œê·¸ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ì˜ˆì œ íŒŒì¼

ëª¨ë“  íŒŒì¼ì€ `data-stacks/ray-on-eks/examples/raydata-sparklogs-processing-job/`ì— ìˆìŠµë‹ˆë‹¤:

- `rayjob.yaml` - RayJob CRD ì •ì˜
- `configmap.yaml` - Python ì²˜ë¦¬ ì½”ë“œ
- `execute-rayjob.sh` - ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
- `verify-iceberg-data.sh` - ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- `iceberg_verification.py` - Python ê²€ì¦

## êµ¬ì„±

### ë‹¨ê³„ 1: ì¸í”„ë¼ ê°’ ê°€ì ¸ì˜¤ê¸°

**Ray on EKS** ë°°í¬ì—ì„œ S3 ë²„í‚· ì´ë¦„ê³¼ êµ¬ì„± ê²€ìƒ‰:

```bash
cd data-stacks/ray-on-eks/terraform/_local

# S3 ë²„í‚· ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
terraform output s3_bucket_id_spark_history_server

# í´ëŸ¬ìŠ¤í„° ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
terraform output cluster_name

# ë¦¬ì „ ê°€ì ¸ì˜¤ê¸°
terraform output region
```

**ì˜ˆì œ ì¶œë ¥:**
```
s3_bucket_id_spark_history_server = "ray-on-eks-logs-20241022xxxxx"
cluster_name = "ray-on-eks"
region = "us-west-2"
```

### ë‹¨ê³„ 2: Ray ì‘ì—… êµ¬ì„± ì—…ë°ì´íŠ¸

Ray Data ì˜ˆì œ ë””ë ‰í† ë¦¬ë¡œ ì´ë™:

```bash
cd data-stacks/ray-on-eks/examples/raydata-sparklogs-processing-job
```

`execute-rayjob.sh`ë¥¼ í¸ì§‘í•˜ê³  ë‹¤ìŒ ê°’ ì—…ë°ì´íŠ¸:

```bash
# Ray Worker í™•ì¥ (ë¡œê·¸ ë³¼ë¥¨ì— ë”°ë¼ ì¡°ì •)
MIN_WORKERS="2"
MAX_WORKERS="10"
INITIAL_WORKERS="2"

# ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸° (ì„±ëŠ¥ì— ë§ê²Œ íŠœë‹)
BATCH_SIZE="10000"
```

### ë‹¨ê³„ 3: RayJob ì´í•´

`rayjob.yaml`ì€ Ray í´ëŸ¬ìŠ¤í„° êµ¬ì„±ê³¼ Python ì¢…ì†ì„±ì„ ì •ì˜í•©ë‹ˆë‹¤.

**ëŸ°íƒ€ì„ ì¢…ì†ì„±:**
```yaml
runtimeEnvYAML: |
  pip:
    - boto3==1.34.131
    - pyiceberg[glue,s3fs]==0.10.0
    - ray[data]==2.47.1
    - pyarrow==21.0.0         # ë„¤ì´í‹°ë¸Œ Pod Identity ì§€ì›
    - s3fs==2025.9.0
    - fsspec==2025.9.0
```

:::tip PyArrow 21.0.0 Pod Identity ì§€ì›
PyArrow 21.0.0+ëŠ” `AWS_CONTAINER_CREDENTIALS_FULL_URI`ë¥¼ í†µí•´ EKS Pod Identityë¥¼ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•˜ì—¬ ìˆ˜ë™ ìê²© ì¦ëª… êµ¬ì„±ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  Ray PodëŠ” S3 ì ‘ê·¼ì— ìë™ìœ¼ë¡œ Pod Identityë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
:::

**Submitter êµ¬ì„±:**
```yaml
submitterPodTemplate:
  spec:
    serviceAccountName: raydata  # IAMìš© Pod Identity
    containers:
      - name: rayjob-submitter
        image: rayproject/ray:2.47.1-py310
        env:
          - name: S3_BUCKET
            value: "$S3_BUCKET"
          - name: ICEBERG_DATABASE
            value: "raydata_spark_logs"
```

**Ray í´ëŸ¬ìŠ¤í„° ìŠ¤í™:**
```yaml
headGroupSpec:
  rayStartParams:
    dashboard-host: "0.0.0.0"
    num-cpus: "2"
    object-store-memory: "1000000000"
  template:
    spec:
      serviceAccountName: raydata
      containers:
        - name: ray-head
          image: rayproject/ray:2.47.1-py310
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
```

**Worker êµ¬ì„±:**
```yaml
workerGroupSpecs:
  - groupName: log-processor-workers
    replicas: $INITIAL_WORKERS
    minReplicas: $MIN_WORKERS
    maxReplicas: $MAX_WORKERS
    template:
      spec:
        containers:
          - name: ray-worker
            resources:
              requests:
                cpu: "4"
                memory: "8Gi"
              limits:
                cpu: "8"
                memory: "16Gi"
```

## ë°°í¬

### ì‘ì—… ë°°í¬

```bash
export S3_BUCKET="<S3_BUCKET>"
export CLUSTER_NAME="ray-on-eks"
export AWS_REGION="<REGION>"

cd data-stacks/ray-on-eks/examples/raydata-sparklogs-processing-job/

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x execute-rayjob.sh

# ì‘ì—… ë°°í¬
./execute-rayjob.sh deploy
```

ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì „ì œ ì¡°ê±´ ê²€ì¦ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤, ì„œë¹„ìŠ¤ ê³„ì •, Pod Identity)
2. êµ¬ì„± ê²€ì¦ (S3 ë²„í‚·, ê²½ë¡œ, ìê²© ì¦ëª…)
3. ì²˜ë¦¬ ì½”ë“œê°€ í¬í•¨ëœ ConfigMap ì ìš©
4. RayJob ë°°í¬
5. ë°°í¬ ìƒíƒœ ë° ëŒ€ì‹œë³´ë“œ ì ‘ê·¼ í‘œì‹œ

**ì˜ˆìƒ ì¶œë ¥:**
```
==== Validating Prerequisites ====
[INFO] âœ… Prerequisites validated
[INFO] âœ… Terraform module infrastructure detected
[INFO] âœ… Ray service account found

==== Current Configuration ====
AWS Region: us-west-2
Namespace: raydata
Iceberg Database: raydata_spark_logs
Iceberg Table: spark_logs
Iceberg Warehouse: s3://bucket-name/iceberg-warehouse
S3 Logs Path: s3://bucket-name/spark-application-logs/spark-team-a
Workers: 2-10 (initial: 2)

==== Deploying Ray Job Components ====
[INFO] 1. Deploying ConfigMap...
[INFO] 2. Deploying RayJob...
[INFO] âœ… Ray job components deployed successfully
```

### ì‘ì—… ì‹¤í–‰ ëª¨ë‹ˆí„°ë§

**ë¹ ë¥¸ ìƒíƒœ í™•ì¸:**
```bash
# ì‘ì—… ìƒíƒœ í™•ì¸
./execute-rayjob.sh status

# ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ì—… ì§„í–‰ ìƒí™© í™•ì¸
./execute-rayjob.sh monitor
```

**ìˆ˜ë™ ëª¨ë‹ˆí„°ë§:**
```bash
# ì‘ì—… ìƒíƒœ í™•ì¸
kubectl get rayjobs -n raydata -w

# ì˜ˆìƒ ì¶œë ¥:
# NAME                       JOB STATUS   DEPLOYMENT STATUS   START TIME             END TIME
# spark-log-processing-job   RUNNING      Running             2025-10-22T23:47:12Z

# Pod ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
kubectl get pods -n raydata

# ì˜ˆìƒ ì¶œë ¥:
# NAME                                                              READY   STATUS
# spark-log-processing-job-2t8h9-head-2bgdd                         2/2     Running
# spark-log-processing-job-2t8h9-log-processor-worke-worker-vckb4   1/1     Running
# spark-log-processing-job-2t8h9-log-processor-worke-worker-z2hgd   1/1     Running
# spark-log-processing-job-cjgff                                    1/1     Running
```

### ì‘ì—… ë¡œê·¸ ë³´ê¸°

**ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©:**
```bash
# ì‘ì—… ë¡œê·¸ íŒ”ë¡œìš°
./execute-rayjob.sh logs
```

**ìˆ˜ë™ ë¡œê·¸ ë³´ê¸°:**
```bash
# Submitter Pod ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
SUBMITTER_POD=$(kubectl get pods -n raydata --no-headers | grep spark-log-processing-job | grep -v head | grep -v worker | awk '{print $1}')

# Submitter ë¡œê·¸ íŒ”ë¡œìš°
kubectl logs $SUBMITTER_POD -n raydata -f
```

**ì˜ˆìƒ ë¡œê·¸ ì¶œë ¥:**
```
2025-10-22 16:48:08,808 - spark-log-processor - INFO - âœ… Using EKS Pod Identity credentials from http://169.254.170.23/v1/credentials
2025-10-22 16:48:08,808 - spark-log-processor - INFO - âœ… PyArrow 19.0.1+ will automatically use Pod Identity for S3 access
2025-10-22 16:48:15,003 - spark-log-processor - INFO - âœ… Iceberg catalog initialized: glue
2025-10-22 16:48:16,801 - spark-log-processor - INFO - âœ… Created new data table: raydata_spark_logs.spark_logs
2025-10-22 16:48:17,523 - spark-log-processor - INFO - âœ… Created new metadata table: raydata_spark_logs.spark_logs_processing_metadata
2025-10-22 16:48:18,676 - spark-log-processor - INFO - ğŸ” Starting intelligent folder discovery with metadata tracking...
2025-10-22 16:48:19,142 - spark-log-processor - INFO - ğŸ”„ Processing folder: spark-1b3b78e281664cefb74ca64a1ed5a2d9

Running Dataset: dataset_1_0. Active & requested resources: 2/10 CPU, 244.5KB/2.3GB object store
- ReadText->SplitBlocks(7): 33%|â–ˆâ–ˆâ–ˆâ– | 295/885 [00:08<00:03, 164 row/s]

2025-10-22 16:48:34,742 - spark-log-processor - INFO - ğŸ¯ Processing Summary:
2025-10-22 16:48:34,742 - spark-log-processor - INFO -   ğŸ“Š Total records processed: 647
2025-10-22 16:48:34,742 - spark-log-processor - INFO -   âœ… Successful folders: 1
2025-10-22 16:48:34,742 - spark-log-processor - INFO -   âŒ Failed folders: 0
2025-10-22 16:48:39,154 - SUCC - Job 'spark-log-processing-job' succeeded
```

### Ray Dashboard ì ‘ê·¼

**ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©:**
```bash
# ëŒ€ì‹œë³´ë“œ ì ‘ê·¼ ì§€ì¹¨ í‘œì‹œ
./execute-rayjob.sh dashboard
```

**ìˆ˜ë™ ì„¤ì •:**
```bash
# Ray Head ì„œë¹„ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
RAY_HEAD_SVC=$(kubectl get svc -n raydata --no-headers | grep head-svc | awk '{print $1}')

# Ray Dashboardë¡œ í¬íŠ¸ í¬ì›Œë“œ
kubectl port-forward -n raydata service/$RAY_HEAD_SVC 8265:8265

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8265 ì—´ê¸°
```

**Ray Dashboard ê¸°ëŠ¥:**
- ì‘ì—… ì‹¤í–‰ íƒ€ì„ë¼ì¸
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  (CPU, ë©”ëª¨ë¦¬, Object Store)
- íƒœìŠ¤í¬ ë¶„ì„ ë° ì„±ëŠ¥
- Worker ë…¸ë“œ ìƒíƒœ
- ë¡œê·¸ ì§‘ê³„

### Karpenter ë…¸ë“œ í”„ë¡œë¹„ì €ë‹ í™•ì¸

```bash
# Karpenter ë¡œê·¸ ë³´ê¸°
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter -f

# ë…¸ë“œ ìƒì„± í™•ì¸
kubectl get nodes -l karpenter.sh/initialized=true -w

# ë…¸ë“œ ìš©ëŸ‰ ìœ í˜• í™•ì¸ (ìŠ¤íŒŸ vs ì˜¨ë””ë§¨ë“œ)
kubectl get nodes -L karpenter.sh/capacity-type
```

## ì²˜ë¦¬ ë¡œì§

ì‘ì—…ì€ ì§€ëŠ¥í˜• ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

### 1. Spark ì• í”Œë¦¬ì¼€ì´ì…˜ íƒìƒ‰

```python
# S3ì—ì„œ ëª¨ë“  spark-app-selector í´ë” ë‚˜ì—´
folders = s3.list_folders(f"s3://{bucket}/{prefix}/")

# ì´ë¯¸ ì²˜ë¦¬ëœ í´ë”ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì¿¼ë¦¬
processed = catalog.load_table('spark_logs_processing_metadata').scan()

# ì²˜ë¦¬í•  ìƒˆ í´ë” ì‹ë³„
new_folders = [f for f in folders if f not in processed['spark_app_selector']]
```

### 2. Ray Dataë¡œ JSON ë¡œê·¸ ì½ê¸°

```python
import ray

# Ray ì´ˆê¸°í™” (PyArrow 21.0.0+ëŠ” ìë™ìœ¼ë¡œ Pod Identity ì‚¬ìš©)
ray.init()

# Fluent Bit ì¶œë ¥ì—ì„œ JSON ë¡œê·¸ íŒŒì¼ ì½ê¸°
ds = ray.data.read_text(
    f"s3://{bucket}/{prefix}/{spark_app_selector}/**/*.json"
)
```

### 3. ë¡œê·¸ íŒŒì‹± ë° ë³´ê°•

```python
def parse_log_line(line):
    """Fluent Bit JSON ë¡œê·¸ í˜•ì‹ íŒŒì‹±"""
    log = json.loads(line)
    return {
        'timestamp': log.get('timestamp'),
        'log_level': log.get('log_level'),
        'message': log.get('message'),
        'pod_name': log.get('kubernetes', {}).get('pod_name'),
        'spark_app_selector': log.get('kubernetes', {}).get('labels', {}).get('spark-app-selector'),
        # ... ì¶”ê°€ Kubernetes ë©”íƒ€ë°ì´í„°
    }

# íŒŒì‹± ì ìš©
ds = ds.map(parse_log_line).filter(lambda x: x is not None)
```

### 4. ë©”íƒ€ë°ì´í„° ì¶”ì ê³¼ í•¨ê»˜ Icebergì— ì“°ê¸°

```python
from pyiceberg.catalog import load_catalog

# AWS Glue ì¹´íƒˆë¡œê·¸ ë¡œë“œ (Pod Identity ì‚¬ìš©)
catalog = load_catalog('glue', warehouse=f"s3://{bucket}/iceberg-warehouse/")

# Iceberg í…Œì´ë¸”ì— ë¡œê·¸ ë°ì´í„° ì“°ê¸°
table = catalog.load_table('raydata_spark_logs.spark_logs')
table.append(ds.to_arrow())

# ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì—…ë°ì´íŠ¸
metadata_table = catalog.load_table('raydata_spark_logs.spark_logs_processing_metadata')
metadata_table.append({
    'spark_app_selector': spark_app_selector,
    'status': 'SUCCESS',
    'records_processed': ds.count(),
    'processing_end_time': datetime.now()
})
```

## IAM ê¶Œí•œ

ì‘ì—…ì€ Pod Identityë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (`infra/terraform/ray-operator.tf`ì—ì„œ êµ¬ì„±):

**S3 ê¶Œí•œ:**
```json
{
  "Effect": "Allow",
  "Action": [
    "s3:GetObject",
    "s3:PutObject",
    "s3:ListBucket"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name/spark-application-logs/*",
    "arn:aws:s3:::bucket-name/iceberg-warehouse/*"
  ]
}
```

**Glue ê¶Œí•œ:**
```json
{
  "Effect": "Allow",
  "Action": [
    "glue:GetDatabase",
    "glue:GetTable",
    "glue:CreateTable",
    "glue:UpdateTable",
    "glue:GetPartitions",
    "glue:BatchCreatePartition"
  ],
  "Resource": "*"
}
```

## ê²°ê³¼ í™•ì¸

### ì‘ì—… ì™„ë£Œ í™•ì¸

```bash
# ìµœì¢… ì‘ì—… ìƒíƒœ í™•ì¸
kubectl get rayjob spark-log-processing-job -n raydata

# ì˜ˆìƒ ì¶œë ¥:
# NAME                       JOB STATUS   DEPLOYMENT STATUS   START TIME             END TIME               AGE
# spark-log-processing-job   SUCCEEDED    Complete            2025-10-22T23:47:12Z   2025-10-22T23:48:42Z   2m51s
```

### Iceberg í…Œì´ë¸” ìƒì„± í™•ì¸

**AWS Glue Catalog í™•ì¸:**
```bash
# Glueì— í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
aws glue get-table \
  --database-name raydata_spark_logs \
  --name spark_logs \
  --region us-west-2 \
  --output json | jq '.Table | {Name, Location: .StorageDescriptor.Location, Columns: .StorageDescriptor.Columns | length}'

# ì˜ˆìƒ ì¶œë ¥:
# {
#   "Name": "spark_logs",
#   "Location": "s3://bucket-name/iceberg-warehouse/raydata_spark_logs.db/spark_logs",
#   "Columns": 14
# }
```

**S3 ë°ì´í„° íŒŒì¼ í™•ì¸:**
```bash
# Iceberg ë°ì´í„° ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ë‚˜ì—´
aws s3 ls s3://your-bucket/iceberg-warehouse/raydata_spark_logs.db/spark_logs/ --recursive

# ì˜ˆìƒ ì¶œë ¥:
# iceberg-warehouse/raydata_spark_logs.db/spark_logs/data/00000-0-592550cf-a900-45c7-adf8-c5ab577236ad.parquet
# iceberg-warehouse/raydata_spark_logs.db/spark_logs/metadata/00001-ba180b06-2e59-4133-9ea9-a2af29bb393b.metadata.json
# iceberg-warehouse/raydata_spark_logs.db/spark_logs/metadata/592550cf-a900-45c7-adf8-c5ab577236ad-m0.avro
```

### ë°ì´í„° ì²˜ë¦¬ í™•ì¸

**ì²˜ë¦¬ ìš”ì•½ í™•ì¸:**
```bash
# Submitter Pod ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
SUBMITTER_POD=$(kubectl get pods -n raydata --no-headers | grep spark-log-processing-job | grep -v head | grep -v worker | awk '{print $1}')

# ì²˜ë¦¬ ìš”ì•½ ë³´ê¸°
kubectl logs $SUBMITTER_POD -n raydata | grep -A5 "Processing Summary"

# ì˜ˆìƒ ì¶œë ¥:
# ğŸ¯ Processing Summary:
#   ğŸ“Š Total records processed: 647
#   âœ… Successful folders: 1
#   âŒ Failed folders: 0
#   âœ… Successfully processed: ['spark-1b3b78e281664cefb74ca64a1ed5a2d9']
```

### Iceberg í…Œì´ë¸” ì¿¼ë¦¬

**ì˜µì…˜ 1: AWS Athena ì‚¬ìš©**

1. AWS Athena ì½˜ì†” ì—´ê¸°
2. ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ: `raydata_spark_logs`
3. ì¿¼ë¦¬ ì‹¤í–‰:

```sql
-- í–‰ ìˆ˜ í™•ì¸
SELECT COUNT(*) as total_records
FROM spark_logs;

-- ìƒ˜í”Œ ë°ì´í„° ë³´ê¸°
SELECT
    timestamp,
    log_level,
    pod_name,
    spark_app_selector,
    message
FROM spark_logs
LIMIT 10;

-- ë¡œê·¸ ë ˆë²¨ ë¶„ì„
SELECT
    log_level,
    COUNT(*) as count
FROM spark_logs
WHERE log_level IS NOT NULL
GROUP BY log_level
ORDER BY count DESC;

-- Spark ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¡œê·¸ ë³´ê¸°
SELECT
    spark_app_selector,
    spark_app_name,
    COUNT(*) as log_count
FROM spark_logs
WHERE spark_app_selector IS NOT NULL
GROUP BY spark_app_selector, spark_app_name
ORDER BY log_count DESC;
```

**ì˜ˆìƒ ê²°ê³¼:**
```
total_records: 647

log_level    | count
-------------|------
INFO         | 520
WARN         | 85
ERROR        | 42

spark_app_selector                       | spark_app_name        | log_count
-----------------------------------------|----------------------|----------
spark-1b3b78e281664cefb74ca64a1ed5a2d9   | taxi-trip-ebs-pvc    | 647
```

**ì˜µì…˜ 2: PyIceberg ì‚¬ìš©**

ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±:

```python
#!/usr/bin/env python3
"""PyIcebergë¥¼ ì‚¬ìš©í•œ Iceberg í…Œì´ë¸” ë°ì´í„° ê²€ì¦."""

from pyiceberg.catalog import load_catalog
import pyarrow.compute as pc

# Glue ì¹´íƒˆë¡œê·¸ ë¡œë“œ
catalog = load_catalog(
    'glue',
    **{
        'type': 'glue',
        'glue.region': 'us-west-2',
        'warehouse': 's3://your-bucket/iceberg-warehouse'
    }
)

# í…Œì´ë¸” ë¡œë“œ
table = catalog.load_table('raydata_spark_logs.spark_logs')

# í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
print(f"Table: {table.name()}")
print(f"Location: {table.location()}")
print(f"Schema: {table.schema()}")
print(f"Snapshots: {len(table.snapshots())}")

# ìŠ¤ìº” ë° PyArrowë¡œ ë³€í™˜
arrow_table = table.scan().to_arrow()

print(f"\nğŸ“Š Total Records: {len(arrow_table)}")
print(f"ğŸ“‹ Columns: {arrow_table.num_columns}")
print(f"ğŸ’¾ Size: {arrow_table.nbytes / 1024:.2f} KB")

# ë¡œê·¸ ë ˆë²¨ ë¶„í¬ í‘œì‹œ
if 'log_level' in arrow_table.column_names:
    log_levels = pc.value_counts(arrow_table['log_level'])
    print(f"\nğŸ“ˆ Log Level Distribution:")
    for level in log_levels.to_pylist():
        print(f"  {level['values']}: {level['counts']}")

# ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
print(f"\nğŸ“ Sample Records (first 5):")
print(arrow_table.select(['timestamp', 'log_level', 'pod_name', 'message']).slice(0, 5).to_pandas())
```

ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
```bash
python verify_iceberg_data.py
```

**ì˜µì…˜ 3: DuckDB ì‚¬ìš©**

```python
import duckdb
from pyiceberg.catalog import load_catalog

# ì¹´íƒˆë¡œê·¸ ë° í…Œì´ë¸” ë¡œë“œ
catalog = load_catalog('glue', warehouse='s3://bucket/iceberg-warehouse/')
table = catalog.load_table('raydata_spark_logs.spark_logs')

# Arrowë¡œ ë³€í™˜ ë° DuckDBë¡œ ì¿¼ë¦¬
con = duckdb.connect()
arrow_table = table.scan().to_arrow()

# DuckDB í…Œì´ë¸”ë¡œ ë“±ë¡
con.register('spark_logs', arrow_table)

# SQL ì¿¼ë¦¬ ì‹¤í–‰
result = con.execute("""
    SELECT
        log_level,
        COUNT(*) as count,
        COUNT(DISTINCT pod_name) as unique_pods
    FROM spark_logs
    WHERE log_level IS NOT NULL
    GROUP BY log_level
    ORDER BY count DESC
""").fetchdf()

print(result)
```

### Pod Identity ì‚¬ìš© í™•ì¸

```bash
# Pod Identity ìê²© ì¦ëª… ì‚¬ìš© í™•ì¸
kubectl logs $SUBMITTER_POD -n raydata | grep "Pod Identity\|AWS credentials"

# ì˜ˆìƒ ì¶œë ¥:
# âœ… Using EKS Pod Identity credentials from http://169.254.170.23/v1/credentials
# âœ… PyArrow 19.0.1+ will automatically use Pod Identity for S3 access
```

### ë©”íƒ€ë°ì´í„° í…Œì´ë¸” í™•ì¸

ì‘ì—…ì€ ì²˜ë¦¬ ìƒíƒœë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ë„ ìƒì„±í•©ë‹ˆë‹¤:

```bash
# Athenaë¡œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì¿¼ë¦¬
SELECT
    spark_app_selector,
    status,
    records_processed,
    processing_start_time,
    processing_end_time,
    updated_at
FROM raydata_spark_logs.spark_logs_processing_metadata
ORDER BY updated_at DESC;

# ì˜ˆìƒ ì¶œë ¥:
# spark_app_selector                       | status    | records_processed | processing_start_time      | processing_end_time
# -----------------------------------------|-----------|-------------------|---------------------------|---------------------
# spark-1b3b78e281664cefb74ca64a1ed5a2d9   | SUCCESS   | 647               | 2025-10-22 16:48:19       | 2025-10-22 16:48:34
```

ì´ ë©”íƒ€ë°ì´í„°ëŠ” ì¦ë¶„ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤ - í›„ì† ì‹¤í–‰ì€ ì´ë¯¸ ì²˜ë¦¬ëœ í´ë”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.

## ì„±ëŠ¥ íŠœë‹

### Worker ìˆ˜ ì¡°ì •

```bash
# ë” ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë” ë§ì€ Worker
MIN_WORKERS="5"
MAX_WORKERS="20"
INITIAL_WORKERS="5"
```

### ë°°ì¹˜ í¬ê¸° íŠœë‹

```bash
# ë” í° ë°°ì¹˜ = ë” ì¢‹ì€ ì²˜ë¦¬ëŸ‰, ë” ë§ì€ ë©”ëª¨ë¦¬
BATCH_SIZE="50000"

# ë” ì‘ì€ ë°°ì¹˜ = ë” ì ì€ ë©”ëª¨ë¦¬, ë” ë§ì€ ì˜¤ë²„í—¤ë“œ
BATCH_SIZE="5000"
```

### ë¦¬ì†ŒìŠ¤ í• ë‹¹

`rayjob.yaml` ìˆ˜ì •:

```yaml
# ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì›Œí¬ë¡œë“œìš©
resources:
  requests:
    cpu: "8"
    memory: "32Gi"
  limits:
    cpu: "16"
    memory: "64Gi"
```

### Karpenter ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•

ì»´í“¨íŒ… ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ìš© NodePool ìƒì„±:

```yaml
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: ray-compute-optimized
spec:
  template:
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["c5.4xlarge", "c6i.4xlarge", "c7i.4xlarge"]
```

## ë¬¸ì œ í•´ê²°

### ì‘ì—…ì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ

```bash
# RayJob ìƒíƒœ í™•ì¸
kubectl describe rayjob spark-log-processing-job -n raydata

# Operator í™•ì¸
kubectl logs -n kuberay-operator deployment/kuberay-operator
```

### Worker Podê°€ Pending

```bash
# Karpenter í™•ì¸
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter --tail=100

# ë…¸ë“œ ìš©ëŸ‰ í™•ì¸
kubectl get nodes
kubectl describe node <node-name>
```

### S3 ì ‘ê·¼ ì˜¤ë¥˜

```bash
# Pod Identity í™•ì¸
kubectl describe sa raydata -n raydata

# Podì—ì„œ S3 ì ‘ê·¼ í…ŒìŠ¤íŠ¸
kubectl run -it --rm debug --image=amazon/aws-cli --serviceaccount=raydata -n raydata -- \
  s3 ls s3://your-bucket/spark-application-logs/
```

### Iceberg ì“°ê¸° ì‹¤íŒ¨

ì¼ë°˜ì ì¸ ë¬¸ì œ:
- **íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë°€ë„**: `downcast-ns-timestamp-to-us-on-write`ë¡œ ì´ë¯¸ ì²˜ë¦¬ë¨
- **ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜**: ì“°ê¸° ê°„ ì¼ê´€ëœ ìŠ¤í‚¤ë§ˆ ë³´ì¥
- **ì¹´íƒˆë¡œê·¸ ì ê¸ˆ**: ë™ì¼ í…Œì´ë¸” íŒŒí‹°ì…˜ì— ëŒ€í•œ ë™ì‹œ ì“°ê¸° ë°©ì§€

## ì •ë¦¬

ì‘ì—…ì€ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì •ë¦¬ë©ë‹ˆë‹¤:

```yaml
shutdownAfterJobFinishes: true
ttlSecondsAfterFinished: 600
```

ìˆ˜ë™ ì •ë¦¬:

```bash
# RayJob ì‚­ì œ
kubectl delete rayjob spark-log-processing-job -n raydata

# ConfigMap ì‚­ì œ
kubectl delete configmap spark-log-processor-code -n raydata
```

## ë‹¤ìŒ ë‹¨ê³„

- **ì²˜ë¦¬ ìŠ¤ì¼€ì¤„ë§**: ì£¼ê¸°ì  ì‹¤í–‰ì„ ìœ„í•´ Kubernetes CronJob ì‚¬ìš©
- **ëª¨ë‹ˆí„°ë§ ì¶”ê°€**: ì‹¤íŒ¨ì— ëŒ€í•œ Prometheus ì•ŒëŒ êµ¬ì„±
- **íŒŒí‹°ì…”ë‹ ìµœì í™”**: ë” ë‚˜ì€ ì¿¼ë¦¬ë¥¼ ìœ„í•´ ë‚ ì§œ ë˜ëŠ” app_idë¡œ íŒŒí‹°ì…˜
- **ì¦ë¶„ ì²˜ë¦¬**: ì¬ì²˜ë¦¬ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì²˜ë¦¬ëœ íŒŒì¼ ì¶”ì 

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Ray Data ë¬¸ì„œ](https://docs.ray.io/en/latest/data/data.html)
- [Apache Iceberg](https://iceberg.apache.org/)
- [AWS Glue Catalog](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html)
- [Karpenter](https://karpenter.sh/)
