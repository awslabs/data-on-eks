---
sidebar_position: 1
sidebar_label: Introduction
---

# Job Schedulers

Job schedulers are an essential component of many organizations' infrastructure, helping to automate and manage complex workflows. When deployed on Kubernetes, job schedulers can take advantage of the platform's features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability. Tools like **Apache Airflow**, **Argo Workflow**, and **Amazon MWAA** provide a simple and efficient way to manage and schedule jobs on a Kubernetes cluster.

These tools are well-suited for a wide range of use cases, including data pipelines, machine learning workflows, and batch processing. By leveraging the power of Kubernetes, organizations can simplify and automate the management of their job schedulers, freeing up resources to focus on other areas of the business. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running job schedulers in production.
The following are the most popular job scheduling tools used with data workloads.
This section provides deployment patterns for the following tools and examples to trigger Spark/ML jobs using these schedulers.

1. [Apache Airflow](https://airflow.apache.org/)
2. [Amazon Managed Workflows for Apache Airflow (MWAA)](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html)
3. [Argo Workflow](https://argoproj.github.io/workflows/)
4. [Prefect](https://www.prefect.io/)
5. [AWS Batch](https://aws.amazon.com/batch/)
