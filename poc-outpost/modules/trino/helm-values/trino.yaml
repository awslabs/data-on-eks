# Coordinator Node Memory:
# JVM Heap (maxHeapSize): 32G (100%)
# ├── Query Memory (maxMemoryPerNode): 22GB (~70%)
# ├── Heap Headroom: 9.6GB (30%)
# └── Total: 31.6GB < 32GB ✓

# Worker Node Memory
# JVM Heap (maxHeapSize): 89G (100%)
# ├── Query Memory (maxMemoryPerNode): 71GB (~70%)
# ├── Heap Headroom: 9.6GB
# └── Total: 80.6GB < 89GB ✓
---
image:
  repository: trinodb/trino
  tag: 447
  pullPolicy: IfNotPresent
server:
  workers: 1
  exchangeManager:
    name: filesystem
    baseDir: "s3://${exchange_bucket_id}"
  autoscaling:
    enabled: false
  config:
    #https:
      #enabled: true  # Disable HTTPS for simplicity
      #authenticationType: "OAUTH2"
    query:
      maxMemory: "280GB"  # Total memory across cluster for queries
      initialHashPartitions: 100  # Improved parallel processing
      maxStageRetries: 3
      maxExecutionTime: "24h"
      clientTimeout: "2h"
  coordinatorExtraConfig: |
    web-ui.authentication.type=FORM
    web-ui.enabled=true
    http-server.authentication.type=PASSWORD
    #http-server.http.enabled=true
    http-server.https.enabled=true
    http-server.process-forwarded=true
    http-server.https.port=8443
    http-server.https.keystore.path=/tmp/data/keystore.jks
    http-server.https.keystore.key=${trino_jks_keystore_password}
    #http-server.authentication.allow-insecure-over-http=true
    internal-communication.shared-secret=${trino_communication_encryption}
    #http-server.authentication.oauth2.oidc.discovery=true
    #http-server.authentication.oauth2.issuer=https://${cognito_user_pool_domain}.auth.${region}.amazoncognito.com
    #http-server.authentication.oauth2.client-id=${cpgnito_user_pool_client_id}
    #http-server.authentication.oauth2.client-secret=${cognito_user_pool_client_secret}
    #http-server.authentication.oauth2.callback-url=https://${trino_domain}/oidc-callback
    #http-server.authentication.oauth2.token-url=${cognito_user_pool_domain}.auth.${region}.amazoncognito.com/oauth2/token
    #http-server.authentication.oauth2.auth-url=${cognito_user_pool_domain}.auth.${region}.amazoncognito.com/oauth2/authorize
    #http-server.authentication.oauth2.scopes="openid email profile"
  workerExtraConfig: |
    internal-communication.shared-secret=${trino_communication_encryption}
service:
  type: ClusterIP
  port: 8080

coordinator:
  additionalConfigFiles:
    password-authenticator.properties: |
      password-authenticator.name=file
      file.password-file=/etc/trino/passwords.db
    passwords.db: |
      trino:${trino_user_password}
  additionalVolumes:
    - name: trino-tls
      secret:
        secretName: "${trino_tls}" # The secret containing the TLS certificate and key for Trino
    - name: temp-data
      emptyDir: {}
  additionalVolumeMounts:
    - name: trino-tls
      mountPath: /etc/trino/trino-tls
      readOnly: true
    - name: temp-data
      mountPath: /tmp/data
  additionalExposedPorts:
    https:
      servicePort: 8443
      name: https
      port: 8443
      protocol: TCP


  jvm:
    maxHeapSize: "32G"  # ~80% of container memory
    extraArguments:
      - "-XX:+UseG1GC"
      - "-XX:G1HeapRegionSize=32M"
      - "-XX:+UseGCOverheadLimit"
      - "-XX:+ExitOnOutOfMemoryError"
      - "-XX:ReservedCodeCacheSize=256M"
      - "-Djdk.attach.allowAttachSelf=true"
      - "-XX:+UseContainerSupport"
  config:
    query:
      #maxMemoryPerNode + (maxHeapSize * 0.3) < maxHeapSize
      maxMemoryPerNode: "20GB"  # ~70% of maxHeapSize
      minWorkers: 1
      initialHashPartitions: 100
  resources:
    requests:
      cpu: "4000m"     # Reduced CPU request
      memory: 20Gi     # Reduced memory request
    limits:
      cpu: "6000m"     # Higher limit for spikes
      memory: 40Gi
  annotations:
    karpenter.sh/do-not-disrupt: "true"
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          component: coordinator

worker:
  labels:
    kyverno.io/skip-affinity: "true"
  jvm:
    maxHeapSize: "45G"  # ~80% of container memory (110Gi)
    extraArguments:
      - "-XX:+UseG1GC"
      - "-XX:G1HeapRegionSize=32M"
      - "-XX:+UseGCOverheadLimit"
      - "-XX:+ExitOnOutOfMemoryError"
      - "-XX:ReservedCodeCacheSize=256M"
      - "-Djdk.attach.allowAttachSelf=true"
      - "-XX:+UseContainerSupport"
  config:
    query:
      maxMemoryPerNode: "35GB"  # ~80% of maxHeapSize
  resources:
    requests:
      cpu: "6000m"  # Leave 3000m for system/DaemonSets
      memory: 50Gi  # Leave 16Gi for system/DaemonSets
    limits:
      cpu: "7000m"
      memory: 50Gi
  nodeSelector:
    NodePool: karpenter
    karpenter.sh/capacity-type: "on-demand"
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          component: worker

additionalConfigProperties:
  - "retry-policy=TASK"
  - "exchange.compression-enabled=true"
  - "query.remote-task.max-error-duration=1m"
  - "query.max-hash-partition-count=100"   # Updated from query.hash-partition-count
  - "spill-enabled=true"                   # Updated from experimental.spill-enabled
  - "spiller-spill-path=/tmp/spill"        # Chagne this to SSD mount for faster
  - "memory.heap-headroom-per-node=9.6GB"
  - "optimizer.join-reordering-strategy=AUTOMATIC"  # Updated from join-reordering-strategy
  - "query.max-history=100"
  - "query.client.timeout=30m"
  - "sink.max-buffer-size=1GB"

additionalExchangeManagerProperties:
  - "exchange-manager.name=filesystem"
  - "exchange.base-directories=s3://${exchange_bucket_id}"
  - "exchange.s3.region=${region}"
  - "exchange.s3.iam-role=${irsa_arn}"
  - "exchange.s3.max-error-retries=10"
  - "exchange.s3.upload.part-size=64MB"
  - "exchange.s3.storage-class=OUTPOSTS"
  - "exchange.s3.endpoint=https://s3-outposts.us-west-2.amazonaws.com"

connector:
  name: iceberg

catalogs:
  iceberg: |
    connector.name=iceberg
    iceberg.catalog.type=jdbc
    iceberg.jdbc-catalog.catalog-name=iceberg
    iceberg.jdbc-catalog.driver-class=org.postgresql.Driver
    iceberg.jdbc-catalog.connection-url=${trino_db_url}
    iceberg.jdbc-catalog.connection-user=${trino_db_user}
    iceberg.jdbc-catalog.connection-password=${trino_db_password}
    iceberg.jdbc-catalog.default-warehouse-dir=s3://${bucket_id}
    fs.native-s3.enabled=true
    s3.endpoint=https://s3-outposts.${region}.amazonaws.com
    s3.path-style-access=true

serviceAccount:
  create: true
  name: ${sa}
###################################
# Ingress configuration
###################################
ingress:
  # Enable web ingress resource
  enabled: false

jmx:

  enabled: true
  registryPort: 9080
  serverPort: 9081
  exporter:
    # jmx.exporter.enabled -- Set to true to export JMX Metrics via HTTP for [Prometheus](https://github.com/prometheus/jmx_exporter) consumption
    enabled: true
    image: bitnami/jmx-exporter:latest
    pullPolicy: Always
    port: 5556
    configProperties: |-
      hostPort: localhost:{{- .Values.jmx.registryPort }}
      startDelaySeconds: 0
      ssl: false
      lowercaseOutputName: false
      lowercaseOutputLabelNames: false
      whitelistObjectNames: ["trino.execution:name=QueryManager","trino.execution:name=ClusterSizeMonitor","trino.execution:name=SqlTaskManager","trino.execution.executor:name=TaskExecutor","trino.memory:name=ClusterMemoryManager","java.lang:type=Runtime","trino.memory:type=ClusterMemoryPool,name=general","java.lang:type=Memory","trino.memory:type=MemoryPool,name=general"]
      autoExcludeObjectNameAttributes: true
      excludeObjectNameAttributes:
        "java.lang:type=OperatingSystem":
          - "ObjectName"
        "java.lang:type=Runtime":
          - "ClassPath"
          - "SystemProperties"
      rules:
      - pattern: ".*"
    resources:
      limits:
        cpu: 200m
        memory: 512Mi
      requests:
        cpu: 200m
        memory: 512Mi

serviceMonitor:
  enabled: true
  labels:
    release: kube-prometheus-stack
  interval: "15s"
  coordinator:
    enabled: true
    labels:
      release: kube-prometheus-stack
  worker:
    enabled: true
    labels:
      release: kube-prometheus-stack

initContainers:
  worker:
    []
  coordinator:
  - name: import-ca-cert
    image: eclipse-temurin:17-jdk
    command:
      - /bin/bash
      - -c
      - |
        set -e
        # Chemins des fichiers montés
        CRT_PATH=/etc/trino/trino-tls/tls.crt
        KEY_PATH=/etc/trino/trino-tls/tls.key
        P12_PATH=/tmp/data/keystore.p12
        JKS_PATH=/tmp/data/keystore.jks
        ALIAS=trino
        STOREPASS="${trino_jks_keystore_password}"
        
        # Générer PKCS12
        openssl pkcs12 -export -in $CRT_PATH -inkey $KEY_PATH -out $P12_PATH -name $ALIAS -passout pass:$STOREPASS

        # Convertir PKCS12 en JKS
        keytool -importkeystore -srckeystore $P12_PATH -srcstoretype PKCS12 -srcstorepass $STOREPASS -destkeystore $JKS_PATH -deststoretype JKS -deststorepass $STOREPASS
    volumeMounts:
      - name: trino-tls
        mountPath: /etc/trino/trino-tls
      - name: temp-data
        mountPath: /tmp/data