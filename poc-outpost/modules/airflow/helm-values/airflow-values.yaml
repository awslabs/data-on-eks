########################################
## CONFIG | Airflow Configs
########################################

# Default security context for airflow (deprecated, use `securityContexts` instead)
securityContext:
  fsGroup: 65534

###################################
# Ingress configuration
###################################
ingress:
  # Configs for the Ingress of the web Service
  apiServer:
    # Enable web ingress resource
    enabled: true

    # Annotations for the web Ingress
    annotations:
      alb.ingress.kubernetes.io/group.name: dataengineering
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/scheme: internet-facing # Private Load Balancer can only be accessed within the VPC
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
      alb.ingress.kubernetes.io/healthcheck-path: '/health'
      # Enable the following if you have public/internal domain e.g., https://mycompany.com/
      # alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'
      # alb.ingress.kubernetes.io/certificate-arn: "arn:aws:acm:....................."

    # The path for the web Ingress
    path: "/"

    # The pathType for the above path (used only with Kubernetes v1.19 and above)
    pathType: "Prefix"

    # The Ingress Class for the web Ingress (used only with Kubernetes v1.19 and above)
    ingressClassName: "alb"

###################################
# Airflow executor
###################################
# One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor
executor: "KubernetesExecutor"

###################################
# Airflow database
###################################
data:
  # Otherwise pass connection values in
  metadataConnection:
    user: ${airflow_db_user}
    pass: ${airflow_db_pass}
    protocol: postgresql
    host: ${airflow_db_host}
    port: 5432
    db: ${airflow_db_name}
    sslmode: disable

postgresql:
  enabled: false
###################################
# Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg
###################################
webserverSecretKeySecretName: ${webserver_secret_name}

###################################
# Airflow Worker Config
###################################
workers:
  persistence:
    # Enable persistent volumes
    enabled: false
  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${worker_service_account}
  extraContainers:
    - name: s3-sync
      image: amazon/aws-cli:latest
      command: [ "sh", "-c", "aws s3 sync --delete s3://${s3_bucket_name}/dags ${airflow_dag_path};" ]
      volumeMounts:
        - name: dags
          mountPath: ${airflow_dag_path}
  extraVolumeMounts:
    - name: dags
      mountPath: ${airflow_dag_path}
  extraVolumes:
    - name: dags
      emptyDir: { }
###################################
# Airflow scheduler settings
###################################
scheduler:
  # Airflow 2.0 allows users to run multiple schedulers,
  # However this feature is only recommended for MySQL 8+ and Postgres
  replicas: 2

  # Scheduler pod disruption budget
  podDisruptionBudget:
    enabled: true

    # PDB configuration
    config:
      # minAvailable and maxUnavailable are mutually exclusive
      maxUnavailable: 1

  resources:
    limits:
     cpu: 500m
     memory: 1024Mi
    requests:
     cpu: 500m
     memory: 1024Mi

  # This setting tells kubernetes that its ok to evict
  # when it wants to scale a node down.
  safeToEvict: false
  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${scheduler_service_account}

#extraVolumes is required for DAG GitSync #https://github.com/apache/airflow/issues/27476
  extraVolumes:
    - name: git-sync-ssh-key
      secret:
        secretName: airflow-ssh-secret
    - name: dags
      emptyDir: { }
  extraVolumeMounts:
    - name: dags
      mountPath: ${airflow_dag_path}
  extraContainers:
    - name: s3-sync
      image: amazon/aws-cli:latest
      command: [ "sh", "-c", "while true; do aws s3 sync --delete s3://${s3_bucket_name}/dags ${airflow_dag_path}; sleep 60; done" ]
      volumeMounts:
        - name: dags
          mountPath: ${airflow_dag_path}

###################################
# Airflow webserver settings
###################################
webserver:
  # Number of webservers
  replicas: 2

  # Webserver pod disruption budget
  podDisruptionBudget:
    enabled: true

    # PDB configuration
    config:
      # minAvailable and maxUnavailable are mutually exclusive
      maxUnavailable: 1

  resources:
    limits:
      cpu: 1000m
      memory: 4Gi
    requests:
      cpu: 1000m
      memory: 4Gi

  # Create initial user.
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin

  service:
    #type: ClusterIP
    type: NodePort

  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${webserver_service_account}

#  extraVolumeMounts:
#    - name: config
#      mountPath: /opt/irflow/webserver_config.py
#      subPath: webserver_config.py
#  extraVolumes:
#    - name: config
#      configMap:
#        name: airflow-webserver-config
###################################
# PgBouncer settings
###################################
pgbouncer:
  # Enable PgBouncer
  enabled: true

###################################
# Config settings to go into the mounted airflow.cfg
###################################
config:
  core:
    dags_folder: "${airflow_dag_path}"
    #dags_folder: "/tmp/"
    # This is ignored when used with the official Docker image
    load_examples: 'True'
    executor: '{{ .Values.executor }}'
    remote_logging: 'True'
  logging:
    remote_logging: 'True'
    logging_level: 'INFO'
    colored_console_log: 'True'
    remote_base_log_folder: "s3://${s3_bucket_name}/airflow-logs/"
    # aws_s3_conn is the name of the connection that needs to be created using Airflow admin UI once the deployment is complete
    # Steps can be seen in the docs link here -> https://github.com/apache/airflow/issues/25322
    remote_log_conn_id: 'aws_s3_conn'
    delete_worker_pods: 'False'
    encrypt_s3_logs: 'True'


###################################
# Git sync
###################################
dags:
  persistence:
    # Enable persistent volume for storing dags
    enabled: false

  gitSync:
    enabled: false


dagProcessor:
  extraContainers:
    - name: s3-sync
      image: amazon/aws-cli:latest
      command: ["sh", "-c", "while true; do aws s3 sync --delete s3://${s3_bucket_name}/dags ${airflow_dag_path}; sleep 60; done"]
      volumeMounts:
        - name: dags
          mountPath: ${airflow_dag_path}
  extraVolumes:
    - name: dags
      emptyDir: {}
  extraVolumeMounts:
    - name: dags
      mountPath: ${airflow_dag_path}
  serviceAccount:
    create: false
    name: ${dag_processor_service_account}
