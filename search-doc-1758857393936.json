{"searchDocs":[{"title":"Amazon EMR on Amazon EKS provides up to 61% lower costs and up to 68% performance improvement for Spark workloads","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/emr-on-eks","content":"","keywords":"","version":"Next"},{"title":"How does Amazon EMR on EKS reduce cost and improve performance?​","type":1,"pageTitle":"Amazon EMR on Amazon EKS provides up to 61% lower costs and up to 68% performance improvement for Spark workloads","url":"/data-on-eks/docs/benchmarks/emr-on-eks#how-does-amazon-emr-on-eks-reduce-cost-and-improve-performance","content":" The EMR runtime for Spark is a performance-optimized runtime for Apache Spark that is 100% API compatible with open-source Apache Spark. It’s enabled by default with Amazon EMR on EKS. It helps run Spark workloads faster, leading to lower running costs. It includes multiple performance optimization features, such as Adaptive Query Execution (AQE), dynamic partition pruning, flattening scalar subqueries, bloom filter join, and more.  In addition to the cost benefit brought by the EMR runtime for Spark, Amazon EMR on EKS can take advantage of other AWS features to further optimize cost. For example, you can run Amazon EMR on EKS jobs on Amazon Elastic Compute Cloud (Amazon EC2) Spot Instances, providing up to 90% cost savings when compared to On-Demand Instances. Also, Amazon EMR on EKS supports Arm-based Graviton EC2 instances, which creates a 15% performance improvement and up to 30% cost savings when compared a Graviton2-based M6g to M5 instance type.  The recent graceful executor decommissioning feature makes Amazon EMR on EKS workloads more robust by enabling Spark to anticipate Spot Instance interruptions. Without the need to recompute or rerun impacted Spark jobs, Amazon EMR on EKS can further reduce job costs via critical stability and performance improvements.  Additionally, through container technology, Amazon EMR on EKS offers more options to debug and monitor Spark jobs. For example, you can choose Spark History Server, Amazon CloudWatch, or Amazon Managed Prometheus and Amazon Managed Grafana (for more details, refer to the Monitoring and Logging workshop). Optionally, you can use familiar command line tools such as kubectl to interact with a job processing environment and observe Spark jobs in real time, which provides a fail-fast and productive development experience.  Amazon EMR on EKS supports multi-tenant needs and offers application-level security control via a job execution role. It enables seamless integrations to other AWS native services without a key-pair set up in Amazon EKS. The simplified security design can reduce your engineering overhead and lower the risk of data breach. Furthermore, Amazon EMR on EKS handles security and performance patches so you can focus on building your applications.  ","version":"Next","tagName":"h2"},{"title":"Benchmarking​","type":1,"pageTitle":"Amazon EMR on Amazon EKS provides up to 61% lower costs and up to 68% performance improvement for Spark workloads","url":"/data-on-eks/docs/benchmarks/emr-on-eks#benchmarking","content":" This post provides an end-to-end Spark benchmark solution so you can get hands-on with the performance test process. The solution uses unmodified TPC-DS data schema and table relationships, but derives queries from TPC-DS to support the Spark SQL test case. It is not comparable to other published TPC-DS benchmark results.  Key concepts Transaction Processing Performance Council-Decision Support (TPC-DS) is a decision support benchmark that is used to evaluate the analytical performance of big data technologies. Our test data is a TPC-DS compliant dataset based on the TPC-DS Standard Specification, Revision 2.4 document, which outlines the business model and data schema, relationship, and more. As the whitepaper illustrates, the test data contains 7 fact tables and 17 dimension tables, with an average of 18 columns. The schema consists of essential retailer business information, such as customer, order, and item data for the classic sales channels: store, catalog, and internet. This source data is designed to represent real-world business scenarios with common data skews, such as seasonal sales and frequent names. Additionally, the TPC-DS benchmark offers a set of discrete scaling points (scale factors) based on the approximate size of the raw data. In our test, we chose the 3 TB scale factor, which produces 17.7 billion records, approximately 924 GB compressed data in Parquet file format.  Test approach A single test session consists of 104 Spark SQL queries that were run sequentially. To get a fair comparison, each session of different deployment types, such as Amazon EMR on EKS, was run three times. The average runtime per query from these three iterations is what we analyze and discuss in this post. Most importantly, it derives two summarized metrics to represent our Spark performance:  Total execution time – The sum of the average runtime from three iterations Geomean – The geometric mean of the average runtime Test results In the test result summary (see the following figure), we discovered that the Amazon EMR-optimized Spark runtime used by Amazon EMR on EKS is approximately 2.1 times better than the open-source Spark on Amazon EKS in geometric mean and 3.5 times faster by the total runtime.    The following figure breaks down the performance summary by queries. We observed that EMR runtime for Spark was faster in every query compared to open-source Spark. Query q67 was the longest query in the performance test. The average runtime with open-source Spark was 1019.09 seconds. However, it took 150.02 seconds with Amazon EMR on EKS, which is 6.8 times faster. The highest performance gain in these long-running queries was q72—319.70 seconds (open-source Spark) vs. 26.86 seconds (Amazon EMR on EKS), a 11.9 times improvement.    For full blog link ","version":"Next","tagName":"h2"},{"title":"Running Spark Benchmark Tests on Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark","content":"","keywords":"","version":"Next"},{"title":"Deploying the Benchmark toolkit​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#deploying-the-benchmark-toolkit","content":" In this example, you will provision the following resources required to run Spark Jobs with open source Spark Operator.  This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC.  Creates a new sample VPC, 2 Private Subnets, 2 Public Subnets, and 2 subnets in the RFC6598 space (100.64.0.0/10) for EKS Pods.Creates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with Managed Node Groups for benchmarking and core services, and Karpenter NodePools for Spark workloads.Deploys Metrics server, Spark-operator, Apache Yunikorn, Karpenter, Cluster Autoscaler, Grafana, AMP and Prometheus server.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraform  ","version":"Next","tagName":"h3"},{"title":"Deploy​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#deploy","content":" Clone the repository.  git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks export DOEKS_HOME=$(pwd)   If DOEKS_HOME is ever unset, you can always set it manually using export DATA_ON_EKS=$(pwd) from your data-on-eks directory.  Navigate into the following directory and run install.sh script.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator chmod +x install.sh ./install.sh   Now create an S3_BUCKET variable that holds the name of the bucket created during the install. This bucket will be used in later examples to store output data. If S3_BUCKET is ever unset, you can run the following commands again.  export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server) echo $S3_BUCKET   ","version":"Next","tagName":"h3"},{"title":"Create the Test Dataset for Running the TPCDS Benchmark​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#create-the-test-dataset-for-running-the-tpcds-benchmark","content":" The Benchmark requires an S3 bucket with input data to query and save the results back to. If you don't have a data set in S3 you can use this same cluster to run the Data Generation job to create one.  Once you have an S3 bucket with the example data set, you can run the benchmark Job  ","version":"Next","tagName":"h3"},{"title":"Running the TPCDS Benchmark​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#running-the-tpcds-benchmark","content":" ","version":"Next","tagName":"h2"},{"title":"Scale up the worker nodes​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#scale-up-the-worker-nodes","content":" For these benchmarks we are not measuring the scaling speed but are focusing on the performance of the Spark SQL queries and the job runtime. To ensure the job is not interrupted by scaling activities or Spot interruptions we recommend using a Managed Node Group for the benchmarks, and scaling the capacity up before submitting the jobs  The blueprint creates two Managed Node Groups that we use for these benchmarks:  spark_benchmark_ebs - This nodegroup is configured for instances without NVMe storage such as r6g or c5spark_benchmark_ssd - This nodegroup will setup a RAID over NVMe devices available on the instances. This is perfect for instances with NVMe storage like r6gd, and c5d. These nodegroups are scaled to 0 by default to save on costs, but you can configure the instance type you would like to benchmark on and then set the min_size and desired_size for the node group.  tip The number of nodes required varies based on the size of the instance and the resource requests of the executot Pods used in the benchmark. Currently the benchmark requests 36 executors, each requesting 5vCPU and 26Gi memory, for a total of 180vCPU and 936Gi memory. This will fit on six r6g.12xlarge instances. You can compare the benchmark manifest against the instance types you'd like to use to find the required number of EC2 instances.  ","version":"Next","tagName":"h3"},{"title":"Set the S3 Bucket for input/output​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#set-the-s3-bucket-for-inputoutput","content":" You will need to replace the &lt;S3_BUCKET&gt; placeholders in the benchmark file with the name of the bucket created earlier. You can get that value by running echo $S3_BUCKET.  To do this automatically you can run the following, which will create a .old backup file and do the replacement for you.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/benchmark sed -i.old s/\\&lt;S3_BUCKET\\&gt;/${S3_BUCKET}/g ./tpcds-benchmark-1t-ebs.yaml   ","version":"Next","tagName":"h3"},{"title":"Submit the Benchmark Job​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#submit-the-benchmark-job","content":" Then to begin the data generation execute the command below   kubectl apply -f tpcds-benchmark-1t-ebs.yaml   Once you apply the tpcds-benchmark-1t-ebs.yaml manifest, you should see the the driver and executor Pods coming up. It takes about an hour to finish the execution of a single iteration of the benchmark queries.  You can monitor the status of the job by checking the Spark driver Pod execution status and logs  kubectl get pod -n spark-team-a   Output:  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES benchmark-exec-ebs-exec-1 1/1 Running 0 75s 100.64.251.188 ip-100-64-219-156.ec2.internal &lt;none&gt; &lt;none&gt; benchmark-exec-ebs-exec-10 1/1 Running 0 73s 100.64.213.1 ip-100-64-146-124.ec2.internal &lt;none&gt; &lt;none&gt; ... benchmark-exec-ebs-exec-8 1/1 Running 0 74s 100.64.202.23 ip-100-64-219-156.ec2.internal &lt;none&gt; &lt;none&gt; benchmark-exec-ebs-exec-9 1/1 Running 0 73s 100.64.238.20 ip-100-64-175-12.ec2.internal &lt;none&gt; &lt;none&gt; tpcds-benchmark-1tb-ebs-driver 1/1 Running 0 2m33s 100.64.228.162 ip-100-64-213-174.ec2.internal &lt;none&gt; &lt;none&gt;   The benchmark is also configured to export metrics and logs so you can review the benchmark using the Spark Observability tools explained here.  To get an idea how far along the benchmark is, you can use the Spark Web UI to review which query is currently being executed. Port forward to the Benchmark Driver to see the UI:  kubectl port-forward -n spark-team-a service/tpcds-benchmark-1tb-ebs-ui-svc 4040:4040   Then open browser and enter localhost:4040. You can review the jobs that are running and that have completed. The benchmark will run the SQL queries sequentially so you can estimate how far along the job is by checking the query number.    Once the benchmark has completed you can scale the Nodgroup back to zero to save costs, and you can delete the remaining SparkApplication with the commands below:  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/benchmark kubectl delete -f tpcds-benchmark-1t-ebs.yaml   ","version":"Next","tagName":"h3"},{"title":"Reviewing the Results​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#reviewing-the-results","content":" When the benchmark completes it will place the results from the benchmark in the S3 bucket specified in the manifest file (default: s3a://&lt;S3_BUCKET&gt;/TPCDS-TEST-1T-RESULT). You can navigate to the S3 console for the corresponding S3 bucket and enter the directory:    When you enter the results directory you will see a list of folders which correspond to the timestamps the job was run:    You can find the latest result by selecting the timestamp that's largest, or find the folder that corresponds to the time of your test. Inside this folder you will see a file with a name like part-00000-000000000-0000-0000-0000-000000000-0000.json, this file includes the full spark configuration used for the job.  Inside the subfolder named summary.csv, the part-00000-000000000-0000-0000-0000-000000000-0000.csv file includes the results of the benchmark.  If you open this csv file you will see 4 columns of data which show the time taken to process each query. The file does not include column headers, the columns from left to right are:  the TPCDS Query numberthe median time that it took to process that querythe minimum time that it took to process that querythe maximum time that it took to process that query  tip If the benchmark ran for a single iiteration (as is the default) then all three columns will display the same times.  This image shows the output for 3 iterations with column headers added for clarity:  ","version":"Next","tagName":"h2"},{"title":"Cost Considerations​","type":1,"pageTitle":"Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/running-the-benchmark#cost-considerations","content":" When utilizing c5d instances for data generation, it's important to keep cost implications in mind. These compute-optimized instances with local NVMe storage offer high performance but can be more expensive than standard c5 instances. To optimize costs, it's crucial to carefully monitor usage and scale resources appropriately. The local NVMe storage provides fast I/O, but data persistence is not guaranteed, so you should factor in the cost of data transfer and backup solutions. Spot instances can offer significant savings for interruptible workloads. Additionally, reserving instances for long-term, predictable usage can lead to substantial discounts. Also, it's essential to terminate these instances when they're no longer needed by adjusting the nodegroup's minimum and desired size to 0. This practice helps avoid unnecessary costs from idle resources.  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment  This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation","content":"","keywords":"","version":"Next"},{"title":"Deploying the data generation toolkit​","type":1,"pageTitle":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation#deploying-the-data-generation-toolkit","content":" In this example, you will provision the following resources required to run Spark Jobs with open source Spark Operator.  This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC.  Creates a new sample VPC, 2 Private Subnets, 2 Public Subnets, and 2 subnets in the RFC6598 space (100.64.0.0/10) for EKS Pods.Creates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with Managed Node Groups for benchmarking and core services, and Karpenter NodePools for Spark workloads.Deploys Metrics server, Spark-operator, Apache Yunikorn, Karpenter, Cluster Autoscaler, Grafana, AMP and Prometheus server.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraform  ","version":"Next","tagName":"h3"},{"title":"Deploy​","type":1,"pageTitle":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation#deploy","content":" Clone the repository.  git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks export DOEKS_HOME=$(pwd)   If DOEKS_HOME is ever unset, you can always set it manually using export DATA_ON_EKS=$(pwd) from your data-on-eks directory.  Export the following environment variables to set the minimum and desired number of ssd enabled c5d12xlarge instances. In our tests, we've set both of these to 6 based on the size of the dataset. Please adjust the number of instances as per your requirement and the size of the dataset you plan to run.  export TF_VAR_spark_benchmark_ssd_min_size=6 export TF_VAR_spark_benchmark_ssd_desired_size=6   Note : If you don't have access to c5d instances, feel free to use other EC2 instances that are equipped with local NVMe-based SSD block level storage. NVMe-based SSD instance storage enabled EC2 instances are a great fit for running the Spark benchmark data generation toolkit.  Navigate into the following directory and run install.sh script.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator chmod +x install.sh ./install.sh   Now create an S3_BUCKET variable that holds the name of the bucket created during the install. This bucket will be used in later examples to store output data. If S3_BUCKET is ever unset, you can run the following commands again.  export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_history_server) echo $S3_BUCKET   ","version":"Next","tagName":"h3"},{"title":"Generating Test Dataset for Running the TPCDS Benchmark​","type":1,"pageTitle":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation#generating-test-dataset-for-running-the-tpcds-benchmark","content":" In order to generate the dataset for TPCDS benchmark tests, you will need to configure the S3 bucket name in the data generation manifest.  You will need to replace the &lt;S3_BUCKET&gt; placeholders in the benchmark file with the name of the bucket created earlier. You can get that value by running echo $S3_BUCKET.  To do this automatically you can run the following, which will create a .old backup file and do the replacement for you.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/benchmark sed -i.old s/\\&lt;S3_BUCKET\\&gt;/${S3_BUCKET}/g ./tpcds-benchmark-data-generation-1t.yaml   Then to begin the data generation execute the command below   kubectl apply -f tpcds-benchmark-data-generation-1t.yaml   Once you apply the tpcds-benchmark-data-generation-1t.yaml manifest, you should see the the driver and executor Pods coming up. It takes about an hour to finish the execution of the test data generation script. Once the execution is completed, you can see go into the AWS S3 console and validate the bucket size.  Navigate to the S3 bucket that got created as part of running the blueprint. Tick the checkbox besides the folder named TPCDS-TEST-1TB and click on Actions dropdown and then click on Calculate total size option as shown below.    For our dataset, the total size is 310 GB.    Once you go inside the TPCDS-TEST-1TB folder, you should see lot of subfolders that got generated (as shown below).    Each subfolder should have a .parquet file inside it that contains the generated data.    Also, check the Spark driver Pod execution status and logs to see if there are any errors.  kubectl get pod -n spark-team-a   Output:  NAME READY STATUS RESTARTS AGE tpcds-data-generation-1tb-driver 0/1 Completed 0 59m   The log snippet of the tpcds-data-generation-1tb-driver pod should look like below  Driver Pod Log Snippet 👈  ","version":"Next","tagName":"h2"},{"title":"Cost Considerations​","type":1,"pageTitle":"Data Generation for Running Spark Benchmark Tests on Amazon EKS","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/data-generation#cost-considerations","content":" When utilizing c5d instances for data generation, it's important to keep cost implications in mind. These compute-optimized instances with local NVMe storage offer high performance but can be more expensive than standard c5 instances. To optimize costs, it's crucial to carefully monitor usage and scale resources appropriately. The local NVMe storage provides fast I/O, but data persistence is not guaranteed, so you should factor in the cost of data transfer and backup solutions. Spot instances can offer significant savings for interruptible workloads. Additionally, reserving instances for long-term, predictable usage can lead to substantial discounts. Also, it's essential to terminate these instances when they're no longer needed by adjusting the nodegroup's minimum and desired size to 0. This practice helps avoid unnecessary costs from idle resources.  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment  This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Introduction to Spark Benchmarks on Amazon EKS 🚀","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark","content":"","keywords":"","version":"Next"},{"title":"📊 TPC-DS Benchmark for Spark​","type":1,"pageTitle":"Introduction to Spark Benchmarks on Amazon EKS 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark#-tpc-ds-benchmark-for-spark","content":" The TPC-DS benchmark is an industry-standard benchmark created by the Transaction Processing Performance Council (TPC) to evaluate the performance of decision support systems. It is widely used to assess the efficiency of SQL-based data processing engines in handling complex analytical queries. Databricks has optimized TPC-DS for Apache Spark, making it highly relevant for evaluating Spark workloads in cloud and on-premise environments.  The TPC-DS benchmark tests cover a range of complex, data-intensive queries that simulate realistic decision support systems, making it ideal for assessing Spark's performance with structured and semi-structured data.  📈 Some Details on TPC-DS Benchmarks for Spark  The TPC-DS benchmark for Spark on Databricks includes the following features and considerations:  Dataset Size: TPC-DS provides standardized datasets ranging from a few gigabytes to multiple terabytes, allowing users to test Spark's scalability.Query Suite: The benchmark includes 99 queries that simulate real-world decision support scenarios. These queries test Spark's ability to handle joins, aggregations, and complex nested queries efficiently.Optimization Techniques: Databricks has implemented various Spark-specific optimizations to improve TPC-DS query performance, such as Catalyst Optimizer, Adaptive Query Execution (AQE), and optimized shuffle operations.Execution Environment: The benchmark tests typically run in a distributed environment and cloud platform (such as Amazon EKS), leveraging optimized runtime for Apache Spark.  Note: The results of TPC-DS benchmarks on Spark can vary significantly depending on cluster configurations, instance types, and Spark settings.  ","version":"Next","tagName":"h2"},{"title":"✨ Why We Need the Benchmark Tests​","type":1,"pageTitle":"Introduction to Spark Benchmarks on Amazon EKS 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark#-why-we-need-the-benchmark-tests","content":" Benchmark tests are crucial for the following reasons:  Performance Evaluation: They provide an objective measure to evaluate and compare the speed, efficiency, and scalability of Spark engines on various cloud and hardware configurations. Optimization Insights: By analyzing benchmark results, users can gain insights into specific areas where Spark might be optimized to handle large-scale, complex analytical queries. Infrastructure Comparison: Running TPC-DS on Spark on Amazon EKS allows teams to make data-driven decisions on the best platforms for their data workloads. Operational Readiness: Ensures that the Spark setup is ready to handle large datasets and complex analytical tasks, reducing risks in production environments.  ","version":"Next","tagName":"h3"},{"title":"🎁 What We Offer​","type":1,"pageTitle":"Introduction to Spark Benchmarks on Amazon EKS 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark#-what-we-offer","content":" With Data-on-EKS blueprints, we offer  Ready to be deployed Infrastructure-as-Code(Terraform) templatesBenchmark Test Data Generation ToolkitBenchmark Execution Toolkit  ","version":"Next","tagName":"h3"},{"title":"💰 A Note on Cost​","type":1,"pageTitle":"Introduction to Spark Benchmarks on Amazon EKS 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark#-a-note-on-cost","content":" To minimize costs, we recommend terminating the C5d instances once the benchmark data generation toolkit completes execution and the data has been successfully stored in the S3 bucket. This approach ensures that resources are only used when necessary, helping to keep expenses under control.  ","version":"Next","tagName":"h3"},{"title":"🔗 Additional Resources​","type":1,"pageTitle":"Introduction to Spark Benchmarks on Amazon EKS 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/spark-operator-eks-benchmark#-additional-resources","content":" TPCDS Specification ","version":"Next","tagName":"h2"},{"title":"EMR on EKS Best Practices","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/analytics/emr-on-eks","content":"","keywords":"","version":"Next"},{"title":"EMR Containers Best Practices Guides​","type":1,"pageTitle":"EMR on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/emr-on-eks#emr-containers-best-practices-guides","content":" Amazon EMR on Amazon EKS enables you to submit Apache Spark jobs on demand on Amazon Elastic Kubernetes Service (EKS) without provisioning clusters. With EMR on EKS, you can consolidate analytical workloads with your other Kubernetes-based applications on the same Amazon EKS cluster to improve resource utilization and simplify infrastructure management.  This link provides the best practices and templates to get started with Amazon EMR on EKS. We publish this guide on GitHub so we could iterate the content quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community.  Checkout the EMR on EKS Best practices GitHub docs here  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"EMR on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/emr-on-eks#architecture","content":" The following diagram illustrates the solution architecture Amazon EMR on EKS.   ","version":"Next","tagName":"h3"},{"title":"Kubeflow Spark Operator Benchmarks 🚀","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark","content":"","keywords":"","version":"Next"},{"title":"✨ Why We Need the Benchmark Tests​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#-why-we-need-the-benchmark-tests","content":" Benchmark testing is a critical step in assessing the efficiency and reliability of the Spark Operator when handling large-scale job submissions. These tests provide valuable insights into:  Identifying performance bottlenecks: Pinpointing areas where the system struggles under heavy loads.Ensuring optimized resource utilization: Ensuring that CPU, memory, and other resources are used efficiently.Evaluating system stability under heavy workloads: Testing the system’s ability to maintain performance and reliability under extreme conditions.  By conducting these tests, you can ensure that the Spark Operator is capable of handling real-world, high-demand scenarios effectively.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#prerequisites","content":" Before running the benchmark tests, ensure you have deployed the Spark Operator EKS cluster by following the instructions here.Access to the necessary AWS resources and permissions to modify EKS configurations.Familiarity with Terraform, Kubernetes, and Locust for load testing.  ","version":"Next","tagName":"h3"},{"title":"Updates to the Cluster​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#updates-to-the-cluster","content":" To prepare the cluster for benchmark testing, apply the following modifications:  Step 1: Update Spark Operator Helm Configuration  Uncomment the specified Spark Operator Helm values in the file analytics/terraform/spark-k8s-operator/addons.tf (from the -- Start to -- End section). Then, run terraform apply to apply the changes.  These updates ensure that:  The Spark Operator and webhook pods are deployed on a dedicated c5.9xlarge instance using Karpenter.The instance provides 36 vCPUs to handle 6000 application submissions.High CPU and memory resources are allocated for both the Controller pod and Webhook pods.  Here’s the updated configuration:  enable_spark_operator = true spark_operator_helm_config = { version = &quot;2.1.0&quot; timeout = &quot;120&quot; values = [ &lt;&lt;-EOT controller: replicas: 1 # -- Reconcile concurrency, higher values might increase memory usage. # -- Increased from 10 to 20 to leverage more cores from the instance workers: 20 # -- Change this to True when YuniKorn is deployed batchScheduler: enable: false # default: &quot;yunikorn&quot; # -- Start: Uncomment this section in the code for Spark Operator scale test # -- Spark Operator is CPU bound so add more CPU or use compute optimized instance for handling large number of job submissions nodeSelector: NodeGroupType: spark-operator-benchmark resources: requests: cpu: 33000m memory: 50Gi webhook: nodeSelector: NodeGroupType: spark-operator-benchmark resources: requests: cpu: 1000m memory: 10Gi # -- End: Uncomment this section in the code for Spark Operator scale test spark: jobNamespaces: - default - spark-team-a - spark-team-b - spark-team-c serviceAccount: create: false rbac: create: false prometheus: metrics: enable: true port: 8080 portName: metrics endpoint: /metrics prefix: &quot;&quot; podMonitor: create: true labels: {} jobLabel: spark-operator-podmonitor podMetricsEndpoint: scheme: http interval: 5s EOT ] }   Step 2: Prometheus Best practices for Large scale clusters  To efficiently monitor 32,000+ pods across 200 nodes, Prometheus should run on a dedicated node with increased CPU and memory allocation. Ensure Prometheus is deployed on core node groups using NodeSelectors in the Prometheus Helm chart. This prevents interference from workload pods.At scale, Prometheus can consume significant CPU and memory, so running it on dedicated infrastructure ensures it doesn’t compete with your apps. It’s common to dedicate a node or node pool solely to monitoring components (Prometheus, Grafana, etc.) using node selectors or taints.Prometheus is memory-intensive and, when monitoring hundreds or thousands of pods, will also demand substantial CPU​Allocating dedicated resources (and even using Kubernetes priority classes or QoS to favor Prometheus) helps keep your monitoring reliable under stress.Please note that full observability stack (metrics, logs, tracing) might consume roughly one-third of your infrastructure resources at scale​, so plan capacity accordingly.Allocate ample memory and CPU from the start, and prefer requests without strict low limits for Prometheus. For example, if you estimate Prometheus needs ~8 GB, don’t cap it at 4 GB. It’s better to reserve an entire node or a large chunk of one for Prometheus.  Step 3: Configure VPC CNI for High Pod Density:  Modify analytics/terraform/spark-k8s-operator/eks.tf to enable prefix delegation in the vpc-cni addon. This increases the pod capacity per node from 110 to 200.  cluster_addons = { vpc-cni = { configuration_values = jsonencode({ env = { ENABLE_PREFIX_DELEGATION = &quot;true&quot; WARM_PREFIX_TARGET = &quot;1&quot; } }) } }   Important Note: After making these changes, run terraform apply to update the VPC CNI configuration.  Step 4: Create a Dedicated Node Group for Spark Load Testing  We have created a dedicated Managed node group called spark_operator_bench for placing the Spark Jobs pods. Configured a 200-node managed node group with m6a.4xlarge instances for Spark load test pods. The user-data has been modified to allow up to 220 pods per node.  Note: This step is informational only, and no changes need to be applied manually.  spark_operator_bench = { name = &quot;spark_operator_bench&quot; description = &quot;Managed node group for Spark Operator Benchmarks with EBS using x86 or ARM&quot; min_size = 0 max_size = 200 desired_size = 0 ... cloudinit_pre_nodeadm = [ { content_type = &quot;application/node.eks.aws&quot; content = &lt;&lt;-EOT --- apiVersion: node.eks.aws/v1alpha1 kind: NodeConfig spec: kubelet: config: maxPods: 220 EOT } ] ... }   Step 5: Manually Update Node Group Min Size to 200  caution Running 200 nodes can incur significant costs. If you plan to run this test independently, carefully estimate the expenses in advance to ensure budget feasibility.  Initially, set min_size = 0. Before starting the load test, update min_size and desired_size to 200 in the AWS console. This pre-creates all nodes required for the load test, ensuring all DaemonSets are running.  ","version":"Next","tagName":"h3"},{"title":"Load Test Configuration and Execution​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#load-test-configuration-and-execution","content":" To simulate high-scale concurrent job submissions, we developed Locust scripts that dynamically create Spark Operator templates and submit jobs concurrently by simulating multiple users.  Step 1: Set Up a Python Virtual EnvironmentOn your local machine (Mac or desktop), create a Python virtual environment and install the required dependencies:  cd analytics/terraform/spark-k8s-operator/examples/benchmark/spark-operator-benchmark-kit python3.12 -m venv venv source venv/bin/activate pip install -r requirements.txt   Step 2: Run the Load TestExecute the following command to start the load test:  locust --headless --only-summary -u 3 -r 1 \\ --job-limit-per-user 2000 \\ --jobs-per-min 1000 \\ --spark-namespaces spark-team-a,spark-team-b,spark-team-c   This command:  Starts a test with 3 concurrent users. Each user submits 2000 jobs at a rate of 1000 jobs per minute. Total of 6000 jobs are submitted by this command. Each Spark job consists of 6 pods (1 Driver and 5 executor pods) Generates 36,000 pods across 200 nodes using 3 namespaces. Locust script uses the Spark job template located at: analytics/terraform/spark-k8s-operator/examples/benchmark/spark-operator-benchmark-kit/spark-app-with-webhook.yaml. Spark job uses a simple spark-pi-sleep.jar that sleeps for a specified duration. The testing image is available at: public.ecr.aws/data-on-eks/spark:pi-sleep-v0.0.2  ","version":"Next","tagName":"h3"},{"title":"Results Verification​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#results-verification","content":" The load test runs for approximately 1 hour. During this time, you can monitor the Spark Operator metrics, cluster performance and resource utilization using Grafana. Follow the steps below to access the monitoring dashboard:  Step1: Port-forward Grafana ServiceRun the following command to create a local port-forward, making Grafana accessible from your local machine:  kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n kube-prometheus-stack   This maps port 3000 on your local system to Grafana's service inside the cluster.  Step2: Access GrafanaTo log into Grafana, retrieve the secret name storing the admin credentials:  terraform output grafana_secret_name   Then, use the retrieved secret name to fetch credentials from AWS Secrets Manager:  aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text   Step 3: Access Grafana Dashboard  Open a web browser and navigate to http://localhost:3000.Enter username as admin and password as the retrieved password from the previous command.Navigate to the Spark Operator Load Test Dashboard to visualize:  The number of Spark jobs submitted.Cluster-wide CPU and memory consumption.Pod scaling behavior and resource allocation.  ","version":"Next","tagName":"h3"},{"title":"Summary of Results​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#summary-of-results","content":" tip For a detailed analysis, refer to the Kubeflow Spark Operator Website  CPU Utilization:  The Spark Operator controller pod is CPU-bound, utilizing all 36 cores during peak processing.CPU constraints limit job processing speed, making compute power a key factor for scalability.  Memory Usage:  Memory consumption remains stable, regardless of the number of applications processed.This indicates that memory is not a bottleneck, and increasing RAM would not improve performance.  Job Processing Rate:  The Spark Operator processes applications at ~130 apps per minute.The processing rate is capped by CPU limitations, preventing further scaling without additional compute resources.  Time to Process Jobs:  ~15 minutes to process 2,000 applications.~30 minutes to process 4,000 applications.These numbers align with the observed 130 apps per minute processing rate.  Work Queue Duration Metric Reliability:  The default work queue duration metric becomes unreliable once it exceeds 16 minutes.Under high concurrency, this metric fails to provide accurate insights into queue processing times.  API Server Performance Impact:  Kubernetes API request duration increases significantly under high workload conditions.This is caused by Spark querying executor pods frequently, not a limitation of the Spark Operator itself.The increased API server load affects job submission latency and monitoring performance across the cluster.  ","version":"Next","tagName":"h2"},{"title":"Cleanup​","type":1,"pageTitle":"Kubeflow Spark Operator Benchmarks 🚀","url":"/data-on-eks/docs/benchmarks/spark-operator-eks-benchmark#cleanup","content":" Step 1: Scale Down Node Group  To avoid unnecessary costs, first scale down the spark_operator_bench node group by setting its minimum and desired node count to zero:  Log in to the AWS Console.Navigate to the EKS section.Locate and select the spark_operator_bench node group.Edit the node group and update the Min Size and Desired Size to 0.Save the changes to scale down the nodes.  Step 2: Destroy the ClusterOnce the nodes have been scaled down, you can proceed with cluster teardown using the following script:  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Trino on EKS Best Practices","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/analytics/trino-best-practices","content":"Trino on EKS Best Practices Trino deployment on Amazon Elastic Kubernetes Service (EKS) delivers distributed query processing with cloud-native scalability. Organizations can optimize costs by selecting specific compute instances and storage solutions that match their workload requirements while they combine the power of Trino with the scalability and flexibility of EKS using Karpenter. This guide provides prescriptive guidance for deploying Trino on EKS. It focuses on achieving high scalability and low cost through optimal configurations, effective resource management, and cost-saving strategies. We cover detailed configurations for popular file formats such as Hive and Iceberg. These configurations ensure seamless data access and optimize performance. Our goal is to help you set up a Trino deployment that is both efficient and cost-effective. We have a deployment-ready blueprint for deploying Trino on EKS, which incorporates the best practices discussed here. Refer to these best practices for the rational and further optimization/fine-tuning. Trino Fundamentals 👈 EKS Cluster Configuration 👈 Trino on EKS Setup 👈 Compute, Storage, and Networking Best Practices 👈 Configuring Trino Connectors 👈 Large Scale Query Optimizations 👈","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/intro","content":"Introduction info COMING SOON Please note that this section is currently a work in progress and will provide a collection of best practices for running Data and ML workloads on EKS. These best practices will cover various aspects, including cluster configuration, resource management, data storage, security, monitoring, and more. By following these recommended practices, you can optimize the performance, reliability, and security of your data and ML workloads on EKS. Stay tuned for valuable insights and guidance on how to achieve the best outcomes in your EKS environment. Through working with AWS customers, we’ve identified a number of Best Practices that we have recommended for Spark or other large data workloads. We continue to collect and post those recommendations here. Because this is an ongoing effort, please open an Issue or Pull Request if you find something outdated so we can update it. These Data on EKS Best Practices are meant to expand upon the EKS Best Practices Guide for data-centric use cases (e.g., batch processing, stream processing, and machine learning) and we recommend reviewing the EKS Best Practices as a primer before diving into these. The Data on EKS Best Practices are not comprehensive and we recommend you read through the guidance and determine what’s best for your environment. The recommendations here were built from working with customers one of two designs (listed below). Each data use case (e.g., batch processing, stream processing) aligns more closely to one of the two cluster designs, which we will call out in our recommendations. The first design is dynamic clusters that scale, or “churn”, a lot. These clusters run batch processing with Spark, or other workloads that create pods for a relatively short time but can vary greatly on the scale at any given time. These clusters create and delete resources like pods and nodes, or churn, at a high rate which adds unique pressures to Kubernetes and critical components.The other design is “static” clusters. These clusters are often large but have less volatile scaling behavior and are generally running longer-lived jobs, like streaming or training. Avoid interruptions for these workloads is a key concern and care must be taken when making changes. When we talk about large clusters or high rates of churn, it’s difficult to put a specific number to those phrases because of the complexity of kubernetes scalability. In general, these clusters have &gt;500 nodes and &gt;5000 pods, or are creating/destroying hundreds of resources a minute; however, the scalability constraints are different for every workload (even between two different Spark jobs).","keywords":"","version":"Next"},{"title":"EKS Best Practices","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/eks-best-practices","content":"","keywords":"","version":"Next"},{"title":"EKS Best Practices Guides​","type":1,"pageTitle":"EKS Best Practices","url":"/data-on-eks/docs/bestpractices/eks-best-practices#eks-best-practices-guides","content":" The primary goal of this EKS Best practices is to offer a set of best practices for day 2 operations for Amazon EKS. We elected to publish this guidance to GitHub so we could iterate quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community.  Checkout the EKS Best practices GitHub docs here ","version":"Next","tagName":"h2"},{"title":"TPCDS Spark Benchmark Results for Graviton R6g, R7g, and R8g","type":0,"sectionRef":"#","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/graviton-r-data","content":"","keywords":"","version":"Next"},{"title":"Results​","type":1,"pageTitle":"TPCDS Spark Benchmark Results for Graviton R6g, R7g, and R8g","url":"/data-on-eks/docs/benchmarks/spark-operator-benchmark/graviton-r-data#results","content":" When reviewing the results for the TPCDS benchmark we are interested in the time it takes for the Spark SQL queries to complete, the faster those queries complete the better. The graph below shows the cumulative runtime in seconds for all of the queries for each instance type we tested:    We can similarly display the time for each query per instance, you can see the improvements in runtime for the newer generations.:    In the table below we have taken the Median times from the output for each instance type from a benchmark with 3 iterations of the queries and calculated the performance gained. You can view the raw output data in the raw_data folder here.  To calculate the performance increase we are calculating a ratio of the query times. For example, to determine how much faster the r8g instances were compared to the r6g instances:  Find the times corresponding to each query, using q20-v2.4 as an example the r6g.12xlarge took 2.81s and the r8g.12xlarge took 1.69s.We then divide r5g.12xlarge/r8g.12xlarge, for q20-v2.4 that's 2.81s/1.69s = 1.66. So for this query the r8g.12xlarge was able to complete the queries 1.66 times faster (or a ~66% percent improvement)  The data has been sorted by the last column, showing the performance increase r8g.12xlarge has over the r6g.12xlarge.  Query\tr6g.12xlarge\tr7g.12xlarge\tr8g.12xlarge r7g times faster than r6g\tr8g times faster than r7g\tr8g times faster than r6gq20-v2.4\t2.81\t1.94\t1.69 1.45\t1.14\t1.66 q39b-v2.4\t5.75\t4.26\t3.56 1.35\t1.20\t1.61 q45-v2.4\t7.66\t5.69\t4.92 1.35\t1.15\t1.56 q8-v2.4\t6.77\t5.19\t4.37 1.31\t1.19\t1.55 q73-v2.4\t5.08\t3.96\t3.31 1.28\t1.20\t1.54 q39a-v2.4\t6.60\t4.84\t4.36 1.36\t1.11\t1.51 q81-v2.4\t22.41\t20.14\t14.93 1.11\t1.35\t1.50 q69-v2.4\t7.81\t5.74\t5.26 1.36\t1.09\t1.48 q34-v2.4\t5.80\t4.76\t3.96 1.22\t1.20\t1.47 q97-v2.4\t19.76\t15.13\t13.60 1.31\t1.11\t1.45 q95-v2.4\t48.14\t38.24\t33.14 1.26\t1.15\t1.45 q22-v2.4\t9.79\t7.57\t6.81 1.29\t1.11\t1.44 q24b-v2.4\t71.09\t59.12\t49.64 1.20\t1.19\t1.43 q15-v2.4\t7.62\t6.30\t5.34 1.21\t1.18\t1.43 q58-v2.4\t5.05\t4.12\t3.57 1.22\t1.16\t1.41 q18-v2.4\t10.43\t8.45\t7.44 1.23\t1.14\t1.40 q14b-v2.4\t53.73\t40.88\t38.33 1.31\t1.07\t1.40 q72-v2.4\t22.29\t17.19\t16.08 1.30\t1.07\t1.39 q98-v2.4\t3.78\t3.26\t2.73 1.16\t1.20\t1.39 q46-v2.4\t13.37\t11.00\t9.65 1.21\t1.14\t1.39 q83-v2.4\t2.39\t1.95\t1.73 1.22\t1.12\t1.38 q6-v2.4\t11.11\t8.83\t8.08 1.26\t1.09\t1.37 q31-v2.4\t12.19\t11.12\t8.87 1.10\t1.25\t1.37 q11-v2.4\t23.95\t19.30\t17.72 1.24\t1.09\t1.35 q29-v2.4\t18.91\t15.29\t14.00 1.24\t1.09\t1.35 q61-v2.4\t5.26\t4.62\t3.92 1.14\t1.18\t1.34 q91-v2.4\t3.63\t3.05\t2.71 1.19\t1.12\t1.34 q5-v2.4\t20.57\t16.72\t15.42 1.23\t1.08\t1.33 q54-v2.4\t6.61\t5.61\t4.99 1.18\t1.12\t1.32 q23b-v2.4\t152.46\t130.94\t115.37 1.16\t1.14\t1.32 q51-v2.4\t11.90\t9.90\t9.06 1.20\t1.09\t1.31 q57-v2.4\t7.62\t6.19\t5.81 1.23\t1.06\t1.31 q10-v2.4\t8.38\t6.70\t6.41 1.25\t1.05\t1.31 q24a-v2.4\t75.07\t63.24\t57.78 1.19\t1.09\t1.30 q64-v2.4\t64.48\t53.92\t49.69 1.20\t1.09\t1.30 q3-v2.4\t3.39\t2.44\t2.62 1.39\t0.93\t1.29 q14a-v2.4\t61.18\t50.12\t47.37 1.22\t1.06\t1.29 q65-v2.4\t17.30\t14.65\t13.49 1.18\t1.09\t1.28 q17-v2.4\t6.95\t5.54\t5.44 1.25\t1.02\t1.28 q79-v2.4\t5.12\t4.14\t4.01 1.24\t1.03\t1.28 q47-v2.4\t8.45\t7.56\t6.62 1.12\t1.14\t1.28 q60-v2.4\t4.30\t3.72\t3.38 1.16\t1.10\t1.27 ss_max-v2.4\t10.48\t9.13\t8.29 1.15\t1.10\t1.26 q35-v2.4\t15.90\t13.96\t12.65 1.14\t1.10\t1.26 q68-v2.4\t7.93\t6.64\t6.32 1.19\t1.05\t1.25 q77-v2.4\t2.39\t1.95\t1.92 1.23\t1.02\t1.25 q75-v2.4\t38.27\t32.63\t30.71 1.17\t1.06\t1.25 q42-v2.4\t2.19\t1.96\t1.76 1.11\t1.11\t1.24 q25-v2.4\t5.64\t4.84\t4.55 1.17\t1.06\t1.24 q93-v2.4\t160.84\t142.63\t130.45 1.13\t1.09\t1.23 q38-v2.4\t16.80\t14.77\t13.66 1.14\t1.08\t1.23 q74-v2.4\t19.57\t16.73\t15.97 1.17\t1.05\t1.23 q82-v2.4\t14.10\t12.66\t11.55 1.11\t1.10\t1.22 q4-v2.4\t55.36\t49.70\t45.47 1.11\t1.09\t1.22 q23a-v2.4\t116.74\t101.45\t95.94 1.15\t1.06\t1.22 q30-v2.4\t15.69\t15.56\t12.91 1.01\t1.21\t1.22 q94-v2.4\t20.87\t17.87\t17.18 1.17\t1.04\t1.21 q71-v2.4\t3.83\t3.08\t3.16 1.24\t0.98\t1.21 q86-v2.4\t3.19\t2.89\t2.64 1.10\t1.10\t1.21 q26-v2.4\t5.94\t5.67\t4.91 1.05\t1.15\t1.21 q59-v2.4\t15.73\t14.35\t13.03 1.10\t1.10\t1.21 q27-v2.4\t7.15\t6.26\t5.93 1.14\t1.06\t1.21 q41-v2.4\t1.24\t1.09\t1.03 1.14\t1.06\t1.20 q36-v2.4\t6.15\t5.27\t5.11 1.17\t1.03\t1.20 q56-v2.4\t4.00\t3.46\t3.32 1.15\t1.04\t1.20 q87-v2.4\t15.46\t14.13\t12.96 1.09\t1.09\t1.19 q21-v2.4\t2.31\t2.11\t1.94 1.09\t1.08\t1.19 q32-v2.4\t2.04\t1.83\t1.73 1.11\t1.06\t1.18 q78-v2.4\t82.43\t75.70\t70.26 1.09\t1.08\t1.17 q88-v2.4\t40.39\t36.38\t34.44 1.11\t1.06\t1.17 q48-v2.4\t7.89\t7.24\t6.74 1.09\t1.07\t1.17 q33-v2.4\t4.06\t3.67\t3.47 1.11\t1.06\t1.17 q99-v2.4\t9.36\t8.36\t8.02 1.12\t1.04\t1.17 q16-v2.4\t28.35\t24.72\t24.41 1.15\t1.01\t1.16 q12-v2.4\t2.60\t2.64\t2.24 0.98\t1.18\t1.16 q28-v2.4\t41.47\t37.51\t35.84 1.11\t1.05\t1.16 q76-v2.4\t23.72\t21.55\t20.66 1.10\t1.04\t1.15 q1-v2.4\t6.24\t5.33\t5.43 1.17\t0.98\t1.15 q37-v2.4\t7.81\t7.18\t6.85 1.09\t1.05\t1.14 q90-v2.4\t9.91\t8.97\t8.77 1.10\t1.02\t1.13 q80-v2.4\t22.63\t20.72\t20.13 1.09\t1.03\t1.12 q66-v2.4\t8.09\t7.54\t7.23 1.07\t1.04\t1.12 q67-v2.4\t145.51\t140.91\t130.01 1.03\t1.08\t1.12 q7-v2.4\t7.82\t6.61\t7.00 1.18\t0.94\t1.12 q50-v2.4\t71.71\t70.06\t64.46 1.02\t1.09\t1.11 q19-v2.4\t4.61\t3.98\t4.16 1.16\t0.96\t1.11 q89-v2.4\t4.71\t4.53\t4.27 1.04\t1.06\t1.10 q13-v2.4\t8.75\t8.30\t7.98 1.05\t1.04\t1.10 q63-v2.4\t4.51\t4.36\t4.12 1.03\t1.06\t1.10 q85-v2.4\t14.98\t13.86\t13.72 1.08\t1.01\t1.09 q70-v2.4\t8.20\t8.28\t7.55 0.99\t1.10\t1.09 q62-v2.4\t8.97\t8.60\t8.28 1.04\t1.04\t1.08 q44-v2.4\t19.24\t18.94\t17.78 1.02\t1.07\t1.08 q84-v2.4\t9.72\t9.51\t9.01 1.02\t1.05\t1.08 q96-v2.4\t8.92\t8.26\t8.29 1.08\t1.00\t1.08 q9-v2.4\t34.46\t32.49\t32.07 1.06\t1.01\t1.07 q2-v2.4\t14.12\t13.05\t13.21 1.08\t0.99\t1.07 q52-v2.4\t2.00\t1.97\t1.87 1.01\t1.05\t1.07 q43-v2.4\t4.39\t4.14\t4.13 1.06\t1.00\t1.06 q92-v2.4\t1.53\t1.56\t1.46 0.98\t1.07\t1.05 q40-v2.4\t12.33\t11.43\t11.82 1.08\t0.97\t1.04 q49-v2.4\t26.01\t26.02\t25.04 1.00\t1.04\t1.04 q53-v2.4\t4.53\t4.64\t4.42 0.98\t1.05\t1.03 q55-v2.4\t2.36\t2.15\t2.50 1.10\t0.86\t0.94 ","version":"Next","tagName":"h2"},{"title":"Spark on EKS Best Practices","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices","content":"","keywords":"","version":"Next"},{"title":"EKS Networking​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#eks-networking","content":" ","version":"Next","tagName":"h2"},{"title":"VPC and Subnets Sizing​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#vpc-and-subnets-sizing","content":" VPC IP address exhaustion​  As EKS clusters scale up with additional Spark workloads, the number of pods managed by a cluster can easily grow into the thousands, each consuming an IP address. This creates challenges, since IP addresses within a VPC are limited, and it's not always feasible to recreate a larger VPC or extend the current VPC's CIDR blocks.  Worker nodes and pods both consume IP addresses. By default, VPC CNI has WARM_ENI_TARGET=1 means that ipamd should keep &quot;a full ENI&quot; of available IPs around in the ipamd warm pool for the Pod IP assignment.  Remediation for IP Address exhaustion​  While IP exhaustion remediation methods exist for VPCs, they introduce additional operational complexity and have significant implications to consider. Hence, for new EKS clusters, it is recommended to over-provision the subnets you will use for Pod networking for growth.  For addressing IP address exhaustion, consider adding secondary CIDR blocks to your VPC and creating new subnets from these additional address ranges, then deploying worker nodes in these expanded subnets.  If adding more subnets, is not an option, then you will have to work on optimising the IP address assignment by tweaking CNI Configuration Variables. Refer to configure MINIMUM_IP_TARGET.  ","version":"Next","tagName":"h3"},{"title":"CoreDNS Recommendations​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#coredns-recommendations","content":" DNS Lookup Throttling​  Spark applications running on Kubernetes generate high volumes of DNS lookups when executors communicate with external services.  This occurs because Kubernetes' DNS resolution model requires each pod to query the cluster's DNS service (kube-dns or CoreDNS) for every new connection, and during task executions Spark executors frequently create new connections for communicating with external services. By default, Kubernetes does not cache DNS results at the pod level, meaning each executor pod must perform a new DNS lookup even for previously resolved hostnames.  This behavior is amplified in Spark applications due to their distributed nature, where multiple executor pods simultaneously attempt to resolve the same external service endpoints.This occurs during data ingestion, processing, and when connecting to external databases or shuffle services.  When DNS traffic exceeds 1024 packets per second for a CoreDNS replica, DNS requests will be throttled, resulting in unknownHostException errors.  Remediation​  It is recommended to scale CoreDNS, as your workload scales. Refer to Scaling CoreDNS for more details on implementation choices.  It is also recommended to continuously monitor CoreDNS metrics. Refer to EKS Networking Best Practices for detailed information.  ","version":"Next","tagName":"h3"},{"title":"Reduce Inter AZ Traffic​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#reduce-inter-az-traffic","content":" Inter AZ Costs​  During the shuffle stage, Spark executors may need to exchange data between them. If the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front, which will be charged as Inter-AZ Traffic costs.  Remediation​  For Spark workloads, it is recommended to colocate executor pods and worker nodes in the same AZ. Colocating workloads in the same AZ serves two main purposes:  Reduce inter-AZ traffic costsReduce network latency between executors/Pods  Refer to Inter AZ Network Optimization for having pods co-locate on the same AZ.  ","version":"Next","tagName":"h3"},{"title":"Karpenter Recommendations​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#karpenter-recommendations","content":" Karpenter enhances Spark on EKS deployments by providing rapid node provisioning capability that aligns with Spark's dynamic resource scaling needs. This automated scaling solution improves resource utilization and cost-efficiency by bringing in right-sized nodes as needed. This also allows Spark jobs to scale seamlessly without the need for pre-configured node groups or manual intervention, there by simplifying operational management.  Here are the Karpenter recommendations for scaling compute nodes while running Spark workloads. For complete Karpenter configuration details, refer Karpenter documentation.  Consider creating separate NodePools for driver and executor pods.  ","version":"Next","tagName":"h2"},{"title":"Driver Nodepool​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#driver-nodepool","content":" The Spark driver is a single pod and manages the entire lifecycle of the Spark application. Terminating Spark driver pod, effectively means terminating the entire Spark job.  Configure Driver Nodepool to always use on-demand nodes only. When Spark driver pods run on spot instances, they are vulnerable to unexpected terminations due to spot instance reclamation, resulting in computation loss and interrupted processing that requires manual intervention to restart.Disable consolidation on Driver Nodepool.Use node selectors or taints/tolerations for placing driver pods on this designated Driver NodePool.  ","version":"Next","tagName":"h3"},{"title":"Executor Nodepool​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#executor-nodepool","content":" Configure Spot instances​  In the absence of Amazon EC2 Reserved Instances or Savings Plans, consider using Amazon EC2 Spot Instances for executors to reduce dataplane costs.  When spot instances are interrupted, executors will be terminated and rescheduled on available nodes. For details on interruption behaviour and node termination management, refer to the Handling Interruptions section.  Instance and Capacity type selection​  Using multiple instance types in the node pool enables access to various spot instance pools, increasing capacity availability and optimizing for both price and capacity across the available instance options.  With Weighted Nodepools, node selection can be optimized using weighted nodepools arranged in priority order. By assigning different weights to each nodepool, you can establish a selection hierarchy, such as: Spot (highest weight), followed by Graviton, AMD, and Intel (lowest weight).  Consolidation Configuration​  While enabling consolidation for Spark executor pods can lead to better cluster resource utilization, it's crucial to strike a balance with job performance. Frequent consolidation events can result in slower execution times for Spark jobs, as executors are forced to recompute the shuffle data and RDD blocks.  This impact is particularly noticeable in long-running Spark jobs. To mitigate this, it's essential to carefully tune the consolidation interval.  Enable graceful executor pods shutdown:  spark.executor.decommission.enabled=true: Enables graceful decommissioning of executors, allowing them to complete their current tasks and transfer their cached data before shutting down. This is particularly useful when using spot instances for executors. spark.storage.decommission.enabled=true: Enables the migration of cached RDD blocks from the decommissioning executor to other active executors before shutdown, preventing data loss and the need for recomputation.  To explore other means to save intermediate data computed in Spark Executors, refer to Storage Best Practices.  Handling interruptions during Karpenter Consolidation/Spot Termination​  Perform controlled decommissioning instead of abruptly killing executors when nodes are scheduled for termination. To achieve this:  Configure appropriate TerminationGracePeriod values for Spark workloads.Implement executor-aware termination handling.Ensure shuffle data is saved before nodes are decommissioned.  Spark provides native configurations to control termination behavior:  Controlling executor interruptions  Configs:spark.executor.decommission.enabledspark.executor.decommission.forceKillTimeoutThese configurations are particularly useful in scenarios where executors might be terminated due to spot instance interruptions or Karpenter consolidation events. When enabled, executors will gracefully shutdown by stopping task acceptance and notifying the driver about their decommissioning state.  Controlling executor's BlockManager behavior  Configs:spark.storage.decommission.enabledspark.storage.decommission.shuffleBlocks.enabledspark.storage.decommission.rddBlocks.enabledspark.storage.decommission.fallbackStorage.pathThese settings enable the migration of shuffle and RDD blocks from decommissioning executors to other available executors or to a fallback storage location. This approach helps in dynamic environments by reducing the need to recompute shuffle data or RDD blocks, thereby improving job completion times and resource efficiency.  ","version":"Next","tagName":"h3"},{"title":"Advanced Scheduling Considerations​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#advanced-scheduling-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Default Kubernetes Scheduler behaviour.​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#default-kubernetes-scheduler-behaviour","content":" Default Kubernetes scheduler uses least allocated approach. This strategy aims to distribute pods evenly across cluster, which helps in maintaining availability and a balanced resource utilization across all nodes, rather than packing more pods in fewer nodes.  Most allocated approach on the other hand, aims to favor nodes with most amount of allocated resources, which leads to packing more pods onto nodes that are already heavily allocated. This approach is favourable for Spark jobs, as it aims for high utilization on select nodes at pod scheduling time, leading to better consolidation of nodes. You will have to leverage a custom kube-scheduler with this option enabled, or leverage Custom Schedulers purpose built for more advanced orchestration.  ","version":"Next","tagName":"h3"},{"title":"Custom Schedulers​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#custom-schedulers","content":" Custom schedulers enhance Kubernetes’ native scheduling capabilities by providing advanced features tailored for batch and high-performance computing workloads. Custom schedulers enhance resource allocation by optimizing bin-packing and offering scheduling tailored to specific application needs. Here are popular custom schedulers for running Spark workloads on Kubernetes.  Apache YunikornVolcano  Advantages of leveraging custom schedulers like Yunikorn.  Hierarchical queue system and configurable policies allowing for complex resource management.Gang scheduling, which ensures all related pods (like Spark executors) start together, preventing resource wastage.Resource fairness across different tenants and workloads.  ","version":"Next","tagName":"h3"},{"title":"How will Yunikorn and Karpenter work together?​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#how-will-yunikorn-and-karpenter-work-together","content":" Karpenter and Yunikorn complement each other by handling different aspects of workload management in Kubernetes:  Karpenter focuses on node provisioning and scaling, determining when to add or remove nodes based on resource demands. Yunikorn brings application awareness to scheduling through advanced features like queue management, resource fairness, and gang scheduling.  In a typical workflow, Yunikorn first schedules pods based on application-aware policies and queue priorities. When these pods remain pending due to insufficient cluster resources, Karpenter detects these pending pods and provisions appropriate nodes to accommodate them. This integration ensures both efficient pod placement (Yunikorn) and optimal cluster scaling (Karpenter).  For Spark workloads, this combination is particularly effective: Yunikorn ensures executors are scheduled according to application SLAs and dependencies, while Karpenter ensures the right node types are available to meet those specific requirements.  ","version":"Next","tagName":"h3"},{"title":"Storage Best Practices​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#storage-best-practices","content":" ","version":"Next","tagName":"h2"},{"title":"Node Storage​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#node-storage","content":" By default, the EBS root volumes of worker nodes are set to 20GB. Spark Executors use local storage for temporary data like shuffle data, intermediate results, and temporary files. This default storage of 20GB root volume attached to worker nodes can be limiting in both size and performance. Consider the following options to address your performance and storage size requirements:  Expand the root volume capacity to provide ample space for intermediate Spark data. You will have to arrive at optimal capacity based on average size of the dataset that each executor will be processing and complexity of Spark job.Configure high-performance storage with better I/O and latency.Mount additional volumes on worker nodes for temporary data storage.Leverage dynamically provisioned PVCs that can be attached directly to executor pods.  ","version":"Next","tagName":"h3"},{"title":"Reuse PVC​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#reuse-pvc","content":" This option allows reusing PVCs associated with Spark executors even after the executors are terminated (either due to consolidation activity or preemption in case of Spot instances).  This allows for preserving the intermediate shuffle data and cached data on the PVC. When Spark requests new executor pod to replace the terminated one, the system attempts to reuse an existing PVC that belonged to terminated executor. This option can be enabled by the following configuration:  spark.kubernetes.executor.reusePersistentVolume=true  ","version":"Next","tagName":"h3"},{"title":"External Shuffle services​","type":1,"pageTitle":"Spark on EKS Best Practices","url":"/data-on-eks/docs/bestpractices/analytics/spark-best-practices#external-shuffle-services","content":" Leverage external shuffle services like Apache Celeborn to decouple compute and storage, allowing Spark executors to write data to an external shuffle service instead of local disks. This reduces the risk of data loss and data re-computation due to executor termination or consolidation.  This also allows for better resource management, especially when Spark Dynamic Resource Allocation is enabled. External shuffle service allows Spark to preserve shuffle data even after executors are removed during dynamic resource allocation, preventing the need for recomputation of shuffle data when new executors are added. This enables more efficient scale-down of resources when they're not needed.  Also consider the performance implications of external shuffle services. For smaller datasets or applications with low shuffle data volues, the overhead of setting up and managing external shuffle service might outweigh its benefits.  External shuffle services is recommended when dealing with either shuffle data volumes exceeding 500GB to 1TB per job or long running Spark applications that run for several hours to multiple days.  Refer to this Celeborn Documentation for deployment on Kubernetes and integration configuration with Apache Spark. ","version":"Next","tagName":"h3"},{"title":"Preload container images into data volumes with EBS Snapshots","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/scalability/preload-container-images","content":"","keywords":"","version":"Next"},{"title":"Overview of this script​","type":1,"pageTitle":"Preload container images into data volumes with EBS Snapshots","url":"/data-on-eks/docs/bestpractices/scalability/preload-container-images#overview-of-this-script","content":"   Launch an EC2 instance with Bottlerocket for EKS AMI.Access to instance via Amazon System ManagerPull images to be cached in this EC2 using Amazon System Manager Run Command.Shut down the instance, build the EBS snapshot for the data volume.Terminate the instance.  ","version":"Next","tagName":"h2"},{"title":"Usage Example​","type":1,"pageTitle":"Preload container images into data volumes with EBS Snapshots","url":"/data-on-eks/docs/bestpractices/scalability/preload-container-images#usage-example","content":" git clone https://github.com/aws-samples/bottlerocket-images-cache/ cd bottlerocket-images-cache/ # Using nohup in terminals to avoid disconnections ❯ nohup ./snapshot.sh --snapshot-size 150 -r us-west-2 \\ docker.io/rayproject/ray-ml:2.10.0-py310-gpu,public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest &amp; ❯ tail -f nohup.out 2024-07-15 17:18:53 I - [1/8] Deploying EC2 CFN stack ... 2024-07-15 17:22:07 I - [2/8] Launching SSM . 2024-07-15 17:22:08 I - SSM launched in instance i-07d10182abc8a86e1. 2024-07-15 17:22:08 I - [3/8] Stopping kubelet.service .. 2024-07-15 17:22:10 I - Kubelet service stopped. 2024-07-15 17:22:10 I - [4/8] Cleanup existing images .. 2024-07-15 17:22:12 I - Existing images cleaned 2024-07-15 17:22:12 I - [5/8] Pulling images: 2024-07-15 17:22:12 I - Pulling docker.io/rayproject/ray-ml:2.10.0-py310-gpu - amd64 ... 2024-07-15 17:27:50 I - docker.io/rayproject/ray-ml:2.10.0-py310-gpu - amd64 pulled. 2024-07-15 17:27:50 I - Pulling docker.io/rayproject/ray-ml:2.10.0-py310-gpu - arm64 ... 2024-07-15 17:27:58 I - docker.io/rayproject/ray-ml:2.10.0-py310-gpu - arm64 pulled. 2024-07-15 17:27:58 I - Pulling public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - amd64 ... 2024-07-15 17:31:34 I - public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - amd64 pulled. 2024-07-15 17:31:34 I - Pulling public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - arm64 ... 2024-07-15 17:31:36 I - public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - arm64 pulled. 2024-07-15 17:31:36 I - [6/8] Stopping instance ... 2024-07-15 17:32:25 I - Instance i-07d10182abc8a86e1 stopped 2024-07-15 17:32:25 I - [7/8] Creating snapshot ... 2024-07-15 17:38:36 I - Snapshot snap-0c6d965cf431785ed generated. 2024-07-15 17:38:36 I - [8/8] Cleanup. 2024-07-15 17:38:37 I - Stack deleted. 2024-07-15 17:38:37 I - -------------------------------------------------- 2024-07-15 17:38:37 I - All done! Created snapshot in us-west-2: snap-0c6d965cf431785ed   You can copy the snapshot ID snap-0c6d965cf431785ed and configure it as a snapshot for worker nodes.  Using Snapshot with Amazon EKS and Karpenter  You can specify snapshotID in a Karpenter node class. Add the content on EC2NodeClass:  apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass metadata: name: default spec: amiFamily: Bottlerocket # Ensure OS is BottleRocket blockDeviceMappings: - deviceName: /dev/xvdb ebs: volumeSize: 150Gi volumeType: gp3 kmsKeyID: &quot;arn:aws:kms:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:key/1234abcd-12ab-34cd-56ef-1234567890ab&quot; # Specify KMS ID if you use custom KMS key snapshotID: snap-0123456789 # Specify your snapshot ID here   End-to-End deployment example  For end-to-end deployment examples, refer to the GitHub samples and documentation linked above, or explore the various data analytics blueprints in this repository. ","version":"Next","tagName":"h2"},{"title":"EMR on EKS Observability","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability","content":"","keywords":"","version":"Next"},{"title":"Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafana​","type":1,"pageTitle":"EMR on EKS Observability","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability#monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana","content":" In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases  Checkout the full blog here  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"EMR on EKS Observability","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability#architecture","content":" The following diagram illustrates the solution architecture for scraping Spark Driver and Executors’ metrics, as well as writing to Amazon Managed Service for Prometheus.    ","version":"Next","tagName":"h3"},{"title":"Grafana Dashboard for Spark​","type":1,"pageTitle":"EMR on EKS Observability","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability#grafana-dashboard-for-spark","content":" The following Grafana dashboard displays the EMR on EKS Spark job metrics with Driver and Executor details.   ","version":"Next","tagName":"h3"},{"title":"EMR Runtime with Spark Operator","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"EMR Runtime with Spark Operator","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-spark-operator#introduction","content":" In this post, we will learn to deploy EKS with EMR Spark Operator and execute sample Spark job with EMR runtime.  In this example, you will provision the following resources required to run Spark Applications using the Spark Operator and EMR runtime.  Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Two managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc.Spark Node group with single AZ for running Spark jobs Creates one Data team (emr-data-team-a) Creates new namespace for the teamNew IAM role for the team execution role IAM policy for emr-data-team-aSpark History Server Live UI is configured for monitoring running Spark jobs through an NLB and NGINX ingress controllerDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, AWS for FluentBit, CloudWatchMetrics for EKS  EMR Spark Operator 👈  Deploying the Solution 👈  Execute Sample Spark job with Karpenter 👈  Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h2"},{"title":"Amazon EMR on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks","content":"","keywords":"","version":"Next"},{"title":"Benefits of EMR on EKS​","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks#benefits-of-emr-on-eks","content":" ","version":"Next","tagName":"h2"},{"title":"Simplify management​","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks#simplify-management","content":" You get the same EMR benefits for Apache Spark on EKS that you get on EC2 today. This includes fully managed versions of Apache Spark 2.4 and 3.0, automatic provisioning, scaling, performance optimized runtime, and tools like EMR Studio for authoring jobs and an Apache Spark UI for debugging.  ","version":"Next","tagName":"h3"},{"title":"Reduce Costs​","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks#reduce-costs","content":" With EMR on EKS, your compute resources can be shared between your Apache Spark applications and your other Kubernetes applications. Resources are allocated and removed on-demand to eliminate over-provisioning or under-utilization of these resources, enabling you to lower costs as you only pay for the resources you use.  ","version":"Next","tagName":"h3"},{"title":"Optimize Performance​","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks#optimize-performance","content":" By running analytics applications on EKS, you can reuse existing EC2 instances in your shared Kubernetes cluster and avoid the startup time of creating a new cluster of EC2 instances dedicated for analytics. You can also get 3x faster performance running performance optimized Spark with EMR on EKS compared to standard Apache Spark on EKS.  ","version":"Next","tagName":"h3"},{"title":"EMR on EKS Deployment patterns with Terraform​","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks#emr-on-eks-deployment-patterns-with-terraform","content":" The following Terraform templates are available to deploy.  EMR on EKS with Karpenter: 👈:skin-tone-3: Start Here if you are new to EMR on EKS. This template deploys EMR on EKS cluster and uses Karpenter to scale Spark jobs.EMR on EKS with Spark Operator: This template deploys EMR on EKS cluster with Spark Operator for managing Spark jobs ","version":"Next","tagName":"h2"},{"title":"Networking for Data","type":0,"sectionRef":"#","url":"/data-on-eks/docs/bestpractices/networking","content":"","keywords":"","version":"Next"},{"title":"VPC and IP Considerations​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#vpc-and-ip-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Default VPC CNI Configuration​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#default-vpc-cni-configuration","content":" With the default VPC CNI configuration larger nodes will consume more IP addresses. For example a m5.8xlarge node that is running 10 pods will hold 60 IPs total (to satisfy WARM_ENI_TARGET=1). However a m5.16xlarge node would hold 100 IPs.  The AWS VPC CNI maintains this “warm pool” of IP addresses on the EKS worker nodes to assign to Pods. When more IP addresses are needed for your Pods, the CNI must communicate with EC2 APIs to assign the addresses to your nodes.  ","version":"Next","tagName":"h3"},{"title":"Plan for a large amount of IP address usage in your EKS clusters.​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters","content":" During periods of high churn or large scale out, these EC2 API calls can be rate throttled, which will delay the provisioning of Pods and thus delay the execution of workloads. Also, configuring the VPC CNI to minimize this warm pool can increase the EC2 API calls from your nodes and increase the risk of rate throttling.  ","version":"Next","tagName":"h3"},{"title":"Consider using a secondary CIDR if your IP space is constrained.​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#consider-using-a-secondary-cidr-if-your-ip-space-is-constrained","content":" If you are working with a network that spans multiple connected VPCs or sites the routable address space may be limited. For example, your VPC may be limited to small subnets like below. In this VPC we wouldn’t be able to run more than one m5.16xlarge node without adjusting the CNI configuration.    You can add additional VPC CIDRs from a range that is not routable across VPCs (such as the RFC 6598 range, 100.64.0.0/10). In this case we added 100.64.0.0/16, 100.65.0.0/16, and 100.66.0.0/16 to the VPC (as this is the maximum CIDR size), then created new subnets with those CIDRs. Finally we recreated the node groups in the new subnets, leaving the existing EKS cluster control plane in place.    With this configuration you can still communicate with the EKS cluster control plane from connected VPCs but your nodes and pods have plenty of IP addresses to accommodate your workloads and the warm pool.  ","version":"Next","tagName":"h3"},{"title":"Tuning the VPC CNI​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#tuning-the-vpc-cni","content":" ","version":"Next","tagName":"h2"},{"title":"VPC CNI and EC2 Rate Throttling​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#vpc-cni-and-ec2-rate-throttling","content":" When an EKS worker node is launched it initially has a single ENI with a single IP address attached for the EC2 instance to communicate. As the VPC CNI launches it tries to provision a Warm Pool of IP addresses that can be assigned to Kubernetes Pods (More details in the EKS Best Practices Guide).  The VPC CNI must make AWS EC2 API calls (like AssignPrivateIpV4Address and DescribeNetworkInterfaces) to assign those additional IPs and ENIs to the worker node. When the EKS cluster scales out the number of Nodes or Pods there could be a spike in the number of these EC2 API calls. This surge of calls could encounter rate throttling from the EC2 API to help the performance of the service, and to ensure fair usage for all Amazon EC2 customers. This rate throttling can cause the pool of IP address to be exhausted while the CNI tries to allocate more IPs.  These failures will cause errors like the one below, indicating that the provisioning of the container network namespace has failed because the VPC CNI could not provision an IP address.  Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;xxxxxxxxxxxxxxxxxxxxxx&quot; network for pod &quot;test-pod&quot;: networkPlugin cni failed to set up pod test-pod_default&quot; network: add cmd: failed to assign an IP address to container   This failure delays the launch of the Pod and adds pressure to the kubelet and worker node as this action is retried until the IP address is assigned. To avoid this delay you can configure the CNI to reduce the number of EC2 API calls needed.  ","version":"Next","tagName":"h3"},{"title":"Avoid using WARM_IP_TARGET in large clusters, or cluster with a lot of churn​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn","content":" WARM_IP_TARGET can help limit the “wasted” IPs for small clusters, or clusters that has very low pod churn. However, this environment variable on the VPC CNI needs to be carefully configured in large clusters, as it may increase the number of EC2 API calls for IP attachment and detachment operations by ipamd, increasing the risk and impact of rate throttling. (ipamd stands for IP Address Management Daemon, and is a core component of VPC CNI. It maintains a warm pool of available IPs for fast Pod startup times.)  For clusters that have a lot of Pod churn, it is recommended to set MINIMUM_IP_TARGET to a value slightly higher than the expected number of pods you plan to run on each node. This will allow the CNI to provision all of those IP addresses in a single (or few) calls.   [...] # EKS Addons cluster_addons = { vpc-cni = { configuration_values = jsonencode({ env = { MINIMUM_IP_TARGET = &quot;30&quot; } }) } } [...]   For detailed information of tweaking VPC CNI variables, refer to this documentation on github.  ","version":"Next","tagName":"h3"},{"title":"Limit the number of IPs per node on large instance types with MAX_ENI and max-pods​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods","content":" When using larger instance types such as 16xlarge or 24xlarge the number of IP addresses that can be assigned per ENI can be fairly large. For example, a c5.18xlarge instance type with the default CNI configuration of WARM_ENI_TARGET=1 would end up holding 100 IP addresses (50 IPs per ENI * 2 ENIs) when running a handful of pods.  For some workloads the CPU, Memory, or other resource will limit the number of Pods on that c5.18xlarge before we need more than 50 IPs. In this case you may want to be able to run 30-40 pods maximum on that instance.   [...] # EKS Addons cluster_addons = { vpc-cni = { configuration_values = jsonencode({ env = { MAX_ENI = &quot;1&quot; } }) } } [...]   Setting the MAX_ENI=1 option on the CNI and that this will limit the number of IP addresses each node is able to provision, but it does not limit the number of pod that kubernetes will try to schedule to the nodes. This can lead to a situation where pods are scheduled to nodes that are unable to provision more IP addresses.  To limit the IPs and stop k8s from scheduling too many pods you will need to:  Update the CNI configuration environment variables to set MAX_ENI=1Update the --max-pods option for the kubelet on the worker nodes.  To configure the --max-pods option you can update the userdata for your worker nodes to set this option via the --kubelet -extra-args in the bootstrap.sh script. By default this script configures the max-pods value for the kubelet, the --use-max-pods false` option disables this behavior when providing your own value:   eks_managed_node_groups = { system = { instance_types = [&quot;m5.xlarge&quot;] min_size = 0 max_size = 5 desired_size = 3 pre_bootstrap_user_data = &lt;&lt;-EOT EOT bootstrap_extra_args = &quot;--use-max-pods false --kubelet-extra-args '--max-pods=&lt;your_value&gt;'&quot; }   One problem is the number of IPs per ENI is different based on the Instance type (for example a m5d.2xlarge can have 15 IPs per ENI, where a m5d.4xlarge can hold 30 IPs per ENI). This means hard-coding a value for max-pods may cause problems if you change instance types or in mixed-instance environments.  In the EKS Optimized AMI releases there is a script included that can be used to help calculate the AWS Recommended max-pods value. If you’d like to automate this calculation for mixed instances you will also need to update the userdata for your instances to use the --instance-type-from-imds flag to autodiscover the instance type from instance metadata.   eks_managed_node_groups = { system = { instance_types = [&quot;m5.xlarge&quot;] min_size = 0 max_size = 5 desired_size = 3 pre_bootstrap_user_data = &lt;&lt;-EOT /etc/eks/max-pod-calc.sh --instance-type-from-imds —cni-version 1.13.4 —cni-max-eni 1 EOT bootstrap_extra_args = &quot;--use-max-pods false --kubelet-extra-args '--max-pods=&lt;your_value&gt;'&quot; }   Maxpods with Karpenter​  By default, Nodes provisioned by Karpenter will have the max pods on a node based on the node instance type. To configure the --max-pods option as mentioned above by defining at the Provisioner level by specifying maxPods within the .spec.kubeletConfiguration . This value will be used during Karpenter pod scheduling and passed through to --max-pods on kubelet startup.  Below is the example Provisioner spec:  apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: providerRef: name: default requirements: - key: &quot;karpenter.k8s.aws/instance-category&quot; operator: In values: [&quot;c&quot;, &quot;m&quot;, &quot;r&quot;] - key: &quot;karpenter.sh/capacity-type&quot; # If not included, the webhook for the AWS cloud provider will default to on-demand operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] # Karpenter provides the ability to specify a few additional Kubelet args. # These are all optional and provide support for additional customization and use cases. kubeletConfiguration: maxPods: 30   ","version":"Next","tagName":"h3"},{"title":"Application​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#application","content":" ","version":"Next","tagName":"h2"},{"title":"Scaling CoreDNS​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#scaling-coredns","content":" Default Behavior​  Route 53 Resolver enforces a limit of 1024 packets per second per network interface for each EC2 instance, and this limit is not adjustable. In EKS clusters, CoreDNS runs with two replicas by default, with each replica on a separate EC2 instance. When DNS traffic exceeds 1024 packets per second for a CoreDNS replica, DNS requests will be throttled, resulting in unknownHostException errors.  Remediation​  To address the scalability of default coreDNS, consider implementing one of the following two options:  Enable coreDNS auto-scaling.Implement Node local cache.  While scaling out CoreDNS it is also crucial to distribute replicas across different nodes. Co-locating CoreDNS on same nodes, will again end up throttling the ENI, rendering additional replicas ineffective. In order to distribute CoreDNS across nodes, apply node anti-affinity policy to the pods:  affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname   CoreDNS Monitoring​  It is recommended to continuously monitor CoreDNS metrics. Refer to EKS Networking Best Practices for detailed information.  ","version":"Next","tagName":"h3"},{"title":"DNS Lookups and ndots​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#dns-lookups-and-ndots","content":" In Kubernetes Pods with the default DNS configuration have a resolv.conf file like so:  nameserver 10.100.0.10 search namespace.svc.cluster.local svc.cluster.local cluster.local ec2.internal options ndots:5   The domain names listed in the search line are appended to DNS names that are not fully qualified domain names (FQDN). For example, if a pod tries to connect to a Kubernetes service using servicename.namespace the domains would be appended in order until the DNS name matched the full kubernetes service name:  servicename.namespace.namespace.svc.cluster.local &lt;--- Fails with NXDOMAIN servicename.namespace.svc.cluster.local &lt;-- Succeed   Whether or not a domain is fully qualified is determined by the ndots option in the resolv.conf. This option defines the number of dots that must be in a domain name before the search domains are skipped. These additional searches can add latency to connections to external resources like S3 and RDS endpoints.  The default ndots setting in Kubernetes is five, if your application isn’t talking to other pods in the cluster, we can set the ndots to a low value like “2”. This is a good starting point, because it still allows your application to do service discovery within the same namespace and in other namespaces within the cluster, but allows a domain like s3.us-east-2.amazonaws.com to be recognized as a FQDN (skipping the search domains).  Here’s an example pod manifest from the Kubernetes documentation with ndots set to “2”:  apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsConfig: options: - name: ndots value: &quot;2&quot;   info While setting ndots to “2” in your pod deployment is a reasonable place to start, this will not universally work in all situations and shouldn’t be applied across the entire cluster. The ndots configuration needs to be configured at the Pod or Deployment level. Reducing this setting at the Cluster level CoreDNS configuration is not recommended.  ","version":"Next","tagName":"h3"},{"title":"Inter AZ Network Optimization​","type":1,"pageTitle":"Networking for Data","url":"/data-on-eks/docs/bestpractices/networking#inter-az-network-optimization","content":" Some workloads may need to exchange data between Pods in the cluster, like Spark executors during the shuffle stage. If the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front. Hence, for these workloads, it is recommended to colocate executors or worker pods in the same AZ. Colocating workloads in the same AZ serves two main purposes:  Reduce inter-AZ traffic costsReduce network latency between executors/Pods  To have pods co-located on the same AZ, we can use podAffinity based scheduling constraints. The scheduling constraint preferredDuringSchedulingIgnoredDuringExecution can be enforced in the Pod spec. For example, ins Spark we can use a custom template for our driver and executor pods:  spec: executor: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: sparkoperator.k8s.io/app-name operator: In values: - &lt;&lt;spark-app-name&gt;&gt; topologyKey: topology.kubernetes.io/zone ...   You can also leverage Kubernetes Topology Aware Routing to have Kubernetes services route traffic in more efficient means once pods have been created: https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/  info Having all executors located in a single AZ, means that AZ will be a single point of failure. This is a trade off you should consider between lowering network cost and latency, and the event of an AZ failure interrupting workloads. If your workload is running on instances with constrained capacity you may consider using multiple AZs to avoid Insufficient Capacity errors. ","version":"Next","tagName":"h3"},{"title":"Data Analytics on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics","content":"Data Analytics on EKS Running data analytics tools on Kubernetes can provide a number of benefits for organizations looking to extract insights from large and complex data sets. Tools such as Apache Spark and DASK are designed to run on a cluster of machines, making them well-suited for deployment on Kubernetes. The Spark Operator for Kubernetes is a popular Kubernetes operator that simplifies the deployment and management of Apache Spark on Kubernetes. By using the Spark Operator, organizations can take advantage of features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of their data analytics pipelines. This can greatly simplify and automate the deployment, scaling, and management of these complex applications, freeing up data scientists and engineers to focus on the analysis and interpretation of the data. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running data analytics platforms in production. Spark OperatorSpark SubmitKarpenterApache YuniKornVolcano","keywords":"","version":"Next"},{"title":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids","content":"","keywords":"","version":"Next"},{"title":"EMR support for NVIDIA RAPIDS Accelerator for Apache Spark​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#emr-support-for-nvidia-rapids-accelerator-for-apache-spark","content":" Integration of Amazon EMR with NVIDIA RAPIDS Accelerator for Apache Spark​ Amazon EMR on EKS now extends its support to include the use of GPU instance types with the NVIDIA RAPIDS Accelerator for Apache Spark. As the use of artificial intelligence (AI) and machine learning (ML) continues to expand in the realm of data analytics, there's an increasing demand for rapid and cost-efficient data processing, which GPUs can provide. The NVIDIA RAPIDS Accelerator for Apache Spark enables users to harness the superior performance of GPUs, leading to substantial infrastructure cost savings.  ","version":"Next","tagName":"h3"},{"title":"Features​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#features","content":" Highlighted Features​ Experience a performance boost in data preparation tasks, allowing you to transition quickly to the subsequent stages of your pipeline. This not only accelerates model training but also liberates data scientists and engineers to concentrate on priority tasks. Spark 3 ensures seamless coordination of end-to-end pipelines - from data ingestion, through model training, to visualization. The same GPU-accelerated setup can serve both Spark and machine learning or deep learning frameworks. This obviates the need for discrete clusters and provides GPU acceleration to the entire pipeline. Spark 3 extends support for columnar processing in the Catalyst query optimizer. The RAPIDS Accelerator can plug into this system to speed up SQL and DataFrame operators. When the query plan is actioned, these operators can then utilize the GPUs within the Spark cluster for improved performance. NVIDIA has introduced an innovative Spark shuffle implementation designed to optimize data exchange between Spark tasks. This shuffle system is built on GPU-boosted communication libraries, including UCX, RDMA, and NCCL, which significantly enhance data transfer rates and overall performance.-  Deploying the Solution 👈  ","version":"Next","tagName":"h3"},{"title":"Launching XGBoost Spark Job​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#launching-xgboost-spark-job","content":" Training Dataset​  Fannie Mae’s Single-Family Loan Performance Data has a comprehensive dataset starting from 2013. It provides valuable insights into the credit performance of a portion of Fannie Mae’s single-family book of business. This dataset is designed to assist investors in better understanding the credit performance of single-family loans owned or guaranteed by Fannie Mae.  Step 1: Building a Custom Docker Image​  To pull the Spark Rapids base image from the EMR on EKS ECR repository located in us-west-2, log in:  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 895885662937.dkr.ecr.us-west-2.amazonaws.com   If you're located in a different region, please refer to: this guide.  To build your Docker image locally, use the following command:  Build the custom Docker image using the provided Dockerfile. Choose a tag for the image, such as 0.10.  info Please note that the build process may take some time, depending on your network speed. Keep in mind that the resulting image size will be approximately 23.5GB.  cd ~/data-on-eks/ai-ml/emr-spark-rapids/examples/xgboost docker build -t emr-6.10.0-spark-rapids-custom:0.10 -f Dockerfile .   Replace &lt;ACCOUNTID&gt; with your AWS account ID. Log in to your ECR repository with the following command:  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin &lt;ACCOUNTID&gt;.dkr.ecr.us-west-2.amazonaws.com   To push your Docker image to your ECR, use:  $ docker tag emr-6.10.0-spark-rapids-custom:0.10 &lt;ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/emr-6.10.0-spark-rapids-custom:0.10 $ docker push &lt;ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/emr-6.10.0-spark-rapids-custom:0.10   You can use this image during the job execution in Step3 .  ","version":"Next","tagName":"h3"},{"title":"Step2: Acquire the Input Data (Fannie Mae’s Single-Family Loan Performance Data)​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#step2-acquire-the-input-data-fannie-maes-single-family-loan-performance-data","content":" This dataset is sourced from Fannie Mae’s Single-Family Loan Performance Data. All rights are held by Fannie Mae.  Go to the Fannie Mae websiteClick on Single-Family Loan Performance Data Register as a new user if you are using the website for the first timeUse the credentials to login Select HPClick on Download Data and choose Single-Family Loan Performance DataYou will find a tabular list of Acquisition and Performance` files sorted based on year and quarter. Click on the file to download. You can download three years(2020, 2021 and 2022 - 4 files for each year and one for each quarter) worth of data that will be used in our example job. e.g.,: 2017Q1.zipUnzip the download file to extract the csv file to your local machine. e.g.,: 2017Q1.csvCopy only the CSV files to an S3 bucket under ${S3_BUCKET}/${EMR_VIRTUAL_CLUSTER_ID}/spark-rapids-emr/input/fannie-mae-single-family-loan-performance/. The example below uses three years of data (one file for each quarter, 12 files in total). Note: ${S3_BUCKET} and ${EMR_VIRTUAL_CLUSTER_ID} values can be extracted from Terraform outputs.   aws s3 ls s3://emr-spark-rapids-&lt;aws-account-id&gt;-us-west-2/949wt7zuphox1beiv0i30v65i/spark-rapids-emr/input/fannie-mae-single-family-loan-performance/ 2023-06-24 21:38:25 2301641519 2000Q1.csv 2023-06-24 21:38:25 9739847213 2020Q2.csv 2023-06-24 21:38:25 10985541111 2020Q3.csv 2023-06-24 21:38:25 11372073671 2020Q4.csv 2023-06-23 16:38:36 9603950656 2021Q1.csv 2023-06-23 16:38:36 7955614945 2021Q2.csv 2023-06-23 16:38:36 5365827884 2021Q3.csv 2023-06-23 16:38:36 4390166275 2021Q4.csv 2023-06-22 19:20:08 2723499898 2022Q1.csv 2023-06-22 19:20:08 1426204690 2022Q2.csv 2023-06-22 19:20:08 595639825 2022Q3.csv 2023-06-22 19:20:08 180159771 2022Q4.csv   ","version":"Next","tagName":"h3"},{"title":"Step3: Run the EMR Spark XGBoost Job​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#step3-run-the-emr-spark-xgboost-job","content":" Here, we will utilize a helper shell script to execute the job. This script requires user input.  This script will ask for certain inputs that you can obtain from Terraform outputs. See the example below.  cd ai-ml/emr-spark-rapids/examples/xgboost/ &amp;&amp; chmod +x execute_spark_rapids_xgboost.sh ./execute_spark_rapids_xgboost.sh # Example inputs shown below Did you copy the fannie-mae-single-family-loan-performance data to S3 bucket(y/n): y Enter the customized Docker image URI: public.ecr.aws/o7d8v7g9/emr-6.10.0-spark-rapids:0.11 Enter EMR Virtual Cluster AWS Region: us-west-2 Enter the EMR Virtual Cluster ID: 949wt7zuphox1beiv0i30v65i Enter the EMR Execution Role ARN: arn:aws:iam::&lt;ACCOUNTID&gt;:role/emr-spark-rapids-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-spark-rapids/emr-ml-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates, Input data and Output data.&lt;bucket-name&gt;: emr-spark-rapids-&lt;ACCOUNTID&gt;-us-west-2 Enter the number of executor instances (4 to 8): 8   Verify the pod status    info Note that the first execution might take longer as it needs to download the image for the EMR Job Pod, Driver, and Executor pods. Each pod may take up to 8 minutes to download the Docker image. Subsequent runs should be faster (usually under 30 seconds), thanks to image caching  ","version":"Next","tagName":"h3"},{"title":"Step4: Verify the Job results​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#step4-verify-the-job-results","content":" Log in to check the Spark driver pod logs from either CloudWatch logs or your S3 bucket.  Here's a sample output from the log file:  /emr-on-eks-logs/emr-spark-rapids/emr-ml-team-a spark-rapids-emr/949wt7zuphox1beiv0i30v65i/jobs/0000000327fe50tosa4/containers/spark-0000000327fe50tosa4/spark-0000000327fe50tosa4-driver/stdout   The following is a sample output from the above log file:  Raw Dataframe CSV Rows count : 215386024 Raw Dataframe Parquet Rows count : 215386024 ETL takes 222.34674382209778  Training takes 95.90932035446167 seconds If features_cols param set, then features_col param is ignored.  Transformation takes 63.999391317367554 seconds +--------------+--------------------+--------------------+----------+ |delinquency_12| rawPrediction| probability|prediction| +--------------+--------------------+--------------------+----------+ | 0|[10.4500541687011...|[0.99997103214263...| 0.0| | 0|[10.3076572418212...|[0.99996662139892...| 0.0| | 0|[9.81707763671875...|[0.99994546175003...| 0.0| | 0|[9.10498714447021...|[0.99988889694213...| 0.0| | 0|[8.81903457641601...|[0.99985212087631...| 0.0| +--------------+--------------------+--------------------+----------+ only showing top 5 rows  Evaluation takes 3.8372223377227783 seconds Accuracy is 0.996563056111921  ","version":"Next","tagName":"h3"},{"title":"ML Pipeline for Fannie Mae Single Loan Performance Dataset​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#ml-pipeline-for-fannie-mae-single-loan-performance-dataset","content":" Step1: Preprocess and clean the dataset to handle missing values, categorical variables, and other data inconsistencies.This may involve techniques like data imputation,one-hot encoding, and data normalization.  Step2: Create additional features from the existing ones that might provide more useful information for predicting loan performance. For example, you could extract features like loan-to-value ratio, borrower's credit score range, or loan origination year.  Step3: Divide the dataset into two parts: one for training the XGBoost model and one for evaluating its performance. This allows you to assess how well the model generalizes to unseen data.  Step4: Feed the training dataset into XGBoost to train the model. XGBoost will analyze the loan attributes and their corresponding loan performance labels to learn the patterns and relationships between them. The objective is to predict whether a loan is likely to default or perform well based on the given features.  Step5: Once the model is trained, use the evaluation dataset to assess its performance. This involves analyzing metrics such as accuracy, precision, recall, or area under the receiver operating characteristic curve (AUC-ROC) to measure how well the model predicts loan performance  Step6: If the performance is not satisfactory, you can tune the XGBoost hyperparameters, such as the learning rate, tree depth, or regularization parameters, to improve the model's accuracy or address issues like overfitting.  Step7: Finally, with a trained and validated XGBoost model, you can use it to make predictions on new, unseen loan data. These predictions can help in identifying potential risks associated with loan default or evaluating loan performance.    ","version":"Next","tagName":"h3"},{"title":"GPU Monitoring with DCGM Exporter, Prometheus and Grafana​","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-spark-rapids#gpu-monitoring-with-dcgm-exporter-prometheus-and-grafana","content":" Observability plays a crucial role in managing and optimizing hardware resources such as GPUs, particularly in machine learning workloads where the GPU utilization is high. The ability to monitor GPU usage in real-time, identify trends, and detect anomalies can significantly impact performance tuning, troubleshooting, and efficient resource utilization.  NVIDIA GPU Operator plays a key role in GPU observability. It automates the deployment of the necessary components to run GPU workloads on Kubernetes. One of its components, the DCGM (Data Center GPU Manager) Exporter, is an open-source project that exports GPU metrics in a format that can be ingested by Prometheus, a leading open-source monitoring solution. These metrics include GPU temperature, memory usage, GPU utilization, and more. The DCGM Exporter allows you to monitor these metrics on a per-GPU basis, providing granular visibility into your GPU resources.  The NVIDIA GPU Operator, in combination with the DCGM Exporter, exports GPU metrics to a Prometheus server. With its flexible query language, Prometheus allows you to slice and dice data to generate insights into resource usage patterns.  However, Prometheus is not designed for long-term data storage. This is where the Amazon Managed Service for Prometheus (AMP) comes into play. It provides a fully-managed, secure, and scalable service for Prometheus that makes it easy to analyze operational data at scale without having to manage the underlying infrastructure.  Visualizing these metrics and creating informative dashboards is where Grafana excels. Grafana is an open-source platform for monitoring and observability, offering rich visualizations to represent collected metrics intuitively. When combined with Prometheus, Grafana can display the GPU metrics collected by the DCGM Exporter in a user-friendly manner.  The NVIDIA GPU Operator is configured to export metrics to the Prometheus server, which then remote-writes these metrics to Amazon Managed Prometheus (AMP). As a user, you can log into the Grafana WebUI, deployed as part of the blueprint, and add AMP as a data source. Following this, you can import the open-source GPU monitoring dashboard that presents GPU metrics in an easily digestible format, facilitating real-time performance monitoring and resource optimization.  NVIDIA GPU Operator: Installed on your Kubernetes cluster, the NVIDIA GPU Operator is responsible for managing the lifecycle of GPU resources. It deploys NVIDIA drivers and the DCGM Exporter on each GPU-equipped node.DCGM Exporter: The DCGM Exporter runs on each node, collecting GPU metrics and exposing them to Prometheus.Prometheus: Prometheus is a time-series database that collects metrics from various sources, including the DCGM Exporter. It pulls metrics from the exporter at regular intervals and stores them. In this setup, you would configure Prometheus to remote-write the collected metrics to AMP.Amazon Managed Service for Prometheus (AMP): AMP is a fully-managed Prometheus service provided by AWS. It takes care of long-term storage, scalability, and security of your Prometheus data.Grafana: Grafana is a visualization tool that can query AMP for the collected metrics, and display them on informative dashboards.  In this blueprint, we leverage DCGM to write GPU metrics to both Prometheus and Amazon Managed Prometheus (AMP). To verify the GPU metrics, you can use Grafana by running the following command:  kubectl port-forward svc/grafana 3000:80 -n grafana `` Login to Grafana using `admin` as the username, and retrieve the password from Secrets Manager using the following AWS CLI command: ```bash aws secretsmanager get-secret-value --secret-id emr-spark-rapids-grafana --region us-west-2   Once logged in, add the AMP datasource to Grafana and import the Open Source GPU monitoring dashboard. You can then explore the metrics and visualize them using the Grafana dashboard, as shown in the screenshot below.    Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h3"},{"title":"EMR on EKS with Karpenter","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#introduction","content":" In this pattern, you will deploy an EMR on EKS cluster and use Karpenter Nodepools for scaling Spark jobs.  Architecture  This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults if you are new to EMR on EKS and only customize if you have viable alternative option available for replacement.  In terms of infrastructure, here are the resources that are created by this pattern  Creates an EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)One managed node group Core Node group with 3 instances spanning multi-AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc. Enables EMR on EKS Creates two namespaces (emr-data-team-a, emr-data-team-b) for data teamsCreates Kubernetes role and role binding(emr-containers user) for both namespacesIAM roles for both teams needed for job executionUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service accountCreate EMR Virtual Cluster for emr-data-team-a &amp; emr-data-team-b and IAM policies for both  You can see the list of add-ons available below.  tip We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as core-node-group as provided by this pattern.  danger We don't recommend removing critical add-ons (Amazon VPC CNI, CoreDNS, Kube-proxy).  Add-on\tEnabled by default?\tBenefits\tLinkAmazon VPC CNI\tYes\tVPC CNI is available as an EKS add-on and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods\tVPC CNI Documentation CoreDNS\tYes\tCoreDNS is available as an EKS add-on and is responsible for resolving DNS queries for spark application and for Kubernetes cluster\tEKS CoreDNS Documentation Kube-proxy\tYes\tKube-proxy is available as an EKS add-on and it maintains network rules on your nodes and enables network communication to your spark application pods\tEKS kube-proxy Documentation Amazon EBS CSI driver\tYes\tEBS CSI driver is available as an EKS add-on and it allows EKS clusters to manage the lifecycle of EBS volumes\tEBS CSI Driver Documentation Karpenter\tYes\tKarpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters\tKarpenter Documentation Cluster Autoscaler\tYes\tKubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as core-node-group) in the cluster\tCluster Autoscaler Documentation Cluster proportional autoscaler\tYes\tThis is responsible for scaling CoreDNS pods in your Kubernetes cluster\tCluster Proportional Autoscaler Documentation Metrics server\tYes\tKubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster\tEKS Metrics Server Documentation Prometheus\tYes\tPrometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost\tPrometheus Documentation Amazon Managed Prometheus\tYes\tThis is responsible for storing and scaling of EKS cluster and spark application metrics\tAmazon Managed Prometheus Documentation Kubecost\tYes\tKubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels\tEKS Kubecost Documentation CloudWatch metrics\tNo\tCloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard\tCloudWatch Container Insights Documentation AWS for Fluent-bit\tNo\tThis can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system\tAWS For Fluent-bit Documentation FSx for Lustre CSI driver\tNo\tThis can be used for running Spark application using FSx for Lustre\tFSx for Lustre CSI Driver Documentation  Customizing Add-ons 👈  Deploying the Solution 👈  ","version":"Next","tagName":"h2"},{"title":"Run Sample Spark job​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#run-sample-spark-job","content":" The pattern shows how to run spark jobs in a multi-tenant EKS cluster. The examples showcases two data teams using namespaces emr-data-team-a and emr-data-team-b mapped to their EMR virtual clusters. You can use different Karpenter Nodepools for each team so that they can submit jobs that are unique to their workload. Teams can also use different storage requirements to run their Spark jobs. For example, you can use compute optimized Nodepool that has taints and specify tolerations using pod templates so that you can run spark on compute optimized EC2 instances. In terms of storage, you can decide whether to use EC2 instance-store or EBS or FSx for lustre volumes for data processing. The default storage that is used in these examples is EC2 instance store because of performance benefit  spark-compute-optimized Nodepool to run spark jobs on c5d instances.spark-memory-optimized Nodepool to run spark jobs on r5d instances.spark-graviton-memory-optimized Nodepool to run spark jobs on r6gd Graviton instances(ARM64).  spark-compute-optimizedspark-memory-optimizedspark-graviton-memory-optimized In this tutorial, you will use Karpenter Nodepool that uses compute optimized instances. This template leverages the Karpenter AWSNodeTemplates. To view Karpenter Nodepool for compute optimized instances, Click to toggle content! Verify the Karpenter NodeClass and Nodepool code here To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding tolerations to your pod templates For example, spec: tolerations: - key: &quot;spark-compute-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; Execute the sample PySpark Job to trigger compute optimized Karpenter Nodepool The following script requires four input parameters virtual_cluster_id, job_execution_role_arn, cloudwatch_log_group_name &amp; S3_Bucket to store PySpark scripts, Pod templates and Input data. These values are auto populated by execute_emr_eks_job.sh. caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-compute-provisioner/ ./execute_emr_eks_job.sh Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3 Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name&gt;: s3://example-bucket Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution kubectl get pods --namespace=emr-data-team-a -w   ","version":"Next","tagName":"h2"},{"title":"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool","content":" This pattern uses EBS volumes for data processing and compute optimized Nodepool. You can modify the Nodepool by changing nodeselector in driver and executor pod templates. In order to change Nodepools, simply update your pod templates to desired Nodepool   nodeSelector: NodeGroupType: &quot;SparkComputeOptimized&quot;   You can also update EC2 instances that doesn't include instance store volumes (for example c5.xlarge) and remove c5d's if needed for this exercise  We will create Storageclass that will be used by drivers and executors. We'll create static Persistent Volume Claim (PVC) for the driver pod but we'll use dynamically created ebs volumes for executors.  Create StorageClass and PVC using example provided  cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/ kubectl apply -f ebs-storageclass-pvc.yaml   Let's run the job  cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/ ./execute_emr_eks_job.sh Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3 Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name&gt;: s3://example-bucket   You'll notice the PVC spark-driver-pvc will be used by driver pod but Spark will create multiple ebs volumes for executors mapped to Storageclass emr-eks-karpenter-ebs-sc. All dynamically created ebs volumes will be deleted once the job completes  ","version":"Next","tagName":"h3"},{"title":"Running Sample Spark job using FSx for Lustre​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#running-sample-spark-job-using-fsx-for-lustre","content":" Amazon FSx for Lustre is a fully managed shared storage option built on the world’s most popular high-performance file system. You can use FSx to store shuffle files and also to store intermediate data processing tasks in a data pipeline. You can read more about FSX for Lustre in documentation and learn how to use this storage with EMR on EKS in our best practices guide  In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage. There are two ways to use FSx for Lustre  using static FSx for Lustre volumesusing dynamically created FSx for Lustre volumes  fsx-staticfsx-dynamic Execute Spark Job by using FSx for Lustre with statically provisioned volume and compute optimized Karpenter Nodepool. Fsx for Lustre Terraform module is disabled by default. Follow the customizing add-ons steps before running Spark jobs. Execute the Spark job using the below shell script. This script requires input parameters which can be extracted from terraform apply output values. caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-static-pvc-shuffle-storage ./fsx-static-spark.sh Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w This will show the mounted /data directory with FSx DNS name kubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- ls -lah /static   ","version":"Next","tagName":"h3"},{"title":"Running Sample Spark job using Apache YuniKorn Batch Scheduler​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#running-sample-spark-job-using-apache-yunikorn-batch-scheduler","content":" Apache YuniKorn is an open-source, universal resource scheduler for managing distributed big data processing workloads such as Spark, Flink, and Storm. It is designed to efficiently manage resources across multiple tenants in a shared, multi-tenant cluster environment. Some of the key features of Apache YuniKorn include:  Flexibility: YuniKorn provides a flexible and scalable architecture that can handle a wide variety of workloads, from long-running services to batch jobs.Dynamic Resource Allocation: YuniKorn uses a dynamic resource allocation mechanism to allocate resources to workloads on an as-needed basis, which helps to minimize resource wastage and improve overall cluster utilization.Priority-based Scheduling: YuniKorn supports priority-based scheduling, which allows users to assign different levels of priority to their workloads based on business requirements.Multi-tenancy: YuniKorn supports multi-tenancy, which enables multiple users to share the same cluster while ensuring resource isolation and fairness.Pluggable Architecture: YuniKorn has a pluggable architecture that allows users to extend its functionality with custom scheduling policies and pluggable components.  Apache YuniKorn is a powerful and versatile resource scheduler that can help organizations efficiently manage their big data workloads while ensuring high resource utilization and workload performance.  Apache YuniKorn Architecture  Apache YuniKorn Gang Scheduling with Karpenter  Apache YuniKorn Scheduler add-on is disabled by default. Follow the steps to deploy the Apache YuniKorn add-on and execute the Spark job.  Update the analytics/terraform/emr-eks-karpenter/variables.tf file with the following  variable &quot;enable_yunikorn&quot; { default = true description = &quot;Enable Apache YuniKorn Scheduler&quot; type = bool }   Execute terrafrom apply again. This will deploy FSx for Lustre add-on and all the necessary resources.  terraform apply -auto-approve   This example demonstrates the Apache YuniKorn Gang Scheduling with Karpenter Autoscaler.  cd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-yunikorn-gangscheduling ./execute_emr_eks_job.sh   Verify the job executionApache YuniKorn Gang Scheduling will create pause pods for total number of executors requested.  kubectl get pods --namespace=emr-data-team-a -w   Verify the driver and executor pods prefix with tg- indicates the pause pods. These pods will be replaced with the actual Spark Driver and Executor pods once the Nodes are scaled and ready by the Karpenter.    Delta Lake Table Format 👈  ","version":"Next","tagName":"h3"},{"title":"Run Interactive Workload with Managed Endpoint​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#run-interactive-workload-with-managed-endpoint","content":" Managed endpoint is a gateway that provides connectivity from EMR Studio to EMR on EKS so that you can run interactive workloads. You can find out more information about it here.  ","version":"Next","tagName":"h2"},{"title":"Creating a managed endpoint​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#creating-a-managed-endpoint","content":" In this example, we will create a managed endpoint under one of the data teams.  Navigate to folder and execute script: cd analytics/terraform/emr-eks-karpenter/examples/managed-endpoints ./create-managed-endpoint.sh   Enter the EMR Virtual Cluster Id: 4ucrncg6z4nd19vh1lidna2b3 Provide your EMR on EKS team (emr-data-team-a or emr-data-team-b): emr-eks-data-team-a Enter your AWS Region: us-west-2 Enter a name for your endpoint: emr-eks-team-a-endpoint Provide an S3 bucket location for logging (i.e. s3://my-bucket/logging/): s3://&lt;bucket-name&gt;/logs Enter the EMR Execution Role ARN (i.e. arn:aws:00000000000000000:role/EMR-Execution-Role): arn:aws:iam::181460066119:role/emr-eks-karpenter-emr-data-team-a   The script will provide the following:  JSON configuration file for the Managed EndpointConfiguration settings: Default 8G Spark DriverCloudWatch monitoring, with logs stored in the S3 bucket provided Proper endpoint creation with appropriate security group to allow using KarpenterOutputs: Managed Endpoint ID and Load Balancer ARN.  Once you have created a managed endpoint, you can follow the instructions here to configure EMR Studio and associate the Managed endpoint to a workspace.  ","version":"Next","tagName":"h3"},{"title":"Cleanup of Endpoint resources​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#cleanup-of-endpoint-resources","content":" To delete the managed endpoint, simply run the following command:  aws emr-containers delete-managed-endpoint --id &lt;Managed Endpoint ID&gt; --virtual-cluster-id &lt;Virtual Cluster ID&gt;   ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter#cleanup","content":" Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h2"},{"title":"Observability Spark on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#introduction","content":" In this post, we will learn the Observability for Spark on EKS. We will use Spark History Server to watch Spark Applications logs and check the Spark job progress via the Spark Web UI. Amazon Managed Service for Prometheus is used to collect and store the metrics generated by Spark Applications and Grafana is used to build dashboards for monitoring use cases.  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#deploying-the-solution","content":" We will reuse the previous Spark on Operator example. Please follow this link to provision resources  ","version":"Next","tagName":"h2"},{"title":"Set up data and py script​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#set-up-data-and-py-script","content":" let's navigate to one example folder under spark-k8s-operator and run the shell script to upload data and py script to the S3 bucket created by terraform above.  cd data-on-eks/analytics/terraform/spark-k8s-operator/examples/cluster-autoscaler/nvme-ephemeral-storage   Run the taxi-trip-execute.sh script with the following input. You will use the S3_BUCKET variable created earlier. Additionally, you must change YOUR_REGION_HERE with the region of your choice, us-west-2 for example.  This script will download some example taxi trip data and create duplicates of it in order to increase the size a bit. This will take a bit of time and will require a relatively fast internet connection.  cd ${DOEKS_HOME}/analytics/scripts/ chmod +x taxi-trip-execute.sh ./taxi-trip-execute.sh ${S3_BUCKET} YOUR_REGION_HERE   You can return to the blueprint directory and continue with the example  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator   ","version":"Next","tagName":"h2"},{"title":"Spark Web UI​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#spark-web-ui","content":" When you submit a Spark application, Spark context is created which ideally gives you Spark Web UI to monitor the execution of the application. Monitoring includes the following.  Spark configurations usedSpark Jobs, stages, and tasks detailsDAG executionDriver and Executor resource utilizationApplication logs and many more   When your application is done with the processing, Spark context will be terminated so your Web UI as well. and if you wanted to see the monitoring for already finished application, we cannot do it.  To try Spark web UI, let's update &lt;S3_BUCKET&gt; with your bucket name and &lt;JOB_NAME&gt; with &quot;nvme-taxi-trip&quot; in nvme-ephemeral-storage.yaml   kubectl apply -f nvme-ephemeral-storage.yaml   Then run port forward command to expose spark web service.  kubectl port-forward po/taxi-trip 4040:4040 -nspark-team-a   Then open browser and enter localhost:4040. You can view your spark application like below.    ","version":"Next","tagName":"h2"},{"title":"Spark History Server​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#spark-history-server","content":" As mentioned above, spark web UI will be terminated once the spark job is done. This is where Spark history Server comes into the picture, where it keeps the history (event logs) of all completed applications and its runtime information which allows you to review metrics and monitor the application later in time.  In this example, we installed Spark history Server to read logs from S3 bucket. In your spark application yaml file, make sure you have the following setting:   sparkConf: &quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;: &quot;com.amazonaws.auth.InstanceProfileCredentialsProvider&quot; &quot;spark.hadoop.fs.s3a.impl&quot;: &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot; &quot;spark.eventLog.enabled&quot;: &quot;true&quot; &quot;spark.eventLog.dir&quot;: &quot;s3a://&lt;your bucket&gt;/logs/&quot;   Run port forward command to expose spark-history-server service.  kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server   Then open browser and enter localhost:18085. You can view your spark history server like below.  ","version":"Next","tagName":"h2"},{"title":"Prometheus​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#prometheus","content":" Spark users must add the following config to spark application yaml file to extract the metrics from Spark Driver and Executors. In the example, they are added into nvme-ephemeral-storage.yaml already.   &quot;spark.ui.prometheus.enabled&quot;: &quot;true&quot; &quot;spark.executor.processTreeMetrics.enabled&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/scrape&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/path&quot;: &quot;/metrics/executors/prometheus/&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/port&quot;: &quot;4040&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/scrape&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/path&quot;: &quot;/metrics/driver/prometheus/&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/port&quot;: &quot;4040&quot; &quot;spark.metrics.conf.*.sink.prometheusServlet.class&quot;: &quot;org.apache.spark.metrics.sink.PrometheusServlet&quot; &quot;spark.metrics.conf.*.sink.prometheusServlet.path&quot;: &quot;/metrics/driver/prometheus/&quot; &quot;spark.metrics.conf.master.sink.prometheusServlet.path&quot;: &quot;/metrics/master/prometheus/&quot; &quot;spark.metrics.conf.applications.sink.prometheusServlet.path&quot;: &quot;/metrics/applications/prometheus/&quot;   Run port forward command to expose prometheus service.  kubectl port-forward service/prometheus-server 8080:80 -n prometheus   Then open browser and enter localhost:8080. You can view your prometheus server like below.  ","version":"Next","tagName":"h2"},{"title":"Grafana​","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/observability-spark-on-eks#grafana","content":" Grafana has been installed. Use the command below to access with port forward.  get grafana password  kubectl port-forward service/grafana 8080:80 -n grafana   login username is admin and password can get from secrets manager. You can import dashboard with ID: 7890.   ","version":"Next","tagName":"h2"},{"title":"DataHub on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#introduction","content":" DataHub is an open source data catalog that enables end-to-end data discovery, data observability, and data governance. This extensive metadata platform allows users to collect, store, and explore metadata from various sources, such as databases, data lakes, streaming platforms, and ML feature stores. DataHub provides many features, a rich UI for searching and browsing metadata, as well as an API for integrating with other applications.  This blueprint deploys DataHub on an EKS cluster, using Amazon OpenSearch Service, Amazon Managed Streaming for Apache Kafka (Amazon MSK), and Amazon RDS for MySQL as the storage layer for the underlying data model and indexes.  ","version":"Next","tagName":"h2"},{"title":"DataHub on AWS​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#datahub-on-aws","content":" On AWS, DataHub can run on EKS cluster. By using EKS, you can leverage the power and flexibility of Kubernetes to deploy and scale DataHub components, and take advantage of other AWS services and features, such as IAM, VPC, and CloudWatch, to monitor and secure the DataHub cluster.  DataHub also depends on many underlying infrastructure and services to function, including a message broker, a search engine, graph database, and a relational database like MySQL or PostgreSQL. AWS offers a range of managed and serverless services that can meet the needs of DataHub and simplify its deployment and operation.  DataHub can use Amazon Managed Streaming for Apache Kafka (MSK) as the messaging layer for metadata ingestion and consumption. MSK is a fully managed Apache Kafka service, so you don't need to handles the provisioning, configuration, and maintenance of Kafka cluster.DataHub stores metadata in both relational database and a search engine. For the relational database, this blueprint uses Amazon RDS for MySQL, which is also a managed service that simplifies the setup and operation of MySQL databases. RDS for MySQL also provides the high availability, security, and other features DataHub needs to store the metadata.For search engine, this blueprint uses Amazon OpenSearch service to provide fast and scalable search capabilities for the metadata.This blueprint deployes a Schema Registry service on EKS for DataHub. You may also choose to use Glue Schema Registry (https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html) instead. Support for Glue Schema Registry will be included in future release of this blueprint.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#deploying-the-solution","content":" This blueprint deploys an EKS Cluster into a new VPC by default:  Creates a new sample VPC, 2 Private Subnets and 2 Public SubnetsCreates Internet gateway for Public Subnets and NAT Gateway for Private Subnets  You may also deploy to an existing VPC by setting value for create_vpc variable to false and specify vpc_id, private_subnet_ids, and vpc_cidr values.  Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with core managed node group, on-demand node group and Spot node group for Spark workloads.Deploys Metrics server, Cluster Autoscaler, Prometheus server and AMP workspace, and AWS LoadBalancer Controller.  It then provisions the storage services for DataHub.  Creates security group, and an OpenSearch domain with one data node in each of the private subnets / AZs that EKS cluster is deployed on.Creates security group, kms key, and configuration for MSK. Creates the MSK cluster with one broker in each of the private subnets.Creates an RDS MySQL db instance with multi-AZ enabled.  Finally it deployes the datahub-prerequisites and datahub helm charts to setup the datahub pods / services on the EKS cluster. Ingress is enabled (as configured in datahub_values.yaml) and AWS LoadBalancer Controller will provision an ALB to expose the DataHub frontend UI.  info You may customize the blueprint by changing values in variables.tf, to deploy to a different region (default to us-west-2 ), use different cluster name, number of subnets / AZs, or disable addons like fluentbit  info If you already have opensearch service in the account, the service-linked role for OpenSearch exists already. You will need to change default value for variable create_iam_service_linked_role_es to false to avoid error in deployment.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraform  Also, you need opensearch service-linked role created in the account. To verify and create the role if needed, run:  aws iam create-service-linked-role --aws-service-name opensearchservice.amazonaws.com || true   If the role has already been successfully created, you will see:  An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForOpenSearch has been taken in this account, please try a different suffix.  ","version":"Next","tagName":"h3"},{"title":"Deploy​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#deploy","content":" Clone the repository  git clone https://github.com/awslabs/data-on-eks.git   Navigate into one of the example directories and run install.sh script  cd data-on-eks/analytics/terraform/datahub-on-eks chmod +x install.sh ./install.sh   caution install.sh script runs terraform apply -target for each module in order. The module dependencies are configured so they will be applied in the right sequence when you just run terraform apply. However, provisioning addons and MSK, OpenSearch, and RDS instances often take longer than 15 minutes, causing error when provisioning kubernetes and helm resources/modules afterwards due to expired auth token. So if you use terraform apply instead running install.sh, you may need to run it multiple times and terraform will resume the failed resource with a new token each time and complete the deployment eventually.  ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#verify-deployment","content":" After the deployment completes, we can access the DataHub UI and test importing metadata from sample datasources. This blueprint creates the Ingress object for the datahub FrontEnd UI with internal LoadBalancer (can only be accessed within the VPC). You may find the URL to the datahub frontend from the output frontend_url, or by running kubectl command below:  kubectl get ingress datahub-datahub-frontend -n datahub # OUTPUT should looks like below NAME CLASS HOSTS ADDRESS PORTS AGE datahub-datahub-frontend &lt;none&gt; * k8s-datahub-datahubd-xxxxxxxxxx-xxxxxxxxxx.&lt;region&gt;.elb.amazonaws.com 80 nn   Copy the ADDRESS field from the output, then open browser and enter the URL as http://&lt;address&gt;/. Enter datahub as the user name when prompted. The default password is set through Kubernetes secret, you can reveal it with command:  kubectl get secret datahub-user-secret -n datahub -o jsonpath='{.data.*}' | base64 -d   After logging in, we will get the DataHub UI like below.    ","version":"Next","tagName":"h3"},{"title":"Testing​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#testing","content":" Follow steps from this blog to populate metadata from AWS Glue Data Catalog and Amazon Redshift, and business glossary and data lineage, into DataHub.  ","version":"Next","tagName":"h2"},{"title":"Cleanup​","type":1,"pageTitle":"DataHub on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/datahub-on-eks#cleanup","content":" To clean up your environment, run the cleanup.sh script.script  chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Run Apache Beam pipelines with Spark on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam","content":"","keywords":"","version":"Next"},{"title":"Beam on Amazon EKS​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#beam-on-amazon-eks","content":" The Spark Operator for Kubernetes simplifies the deployment and management of Apache Spark on Kubernetes. By using the Spark Operator, we can directly submit Apache Beam pipelines as Spark Applications and deploy and manage them on EKS cluster, taking advantage of features such as automatic scaling and self-healing capabilities on the robust and managed infrastructure of EKS.  ","version":"Next","tagName":"h2"},{"title":"Solution overview​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#solution-overview","content":" In this solution, we will show how to deploy your Beam pipeline, written in Python, on an EKS cluster with Spark Operator. It uses the example pipeline from Apache Beam github repo.    ","version":"Next","tagName":"h2"},{"title":"Deploy Beam pipeline​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#deploy-beam-pipeline","content":" Deploying the Spark-Operator-on-EKS solution 👈  ","version":"Next","tagName":"h2"},{"title":"Step 1: Build custom Docker Image with Spark and Beam SDK​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#step-1-build-custom-docker-image-with-spark-and-beam-sdk","content":" Create a custom spark runtime image from the office spark base image, with a Python virtual environment and Apache Beam SDK pre-installed.  Review the sample DockerfileCustomize the Dockerfile as needed for your environmentBuild the Docker image and push the image to ECR  cd examples/beam aws ecr create-repository --repository-name beam-spark-repo --region us-east-1 docker build . --tag ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/beam-spark-repo:eks-beam-image --platform linux/amd64,linux/arm64 aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com docker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/beam-spark-repo:eks-beam-image   We have created a docker image and published in ECR.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Build and package the Beam pipeline with dependencies​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#step-2-build-and-package-the-beam-pipeline-with-dependencies","content":" With python 3.11 installed, create a Python virtual environment and install dependencies required for building the Beam pipeline:  python3 -m venv build-environment &amp;&amp; \\ source build-environment/bin/activate &amp;&amp; \\ python3 -m pip install --upgrade pip &amp;&amp; \\ python3 -m pip install apache_beam==2.58.0 \\ s3fs \\ boto3   Download the wordcount.py example pipeline and the sample input file. The wordcount Python example demonstrates an Apache Beam pipeline with the following stages: read files, split words, map, group, and sum word counts, and write output to files.  curl -O https://raw.githubusercontent.com/apache/beam/master/sdks/python/apache_beam/examples/wordcount.py curl -O https://raw.githubusercontent.com/cs109/2015/master/Lectures/Lecture15b/sparklect/shakes/kinglear.txt   Upload the input text file to the S3 bucket.  aws s3 cp kinglear.txt s3://${S3_BUCKET}/   To run an Apache Beam Python pipeline on Spark, you may package the pipeline and all its dependencies into a single jar file. Use the below command to create the &quot;fat&quot; jar for the wordcount pipeline with all parameters, without actually executing the pipeline:  python3 wordcount.py --output_executable_path=./wordcountApp.jar \\ --runner=SparkRunner \\ --environment_type=PROCESS \\ --environment_config='{&quot;command&quot;:&quot;/opt/apache/beam/boot&quot;}' \\ --input=s3://${S3_BUCKET}/kinglear.txt \\ --output=s3://${S3_BUCKET}/output.txt   Upload the jar file to the S3 bucket to be used by the spark application.  aws s3 cp wordcountApp.jar s3://${S3_BUCKET}/app/   ","version":"Next","tagName":"h3"},{"title":"Step 3: Create and run the pipeline as SparkApplication​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#step-3-create-and-run-the-pipeline-as-sparkapplication","content":" In this step, we create the manifest file for the SparkApplication object to submit the Apache Beam pipeline as a Spark application. Run the below commands to create a BeamApp.yaml file substituting the ACCOUNT_ID and S3_BUCKET values from the build environment.  envsubst &lt; beamapp.yaml &gt; beamapp.yaml   This command will replace the env variables in file beamapp.yaml.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Execute Spark Job​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#step-4-execute-spark-job","content":" Apply the YAML configuration file to create the SparkApplication on your EKS cluster to execute the Beam pipeline:  kubectl apply -f beamapp.yaml   ","version":"Next","tagName":"h3"},{"title":"Step 5: Monitor and review the pipeline job​","type":1,"pageTitle":"Run Apache Beam pipelines with Spark on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-beam#step-5-monitor-and-review-the-pipeline-job","content":" Monitor and review the pipeline job The word count Beam pipeline may take a couple of minutes to execute. There are a few ways to monitor its status and review job details.  We can use the Spark history server to check the running job  We used the spark-k8s-operator pattern to create the EKS cluster, which had already installed and configured a spark-history-server. Run the command below to start port-forwarding, then click the Preview menu and select Preview Running Application:  kubectl port-forward svc/spark-history-server 8080:80 -n spark-history-server   Open a new browser window and go to this address: http://127.0.0.1:8080/.  Once the job completes successfully, in about 2 minutes, the output files (output.txt-*) containing words found in the input text and the count of each occurrence can be downloaded from the S3 bucket by running the below commands to copy the outputs to your build environment.  mkdir job_output &amp;&amp; cd job_output aws s3 sync s3://$S3_BUCKET/ . --include &quot;output.txt-*&quot; --exclude &quot;kinglear*&quot; --exclude app/*   Output looks like below:  ... particular: 3 wish: 2 Either: 3 benison: 2 Duke: 30 Contending: 1 say'st: 4 attendance: 1 ...   Cleanup 👈 ","version":"Next","tagName":"h3"},{"title":"Spark Operator on EKS with IPv6","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6","content":"","keywords":"","version":"Next"},{"title":"Deploy the EKS Cluster with all the add-ons and infrastructure needed to test this example​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#deploy-the-eks-cluster-with-all-the-add-ons-and-infrastructure-needed-to-test-this-example","content":" The Terraform blueprint will provision the following resources required to run Spark Jobs with open source Spark Operator on Amazon EKS IPv6  A Dual Stack Amazon Virtual Private Cloud (Amazon VPC) with 3 Private Subnets and 3 Public SubnetsAn Internet gateway for Public Subnets, NAT Gateway for Private Subnets and Egress-only Internet gatewayAn Amazon EKS cluster in IPv6 mode (version 1.30)Amazon EKS core-managed node group used to host some of the add-ons that we’ll provision on the clusterDeploys Spark-k8s-operator, Apache Yunikorn, Karpenter, Prometheus and Grafana server.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraform  Before installing the cluster create a EKS IPv6 CNI policy. Follow the instructions from the link:AmazonEKS_CNI_IPv6_Policy   ","version":"Next","tagName":"h3"},{"title":"Clone the repository​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#clone-the-repository","content":" git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks export DOEKS_HOME=$(pwd)   ","version":"Next","tagName":"h3"},{"title":"Initialize Terraform​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#initialize-terraform","content":" Navigate into the example directory and run the initialization script install.sh.  cd ${DOEKS_HOME}/analytics//terraform/spark-eks-ipv6/ chmod +x install.sh ./install.sh   ","version":"Next","tagName":"h3"},{"title":"Export Terraform Outputs​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#export-terraform-outputs","content":" export CLUSTER_NAME=$(terraform output -raw cluster_name) export AWS_REGION=$(terraform output -raw region) export S3_BUCKET=$(terraform output -raw s3_bucket_id_spark_event_logs_example_data)   The S3_BUCKET variable that holds the name of the bucket created during the install. This bucket will be used in later examples to store output data.  ","version":"Next","tagName":"h3"},{"title":"Update kubeconfig​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#update-kubeconfig","content":" Update the kubeconfig to verify the deployment.  aws eks --region $AWS_REGION update-kubeconfig --name $CLUSTER_NAME   ","version":"Next","tagName":"h3"},{"title":"Verify the deployment​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#verify-the-deployment","content":" Examine the IP addresses assigned to the cluster nodes and the pods. You will notice that both have IPv6 addresses allocated.  kubectl get node -o custom-columns='NODE_NAME:.metadata.name,INTERNAL-IP:.status.addresses[?(@.type==&quot;InternalIP&quot;)].address' NODE_NAME INTERNAL-IP ip-10-1-0-212.us-west-2.compute.internal 2600:1f13:520:1303:c87:4a71:b9ea:417c ip-10-1-26-137.us-west-2.compute.internal 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa ip-10-1-46-28.us-west-2.compute.internal 2600:1f13:520:1305:5ee5:b994:c0c2:e4da   kubectl get pods -A -o custom-columns='NAME:.metadata.name,NodeIP:.status.hostIP,PodIP:status.podIP' NAME NodeIP PodIP .... karpenter-5fd95dffb8-l8j26 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b:: karpenter-5fd95dffb8-qpv55 2600:1f13:520:1303:c87:4a71:b9ea:417c 2600:1f13:520:1303:60ac:: kube-prometheus-stack-grafana-9f5c9d8fc-zgn98 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::a kube-prometheus-stack-kube-state-metrics-98c74d866-56275 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::9 kube-prometheus-stack-operator-67df8bc57d-2d8jh 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::b kube-prometheus-stack-prometheus-node-exporter-5qrqs 2600:1f13:520:1303:c87:4a71:b9ea:417c 2600:1f13:520:1303:c87:4a71:b9ea:417c kube-prometheus-stack-prometheus-node-exporter-hcpvk 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa kube-prometheus-stack-prometheus-node-exporter-ztkdm 2600:1f13:520:1305:5ee5:b994:c0c2:e4da 2600:1f13:520:1305:5ee5:b994:c0c2:e4da prometheus-kube-prometheus-stack-prometheus-0 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::7 spark-history-server-6c9f9d7cc4-xzj4c 2600:1f13:520:1305:5ee5:b994:c0c2:e4da 2600:1f13:520:1305:64b::1 spark-operator-84c6b48ffc-z2glj 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::5 spark-operator-webhook-init-kbl4s 2600:1f13:520:1305:5ee5:b994:c0c2:e4da 2600:1f13:520:1305:64b::2 yunikorn-admission-controller-d675f89c5-f2p47 2600:1f13:520:1303:c87:4a71:b9ea:417c 2600:1f13:520:1303:c87:4a71:b9ea:417c yunikorn-scheduler-59d6879975-2rh4d 2600:1f13:520:1304:15b2:b8a3:7f63:cbfa 2600:1f13:520:1304:a79b::4 ....   ","version":"Next","tagName":"h3"},{"title":"Execute Sample Spark job with Karpenter​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#execute-sample-spark-job-with-karpenter","content":" Navigate to example directory and submit the Spark job.  cd ${DOEKS_HOME}/analytics/terraform/spark-eks-ipv6/examples/karpenter kubectl apply -f pyspark-pi-job.yaml   Monitor the job status using the below command. You should see the new nodes triggered by the Karpenter.  kubectl get pods -n spark-team-a -w   ","version":"Next","tagName":"h3"},{"title":"Apache YuniKorn Gang Scheduling with NVMe based SSD disk for shuffle storage​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#apache-yunikorn-gang-scheduling-with-nvme-based-ssd-disk-for-shuffle-storage","content":" Gang Scheduling Spark jobs using Apache YuniKorn and Spark Operator  cd ${DOEKS_HOME}/analytics/terraform/spark-eks-ipv6/examples/karpenter/nvme-yunikorn-gang-scheduling   Run the taxi-trip-execute.sh script with the following input. You will use the S3_BUCKET variable created earlier. Additionally, you must change YOUR_REGION_HERE with the region of your choice, us-west-2 for example.  This script will download some example taxi trip data and create duplicates of it in order to increase the size a bit. This will take a bit of time and will require a relatively fast internet connection.  ${DOEKS_HOME}/analytics/scripts/taxi-trip-execute.sh ${S3_BUCKET} YOUR_REGION_HERE   Once our sample data is uploaded you can run the Spark job. You will need to replace the &lt;S3_BUCKET&gt; placeholders in this file with the name of the bucket created earlier. You can get that value by running echo $S3_BUCKET.  To do this automatically you can run the following, which will create a .old backup file and do the replacement for you.  sed -i.old s/\\&lt;S3_BUCKET\\&gt;/${S3_BUCKET}/g ./nvme-storage-yunikorn-gang-scheduling.yaml   Now that the bucket name is in place you can create the Spark job.  kubectl apply -f nvme-storage-yunikorn-gang-scheduling.yaml   ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"Spark Operator on EKS with IPv6","url":"/data-on-eks/docs/blueprints/data-analytics/spark-eks-ipv6#cleanup","content":" This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ${DOEKS_HOME}/analytics/terraform/spark-eks-ipv6 &amp;&amp; chmod +x cleanup.sh ./cleanup.sh   caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h2"},{"title":"Spark Operator with YuniKorn","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Spark Operator with YuniKorn","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn#introduction","content":" The EKS Cluster design for the Data on EKS blueprint is optimized for running Spark applications with Spark Operator and Apache YuniKorn as the batch scheduler. This blueprint leverage Karpenter to scale the worker nodes, AWS for FluentBit is employed for logging, and a combination of Prometheus, Amazon Managed Prometheus, and open source Grafana are used for observability. Additionally, the Spark History Server Live UI is configured for monitoring running Spark jobs through an NLB and NGINX ingress controller.  Spark workloads with Karpenter 👈  NVMe SSD Instance Storage for Spark Shuffle data 👈  Spark Operator 👈  Deploying the Solution 👈  Execute Sample Spark job with Karpenter 👈  Karpenter Nodepool weights with Graviton and Intel 👈  Cleanup 👈 ","version":"Next","tagName":"h2"},{"title":"Distributed Data Processing with Ray Data","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing","content":"","keywords":"","version":"Next"},{"title":"What is Ray Data?​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#what-is-ray-data","content":" Ray Data is a scalable, framework-agnostic data processing library built on top of Ray, designed for distributed data analytics and machine learning workloads. It provides:  Distributed Processing: Parallel data processing across multiple Ray worker nodesLazy Evaluation: Operations are optimized and executed only when results are neededRich Data Connectors: Native support for various data sources including S3, databases, and file systemsMemory Management: Efficient handling of large datasets that don't fit in memoryIntegration with ML Libraries: Seamless integration with pandas, NumPy, and PyArrow  ","version":"Next","tagName":"h2"},{"title":"Why Ray Data? Is this an Alternative Tool to Spark?​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#why-ray-data-is-this-an-alternative-tool-to-spark","content":" Ray Data is complementary to Spark, not a direct replacement. While both are distributed data processing frameworks, they serve different use cases:    Ray Data excels when you need:  Python-native data processing with familiar pandas/NumPy APIsTight integration with machine learning pipelinesReal-time or streaming data processingComplex iterative algorithms  Spark remains ideal for:  Large-scale ETL operationsComplex SQL-based analyticsEnterprise data warehouse workloadsCross-language support (Scala, Java, Python, R)  ","version":"Next","tagName":"h2"},{"title":"Problem Statement​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#problem-statement","content":" When Apache Spark applications run on Kubernetes, they generate extensive logs that are captured by Fluent Bit and written to S3. However, these logs present several challenges for data engineers:  Unstructured Format: Spark logs are written as raw text files without a consistent schemaNo Query Capability: Engineers cannot easily query logs using SQL-based tools like Amazon AthenaMetadata Enrichment: Fluent Bit adds Kubernetes metadata as JSON, creating mixed formatsPerformance Issues: Scanning raw log files for troubleshooting is time-consuming and expensive    Solution: Use Ray Data to periodically process these unstructured logs, apply a consistent schema, and write them to Apache Iceberg tables. This enables:  ✅ SQL queries via Amazon Athena✅ Structured data with defined schema✅ Efficient columnar storage format✅ Time-travel and versioning capabilities  ","version":"Next","tagName":"h3"},{"title":"Log Snippet in S3 Before Processing​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#log-snippet-in-s3-before-processing","content":" Here's what Spark logs look like when written to S3 by Fluent Bit:  { &quot;log&quot;: &quot;2024-01-15 14:23:45 INFO SparkContext: Running Spark version 3.5.0\\n&quot;, &quot;stream&quot;: &quot;stdout&quot;, &quot;time&quot;: &quot;2024-01-15T14:23:45.123456Z&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;spark-driver-abc123&quot;, &quot;namespace_name&quot;: &quot;spark-team-a&quot;, &quot;pod_id&quot;: &quot;12345678-1234-1234-1234-123456789012&quot;, &quot;labels&quot;: { &quot;spark-role&quot;: &quot;driver&quot;, &quot;spark-app-id&quot;: &quot;spark-application-12345&quot; }, &quot;container_name&quot;: &quot;spark-driver&quot;, &quot;container_image&quot;: &quot;spark:3.5.0&quot; } } { &quot;log&quot;: &quot;2024-01-15 14:23:46 INFO ResourceUtils: Using Spark's default log4j profile\\n&quot;, &quot;stream&quot;: &quot;stdout&quot;, &quot;time&quot;: &quot;2024-01-15T14:23:46.234567Z&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;spark-driver-abc123&quot;, &quot;namespace_name&quot;: &quot;spark-team-a&quot;, &quot;pod_id&quot;: &quot;12345678-1234-1234-1234-123456789012&quot;, &quot;labels&quot;: { &quot;spark-role&quot;: &quot;driver&quot;, &quot;spark-app-id&quot;: &quot;spark-application-12345&quot; }, &quot;container_name&quot;: &quot;spark-driver&quot;, &quot;container_image&quot;: &quot;spark:3.5.0&quot; } }   Key Challenges:  Each log line is wrapped in JSON with Kubernetes metadataThe actual log message is embedded in the log fieldNo structured schema for querying specific log levels or componentsRedundant metadata repeated for each log line  Fluent Bit Enrichment Fluent Bit automatically enriches each log line with Kubernetes metadata including pod name, namespace, labels, and container information. This enrichment is configured in the aws-for-fluentbit-values.yaml file. While this metadata is valuable for debugging, it creates a mixed format that's difficult to query efficiently.  ","version":"Next","tagName":"h3"},{"title":"📋 Architecture Overview​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-architecture-overview","content":" ","version":"Next","tagName":"h2"},{"title":"How Ray Data Transforms Log Processing​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#how-ray-data-transforms-log-processing","content":" Ray Data periodically fetches new logs from S3, processes them in parallel, and writes structured data to Apache Iceberg tables. The solution includes:    ","version":"Next","tagName":"h3"},{"title":"Key Features of Ray Data Processing​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#key-features-of-ray-data-processing","content":" 📊 Schema Extraction and Parsing​  Ray Data intelligently extracts structured fields from unstructured logs:  🕐 timestamp - Parsed from the log message🏷️ log_level - Extracted levels (INFO, WARN, ERROR, DEBUG)🔧 component - Spark component (SparkContext, ResourceUtils, etc.)📝 message - The actual log content🏠 pod_name &amp; namespace - From Kubernetes metadata👷 spark_role - Driver or Executor identification🆔 application_id - Unique Spark application identifier  🔍 Intelligent Filtering and Querying​  Once processed, you can easily query logs using SQL:  -- Find all ERROR logs for a specific application SELECT timestamp, component, message FROM spark_logs WHERE log_level = 'ERROR' AND application_id = 'spark-application-12345' AND timestamp &gt; '2024-01-15 00:00:00' ORDER BY timestamp DESC; -- Analyze log patterns by component SELECT component, log_level, COUNT(*) as count FROM spark_logs WHERE namespace = 'spark-team-a' GROUP BY component, log_level ORDER BY count DESC; -- Track application lifecycle events SELECT timestamp, message FROM spark_logs WHERE component = 'SparkContext' AND (message LIKE '%Starting%' OR message LIKE '%Stopping%') ORDER BY timestamp;   🎯 Metadata Management​  ✅ Idempotent Processing - Tracks processed folders to avoid reprocessing📋 Metadata Table - Maintains processing history and state🔄 Auto-Discovery - Automatically finds new log folders in S3⚡ Incremental Updates - Processes only new data for efficiency  ","version":"Next","tagName":"h3"},{"title":"🚀 Getting Started​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#prerequisites","content":" Before deploying this blueprint, ensure you have:  ✅ S3 bucket with Spark application logs: Follow the Spark Operator blueprint to generate Spark logs.Note: Execute the steps in Put sample data in S3 section of the Execute Sample Spark job with Karpenter step to populate the S3 bucket with Spark Application Logs.✅ AWS CLI configured with appropriate permissions✅ kubectl✅ Terraform installed (&gt;= 1.0)  Generate Spark Logs First The Ray Data pipeline processes Spark application logs. Make sure you've run the taxi-trip example from the Spark Operator blueprint to populate your S3 bucket with logs. 📁 Spark Logfile Structure in S3: s3://${S3_BUCKET}/ └── spark-application-logs/ └── spark-team-a/ ├── spark-application-1234567890-driver/ │ └── stdout ├── spark-application-1234567890-exec-1/ │ └── stdout └── spark-application-1234567890-exec-2/ └── stdout Each stdout file contains JSON-formatted logs with Kubernetes metadata enrichment from Fluent Bit.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Enable Ray Data Processing​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#step-1-enable-ray-data-processing","content":" Deploy the EKS cluster with Ray Data components by enabling the enable_raydata variable. This will install:  KubeRay Operator - Manages Ray clusters on KubernetesRay Custom Resources - RayJob and RayCluster CRDsAWS Resources - IAM roles, S3 access policies, and Glue databaseRay Data Pipeline - Namespace, service accounts, and RBAC  Using TerraformUsing Install Script cd analytics/terraform/spark-k8s-operator # Deploy EKS cluster with Ray Data support enabled export TF_VAR_enable_raydata=true terraform init terraform plan terraform apply -auto-approve   Deployment Time The full deployment takes approximately 20-25 minutes to create the EKS cluster, install operators, and configure all Ray Data components.  This deployment creates:  🎯 KubeRay Operator for Ray job orchestration🔐 Ray Service Account with IRSA (IAM Roles for Service Accounts)📝 IAM Roles with S3 and Glue permissions📊 AWS Glue Database for Iceberg catalog🌐 Kubernetes Namespace (raydata)  ","version":"Next","tagName":"h3"},{"title":"Step 2: Verify KubeRay Operator Installation​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#step-2-verify-kuberay-operator-installation","content":" Confirm that the KubeRay Operator is running successfully:  kubectl get po -n kuberay-operator   Expected output:  NAME READY STATUS RESTARTS AGE kuberay-operator-74fcdcc6bf-gpl5p 1/1 Running 0 10h   ","version":"Next","tagName":"h3"},{"title":"Step 3: Configure Ray Job​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#step-3-configure-ray-job","content":" Navigate to the example directory and update the S3 configuration in the deployment script.  cd examples/raydata-sparklogs-processing-job   Replace S3_BUCKET, CLUSTER_NAME and AWS_REGION variables in the execute-rayjob.sh shell script before running.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Deploy the Ray Cluster & Execute Ray Job​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#step-4-deploy-the-ray-cluster--execute-ray-job","content":" # Make script executable chmod +x execute-rayjob.sh # Deploy the processing job ./execute-rayjob.sh deploy   ","version":"Next","tagName":"h3"},{"title":"📊 Monitoring the RayJob Deployment​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-monitoring-the-rayjob-deployment","content":" ","version":"Next","tagName":"h2"},{"title":"Check Job Status​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#check-job-status","content":" Monitor your Ray job with these commands:  # Monitor job progress in real-time ./execute-rayjob.sh monitor # Check current status ./execute-rayjob.sh status # View processing logs ./execute-rayjob.sh logs   Check RayJob Logs​  2025-07-27 22:04:46,324 - spark-log-processor - INFO - ✅ Successfully processed 1287 records from spark-fb094270bf654473b372d0f773e86687 2025-07-27 22:04:46,324 - spark-log-processor - INFO - 🎯 Processing Summary: 2025-07-27 22:04:46,324 - spark-log-processor - INFO - 📊 Total records processed: 1287 2025-07-27 22:04:46,324 - spark-log-processor - INFO - ✅ Successful folders: 1 2025-07-27 22:04:46,324 - spark-log-processor - INFO - ❌ Failed folders: 0 2025-07-27 22:04:46,324 - spark-log-processor - INFO - ✅ Successfully processed: ['spark-fb094270bf654473b372d0f773e86687'] 2025-07-27 22:04:46,324 - spark-log-processor - INFO - ✅ Metadata-driven incremental processing completed   What's Happening Behind the Scenes? When you deploy the RayJob, the following automated process occurs: 🚀 Ray Cluster Initialization - KubeRay Operator creates a Ray cluster with head and worker nodes🔍 S3 Discovery - Ray Data scans the configured S3 bucket path for folders matching spark-* pattern📊 Metadata Check - Queries the Iceberg metadata table to identify unprocessed folders📥 Parallel Processing - Ray workers read JSON log files from S3 in parallel🔄 Data Transformation - Extracts structured fields from JSON logs (timestamp, log level, component, etc.)✍️ Iceberg Writing - Writes transformed data to Apache Iceberg tables with ACID guarantees📝 Metadata Update - Records processing status in metadata table for idempotency🎯 Completion - Shuts down Ray cluster after successful processing The entire process is idempotent - you can safely re-run it without duplicating data, as it only processes new log folders.  ","version":"Next","tagName":"h3"},{"title":"Access Ray Dashboard​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#access-ray-dashboard","content":" 🎨 Ray Dashboard Access 👈  ","version":"Next","tagName":"h3"},{"title":"✅ Data Verification​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-data-verification","content":" ","version":"Next","tagName":"h2"},{"title":"S3 Bucket Structure​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#s3-bucket-structure","content":" Ray Data uses the same S3 bucket for both input Spark logs and output Iceberg data, organized in separate paths:  s3://your-spark-logs-bucket/ ├── spark-application-logs/ # 📥 Input: Raw Spark logs from Fluent Bit │ └── spark-team-a/ │ ├── spark-application-1234567890-driver/ │ │ └── stdout # JSON logs with Kubernetes metadata │ ├── spark-application-1234567890-exec-1/ │ │ └── stdout │ └── spark-application-1234567890-exec-2/ │ └── stdout │ └── iceberg-warehouse/ # 📤 Output: Processed Iceberg data └── raydata_spark_logs.db/ └── spark_logs/ ├── metadata/ # Iceberg metadata files │ ├── 00000-xxx.metadata.json │ ├── snap-xxx.avro # Snapshots for time travel │ └── version-hint.text └── data/ # Actual data in Parquet format ├── 00000-0-xxx.parquet ├── 00001-0-xxx.parquet └── ...   Same Bucket, Different Paths Input Path: s3://bucket/spark-application-logs/ - Contains raw JSON logsOutput Path: s3://bucket/iceberg-warehouse/ - Contains structured Parquet filesStorage Format: Iceberg uses efficient columnar Parquet format with metadata for ACID transactions  In AWS S3 Console, it should looks like below:    ","version":"Next","tagName":"h3"},{"title":"Log Snippet After RayData Processing​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#log-snippet-after-raydata-processing","content":" Here's how the data transformation looks before and after Ray Data processing:  Before Processing (Raw S3 JSON)After Processing (Structured Iceberg) Raw Fluent Bit logs in S3 - Each log line wrapped in JSON with redundant metadata: { &quot;log&quot;: &quot;2024-01-15 14:23:45 INFO SparkContext: Running Spark version 3.5.0\\n&quot;, &quot;stream&quot;: &quot;stdout&quot;, &quot;time&quot;: &quot;2024-01-15T14:23:45.123456Z&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;spark-driver-abc123&quot;, &quot;namespace_name&quot;: &quot;spark-team-a&quot;, &quot;pod_id&quot;: &quot;12345678-1234-1234-1234-123456789012&quot;, &quot;labels&quot;: { &quot;spark-role&quot;: &quot;driver&quot;, &quot;spark-app-id&quot;: &quot;spark-application-12345&quot; }, &quot;container_name&quot;: &quot;spark-driver&quot;, &quot;container_image&quot;: &quot;spark:3.5.0&quot; } } { &quot;log&quot;: &quot;2024-01-15 14:23:46 ERROR TaskSchedulerImpl: Lost executor 1: Container killed\\n&quot;, &quot;stream&quot;: &quot;stderr&quot;, &quot;time&quot;: &quot;2024-01-15T14:23:46.234567Z&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;spark-executor-def456&quot;, &quot;namespace_name&quot;: &quot;spark-team-a&quot;, &quot;labels&quot;: { &quot;spark-role&quot;: &quot;executor&quot;, &quot;spark-app-id&quot;: &quot;spark-application-12345&quot; } } }   ","version":"Next","tagName":"h3"},{"title":"Option 1: Query Iceberg Tables​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#option-1-query-iceberg-tables","content":" Use the built-in data verification script provided in the blueprint that automatically sets up a Python virtual environment and all required dependencies:  from your environment.  # Make script executable chmod +x verify-iceberg-data.sh   Replace S3_BUCKET and AWS_REGION variables in the verify-iceberg-data.sh shell script before running.  ./verify-iceberg-data.sh   The script automatically...  ✅ Creates an isolated Python virtual environment✅ Installs PyIceberg and all dependencies (pyiceberg[glue,s3fs]==0.7.0)✅ Connects to AWS Glue catalog and Iceberg tables✅ Performs comprehensive data validation✅ Cleans up temporary files and environment after completion  📋 Sample Script Output 👈  ","version":"Next","tagName":"h3"},{"title":"Option 2: Use AWS CLI​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#option-2-use-aws-cli","content":" Check table metadata without querying data:  # View Iceberg table in Glue catalog aws glue get-table \\ --database-name raydata_spark_logs \\ --name spark_logs \\ --query 'Table.StorageDescriptor.Location'   ","version":"Next","tagName":"h3"},{"title":"🧹 Cleanup​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-cleanup","content":" To clean up resources:  # Remove Ray job only (preserve infrastructure) ./execute-rayjob.sh cleanup # Remove all infrastructure cd analytics/terraform/spark-k8s-operator terraform destroy -var=&quot;enable_raydata_processing=true&quot;   ","version":"Next","tagName":"h2"},{"title":"🌟 Scale Your Data Pipeline​","type":1,"pageTitle":"Distributed Data Processing with Ray Data","url":"/data-on-eks/docs/blueprints/data-analytics/ray-data-processing#-scale-your-data-pipeline","content":" Scale Processing: Adjust Ray worker counts in rayjob.yaml for larger workloadsAdd Analytics: Create dashboards using Amazon QuickSight or GrafanaAutomate: Schedule regular processing with Kubernetes CronJobsExtend: Process other data types like metrics, events, or application data  Learn More 📚 Ray Data Documentation🧊 Apache Iceberg Documentation🎯 KubeRay Documentation☁️ AWS Glue Catalog  This blueprint demonstrates how Ray Data and Apache Iceberg can work together to build scalable, reliable data processing pipelines on Amazon EKS. The combination provides a modern data lake architecture with distributed processing capabilities, ACID transactions, and intelligent metadata management.  .feature-grid{grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:1.5rem;margin:2rem 0;display:grid}.feature-card{border:1px solid var(--ifm-color-emphasis-300);background:var(--ifm-background-surface-color);border-radius:8px;padding:1.5rem;transition:transform .2s,box-shadow .2s}.feature-card:hover{transform:translateY(-2px);box-shadow:0 4px 12px #0000001a}.feature-card h3{margin-top:0;margin-bottom:.5rem;font-size:1.2rem}.feature-card p{color:var(--ifm-color-content-secondary);margin:0} ","version":"Next","tagName":"h2"},{"title":"Distributed Databases on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases","content":"Distributed Databases on EKS info Note: The blueprints for distributed databases and query engines are currently in the process of development. Documentation will be updated once a deployment example has been added to the repository. Running distributed databases and query engines on Kubernetes can provide a number of benefits for organizations looking to manage and process large amounts of data in real-time. Kubernetes provides features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of these systems. There are a number of popular distributed databases and query engines that have emerged to support this use case, including Apache Cassandra, Amazon DynamoDB, and Apache Presto. These systems make it easy to manage and process large amounts of data in real-time, and provide features such as scalability, high availability, and real-time data processing. By leveraging the power of Kubernetes, organizations can simplify and automate the deployment, scaling, and management of these complex systems, freeing up resources to focus on other areas of the business. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running distributed databases and query engines in production.","keywords":"","version":"Next"},{"title":"Superset on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#introduction","content":" Apache Superset is a popular open source data exploration and visualization platform. Superset provides a rich set of data visualizations and easy ad-hoc query and analysis functionalities for data scientists, analysts, and business users.  This blueprint deploys Superset on an EKS cluster using Postgres as the backend database and Amazon Elastic Block Store (Amazon EBS) for persistent storage.  ","version":"Next","tagName":"h2"},{"title":"Superset on AWS​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#superset-on-aws","content":" On AWS, Superset can run on an EKS cluster. By using EKS, you can leverage Kubernetes for deployment, scaling, and management of Superset services. Other AWS services like VPC, IAM, and EBS provide the networking, security, and storage capabilities.  Key AWS services used:  Amazon EKS as the managed Kubernetes cluster to run Superset pods and services.Amazon EBS to provide a scalable block store for Superset persistent storage.Amazon ECR to store Docker container images for Superset and dependencies  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#deploying-the-solution","content":" The blueprint performs the following to deploy Superset on EKS:  Create a new VPC with public and private subnetsProvision an EKS cluster control plane and managed worker nodesCreate an Amazon EBS file system and access pointBuild Docker images and push to Amazon ECRInstall Superset and services on EKS via Helm chartExpose Superset UI through a load balancer  Ingress is enabled and AWS LoadBalancer Controller will provision an ALB to expose the Superset frontend UI.  info You may customize the blueprint by changing values in variables.tf, to deploy to a different region (default to us-west-1 ), use different cluster name, number of subnets / AZs, or disable addons like fluentbit  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraformHelm  ","version":"Next","tagName":"h3"},{"title":"Deploy​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#deploy","content":" Clone the repository  git clone https://github.com/awslabs/data-on-eks.git   Navigate into one of the example directories and run install.sh script  cd data-on-eks/analytics/terraform/superset-on-eks chmod +x install.sh ./install.sh   or simply  terraform init terraform apply --auto-approve   ","version":"Next","tagName":"h3"},{"title":"Architecture Overview​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#architecture-overview","content":"   ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#verify-deployment","content":" After the deployment completes, we can access the Superset UI . For demo purpose, this blueprint creates the Ingress object for the Superset FrontEnd UI with public LoadBalancer and also number of pods across 2 AZ's in the corenode and superset node respectively.    You may find the URL to the Superset frontend from the output superset_url, or by running kubectl command below:  kubectl get ingress -n superset # OUTPUT should looks like below NAME CLASS HOSTS ADDRESS PORTS AGE superset-ingress aws-alb * k8s-superset-***.***.elb.amazonaws.com 80 125m   Copy the ADDRESS field from the output, then open browser and enter the URL as http://&lt;address&gt;/. Enter admin as both user name and password when prompted. We can view the Superset UI like below.    In order to visualize data, we need to first connect to Postgres database. the IP address of the database can be obtained by describing a pod 'superset-postgresql-0'. Basically the database is hosted on the superset-node  k describe po superset-postgresql-0 -n superset   After obtaining the IP address, database connection can be established as per the screenshot below  Once the database is connected, it has to be configured to allow file upload. This features allows, csv and other format files to be uploaded as a new table. Please refer to the screeshots below  Step -1 : Edit database configuration and navigate to 'ADVANCED' settings    Step - 2 : Under Security scroll down to the very end and 'check allow file uploads to the database'    Step -3 : Create a dataset by uploading a file    Step - 4 : To show a sample visualization a sample CSV of COVID research across various countries was uploaded. Here are a few visualizations that shows countries porgess with respect to trial of various vaccines      ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"Superset on EKS","url":"/data-on-eks/docs/blueprints/data-analytics/superset-on-eks#cleanup","content":" To clean up your environment, run the cleanup.sh script.  chmod +x cleanup.sh ./cleanup.sh   otherwise  terraform destroy --auto-approve  ","version":"Next","tagName":"h2"},{"title":"Aerospike Database Enterprise on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases/aerospike","content":"","keywords":"","version":"Next"},{"title":"Key Benefits of Aerospike on EKS​","type":1,"pageTitle":"Aerospike Database Enterprise on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/aerospike#key-benefits-of-aerospike-on-eks","content":" Predictable Low Latency: Aerospike ensures sub-millisecond response times, which is critical for applications requiring immediate data access, such as fraud detection, recommendation engines, and real-time bidding systems. Seamless Scalability: Aerospike’s Hybrid-Memory Architecture allows for efficient horizontal and vertical scaling, accommodating data growth from terabytes to petabytes without compromising performance. Deploying on EKS enables dynamic resource allocation to meet varying workload demands effectively. High Availability and Resilience: With features like Cross-Datacenter Replication (XDR), Aerospike provides fine-grained control for asynchronous data replication across geographically dispersed clusters. This ensures data availability and compliance with data locality regulations. EKS enhances this resilience by distributing workloads across multiple availability zones. Operational Efficiency: The Aerospike Kubernetes Operator automates routine tasks such as configuration, scaling, and upgrading Aerospike clusters. This reduces operational complexity and overhead, allowing teams to focus on delivering value rather than managing infrastructure. Cost Optimization: Leveraging Aerospike’s efficient use of resources and EKS’s flexible infrastructure, organizations can achieve significant cost savings compared to traditional deployments. Aerospike’s ability to deliver high performance with a reduced hardware footprint translates to lower total cost of ownership.  ","version":"Next","tagName":"h2"},{"title":"Community and Support​","type":1,"pageTitle":"Aerospike Database Enterprise on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/aerospike#community-and-support","content":" For assistance and to engage with the community:  Aerospike Documentation: Comprehensive guides and references are available in the Aerospike Documentation. GitHub Repository: Access the AKO source code, report issues, and contribute at the aerospike-kubernetes-operator GitHub repository. Community Forum: Join discussions and seek advice on the Aerospike Community Forum.  By deploying Aerospike Database Enterprise Edition on Amazon EKS, organizations can harness the combined strengths of Aerospike’s high-performance data management and Kubernetes orchestration capabilities, resulting in a scalable, resilient, and cost-effective solution for real-time data applications.  ","version":"Next","tagName":"h2"},{"title":"Deploying Aerospike on EKS​","type":1,"pageTitle":"Aerospike Database Enterprise on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/aerospike#deploying-aerospike-on-eks","content":" To deploy the Aerospike Database Enterprise Edition on an EKS cluster, we recommend using the Aerospike on EKS blueprint maintained by Aerospike, an AWS Partner and the maintainer of the Aerospike Kubernetes Operator.  If you encounter any issues with the blueprint or the operator, please report them on the relevant Aerospike GitHub repository. ","version":"Next","tagName":"h2"},{"title":"ClickHouse on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases/clickhouse","content":"ClickHouse on EKS ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP) that is open sourced under the Apache 2.0 license. OLAP is software technology you can use to analyze business data from different points of view. Organizations collect and store data from multiple data sources, such as websites, applications, smart meters, and internal systems. OLAP helps organizations process and benefit from a growing amount of information by combining and groups this data into categories to provide actionable insights for strategic planning. For example, a retailer stores data about all the products it sells, such as color, size, cost, and location. The retailer also collects customer purchase data, such as the name of the items ordered and total sales value, in a different system. OLAP combines the datasets to answer questions such as which color products are more popular or how product placement impacts sales. Some key benefits of Clickhouse include: Real-Time Analytics: ClickHouse can handle real-time data ingestion and analysis, making it suitable for use cases such as monitoring, logging, and event data processing.High Performance: ClickHouse is optimized for analytical workloads, providing fast query execution and high throughput.Scalability: ClickHouse is designed to scale horizontally across multiple nodes, allowing users to store and process petabytes of data across a distributed cluster. It supports sharding and replication for high availability and fault tolerance.Column-Oriented Storage: ClickHouse organizes data by columns rather than rows, which allows for efficient compression and faster query processing, especially for queries that involve aggregations and scans of large datasets.SQL Support: ClickHouse supports a subset of SQL, making it familiar and easy to use for developers and analysts who are already familiar with SQL-based databases.Integrated Data Formats: ClickHouse supports various data formats, including CSV, JSON, Apache Avro, and Apache Parquet, making it flexible for ingesting and querying different types of data. To deploy Clickhouse on EKS, we recommend this Clickhouse on EKS blueprint from Altinity, an AWS Partner who maintains the Clickouse Kubernetes Operator. If you have any issues with the blueprint or operator, please create an issue on the corresponding Altinity GitHub repository.","keywords":"","version":"Next"},{"title":"S3 Tables with Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables","content":"","keywords":"","version":"Next"},{"title":"What is S3 Tables?​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#what-is-s3-tables","content":" Amazon S3 Tables is a fully managed tabular data store purpose-built to optimize performance, simplify security, and provide cost-efficient storage for large-scale analytics workloads. It integrates directly with services like Amazon EMR, Amazon Athena, Amazon Redshift, AWS Glue, and AWS Lake Formation, offering a seamless experience for running analytics and machine learning workloads.  ","version":"Next","tagName":"h2"},{"title":"Why Run S3 Tables on Amazon EKS?​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#why-run-s3-tables-on-amazon-eks","content":" For users who have adopted Amazon EKS for Spark workloads and are using table formats like Iceberg, leveraging S3 Tables offers advantages in performance, cost-efficiency, and security controls. This integration allows organizations to combine Kubernetes-native features with the capabilities of S3 Tables, potentially improving query performance and resource scaling within their existing environment. By following the steps detailed in this document, users can seamlessly integrate S3 Tables into their EKS setup, providing a flexible and complementary solution for analytics workloads.  ","version":"Next","tagName":"h2"},{"title":"How S3 Tables Differ from Iceberg Table Format​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#how-s3-tables-differ-from-iceberg-table-format","content":" While S3 Tables use Apache Iceberg as an underlying implementation, they offer enhanced capabilities specifically designed for AWS customers:  🛠️ Automatic Compaction: S3 Tables implements automatic compaction, which intelligently optimizes data storage in the background by combining smaller files into larger, more efficient ones. This process reduces storage costs, improves query speed, and operates continuously without manual intervention. 🔄 Table Maintenance: It offers critical maintenance tasks like snapshot management and unreferenced file removal. This continuous optimization ensures that tables remain performant and cost-effective without manual intervention, reducing operational overhead and allowing teams to focus on data insights. ❄️ Apache Iceberg Support: Offers built-in support for Apache Iceberg, which simplifies managing data lakes at scale while improving query performance and reducing costs. Consider using S3 Tables for your data lake if you want to experience the following results. 🔒 Simplified Security: S3 Tables treat your tables as AWS resources, enabling fine-grained AWS Identity and Access Management (IAM) permissions at the table level. This simplifies data governance, enhances security, and makes access control more intuitive and manageable with your familiar AWS services. ⚡ Enhanced Performance: Amazon S3 Tables introduce a new type of bucket, purpose-built for storing Apache Iceberg tables. Table buckets deliver up to 3x faster query performance and up to 10x higher transactions per second compared to storing Iceberg tables in general-purpose S3 buckets. This performance enhancement supports high-frequency updates, real-time ingestion, and more demanding workloads, ensuring scalability and responsiveness as data volumes grow. 🛠️ Integration with AWS Services: S3 Tables are tightly integrated with AWS analytics services such as Athena, Redshift, EMR, and Glue, providing native support for analytics workloads.  Deploying the Solution 👈  ","version":"Next","tagName":"h2"},{"title":"Execute Sample Spark job​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#execute-sample-spark-job","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Create the S3 Tables compatible Apache Spark Docker Image​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-1-create-the-s3-tables-compatible-apache-spark-docker-image","content":" Create a Docker image with necessary jars for S3 tables communication.  Review the sample DockerfileNote the key jar files for S3 Tables interaction, including Iceberg, AWS SDK bundle, and S3 Tables Catalog for Iceberg runtimeCustomize the Dockerfile as needed for your environmentBuild the Docker image and push the image to your preferred container registry  We have created a docker image and published in ECR for the demo purpose only.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Create Test Data for the job​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-2-create-test-data-for-the-job","content":" Navigate to the example directory and generate sample employee data for Spark job input using this shell script.  cd analytics/terraform/spark-k8s-operator/examples/s3-tables ./input-data-gen.sh   This script will create a file named employee_data.csv in your current directory. By default, it generates 100 records.  Note: If you need to adjust the number of records, you can modify the input-data-gen.sh script. Look for the loop that generates the data and change the iteration count as needed.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Upload Test Input data to Amazon S3 Bucket​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-3-upload-test-input-data-to-amazon-s3-bucket","content":" Replace &lt;YOUR_S3_BUCKET&gt; with the name of the S3 bucket created by your blueprint and run the below command.  aws s3 cp employee_data.csv s3://&lt;S3_BUCKET&gt;/s3table-example/input/   This command will upload the CSV file to your S3 bucket. The Spark job will later reference this path to read the input data. Ensure you have the necessary permissions to write to this bucket before executing the command.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Upload PySpark Script to S3 Bucket​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-4-upload-pyspark-script-to-s3-bucket","content":" The following script is the snippet of the Spark job where you can see the Spark config required to work with S3 Tables.  def main(args): if len(args) != 3: logger.error(&quot;Usage: spark-etl [input-csv-path] [s3table-arn]&quot;) sys.exit(1) # Input parameters input_csv_path = args[1] # Path to the input CSV file s3table_arn = args[2] # s3table arn # Initialize Spark session logger.info(&quot;Initializing Spark Session&quot;) spark = (SparkSession .builder .appName(f&quot;{AppName}_{dt_string}&quot;) .config(&quot;spark.sql.extensions&quot;, &quot;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions&quot;) .config(&quot;spark.sql.catalog.s3tablesbucket&quot;, &quot;org.apache.iceberg.spark.SparkCatalog&quot;) .config(&quot;spark.sql.catalog.s3tablesbucket.catalog-impl&quot;, &quot;software.amazon.s3tables.iceberg.S3TablesCatalog&quot;) .config(&quot;spark.sql.catalog.s3tablesbucket.warehouse&quot;, s3table_arn) .config('spark.hadoop.fs.s3.impl', &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;) .config(&quot;spark.sql.defaultCatalog&quot;, &quot;s3tablesbucket&quot;) .getOrCreate()) spark.sparkContext.setLogLevel(&quot;INFO&quot;) logger.info(&quot;Spark session initialized successfully&quot;) namespace = &quot;doeks_namespace&quot; table_name = &quot;employee_s3_table&quot; full_table_name = f&quot;s3tablesbucket.{namespace}.{table_name}&quot; ...   Replace S3_BUCKET with the name of the S3 bucket created by your blueprint and run the below command to upload sample Spark job to S3 buckets.  aws s3 cp s3table-iceberg-pyspark.py s3://&lt;S3_BUCKET&gt;/s3table-example/scripts/   Navigate to example directory and submit the Spark job.  ","version":"Next","tagName":"h3"},{"title":"Step 5: Create Amazon S3 table bucket​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-5-create-amazon-s3-table-bucket","content":" This is the main step where you will create an S3 table bucket that will be used for S3 Tables, which your PySpark job will access later.  Replace &lt;S3TABLE_BUCKET_NAME&gt; with your desired bucket name. Replace &lt;REGION&gt; with your AWS region.  aws s3tables create-table-bucket \\ --region &quot;&lt;REGION&gt;&quot; \\ --name &quot;&lt;S3TABLE_BUCKET_NAME&gt;&quot;   Make note of the S3TABLE BUCKET ARN generated by this command. Verify the S3 table bucket ARN from AWS Console.    ","version":"Next","tagName":"h3"},{"title":"Step 6: Update Spark Operator YAML File​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-6-update-spark-operator-yaml-file","content":" Update the Spark Operator YAML file as below:  Open s3table-spark-operator.yaml file in your preferred text editor.Replace &lt;S3_BUCKET&gt; with your S3 bucket created by this blueprint(Check Terraform outputs). S3 bucket is the place where you copied the test data and sample spark job in the above steps.REPLACE &lt;S3TABLE_BUCKET_ARN&gt; with your S3 table bucket ARN captured in the previous step.  You can see the snippet of Spark Operator Job config below.  --- apiVersion: &quot;sparkoperator.k8s.io/v1beta2&quot; kind: SparkApplication metadata: name: &quot;s3table-example&quot; namespace: spark-team-a labels: app: &quot;s3table-example&quot; applicationId: &quot;s3table-example-nvme&quot; spec: type: Python sparkVersion: &quot;3.5.3&quot; mode: cluster # CAUTION: Unsupported test image # This image is created solely for testing and reference purposes. # Before use, please: # 1. Review the Dockerfile used to create this image # 2. Create your own image that meets your organization's security requirements image: &quot;public.ecr.aws/data-on-eks/spark:3.5.3-scala2.12-java17-python3-ubuntu-s3table0.1.3-iceberg1.6.1&quot; imagePullPolicy: IfNotPresent mainApplicationFile: &quot;s3a://&lt;S3_BUCKET&gt;/s3table-example/scripts/s3table-iceberg-pyspark.py&quot; arguments: - &quot;s3a://&lt;S3_BUCKET&gt;/s3table-example/input/&quot; - &quot;&lt;S3TABLE_BUCKET_ARN&gt;&quot; sparkConf: &quot;spark.app.name&quot;: &quot;s3table-example&quot; &quot;spark.kubernetes.driver.pod.name&quot;: &quot;s3table-example&quot; &quot;spark.kubernetes.executor.podNamePrefix&quot;: &quot;s3table-example&quot; &quot;spark.local.dir&quot;: &quot;/data&quot; &quot;spark.speculation&quot;: &quot;false&quot; &quot;spark.network.timeout&quot;: &quot;2400&quot; &quot;spark.hadoop.fs.s3a.connection.timeout&quot;: &quot;1200000&quot; &quot;spark.hadoop.fs.s3a.path.style.access&quot;: &quot;true&quot; &quot;spark.hadoop.fs.s3a.connection.maximum&quot;: &quot;200&quot; &quot;spark.hadoop.fs.s3a.fast.upload&quot;: &quot;true&quot; &quot;spark.hadoop.fs.s3a.readahead.range&quot;: &quot;256K&quot; &quot;spark.hadoop.fs.s3a.input.fadvise&quot;: &quot;random&quot; &quot;spark.hadoop.fs.s3a.aws.credentials.provider.mapping&quot;: &quot;com.amazonaws.auth.WebIdentityTokenCredentialsProvider=software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider&quot; &quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;: &quot;software.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider&quot; # AWS SDK V2 https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/aws_sdk_upgrade.html ...   ","version":"Next","tagName":"h3"},{"title":"Step 7: Execute Spark Job​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-7-execute-spark-job","content":" Apply the updated YAML configuration file to your Kubernetes cluster to submit and execute the Spark job:  cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/s3-tables kubectl apply -f s3table-spark-operator.yaml   This will schedule the Spark job on the EKS cluster. Spark Operator handles submitting the job to the Kubernetes API Server.  Kubernetes will schedule the Spark driver and executor pods to run on separate worker nodes. Karpenter will automatically provision new nodes if required, based on the nodepool configuration in the Terraform scripts.  Monitor the logs of the Spark driver pod to track job progress. The pods default to c5d instances, but you can modify the YAML and Karpenter nodepool to use different EC2 instance types if needed.  ","version":"Next","tagName":"h3"},{"title":"Step 8: Verify the Spark Driver log for the output​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#step-8-verify-the-spark-driver-log-for-the-output","content":" List the pods running under the spark-team-a namespace:  kubectl get pods -n spark-team-a   Verify the driver logs to see the full output of the Spark job. The job reads the CSV data from the S3 bucket and writes it back to the S3 Tables bucket using the Iceberg format. It also counts the number of records processed and displays the first 10 records:  kubectl logs &lt;spark-driver-pod-name&gt; -n spark-team-a   You should see the Spark driver pod transition to a Succeeded state when the job completes successfully and the log should show output like below.  ... [2025-01-07 22:07:44,185] INFO @ line 59: Previewing employee data schema root |-- id: integer (nullable = true) |-- name: string (nullable = true) |-- level: string (nullable = true) |-- salary: double (nullable = true) .... 25/01/07 22:07:44 INFO CodeGenerator: Code generated in 10.594982 ms +---+-----------+------+--------+ |id |name |level |salary | +---+-----------+------+--------+ |1 |Employee_1 |Mid |134000.0| |2 |Employee_2 |Senior|162500.0| |3 |Employee_3 |Senior|174500.0| |4 |Employee_4 |Exec |69500.0 | |5 |Employee_5 |Senior|54500.0 | |6 |Employee_6 |Mid |164000.0| |7 |Employee_7 |Junior|119000.0| |8 |Employee_8 |Senior|54500.0 | |9 |Employee_9 |Senior|57500.0 | |10 |Employee_10|Mid |152000.0| +---+-----------+------+--------+ only showing top 10 rows ....   You should see a new Table and Namespace once the job is successful as shown in the image below.    The following commands will show the additional information on the S3 Tables.  ","version":"Next","tagName":"h3"},{"title":"Verify the S3Table using S3Table API​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#verify-the-s3table-using-s3table-api","content":" Use the S3Table API to confirm the table was created successfully. Just replace the &lt;ACCOUNT_ID&gt; and run the command.  aws s3tables get-table --table-bucket-arn arn:aws:s3tables:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:bucket/doeks-spark-on-eks-s3table \\ --namespace doeks_namespace \\ --name employee_s3_table   Output looks like below:  Output looks like below. { &quot;name&quot;: &quot;employee_s3_table&quot;, &quot;type&quot;: &quot;customer&quot;, &quot;tableARN&quot;: &quot;arn:aws:s3tables:us-west-2:&lt;ACCOUNT_ID&gt;:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf&quot;, &quot;namespace&quot;: [ &quot;doeks_namespace&quot; ], &quot;versionToken&quot;: &quot;aafc39ddd462690d2a0c&quot;, &quot;metadataLocation&quot;: &quot;s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3/metadata/00004-62cc4be3-59b5-4647-a78d-1cdf69ec5ed8.metadata.json&quot;, &quot;warehouseLocation&quot;: &quot;s3://55511111-7a03-4513-asdfsafdsfdsf--table-s3&quot;, &quot;createdAt&quot;: &quot;2025-01-07T22:14:48.689581+00:00&quot;, &quot;createdBy&quot;: &quot;&lt;ACCOUNT_ID&gt;&quot;, &quot;modifiedAt&quot;: &quot;2025-01-09T00:06:09.222917+00:00&quot;, &quot;ownerAccountId&quot;: &quot;&lt;ACCOUNT_ID&gt;&quot;, &quot;format&quot;: &quot;ICEBERG&quot; }   ","version":"Next","tagName":"h3"},{"title":"Monitor the table maintenance job status:​","type":1,"pageTitle":"S3 Tables with Amazon EKS","url":"/data-on-eks/docs/blueprints/data-analytics/spark-operator-s3tables#monitor-the-table-maintenance-job-status","content":" aws s3tables get-table-maintenance-job-status --table-bucket-arn arn:aws:s3tables:us-west-2:&quot;\\&lt;ACCOUNT_ID&gt;:bucket/doeks-spark-on-eks-s3table --namespace doeks_namespace --name employee_s3_table   This command provides information about Iceberg compaction, snapshot management, and unreferenced file removal processes.  { &quot;tableARN&quot;: &quot;arn:aws:s3tables:us-west-2:&lt;ACCOUNT_ID&gt;:bucket/doeks-spark-on-eks-s3table/table/55511111-7a03-4513-b921-e372b0030daf&quot;, &quot;status&quot;: { &quot;icebergCompaction&quot;: { &quot;status&quot;: &quot;Successful&quot;, &quot;lastRunTimestamp&quot;: &quot;2025-01-08T01:18:08.857000+00:00&quot; }, &quot;icebergSnapshotManagement&quot;: { &quot;status&quot;: &quot;Successful&quot;, &quot;lastRunTimestamp&quot;: &quot;2025-01-08T22:17:08.811000+00:00&quot; }, &quot;icebergUnreferencedFileRemoval&quot;: { &quot;status&quot;: &quot;Successful&quot;, &quot;lastRunTimestamp&quot;: &quot;2025-01-08T22:17:10.377000+00:00&quot; } } }   info To work with S3 Tables on EKS, both Node-level policies and Pod-level policies are required. Node-level policies: These are added to the Karpenter Node IAM role. For reference, you can view the permissions configuration in the addons.tf file. Pod-level policies: These are necessary for creating namespaces, managing tables, and reading/writing data to the tables. https://github.com/awslabs/data-on-eks/blob/e3f1a6b08d719fc69f61d18b57cd5ad09cb01bd5/analytics/terraform/spark-k8s-operator/main.tf#L98C1-L156C2 are granted through IAM Roles for Service Accounts (IRSA) for the spark-team-a namespace. This ensures that the Spark job pods have the required access to perform operations on S3 Tables. By configuring these permissions appropriately, you can ensure seamless execution of Spark jobs and secure access to resources. Please note that these policies can further be adjusted and make it more granular based on your security requirements.  Using S3 Tables with JupyterLab 👈  Cleanup 👈 ","version":"Next","tagName":"h3"},{"title":"Deploying Apache Pinot (🍷) on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#architecture","content":"   In this setup we deploy all Apache Pinot components in private subnets across 3 availability zones. This allows for greater flexibility and resilience. Most pinot components can run on latest generation general purpose compute instances (m7i) except for server component which requires memory optimized instance types (r7i). We also setup internal NLB to easily communicate with Controller and Broker components.  Note: All Apache Pinot components run on StatefulSet.  Note: This blueprint doesn't leverage DeepStore currently and uses EBS volumes to store table segments on server.  Note: Based on your use case, you will need to update the cluster size and configuration to better suite your use case. You can read more about Apache Pinot capacity planning here and here.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites 📝​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#prerequisites-","content":" Ensure that you have following tools installed on your machine.  aws clikubectlterraform  ","version":"Next","tagName":"h2"},{"title":"Deployment ⚙️​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#deployment-️","content":" ","version":"Next","tagName":"h2"},{"title":"Deploy the EKS Cluster with Apache Pinot​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#deploy-the-eks-cluster-with-apache-pinot","content":" First, clone the repository.  git clone https://github.com/awslabs/data-on-eks.git   Navigate to apache pinot folder and create terraform.tfvars to provide desired values for all the variables. This is also the time to update any other input variables or make any other changes to the terraform template.  cd data-on-eks/distributed-databases/pinot touch terraform.tfvars   Sample terraform.tfvars​  name = &quot;pinot-on-eks&quot; region = &quot;us-west-2&quot; eks_cluster_version = &quot;1.25&quot; ...   Once you have updated your variables, you can run the install script to deploy your pre-configured EKS cluster with Apache Pinot.  ./install.sh   ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#verify-deployment","content":" Verify the Amazon EKS Cluster  aws eks describe-cluster --name pinot-on-eks   Update local kubeconfig so we can access kubernetes cluster.  aws eks update-kubeconfig --name pinot-on-eks --region us-west-2   First, lets verify that we have worker nodes running in the cluster.  kubectl get nodes   Output​  NAME STATUS ROLES AGE VERSION ip-10-1-189-200.us-west-2.compute.internal Ready &lt;none&gt; 12d v1.24.17-eks-43840fb ip-10-1-46-117.us-west-2.compute.internal Ready &lt;none&gt; 12d v1.24.17-eks-43840fb ip-10-1-84-80.us-west-2.compute.internal Ready &lt;none&gt; 12d v1.24.17-eks-43840fb   Next, lets verify all the pods are running.  kubectl get pods -n pinot   Output​  NAME READY STATUS RESTARTS AGE pinot-broker-0 1/1 Running 0 11d pinot-broker-1 1/1 Running 0 11d pinot-broker-2 1/1 Running 0 11d pinot-controller-0 1/1 Running 0 11d pinot-controller-1 1/1 Running 0 11d pinot-controller-2 1/1 Running 0 11d pinot-minion-stateless-86cf65f89-rlpwn 1/1 Running 0 12d pinot-minion-stateless-86cf65f89-tkbjf 1/1 Running 0 12d pinot-minion-stateless-86cf65f89-twp8n 1/1 Running 0 12d pinot-server-0 1/1 Running 0 11d pinot-server-1 1/1 Running 0 11d pinot-server-2 1/1 Running 0 11d pinot-zookeeper-0 1/1 Running 0 12d pinot-zookeeper-1 1/1 Running 0 12d pinot-zookeeper-2 1/1 Running 0 12d   We have also deployed prometheus and grafana under monitoring namespace. So also make sure all the pods for monitoring are also running.  kubectl get pods -n monitoring   Output​  prometheus-grafana-85b4584dbf-4l72l 3/3 Running 0 12d prometheus-kube-prometheus-operator-84dcddccfc-pv8nv 1/1 Running 0 12d prometheus-kube-state-metrics-57f6b6b4fd-txjtb 1/1 Running 0 12d prometheus-prometheus-kube-prometheus-prometheus-0 2/2 Running 0 4d3h prometheus-prometheus-node-exporter-4jh8q 1/1 Running 0 12d prometheus-prometheus-node-exporter-f5znb 1/1 Running 0 12d prometheus-prometheus-node-exporter-f9xrz 1/1 Running 0 12d   Now lets access Apache Pinot Console using the below command. Console consist of Cluster Manager, Query Explorer, Zookeeper Browser and Swagger REST API Explorer.  kubectl port-forward service/pinot-controller 9000:9000 -n pinot   This will allow you to access Apache Pinot Console like the one shown below using http://localhost:9000    Apache Pinot supports exporting metrics using Prometheus JMX exporter that is packaged within the Apache Pinot docker image. Lets ensure metrics from all Apache Pinot components are getting published to prometheus.  kubectl port-forward service/prometheus-kube-prometheus-prometheus 9090:9090 -n monitoring   Navigate to the prometheus UI at http://localhost:9090, type pinot in the search box and you should be able to see all the metrics.    Next, Let's use Grafana to visualize the Apache Pinot metrics. In order to access Grafana, we need to get the grafana password from AWS Secrets Manager.  aws secretsmanager get-secret-value --secret-id pinot-on-eks-grafana | jq '.SecretString' --raw-output   Now use the port-forwarding to access Grafana at port 8080  kubectl port-forward service/prometheus-grafana 8080:80 -n monitoring   Login to grafana dashboard using admin and password retrieved in the previous step and then navigate to Dashboard and click New and then Import. Use the file pinot.json under data-on-eks/distributed-database/pinot/dashboard to create a pinot dashboard.    To learn more about the monitoring of Apache Pinot using Prometheus and Grafana use the official guide.  ","version":"Next","tagName":"h3"},{"title":"Additional Deployment (Optional) 🏆​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#additional-deployment-optional-","content":" ","version":"Next","tagName":"h2"},{"title":"Deploy Apache Kafka for Streaming Data​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#deploy-apache-kafka-for-streaming-data","content":" Apache Pinot can ingest data from streaming data sources (real-time) as well as batch data sources (offline). In this example, we will leverage Apache Kafka to push real-time data to a topic.  If you already have Apache Kafka running in your EKS cluster or you are leveraging Amazon Managed Streaming for Apache Kafka (MSK) you can skip this step. Otherwise, follow the steps below to install Kafka in your EKS cluster.  Note: Following deployment configure Kafka Brokers with PLAINTEXT listeners for simplified deployment. Modify the kafka-values.yaml file for production deployment  helm repo add bitnami https://charts.bitnami.com/bitnami helm install -n pinot pinot-kafka bitnami/kafka --values ./helm/kafka-values.yaml   Output​  NAME: pinot-kafka LAST DEPLOYED: Tue Oct 24 01:10:25 2023 NAMESPACE: pinot STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: kafka CHART VERSION: 26.2.0 APP VERSION: 3.6.0 ** Please be patient while the chart is being deployed ** Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster: pinot-kafka.pinot.svc.cluster.local Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster: pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 pinot-kafka-controller-1.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 pinot-kafka-controller-2.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 To create a pod that you can use as a Kafka client run the following commands: kubectl run pinot-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.6.0-debian-11-r0 --namespace pinot --command -- sleep infinity kubectl exec --tty -i pinot-kafka-client --namespace pinot -- bash PRODUCER: kafka-console-producer.sh \\ --broker-list pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092,pinot-kafka-controller-1.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092,pinot-kafka-controller-2.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 \\ --topic test CONSUMER: kafka-console-consumer.sh \\ --bootstrap-server pinot-kafka.pinot.svc.cluster.local:9092 \\ --topic test \\ --from-beginning   Use the command mentioned above to create Kafka Client pod within your namespace.  kubectl run pinot-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.6.0-debian-11-r0 --namespace pinot --command -- sleep infinity   and then attach to the container shell  kubectl exec --tty -i pinot-kafka-client --namespace pinot -- bash   Create Kafka topics using the below commands, which will then be used to publish messages.  kafka-topics.sh --bootstrap-server pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 --topic flights-realtime --create --partitions 1 --replication-factor 1 kafka-topics.sh --bootstrap-server pinot-kafka-controller-0.pinot-kafka-controller-headless.pinot.svc.cluster.local:9092 --topic flights-realtime-avro --create --partitions 1 --replication-factor 1   and then exit from the container shell  exit   Use provided example/pinot-realtime-quickstart.yml to create tables and publish sample data to the above topics, which will then get ingested into tables.  kubectl apply -f example/pinot-realtime-quickstart.yml   Now, let's navigate back to Query Console and then click one of the tables. You should be able to see the newly created tables and data coming into tables.  kubectl port-forward service/pinot-controller 9000:9000 -n pinot     ","version":"Next","tagName":"h3"},{"title":"Cleanup 🧹​","type":1,"pageTitle":"Deploying Apache Pinot (🍷) on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/pinot#cleanup-","content":" To delete all the components provisioned as part of this blueprint, using the following command to destroy all the resources.  ./cleanup.sh   caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ex. Delete kafka-on-eks EBS volumes ","version":"Next","tagName":"h2"},{"title":"Job Schedulers","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/job-schedulers","content":"Job Schedulers Job schedulers are an essential component of many organizations' infrastructure, helping to automate and manage complex workflows. When deployed on Kubernetes, job schedulers can take advantage of the platform's features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability. Tools like Apache Airflow, Argo Workflow, and Amazon MWAA provide a simple and efficient way to manage and schedule jobs on a Kubernetes cluster. These tools are well-suited for a wide range of use cases, including data pipelines, machine learning workflows, and batch processing. By leveraging the power of Kubernetes, organizations can simplify and automate the management of their job schedulers, freeing up resources to focus on other areas of the business. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running job schedulers in production. The following are the most popular job scheduling tools used with data workloads. This section provides deployment patterns for the following tools and examples to trigger Spark/ML jobs using these schedulers. Apache AirflowAmazon Managed Workflows for Apache Airflow (MWAA)Argo WorkflowPrefectAWS Batch","keywords":"","version":"Next"},{"title":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#introduction","content":" CloudNativePG is an open sourceoperatordesigned to manage PostgreSQL workloads Kubernetes.  It defines a new Kubernetes resource called Cluster representing a PostgreSQL cluster made up of a single primary and an optional number of replicas that co-exist in a chosen Kubernetes namespace for High Availability and offloading of read-only queries.  Applications that reside in the same Kubernetes cluster can access the PostgreSQL database using a service which is solely managed by the operator, without having to worry about changes of the primary role following a failover or a switchover. Applications that reside outside the Kubernetes cluster, need to configure a Service or Ingress object to expose the Postgres via TCP. Web applications can take advantage of the native connection pooler based on PgBouncer.  CloudNativePG was originally built by EDB, then released open source under Apache License 2.0 and submitted for CNCF Sandbox in April 2022. The source code repository is in Github.  More details about the project will be found on this link  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#deploying-the-solution","content":" Let's go through the deployment steps  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraformpsql  ","version":"Next","tagName":"h3"},{"title":"Deploy the EKS Cluster with CloudNativePG Operator​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#deploy-the-eks-cluster-with-cloudnativepg-operator","content":" First, clone the repository  git clone https://github.com/awslabs/data-on-eks.git   Navigate into cloudnative-postgres folder and run install.sh script. By default the script deploys EKS cluster to us-west-2 region. Update variables.tf to change the region. This is also the time to update any other input variables or make any other changes to the terraform template.  cd data-on-eks/distributed-databases/cloudnative-postgres ./install.sh   ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#verify-deployment","content":" Verify the Amazon EKS Cluster  aws eks describe-cluster --name cnpg-on-eks   Update local kubeconfig so we can access kubernetes cluster  aws eks update-kubeconfig --name cnpg-on-eks --region us-west-2   First, lets verify that we have worker nodes running in the cluster.  kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-1-10-192.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-10-249.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-11-38.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-12-195.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af   Next, lets verify all the pods are running.  kubectl get pods --namespace=monitoring NAME READY STATUS RESTARTS AGE alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 1 (4d17h ago) 4d17h kube-prometheus-stack-grafana-7f8b9dc64b-sb27n 3/3 Running 0 4d17h kube-prometheus-stack-kube-state-metrics-5979d9d98c-r9fxn 1/1 Running 0 60m kube-prometheus-stack-operator-554b6f9965-zqszr 1/1 Running 0 60m prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 0 4d17h kubectl get pods --namespace=cnpg-system NAME READY STATUS RESTARTS AGE cnpg-on-eks-cloudnative-pg-587d5d8fc5-65z9j 1/1 Running 0 4d17h   ","version":"Next","tagName":"h3"},{"title":"Deploy a PostgreSQL cluster​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#deploy-a-postgresql-cluster","content":" First of all, we need to create a storageclass using the ebs-csi-driver, a demo namespace and kubernetes secrets for login/password for database authentication app-auth. Check examples folder for all kubernetes manifests.  Storage​  For running a highly scalable and durable self-managed PostgreSQL database on Kubernetes with Amazon EKS and EC2, it is recommended to use Amazon Elastic Block Store (EBS) volumes that provide high performance and fault tolerance. The preferred EBS volume types for this use case are:  1.Provisioned IOPS SSD (io2 or io1):  Designed for I/O-intensive workloads such as databases.Offers consistent and low-latency performance.Allows you to provision a specific number of IOPS (input/output operations per second) according to your requirements.Provides up to 64,000 IOPS per volume and 1,000 MB/s throughput, making it suitable for demanding database workloads.  2.General Purpose SSD (gp3 or gp2):  Suitable for most workloads and offers a balance between performance and cost.Provides a baseline performance of 3,000 IOPS and 125 MB/s throughput per volume, which can be increased if needed (up to 16,000 IOPS and 1,000 MB/s for gp3).Recommended for less I/O-intensive database workloads or when cost is a primary concern.  You can find both storageclass template in examples folder.  kubectl create -f examples/storageclass.yaml kubectl create -f examples/auth-prod.yaml   As with any other deployment in Kubernetes, to deploy a PostgreSQL cluster you need to apply a configuration file that defines your desired Cluster. CloudNativePG operator offers two type of Bootstrapping a new database:  Bootstrap an empty clusterBootstrap From another cluster.  In this first example, we are going to create a new empty database cluster using initdbflags. We are going to use the template below by modifying the IAM role for IRSA configuration 1 and S3 bucket for backup restore process and WAL archiving 2. The Terraform could already created this use terraform output to extract these parameters:  cd data-on-eks/distributed-databases/cloudnative-postgres terraform output barman_backup_irsa = &quot;arn:aws:iam::&lt;your_account_id&gt;:role/cnpg-on-eks-prod-irsa&quot; barman_s3_bucket = &quot;XXXX-cnpg-barman-bucket&quot; configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name cnpg-on-eks&quot;   --- apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: prod namespace: demo spec: description: &quot;Cluster Demo for DoEKS&quot; # Choose your PostGres Database Version imageName: ghcr.io/cloudnative-pg/postgresql:15.2 # Number of Replicas instances: 3 startDelay: 300 stopDelay: 300 replicationSlots: highAvailability: enabled: true updateInterval: 300 primaryUpdateStrategy: unsupervised serviceAccountTemplate: # For backup and restore, we use IRSA for barman tool. # You will find this IAM role on terraform outputs. metadata: annotations: eks.amazonaws.com/role-arn: arn:aws:iam::&lt;&lt;account_id&gt;&gt;:role/cnpg-on-eks-prod-irsa #1 postgresql: parameters: shared_buffers: 256MB pg_stat_statements.max: '10000' pg_stat_statements.track: all auto_explain.log_min_duration: '10s' pg_hba: # - hostssl app all all cert - host app app all password logLevel: debug storage: storageClass: ebs-sc size: 1Gi walStorage: storageClass: ebs-sc size: 1Gi monitoring: enablePodMonitor: true bootstrap: initdb: # Deploying a new cluster database: WorldDB owner: app secret: name: app-auth backup: barmanObjectStore: # For backup, we S3 bucket to store data. # On this Blueprint, we create an S3 check the terraform output for it. destinationPath: s3://&lt;your-s3-barman-bucket&gt; #2 s3Credentials: inheritFromIAMRole: true wal: compression: gzip maxParallel: 8 retentionPolicy: &quot;30d&quot; resources: # m5large: m5xlarge 2vCPU, 8GI RAM requests: memory: &quot;512Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;1Gi&quot; cpu: &quot;2&quot; affinity: enablePodAntiAffinity: true topologyKey: failure-domain.beta.kubernetes.io/zone nodeMaintenanceWindow: inProgress: false reusePVC: false   Once updated, you can apply your template.  kubectl create -f examples/prod-cluster.yaml   Verify that CloudNatvicePG operator has created three pods: one primary and two standby.   kubectl get pods,svc -n demo NAME READY STATUS RESTARTS AGE pod/prod-1 1/1 Running 0 4m36s pod/prod-2 1/1 Running 0 3m45s pod/prod-3 1/1 Running 0 3m9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prod-any ClusterIP 172.20.230.153 &lt;none&gt; 5432/TCP 4m54s service/prod-r ClusterIP 172.20.33.61 &lt;none&gt; 5432/TCP 4m54s service/prod-ro ClusterIP 172.20.96.16 &lt;none&gt; 5432/TCP 4m53s service/prod-rw ClusterIP 172.20.236.1 &lt;none&gt; 5432/TCP 4m53s   The operator created also three services:  -rw: points only to the primary instances of cluster database-ropoints only to hot standby replicas for read-only-workloads-rpoints to any of the instances for read-only workloads  Note that -any points on all the instances.  Another way to check Cluster status is by using cloudnative-pg kubectl plugin offered by the CloudNativePG community,  kubectl cnpg status prod Cluster Summary Name: prod Namespace: demo System ID: 7214866198623563798 PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:15.2 Primary instance: prod-1 Status: Cluster in healthy state Instances: 3 Ready instances: 3 Current Write LSN: 0/6000000 (Timeline: 1 - WAL File: 000000010000000000000005) Certificates Status Certificate Name Expiration Date Days Left Until Expiration ---------------- --------------- -------------------------- prod-ca 2023-06-24 14:40:27 +0000 UTC 89.96 prod-replication 2023-06-24 14:40:27 +0000 UTC 89.96 prod-server 2023-06-24 14:40:27 +0000 UTC 89.96 Continuous Backup status First Point of Recoverability: Not Available Working WAL archiving: OK WALs waiting to be archived: 0 Last Archived WAL: 000000010000000000000005 @ 2023-03-26T14:52:09.24307Z Last Failed WAL: - Streaming Replication status Replication Slots Enabled Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority Replication Slot ---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- ---------------- prod-2 0/6000000 0/6000000 0/6000000 0/6000000 00:00:00 00:00:00 00:00:00 streaming async 0 active prod-3 0/6000000 0/6000000 0/6000000 0/6000000 00:00:00 00:00:00 00:00:00 streaming async 0 active Unmanaged Replication Slot Status No unmanaged replication slots found Instances status Name Database Size Current LSN Replication role Status QoS Manager Version Node ---- ------------- ----------- ---------------- ------ --- --------------- ---- prod-1 29 MB 0/6000000 Primary OK BestEffort 1.19.0 ip-10-1-10-192.us-west-2.compute.internal prod-2 29 MB 0/6000000 Standby (async) OK BestEffort 1.19.0 ip-10-1-12-195.us-west-2.compute.internal prod-3 29 MB 0/6000000 Standby (async) OK BestEffort 1.19.0 ip-10-1-11-38.us-west-2.compute.internal   ","version":"Next","tagName":"h3"},{"title":"Monitoring​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#monitoring","content":" In this example, we deployed a Prometheus and Grafana addons to monitor all database clusters created by CloudNativePG. Let's check Grafana dashboard.  kubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80     ","version":"Next","tagName":"h3"},{"title":"Import database sample​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#import-database-sample","content":" You can expose your database outside the cluster using ingress-controller or kubernetes service type LoadBalancer. However, for internal usage inside your EKS cluster, you can use kubernetes service prod-rw and prod-ro. In this section, we are going to expose read-write service -rwusing kubectl port-forward.   kubectl port-forward svc/prod-rw 5432:5432 -n demo   Now, we use psql cli to import world.sql into our database instance WorldDB using credentials from app-auth secrets.   psql -h localhost --port 5432 -U app -d WorldDB &lt; world.sql # Quick check on db tables. psql -h localhost --port 5432 -U app -d WorldDB -c '\\dt' Password for user app: List of relations Schema | Name | Type | Owner --------+-----------------+-------+------- public | city | table | app public | country | table | app public | countrylanguage | table | app (3 rows)   ","version":"Next","tagName":"h3"},{"title":"Create Backup to S3​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#create-backup-to-s3","content":" Now that we had a running database with data, CloudNativePG operator offers backup-restore feature using barman tool. CloudNativePG allows database admin to create on-demand database or Scheduled backups and for more details on documentations.  In this example, we will create a Backup object to start a backup process immediately.  apiVersion: postgresql.cnpg.io/v1 kind: Backup metadata: name: ondemand spec: cluster: name: prod    kubectl create -f examples/backup-od.yaml   It will take couple minutes to run, then, check the backup process  kubectl describe backup ondemand Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 60s cloudnative-pg-backup Starting backup for cluster prod Normal Starting 60s instance-manager Backup started Normal Completed 56s instance-manager Backup completed   ","version":"Next","tagName":"h3"},{"title":"Restore​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#restore","content":" For restore, we use bootstrap a new cluster using backup file on S3. The backup tool barman manages restore process, but, it doesn't support backup and restore for kubernetes secrets. This must be managed separately, like using csi-secrets-driver with AWS SecretsManager.  First let's delete prod database.  kubectl delete cluster prod -n demo   Then, update your template examples/cluster-restore.yaml with your S3 bucket and IAM role. Note that on restore template, CloudNativePG use externalClusters to point on the database.   kubectl create -f examples/cluster-restore.yaml Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreatingPodDisruptionBudget 7m12s cloudnative-pg Creating PodDisruptionBudget prod-primary Normal CreatingPodDisruptionBudget 7m12s cloudnative-pg Creating PodDisruptionBudget prod Normal CreatingServiceAccount 7m12s cloudnative-pg Creating ServiceAccount Normal CreatingRole 7m12s cloudnative-pg Creating Cluster Role Normal CreatingInstance 7m12s cloudnative-pg Primary instance (from backup) Normal CreatingInstance 6m33s cloudnative-pg Creating instance prod-2 Normal CreatingInstance 5m51s cloudnative-pg Creating instance prod-3   When creating a new cluster, the operator will create a ServiceAccount with IRSA configuration as described on Cluster resources. Make sure the trust policy points the right ServiceAccount.  Let's check if the data were covered as expected.   psql -h localhost --port 5432 -U app -d WorldDB -c '\\dt' Password for user app: List of relations Schema | Name | Type | Owner --------+-----------------+-------+------- public | city | table | app public | country | table | app public | countrylanguage | table | app (3 rows) psql -h localhost --port 5432 -U app -d WorldDB -c 'SELECT CURRENT_TIME;'   ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres#conclusion","content":" CloudNativePG operator provides Level 5 from Operator Capability Levels. In this example, we share a blueprint that deploy the operator as an addon along with its monitoring stack (Prometheus and grafana). Among many features, we highlighted couple of examples on creating cluster, importing data and restoring database in case of disaster (or cluster deletion). More features are available on this documentation ","version":"Next","tagName":"h2"},{"title":"AWS Batch on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch","content":"","keywords":"","version":"Next"},{"title":"Considerations​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#considerations","content":" AWS Batch is meant for offline analytics and data processing tasks, such as reformatting media, training ML models, batch inference, or other compute and data intensive tasks that are not interactive with a user.  In particular, Batch is tuned for running jobs that are greater than three minutes. If your jobs are short (less than a minute), consider packing more work into a single AWS Batch job request to increase the total runtime of the job.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#prerequisites","content":" Ensure that you have the following tools installed locally:  aws clikubectlterraform  ","version":"Next","tagName":"h2"},{"title":"Deploy​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#deploy","content":" To provision this example:  Clone the repository to your local machine. git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/aws-batch Run the install script. /bin/sh install.sh Enter region at command prompt to continue.  The script will run Terraform to stand up all of the resources. Once done, you will see terraform output like below.    The following components are provisioned in your environment:  A sample VPC, with 2 Private Subnets and 2 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node group.EKS Managed Add-ons: VPC_CNI, CoreDNS, EBS_CSI_Driver, CloudWatchAWS Batch resources including An On-Demand compute environment and job queueA Spot compute environment and job queueAn example Batch job definition to run echo &quot;hello world!&quot;  ","version":"Next","tagName":"h2"},{"title":"Validate​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#validate","content":" ","version":"Next","tagName":"h2"},{"title":"Run an example job on your EKS cluster using AWS Batch​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#run-an-example-job-on-your-eks-cluster-using-aws-batch","content":" The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment.  ","version":"Next","tagName":"h2"},{"title":"Run update-kubeconfig command​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#run-update-kubeconfig-command","content":" Run the command from the configure_kubectl_cmd output value from terraform apply. If you do not have this available, you can get the terraform stack output values using the terraform output command.  # DO NOT COPY THIS! This is only an example, see above for what to run. aws eks --region us-east-1 update-kubeconfig --name doeks-batch   ","version":"Next","tagName":"h3"},{"title":"List the nodes​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#list-the-nodes","content":" Once kubectl is configured, you can use it to inspect the cluster nodes and namespaces. To get the node information, run the following command.  kubectl get nodes   The output should look like the following.  NAME STATUS ROLES AGE VERSION ip-10-1-107-168.ec2.internal Ready &lt;none&gt; 3m7s v1.30.2-eks-1552ad0 ip-10-1-141-25.ec2.internal Ready &lt;none&gt; 3m7s v1.30.2-eks-1552ad0   To get the created namespaces of the cluster, run the following command.  kubectl get ns   The output should look like the following.  NAME STATUS AGE amazon-cloudwatch Active 2m22s default Active 10m doeks-aws-batch Active 103s kube-node-lease Active 10m kube-public Active 10m kube-system Active 10m   The namespace doeks-aws-batch will be used by Batch to add Batch-managed EC2 instances for nodes and run jobs on these nodes.  note The AWS Batch kubernetes namespace is configurable as an input variable to terraform. If you chose to change it in the variables.tf file, then you will need to adjust later commands to account for the change.  ","version":"Next","tagName":"h3"},{"title":"Running the \"Hello World!\" job​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#running-the-hello-world-job","content":" The output of terraform apply contained the AWS CLI command to run the example Hello World! job definition on both the On-Demand and Spot job queues. You can view these commands again using terraform output.  To run the example job definition on the On-Demand resources:  Run the provided command from the terraform output run_example_aws_batch_job. It should look something like: JOB_ID=$(aws batch --region us-east-1 submit-job --job-definition arn:aws:batch:us-east-1:653295002771:job-definition/doeks-hello-world:2 --job-queue doeks-JQ1_OD --job-name doeks_hello_example --output text --query jobId) &amp;&amp; echo $JOB_ID ## Output should be the Batch job ID be1f781d-753e-4d10-a7d4-1b6de68574fc The response will populate the JOB_ID shell variable, which you can use in later steps.  ","version":"Next","tagName":"h3"},{"title":"Checking the status​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#checking-the-status","content":" You can use the AWS CLI to check the status of the job from the AWS Batch API:  aws batch --no-cli-pager \\ describe-jobs --jobs $JOB_ID --query &quot;jobs[].[jobId,status]&quot;   This will output something like the following:  [ [ &quot;a13e1cff-121c-4a0b-a9c5-fab953136e20&quot;, &quot;RUNNABLE&quot; ] ]   tip If you see an empty result, it is likely that you are using a different default AWS Region than the one that was deployed to. Adjust the value of your default Region by setting the AWS_DEFAULT_REGION shell variable. export AWS_DEFAULT_REGION=us-east-1   We can monitor the status of Nodes and Pods that Batch manages using kubectl. First, let's track the nodes as they launch and join the cluster:  kubectl get nodes -w   This will continuously monitor the state of EKS Nodes, and periodically output their ready state.  NAME STATUS ROLES AGE VERSION ip-10-1-107-168.ec2.internal Ready &lt;none&gt; 12m v1.30.2-eks-1552ad0 ip-10-1-141-25.ec2.internal Ready &lt;none&gt; 12m v1.30.2-eks-1552ad0 ip-10-1-60-65.ec2.internal NotReady &lt;none&gt; 0s v1.30.2-eks-1552ad0 ip-10-1-60-65.ec2.internal NotReady &lt;none&gt; 0s v1.30.2-eks-1552ad0 ip-10-1-60-65.ec2.internal NotReady &lt;none&gt; 0s v1.30.2-eks-1552ad0 # ... more lines   When the new Batch-managed nodes are launching (new nodes with NotReady status), you can press the Control-c key combination to exit the watch process. This will allow you to monitor the state of pods launching in the AWS Batch namespace:  kubectl get pods -n doeks-aws-batch -w   note The AWS Batch kubernetes namespace is configurable as an input variable to terraform. If you chose to change it in the variables.tf file, then you will need to adjust the previous command to account for the change.  This will continuously monitor the state of the Pods that Batch places on the cluster, and periodically output their state.  NAME READY STATUS RESTARTS AGE aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 Pending 0 0s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 ContainerCreating 0 0s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 1/1 Running 0 17s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 Completed 0 52s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 Completed 0 53s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 Terminating 0 53s aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1 0/1 Terminating 0 53s   Once the Pods are Terminating, you can exit the watch process by hitting the Control-c key combination. To view the status of the job from AWS Batch, use to following command:  aws batch --no-cli-pager \\ describe-jobs --jobs $JOB_ID --query &quot;jobs[].[jobId,status]&quot;   This will show the job ID and the status, which should be SUCCEEDED.  [ [ &quot;a13e1cff-121c-4a0b-a9c5-fab953136e20&quot;, &quot;SUCCEEDED&quot; ] ]   To find the application container logs in the CloudWatch Log Groups management console, we will need the application container's Pod name. The kubestl get pods output did not give us a good way to determine which of those Pods was the one with the application container. Also, once Pods are terminated, the Kubernetes scheduler can no longer provide any information on the job's nodes or Pods. Good thing that AWS Batch keeps a record of the job!  We can use AWS Batch's API to query for the main node's podName and other information. To get information on a specific node from an MNP job, you suffix the job ID with the pattern &quot;#&lt;NODE_INDEX&gt;&quot;. For the main node, which we defined as index &quot;0&quot; in our job definition, that would translate to the following AWS CLI command:  aws batch describe-jobs --jobs &quot;$JOB_ID&quot; --query &quot;jobs[].eksAttempts[].{nodeName: nodeName, podName: podName}&quot;   The output should be similar to the following.  [ { &quot;nodeName&quot;: &quot;ip-10-1-60-65.ec2.internal&quot;, &quot;podName&quot;: &quot;aws-batch.32d8f53f-29dc-31b4-9ce4-13504ccf74c1&quot; } ]   To view the application container logs:  Navigate to the Amazon CloudWatch management console Log Groups panel.In the Log groups listing table, choose the application logs for your cluster. These are identified by your cluster's name, and the suffix application.In the Log streams listing table, enter the value for podName from the previous step. This will highlight two logs for the two containers in the Pod. Choose the log stream for the application container.In the Log events section, in the filter bar, choose Display then choose **View in plain text&quot;. You should see the log messages &quot;Hello World!&quot; in the &quot;log&quot; property of the log event.  ","version":"Next","tagName":"h2"},{"title":"Cleaning up​","type":1,"pageTitle":"AWS Batch on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-batch#cleaning-up","content":" To clean up your environment—remove all AWS Batch resources and kubernetes constructs from your cluster—run the cleanup.sh script.  chmod +x cleanup.sh ./cleanup.sh   To avoid data charges from CloudWatch logs, you should also delete the log groups from your cluster. You can find these by navigating to the Log groups page of the CloudWatch management console. ","version":"Next","tagName":"h2"},{"title":"Argo Workflows on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks","content":"","keywords":"","version":"Next"},{"title":"Prerequisites:​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#prerequisites","content":" Ensure that you have the following tools installed locally:  aws clikubectlterraformArgo WorkflowCLI  ","version":"Next","tagName":"h2"},{"title":"Deploy​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#deploy","content":" To provision this example:  git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/argo-workflow region=&lt;your region&gt; # set region variable for following commands terraform init terraform apply -var region=$region #defaults to us-west-2   Enter yes at command prompt to apply  The following components are provisioned in your environment:  A sample VPC, 2 Private Subnets and 2 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S Metrics Server, CoreDNS Autoscaler, Cluster Autoscaler, AWS for FluentBit, Karpenter, Argo Workflows, Argo Events, Kube Prometheus Stack, Spark Operator and Yunikorn SchedulerK8s roles and rolebindings for Argo Workflows and Argo Events    ","version":"Next","tagName":"h2"},{"title":"Validate​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#validate","content":" The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment.  ","version":"Next","tagName":"h2"},{"title":"Run update-kubeconfig command:​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#run-update-kubeconfig-command","content":" aws eks --region eu-west-1 update-kubeconfig --name argoworkflows-eks   ","version":"Next","tagName":"h3"},{"title":"List the nodes​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#list-the-nodes","content":" kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-1-0-189.eu-west-1.compute.internal Ready &lt;none&gt; 10m v1.29.15-eks-473151a ip-10-1-0-240.eu-west-1.compute.internal Ready &lt;none&gt; 10m v1.29.15-eks-473151a ip-10-1-1-135.eu-west-1.compute.internal Ready &lt;none&gt; 10m v1.29.15-eks-473151a   ","version":"Next","tagName":"h3"},{"title":"List the namespaces in EKS cluster​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#list-the-namespaces-in-eks-cluster","content":" kubectl get ns # Output should look like below NAME STATUS AGE argo-events Active 7m45s argo-workflows Active 8m25s spark-team-a Active 5m51s default Active 25m karpenter Active 21m kube-node-lease Active 25m kube-prometheus-stack Active 8m5s kube-public Active 25m kube-system Active 25m spark-operator Active 5m43s yunikorn Active 5m44s   ","version":"Next","tagName":"h3"},{"title":"Access Argo Workflow WebUI​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#access-argo-workflow-webui","content":" To access Argo Workflow WebUI in your browser, open another terminal and execute the command below:  kubectl port-forward service/argo-workflows-server -n argo-workflows 2746:2746   Sample output:  Forwarding from 127.0.0.1:2746 -&gt; 2746 Forwarding from [::1]:2746 -&gt; 2746 Handling connection for 2746 ...   In your browser, access the WebUI via http://localhost:2746/  The initial username is admin. The login token is autogenerated and you can get it by running the following command:  ARGO_TOKEN=$(argo auth token) echo $ARGO_TOKEN   Sample output:  Bearer k8s-aws-v1.aHR0cHM6Ly9zdHMudXMtd2VzdC0yLmF...     ","version":"Next","tagName":"h3"},{"title":"Submit Spark Job with Argo Workflow​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#submit-spark-job-with-argo-workflow","content":" Export EKS API from terraform output  eks_api_url=https://ABCDEFG1234567890.yl4.eu-west-2.eks.amazonaws.com cat workflow-examples/argo-spark.yaml | sed &quot;s/&lt;your_eks_api_server_url&gt;/$eks_api_url/g&quot; | kubectl apply -f - kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Running 8s   You can also check the workflow status from Web UI    ","version":"Next","tagName":"h3"},{"title":"Submit Spark Job with Spark Operator and Argo Workflow​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#submit-spark-job-with-spark-operator-and-argo-workflow","content":" kubectl apply -f workflow-examples/argo-spark-operator.yaml kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Succeeded 3m58s spark-operator Running 5s   The workflow status from web UI    ","version":"Next","tagName":"h3"},{"title":"Trigger a workflow to create a spark job based on SQS message​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#trigger-a-workflow-to-create-a-spark-job-based-on-sqs-message","content":" ","version":"Next","tagName":"h2"},{"title":"Install eventbus which is for event transmission in argo events​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#install-eventbus-which-is-for-event-transmission-in-argo-events","content":" kubectl apply -f argo-events-manifests/eventbus.yaml   ","version":"Next","tagName":"h3"},{"title":"Deploy eventsource-sqs.yaml to link with external SQS​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#deploy-eventsource-sqsyaml-to-link-with-external-sqs","content":" In this case, we configure a EventSource to license to the queue test1 in region us-east-1. The eventsource is capable of monitoring events across regions, so the Amazon EKS cluster and Amazon SQS queue don’t need to be located in the same Region.  queue_name=test1 region_sqs=us-east-1 cat argo-events-manifests/eventsource-sqs.yaml | sed &quot;s/&lt;region_sqs&gt;/$region_sqs/g;s/&lt;queue_name&gt;/$queue_name/g&quot; | kubectl apply -f -   Let's create that queue in your account.  # create a queue queue_url=$(aws sqs create-queue --queue-name $queue_name --region $region_sqs --output text) # get your queue arn sqs_queue_arn=$(aws sqs get-queue-attributes --queue-url $queue_url --attribute-names QueueArn --region $region_sqs --query &quot;Attributes.QueueArn&quot; --output text) template=`cat argo-events-manifests/sqs-accesspolicy.json | sed -e &quot;s|&lt;sqs_queue_arn&gt;|$sqs_queue_arn|g;s|&lt;your_event_irsa_arn&gt;|$your_event_irsa_arn|g&quot;` aws sqs set-queue-attributes --queue-url $queue_url --attributes $template --region $region_sqs   ","version":"Next","tagName":"h3"},{"title":"Deploy sensor-rbac.yaml and sensor-sqs-spark-crossns.yaml for triggering workflow​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#deploy-sensor-rbacyaml-and-sensor-sqs-spark-crossnsyaml-for-triggering-workflow","content":" kubectl apply -f argo-events-manifests/sensor-rbac.yaml   cd workflow-examples   Update the variables in Shell script and execute  ./taxi-trip-execute.sh   Update YAML file and run the below command  kubectl apply -f sensor-sqs-sparkjobs.yaml   ","version":"Next","tagName":"h3"},{"title":"Verify argo-events namespace​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#verify-argo-events-namespace","content":" kubectl get all,eventbus,EventSource,sensor,sa,role,rolebinding -n argo-events # Output should look like below NAME READY STATUS RESTARTS AGE pod/argo-events-controller-manager-bfb894cdb-26qw7 1/1 Running 0 18m pod/aws-sqs-crossns-spark-sensor-zkgz5-6584787c47-zjm9p 1/1 Running 0 44s pod/aws-sqs-eventsource-544jd-8fccc6f8-w6ssd 1/1 Running 0 4m45s pod/eventbus-default-stan-0 2/2 Running 0 5m21s pod/eventbus-default-stan-1 2/2 Running 0 5m13s pod/eventbus-default-stan-2 2/2 Running 0 5m11s pod/events-webhook-6f8d9fdc79-l9q9w 1/1 Running 0 18m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/eventbus-default-stan-svc ClusterIP None &lt;none&gt; 4222/TCP,6222/TCP,8222/TCP 5m21s service/events-webhook ClusterIP 172.20.4.211 &lt;none&gt; 443/TCP 18m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/argo-events-controller-manager 1/1 1 1 18m deployment.apps/aws-sqs-crossns-spark-sensor-zkgz5 1/1 1 1 44s deployment.apps/aws-sqs-eventsource-544jd 1/1 1 1 4m45s deployment.apps/events-webhook 1/1 1 1 18m NAME DESIRED CURRENT READY AGE replicaset.apps/argo-events-controller-manager-bfb894cdb 1 1 1 18m replicaset.apps/aws-sqs-crossns-spark-sensor-zkgz5-6584787c47 1 1 1 44s replicaset.apps/aws-sqs-eventsource-544jd-8fccc6f8 1 1 1 4m45s replicaset.apps/events-webhook-6f8d9fdc79 1 1 1 18m NAME READY AGE statefulset.apps/eventbus-default-stan 3/3 5m21s NAME AGE eventbus.argoproj.io/default 5m22s NAME AGE eventsource.argoproj.io/aws-sqs 4m46s NAME AGE sensor.argoproj.io/aws-sqs-crossns-spark 45s NAME SECRETS AGE serviceaccount/argo-events-controller-manager 0 18m serviceaccount/argo-events-events-webhook 0 18m serviceaccount/default 0 18m serviceaccount/event-sa 0 16m serviceaccount/operate-workflow-sa 0 53s NAME CREATED AT role.rbac.authorization.k8s.io/operate-workflow-role 2023-07-24T18:52:30Z NAME ROLE AGE rolebinding.rbac.authorization.k8s.io/operate-workflow-role-binding Role/operate-workflow-role 52s   ","version":"Next","tagName":"h3"},{"title":"Test from SQS​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#test-from-sqs","content":" Send a message from SQS: {&quot;message&quot;: &quot;hello&quot;}  aws sqs send-message --queue-url $queue_url --message-body '{&quot;message&quot;: &quot;hello&quot;}' --region $region_sqs   Argo Events would capture the message and trigger Argo Workflows to create a workflow for spark jobs.  kubectl get wf -A # Output should look like below NAMESPACE NAME STATUS AGE MESSAGE argo-workflows aws-sqs-spark-workflow-hh79p Running 11s   Run the command below to check spark application driver pods and executor pods under spark-team-a namespace.  kubectl get po -n spark-team-a # Output should look like below NAME READY STATUS RESTARTS AGE event-wf-sparkapp-tcxl8-driver 1/1 Running 0 45s pythonpi-a72f5f89894363d2-exec-1 1/1 Running 0 16s pythonpi-a72f5f89894363d2-exec-2 1/1 Running 0 16s   See the SQS workflow status in web UI      ","version":"Next","tagName":"h3"},{"title":"Destroy​","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks#destroy","content":" To teardown and remove the resources created in this example:  kubectl delete -f argo-events-manifests/. ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Amazon Managed Workflows for Apache Airflow (MWAA)","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow","content":"","keywords":"","version":"Next"},{"title":"Considerations​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#considerations","content":" Ideally we recommend adding the steps to sync requirements/sync dags to the MWAA S3 Bucket as part of a CI/CD pipeline. Generally Dags development have a different lifecycle than the Terraform code to provision infrastructure. For simplicity, we are providing steps for that using Terraform running AWS CLI commands on null_resource.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites:​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#prerequisites","content":" Ensure that you have the following tools installed locally:  aws clikubectlterraform  ","version":"Next","tagName":"h2"},{"title":"Deploy​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#deploy","content":" To provision this example:  git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/managed-airflow-mwaa chmod +x install.sh ./install.sh   Enter region at command prompt to continue.  Once done, you will see terraform output like below.    The following components are provisioned in your environment:  A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server and cluster autoscalerA MWAA environment in version 2.2.2An EMR virtual cluster registered with the newly created EKSA S3 bucket with DAG code  ","version":"Next","tagName":"h2"},{"title":"Validate​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#validate","content":" The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment.  ","version":"Next","tagName":"h2"},{"title":"Run update-kubeconfig command​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#run-update-kubeconfig-command","content":" Run the command below. You may also copy the command from the terraform output 'configure_kubectl'.  aws eks --region us-west-2 update-kubeconfig --name managed-airflow-mwaa   ","version":"Next","tagName":"h3"},{"title":"List the nodes​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#list-the-nodes","content":" kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-0-0-42.ec2.internal Ready &lt;none&gt; 5h15m v1.26.4-eks-0a21954 ip-10-0-22-71.ec2.internal Ready &lt;none&gt; 5h15m v1.26.4-eks-0a21954 ip-10-0-44-63.ec2.internal Ready &lt;none&gt; 5h15m v1.26.4-eks-0a21954   ","version":"Next","tagName":"h3"},{"title":"List the namespaces in EKS cluster​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#list-the-namespaces-in-eks-cluster","content":" kubectl get ns # Output should look like below default Active 4h38m emr-mwaa Active 4h34m kube-node-lease Active 4h39m kube-public Active 4h39m kube-system Active 4h39m mwaa Active 4h30m   namespace emr-mwaa will be used by EMR for running spark jobs. namespace mwaa will be used by MWAA directly.  ","version":"Next","tagName":"h3"},{"title":"Trigger jobs from MWAA​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#trigger-jobs-from-mwaa","content":" ","version":"Next","tagName":"h2"},{"title":"Log into Apache Airflow UI​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#log-into-apache-airflow-ui","content":" Open the Environments page on the Amazon MWAA consoleChoose an environmentUnder the Details section, click the link for the Airflow UI   Note: You will see red error message once login. That is because the EMR connection has not been setup. The message will be gone after following the steps below to set up the connection and login again.  ","version":"Next","tagName":"h3"},{"title":"Trigger the DAG workflow to execute job in EMR on EKS​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-emr-on-eks","content":" First, you need to set up the connection to EMR virtual cluster in MWAA    Click Add button, Make sure use emr_eks as Connection Id Amazon Web Services as Connection Type Replace the value in Extra based on your terraform output {&quot;virtual_cluster_id&quot;:&quot;&lt;emrcontainers_virtual_cluster_id in terraform output&gt;&quot;, &quot;job_role_arn&quot;:&quot;&lt;emr_on_eks_role_arn in terraform output&gt;&quot;}    Go back to Airflow UI main page, enable the example DAG emr_eks_pi_job and then trigger the job.    While it is running, use the following command to verify the spark jobs:  kubectl get all -n emr-mwaa   You should see output similar to the following:  NAME READY STATUS RESTARTS AGE pod/000000030tk2ihdmr8g-psstj 3/3 Running 0 90s pod/pythonpi-a8051f83b415c911-exec-1 2/2 Running 0 14s pod/pythonpi-a8051f83b415c911-exec-2 2/2 Running 0 14s pod/spark-000000030tk2ihdmr8g-driver 2/2 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/spark-000000030tk2ihdmr8g-ee64be83b4151dd5-driver-svc ClusterIP None &lt;none&gt; 7078/TCP,7079/TCP,4040/TCP 57s NAME COMPLETIONS DURATION AGE job.batch/000000030tk2ihdmr8g 0/1 92s 92s   You can also check the job status in Amazon EMR console. Under the Virtual clusters section, click on Virtual cluster    ","version":"Next","tagName":"h3"},{"title":"Trigger the DAG workflow to execute job in EKS​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-eks","content":" In the Airflow UI, enable the example DAG kubernetes_pod_example and then trigger it.      Verify that the pod was executed successfully  After it runs and completes successfully, use the following command to verify the pod:  kubectl get pods -n mwaa   You should see output similar to the following:  NAME READY STATUS RESTARTS AGE mwaa-pod-test.4bed823d645844bc8e6899fd858f119d 0/1 Completed 0 25s   ","version":"Next","tagName":"h3"},{"title":"Destroy​","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow#destroy","content":" To clean up your environment, run the cleanup.sh script.script  chmod +x cleanup.sh ./cleanup.sh    ","version":"Next","tagName":"h2"},{"title":"Deploying Trino on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/distributed-databases/trino","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#introduction","content":" Trino is an open-source, fast, distributed query engine designed to run SQL queries for big data analytics, over a multitude of data sources including Amazon S3, relational databases, distributed data stores and data warehouses.  When Trino executes a query, it does so by breaking up the execution into a hierarchy of stages, which are implemented as a series of tasks distributed over a network of Trino workers. A Trino cluster consists of a coordinator and many workers for parallel processing, which can be deployed as Kubernetes pods on an EKS cluster. The coordinator and the workers collaborate to access connected data sources, with schemas and references stored in a catalog. To access the data sources, you can use one of the many connectors provided by Trino to adapt Trino. Examples include as Hive, Iceberg and Kafka. More details about Trino project can be found on this link  ","version":"Next","tagName":"h2"},{"title":"Blueprint Solution​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#blueprint-solution","content":" This blueprint will deploy Trino on an EKS cluster (Kubernetes version 1.29) with nodes provisioned using Karpenter (v0.34.0). To optimize on cost and performance, Karpenter will provision On-demand nodes for the Trino coordinator and EC2 Spot instances for Trino workers. With Trino's multi-architectural container images, Karpenter NodePool will allow provisioning of nodes with EC2 instances from different cpu architectures including AWS Graviton based instances. Trino is deployed using the official Helm chart, with custom values provided for users to leverage Hive and Iceberg connectors. The examples will use Glue and Iceberg tables on AWS as the backend data source, using S3 as the storage.  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#deploying-the-solution","content":" Let's go through the deployment steps.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraformTrino CLI client   Toggle to see installation steps of Trino CLI wget https://repo1.maven.org/maven2/io/trino/trino-cli/427/trino-cli-427-executable.jar mv trino-cli-427-executable.jar trino chmod +x trino   ","version":"Next","tagName":"h3"},{"title":"Deploy the EKS Cluster with Trino​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#deploy-the-eks-cluster-with-trino","content":" First, clone the repository  git clone https://github.com/awslabs/data-on-eks.git   Navigate into distributed-databases/trino and run install.sh script. Enter, when prompted, the AWS region you want to provision the resources to (for example, us-west-2).  cd data-on-eks/distributed-databases/trino ./install.sh   ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#verify-deployment","content":" Verify the Amazon EKS Cluster  #Select your own region where you deployed resources aws eks describe-cluster --name trino-on-eks --region us-west-2   Update local kubeconfig so we can access kubernetes cluster (you can also get this command from the terraform output configure_kubectl )  aws eks update-kubeconfig --name trino-on-eks --region us-west-2   First, let's verify that we have worker nodes provisioned by Karpenter in the cluster. Let's also see their availability zone and capacity type (on-demand or spot)  kubectl get nodes --selector=karpenter.sh/nodepool=trino-sql-karpenter -L topology.kubernetes.io/zone -L karpenter.sh/capacity-type -L node.kubernetes.io/instance-type   Output​  NAME STATUS ROLES AGE VERSION ZONE CAPACITY-TYPE INSTANCE-TYPE ip-10-1-11-49.us-west-2.compute.internal Ready &lt;none&gt; 24m v1.29.0-eks-5e0fdde us-west-2b on-demand t4g.medium   We can see above that Karpenter provisioned on-demand node for running Trino coordinator.  info For a distributed Big Data query engine like Trino which runs on a massively parallel processing cluster, it is recommended to deploy the cluster in same availability zone to avoid incurring high Inter-AZ Data Transfer costs. That's why Karpenter NodePool has been configured to launch EKS nodes in same AZ  Now, let's verify the coordinator and worker pods running in trino namespace  kubectl get pods --namespace=trino   Output​  NAME READY STATUS RESTARTS AGE trino-coordinator-5cfd685c8f-mchff 1/1 Running 0 37m   Next, we will port-forward the trino service so it can be accessed locally  kubectl -n trino port-forward service/trino 8080:8080   Now, lets access the Trino UI at http://localhost:8080 through web browser and login with username admin in the login window as shown below:    Trino Web UI will show 0 active worker:    ","version":"Next","tagName":"h3"},{"title":"Using Trino for database querying executions​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#using-trino-for-database-querying-executions","content":" ","version":"Next","tagName":"h2"},{"title":"Example #1: Using the Hive Connector​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#example-1-using-the-hive-connector","content":" In this example, we will set up a Hive metastore using AWS Glue, with the source data stored in S3, and crawler that will infer schema from it to build a Glue table.  Using Trino on EKS with the Glue connector, we will use Trino CLI to run sample SQL queries to retrieve data.  Setup​  Run the hive script from the examples directory to set up the blueprint S3 bucket with the 2022 NYC Taxi dataset (in Parquet), and build Glue metastore:  cd examples/ ./hive-setup.sh   You will see some outputs to show progress, and if successful, will see the name of the Glue table that will store the metadata as hive.  Running the queries​  You should have the Trino CLI installed as part of the prerequisite. The blueprint has the Hive Connector configured with the bucket we set up in the previous section, so you should be able to query the data source without additional settings.  First, port-forward your trino service to access it locally, if you have closed the session from previous section:  kubectl -n trino port-forward service/trino 8080:8080   While the port-forward is running, open another terminal tab where you have Trino CLI and run the following command to access the coordinator:  ./trino http://127.0.0.1:8080 --user admin   Once successful, you will be able to get a prompt to execute commands. You can use help command to see a list of supported commands. The first command you run will trigger the auto-scaling of trino workers from 0 to 1, and take a couple minutes to complete.  For example:  To show a list of catalogs, run query - SHOW CATALOGS; and you can see hive and iceberg catalogs among others configured by the blueprint  Output​   Catalog --------- hive iceberg system tpcds tpch (5 rows) Query 20240215_200117_00003_6jdxw, FINISHED, 1 node Splits: 1 total, 1 done (100.00%) 0.49 [0 rows, 0B] [0 rows/s, 0B/s]   To see the schemas (databases) in Hive catalog, run query - SHOW SCHEMAS FROM hive; :  Output​   Schema -------------------- information_schema taxi_hive_database (2 rows)   Let's use taxi_hive_database and show table in this database -  USE hive.taxi_hive_database;   SHOW TABLES;   Output​  Table ------- hive (1 row)   Finally, to run a simple query to list items - SELECT * FROM hive LIMIT 5;  Output​  vendorid | tpep_pickup_datetime | tpep_dropoff_datetime | passenger_count | trip_distance | ratecodeid | store_and_fwd_flag | pulocationid | dolocation&gt; ----------+-------------------------+-------------------------+-----------------+---------------+------------+--------------------+--------------+-----------&gt; 1 | 2022-09-01 00:28:12.000 | 2022-09-01 00:36:22.000 | 1.0 | 2.1 | 1.0 | N | 100 | 2&gt; 1 | 2022-11-01 00:24:49.000 | 2022-11-01 00:31:04.000 | 2.0 | 1.0 | 1.0 | N | 158 | 1&gt; 1 | 2022-11-01 00:37:32.000 | 2022-11-01 00:42:23.000 | 2.0 | 0.8 | 1.0 | N | 249 | 1&gt; 2 | 2022-09-01 00:02:24.000 | 2022-09-01 00:09:39.000 | 1.0 | 1.32 | 1.0 | N | 238 | 1&gt; 2 | 2022-09-01 00:47:25.000 | 2022-09-01 00:56:09.000 | 1.0 | 2.94 | 1.0 | N |   Cleaning Up Hive Resources​  Exit from Trino CLI with exit command. Run the cleanup script from the examples directory to delete all the resources created from the hive script:  cd data-on-eks/distributed-databases/trino/examples ./hive-cleanup.sh   ","version":"Next","tagName":"h3"},{"title":"Example #2: Using the Iceberg Connector​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#example-2-using-the-iceberg-connector","content":" In this example, we will set up using Apache Iceberg with AWS Glue as the catalog type, and will store the data in Amazon S3 with PARQUET format.  Using Trino on EKS with the Iceberg connector, we will use Trino CLI to create the above resources and run sample SQL queries to insert and retrieve data.  Running the queries​  Let's find out the S3 data bucket created by blueprint. We will use this bucket to store data in Iceberg tables in PARQUET format.  cd data-on-eks/distributed-databases/trino export BUCKET=$(terraform output --state=&quot;./terraform.tfstate&quot; --raw data_bucket) echo $BUCKET   Output​  trino-data-bucket-20240215180855515400000001   Let’s now create an Iceberg schema with tables populated with data from sf10000 schema tables of TPCDS. We will use CREATE TABLE AS SELECT (CTAS) statements. The SQL file examples/trino_sf10000_tpcds_to_iceberg.sql has the below SQL statements:  use tpcds.sf10000; select * from tpcds.sf10000.item limit 10; select * from tpcds.sf10000.warehouse limit 10; /* Drop tables &amp; schema */ drop schema iceberg.iceberg_schema; drop table iceberg.iceberg_schema.warehouse; drop table iceberg.iceberg_schema.item; drop table iceberg.iceberg_schema.inventory; drop table iceberg.iceberg_schema.date_dim; /* Iceberg schema creation */ create schema if not exists iceberg.iceberg_schema with (LOCATION = 's3://trino-data-bucket-20240215180855515400000001/iceberg/'); /* Iceberg Table Creation with CTAS from tpcds tables */ create table if not exists iceberg.iceberg_schema.inventory with (FORMAT = 'PARQUET') as select * from tpcds.sf10000.inventory; create table if not exists iceberg.iceberg_schema.date_dim with (FORMAT = 'PARQUET') as select d_date_sk, cast(d_date_id as varchar(16)) as d_date_id, d_date, d_month_seq, d_week_seq, d_quarter_seq, d_year, d_dow, d_moy, d_dom, d_qoy, d_fy_year, d_fy_quarter_seq, d_fy_week_seq, cast(d_day_name as varchar(9)) as d_day_name, cast(d_quarter_name as varchar(6)) as d_quarter_name, cast(d_holiday as varchar(1)) as d_holiday, cast(d_weekend as varchar(1)) as d_weekend, cast(d_following_holiday as varchar(1)) as d_following_holiday, d_first_dom, d_last_dom, d_same_day_ly, d_same_day_lq, cast(d_current_day as varchar(1)) as d_current_day, cast(d_current_week as varchar(1)) as d_current_week, cast(d_current_month as varchar(1)) as d_current_month, cast(d_current_quarter as varchar(1)) as d_current_quarter from tpcds.sf10000.date_dim; create table if not exists iceberg.iceberg_schema.warehouse with (FORMAT = 'PARQUET') as select w_warehouse_sk, cast(w_warehouse_id as varchar(16)) as w_warehouse_id, w_warehouse_name, w_warehouse_sq_ft, cast(w_street_number as varchar(10)) as w_street_number, w_street_name, cast(w_street_type as varchar(15)) as w_street_type, cast(w_suite_number as varchar(10)) as w_suite_number, w_city, w_county, cast(w_state as varchar(2)) as w_state, cast(w_zip as varchar(10)) as w_zip, w_country, w_gmt_offset from tpcds.sf10000.warehouse; create table if not exists iceberg.iceberg_schema.item with (FORMAT = 'PARQUET') as select i_item_sk, cast(i_item_id as varchar(16)) as i_item_id, i_rec_start_date, i_rec_end_date, i_item_desc, i_current_price, i_wholesale_cost, i_brand_id, cast(i_brand as varchar(50)) as i_brand, i_class_id, cast(i_class as varchar(50)) as i_class, i_category_id, cast(i_category as varchar(50)) as i_category, i_manufact_id, cast(i_manufact as varchar(50)) as i_manufact, cast(i_size as varchar(50)) as i_size, cast(i_formulation as varchar(20)) as i_formulation, cast(i_color as varchar(20)) as i_color, cast(i_units as varchar(10)) as i_units, cast(i_container as varchar(10)) as i_container, i_manager_id, cast(i_product_name as varchar(50)) as i_product_name from tpcds.sf10000.item; /* Select from Iceberg table */ select * from iceberg.iceberg_schema.date_dim limit 10; select * from iceberg.iceberg_schema.item limit 10; select * from iceberg.iceberg_schema.inventory limit 10; /* Running query from Iceberg table */ with inv as (select w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy ,stdev,mean, case mean when 0 then null else stdev/mean end cov from(select w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy ,stddev_samp(inv_quantity_on_hand) stdev,avg(inv_quantity_on_hand) mean from iceberg.iceberg_schema.inventory ,iceberg.iceberg_schema.item ,iceberg.iceberg_schema.warehouse ,iceberg.iceberg_schema.date_dim where inv_item_sk = i_item_sk and inv_warehouse_sk = w_warehouse_sk and inv_date_sk = d_date_sk and d_year =1999 group by w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy) foo where case mean when 0 then 0 else stdev/mean end &gt; 1) select inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean, inv1.cov ,inv2.w_warehouse_sk,inv2.i_item_sk,inv2.d_moy,inv2.mean, inv2.cov from inv inv1,inv inv2 where inv1.i_item_sk = inv2.i_item_sk and inv1.w_warehouse_sk = inv2.w_warehouse_sk and inv1.d_moy=4 and inv2.d_moy=4+1 and inv1.cov &gt; 1.5 order by inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean,inv1.cov,inv2.d_moy,inv2.mean, inv2.cov;   Above SQL commands will execute following actions: Create an Iceberg schema named iceberg_schemaCreate 4 Iceberg tables - warehouse, item, inventory and date_dim with data from same tables of tpcdsQuery data from above Iceberg tables Let's now execute above SQL commands using Trino CLI:  envsubst &lt; examples/trino_sf10000_tpcds_to_iceberg.sql &gt; examples/iceberg.sql ./trino --file 'examples/iceberg.sql' --server http://localhost:8080 --user admin --ignore-errors   You can see completed and running SQL queries in Trino UI web monitor as below:    Let’s open another terminal and see how KEDA is scaling Trino worker pods, when above SQL commands are running:  kubectl get hpa -n trino -w   Output​  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE keda-hpa-keda-scaler-trino-worker Deployment/trino-worker &lt;unknown&gt;/1, &lt;unknown&gt;/1 + 1 more... 1 15 0 37m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 1/1 + 1 more... 1 15 1 38m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 500m/1 + 1 more... 1 15 1 40m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 1 40m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 1 40m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 1 40m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 2 41m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 2 41m keda-hpa-keda-scaler-trino-worker Deployment/trino-worker 0/1, 0/1 + 1 more... 1 15 2 41m   You can see HPA scaling from initial 0 workers to 2 workers with increasing query load and average cpu utilization of workerss:    ","version":"Next","tagName":"h3"},{"title":"Example #3 (Optional): Fault-tolerant execution in Trino​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#example-3-optional-fault-tolerant-execution-in-trino","content":" Fault-tolerant execution is an opt-in mechanism in Trino that was implemented using Project Tardigrade. Without fault-tolerant configuration, Trino query fails whenever any of the component tasks of the query fails due to any reason (for example, a worker node failure or termination). These failed queries have to be restarted from scratch resulting in longer execution time, compute wastage, and spend, especially for long-running queries.  When fault-tolerant execution is configured in Trino with a retry policy, intermediate exchange data is spooled in an external storage like Amazon S3 or HDFS using exchange manager. Trino then retries failed query (if retry policy is configured as &quot;QUERY&quot;) or failed tasks (if retry policy is configured as &quot;TASK&quot;). Trino's remaining workers reuse exchange manager data to retry and complete query in the event of a worker outage or other fault during query execution.  info A QUERY retry policy instructs Trino to retry whole query when an error occurs on a worker node. It is recommended to use this retry policy when the majority of the workload for the Trino cluster comprises many small queries. A TASK retry policy instructs Trino to retry individual tasks in the event of failure. It is recommended to use this policy when Trino executes large batch queries. The cluster can more efficiently retry smaller tasks within the query rather than retry the whole query.  This blueprint has deployed Trino cluster with fault-tolerant configuration with a TASK retry policy in config.properties file in coordinator and worker pods. Let's verify that by opening a bash command shell inside coordinator pod:  COORDINATOR_POD=$(kubectl get pods -l &quot;app.kubernetes.io/instance=trino,app.kubernetes.io/component=coordinator&quot; -o name -n trino) kubectl exec --stdin --tty $COORDINATOR_POD -n trino -- /bin/bash cat /etc/trino/config.properties   Output​  coordinator=true node-scheduler.include-coordinator=false http-server.http.port=8080 query.max-memory=280GB query.max-memory-per-node=22GB discovery.uri=http://localhost:8080 retry-policy=TASK exchange.compression-enabled=true query.low-memory-killer.delay=0s query.remote-task.max-error-duration=1m query.hash-partition-count=50   Blueprint has also configured exchange manager using an Amazon S3 bucket in exchange-manager.properties file in coordinator and worker pods. Let's also verify that inside coordinator pod  cat /etc/trino/exchange-manager.properties   Output​  exchange-manager.name=filesystem exchange.base-directories=s3://trino-exchange-bucket-20240215180855570800000004 exchange.s3.region=us-west-2 exchange.s3.iam-role=arn:aws:iam::xxxxxxxxxx:role/trino-sa-role   Please note down exchange manager S3 bucket name from above. You can explore contents of above S3 bucket in AWS Console. It will be empty when no query is running.  Now, let's exit from bash shell of the coordinator pod  exit   With below steps, we will now test fault-tolerant execution by running a select query and terminate few Trino workers when query is still running.  Find the file trino_select_query_iceberg.sql file in the examples folder, which contains the SQL commands below:  with inv as (select w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy ,stdev,mean, case mean when 0 then null else stdev/mean end cov from(select w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy ,stddev_samp(inv_quantity_on_hand) stdev,avg(inv_quantity_on_hand) mean from iceberg.iceberg_schema.inventory ,iceberg.iceberg_schema.item ,iceberg.iceberg_schema.warehouse ,iceberg.iceberg_schema.date_dim where inv_item_sk = i_item_sk and inv_warehouse_sk = w_warehouse_sk and inv_date_sk = d_date_sk and d_year =1999 group by w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy) foo where case mean when 0 then 0 else stdev/mean end &gt; 1) select inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean, inv1.cov ,inv2.w_warehouse_sk,inv2.i_item_sk,inv2.d_moy,inv2.mean, inv2.cov from inv inv1,inv inv2 where inv1.i_item_sk = inv2.i_item_sk and inv1.w_warehouse_sk = inv2.w_warehouse_sk and inv1.d_moy=4 and inv2.d_moy=4+1 and inv1.cov &gt; 1.5 order by inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean,inv1.cov,inv2.d_moy,inv2.mean, inv2.cov;   Let's now run select query first  ./trino --file 'examples/trino_select_query_iceberg.sql' --server http://localhost:8080 --user admin --ignore-errors   Immediately after above command, when above query is still running, open another terminal and scale down worker pods to just 1 worker, terminating all other workers with command below:  kubectl scale deployment trino-worker -n trino --replicas=1   See Trino Web UI on browser and there is only 1 active worker running now as other workers are terminated:    Go to Amazon S3 console and verify intermediate exchange data spooling in exchange manager S3 bucket with a name starting with trino-exchange-bucket.    Let's now see Trino Web UI monitor again to verify completion of the query despite of 6 failed tasks due to terminated workers (we have encircled them in RED in the screenshot below).  info Please note, number of failed tasks could be different in your Trino Web UI depending upon how many tasks were running on workers that got terminated. Also you can see different number of active workers depending upon worker pods scaled by Horizontal Pod Autoscaler(HPA) using cpu utilization metric    Cleaning Up Iceberg Resources​  Let's open Trino CLI  ./trino http://127.0.0.1:8080 --user admin   Now, let's delete Iceberg tables and schema by running below SQL commands on Trino CLI:  drop table iceberg.iceberg_schema.warehouse; drop table iceberg.iceberg_schema.item; drop table iceberg.iceberg_schema.inventory; drop table iceberg.iceberg_schema.date_dim; drop schema iceberg.iceberg_schema;   Exit from Trino CLI with exit command.  ","version":"Next","tagName":"h3"},{"title":"Cleanup 🧹​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#cleanup-","content":" To delete all the components provisioned as part of this blueprint, using the following command to destroy all the resources.  cd data-on-eks/distributed-databases/trino ./cleanup.sh   caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ex. S3 buckets for Trino Exchange manager  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Deploying Trino on EKS","url":"/data-on-eks/docs/blueprints/distributed-databases/trino#conclusion","content":" Trino is a tool for fast querying vast amounts of data from your data sources.In this example, we shared a terraform based blueprint that deploys Trino with fault-tolerant configuration on Amazon EKS, with add-ons necessary to build a complete EKS cluster (i.e. Karpenter for node autoscaling, Metrics server and HPA for Trino worker pods autoscaling, monitoring with Prometheus/Grafana stack). Among many features, we highlighted a couple of examples on creating an Iceberg or Hive data store using Amazon S3 as storage, and running simple Trino queries for results. We also deployed and scaled Trino workers on Spot instances for cost optimization. We also demonstrated fault-tolerant feature of Trino, which makes it suitable for Spot instances to save costs for long-running batch queries. ","version":"Next","tagName":"h2"},{"title":"Streaming Platforms on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms","content":"Streaming Platforms on EKS info Note: The blueprints for streaming platforms are currently in the process of development. Documentation will be updated once a deployment example has been added to the repository. Running streaming platforms on Kubernetes can provide a number of benefits for organizations looking to process and analyze real-time data streams. Kubernetes provides features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of streaming platforms. There are a number of popular streaming platforms that have emerged to support this use case, including Apache Kafka, Apache Flink, and Apache Pulsar. These platforms make it easy to process and analyze real-time data streams in a containerized environment, and provide features such as real-time data processing, event-driven architecture, and fault-tolerance. By leveraging the power of Kubernetes, organizations can focus on building and processing their streaming data pipelines, rather than worrying about the underlying infrastructure. With its robust ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running streaming platforms in production. KafkaFlink","keywords":"","version":"Next"},{"title":"Self-managed Apache Airflow deployment on Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#introduction","content":" This pattern deploys self-managed Apache Airflow deployment on EKS. This blueprint deploys Airflow on Amazon EKS managed node groups and leverages Karpenter to run the workloads.  Architecture    This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults and only customize if you have viable alternative option available for replacement.  In terms of infrastructure, below are the resources that are created by this pattern:  EKS Cluster Control plane with public endpoint (recommended for demo/poc environment) One managed node group Core Node group with 3 instances spanning multi-AZs for running Apache Airflow and other system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc. Apache Airflow core components (with airflow-core.tf): Amazon RDS PostgreSQL instance and security group for Airflow meta database.Airflow namespaceKubernetes service accounts and AWS IAM roles for service account (IRSA) for Airflow Webserver, Airflow Scheduler, and Airflow Worker.Amazon Elastic File System (EFS), EFS mounts, Kubernetes Storage Class for EFS, and Kubernetes Persistent Volume Claim for mounting Airflow DAGs for Airflow pods.Amazon S3 log bucket for Airflow logs  AWS for FluentBit is employed for logging, and a combination of Prometheus, Amazon Managed Prometheus, and open source Grafana are used for observability. You can see the complete list of add-ons available below.  tip We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as core-node-group as provided by this pattern.  danger We don't recommend removing critical add-ons (Amazon VPC CNI, CoreDNS, Kube-proxy).  Add-on\tEnabled by default?\tBenefits\tLinkAmazon VPC CNI\tYes\tVPC CNI is available as an EKS add-on and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods\tVPC CNI Documentation CoreDNS\tYes\tCoreDNS is available as an EKS add-on and is responsible for resolving DNS queries for spark application and for Kubernetes cluster\tEKS CoreDNS Documentation Kube-proxy\tYes\tKube-proxy is available as an EKS add-on and it maintains network rules on your nodes and enables network communication to your spark application pods\tEKS kube-proxy Documentation Amazon EBS CSI driver\tYes\tEBS CSI driver is available as an EKS add-on and it allows EKS clusters to manage the lifecycle of EBS volumes\tEBS CSI Driver Documentation Amazon EFS CSI driver\tYes\tThe Amazon EFS Container Storage Interface (CSI) driver provides a CSI interface that allows Kubernetes clusters running on AWS to manage the lifecycle of Amazon EFS file systems.\tEFS CSI Driver Documentation Karpenter\tYes\tKarpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters\tKarpenter Documentation Cluster Autoscaler\tYes\tKubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as core-node-group) in the cluster\tCluster Autoscaler Documentation Cluster proportional autoscaler\tYes\tThis is responsible for scaling CoreDNS pods in your Kubernetes cluster\tCluster Proportional Autoscaler Documentation Metrics server\tYes\tKubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster\tEKS Metrics Server Documentation Prometheus\tYes\tPrometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost\tPrometheus Documentation Amazon Managed Prometheus\tYes\tThis is responsible for storing and scaling of EKS cluster and spark application metrics\tAmazon Managed Prometheus Documentation Kubecost\tYes\tKubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels\tEKS Kubecost Documentation CloudWatch metrics\tYes\tCloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard\tCloudWatch Container Insights Documentation AWS for Fluent-bit\tYes\tThis can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system\tAWS For Fluent-bit Documentation AWS Load Balancer Controller\tYes\tThe AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster.\tAWS Load Balancer Controller Documentation  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraform  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#deploying-the-solution","content":" Clone the repository  git clone https://github.com/awslabs/data-on-eks.git   Navigate into self-managed-airflow directory and run install.sh script  cd data-on-eks/schedulers/terraform/self-managed-airflow chmod +x install.sh ./install.sh   ","version":"Next","tagName":"h2"},{"title":"Verify the resources​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#verify-the-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Create kubectl config​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#create-kubectl-config","content":" Update the placeholder for AWS region and run the below command.  mv ~/.kube/config ~/.kube/config.bk aws eks update-kubeconfig --region &lt;region&gt; --name self-managed-airflow   ","version":"Next","tagName":"h3"},{"title":"Describe the EKS Cluster​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#describe-the-eks-cluster","content":" aws eks describe-cluster --name self-managed-airflow   ","version":"Next","tagName":"h3"},{"title":"Verify the EFS PV and PVC created by this deployment​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#verify-the-efs-pv-and-pvc-created-by-this-deployment","content":" kubectl get pvc -n airflow NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE airflow-dags Bound pvc-157cc724-06d7-4171-a14d-something 10Gi RWX efs-sc 73m kubectl get pv -n airflow NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-157cc724-06d7-4171-a14d-something 10Gi RWX Delete Bound airflow/airflow-dags efs-sc 74m   ","version":"Next","tagName":"h3"},{"title":"Verify the EFS Filesystem​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#verify-the-efs-filesystem","content":" aws efs describe-file-systems --query &quot;FileSystems[*].FileSystemId&quot; --output text   ","version":"Next","tagName":"h3"},{"title":"Verify S3 bucket created for Airflow logs​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#verify-s3-bucket-created-for-airflow-logs","content":" aws s3 ls | grep airflow-logs-   ","version":"Next","tagName":"h3"},{"title":"Verify the Airflow deployment​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#verify-the-airflow-deployment","content":" kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1/1 1 1 77m airflow-scheduler 2/2 2 2 77m airflow-statsd 1/1 1 1 77m airflow-triggerer 1/1 1 1 77m airflow-webserver 2/2 2 2 77m   ","version":"Next","tagName":"h3"},{"title":"Fetch Postgres RDS password​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#fetch-postgres-rds-password","content":" Amazon Postgres RDS database password can be fetched from the Secrets manager  Login to AWS console and open secrets managerClick on postgres secret nameClick on Retrieve secret value button to verify the Postgres DB master password  ","version":"Next","tagName":"h3"},{"title":"Login to Airflow Web UI​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#login-to-airflow-web-ui","content":" This deployment creates an Ingress object with public LoadBalancer(internal # Private Load Balancer can only be accessed within the VPC) for demo purpose For production workloads, you can modify airflow-values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port.  Execute the following command to get the ALB DNS name  kubectl get ingress -n airflow NAME CLASS HOSTS ADDRESS PORTS AGE airflow-airflow-ingress alb * k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com 80 88m   The above ALB URL will be different for you deployment. So use your URL and open it in a browser  e.g., Open URL http://k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com/ in a browser  By default, Airflow creates a default user with admin and password as admin  Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user  ","version":"Next","tagName":"h3"},{"title":"Execute Sample Airflow Job​","type":1,"pageTitle":"Self-managed Apache Airflow deployment on Amazon EKS","url":"/data-on-eks/docs/blueprints/job-schedulers/self-managed-airflow#execute-sample-airflow-job","content":" Login to Airflow WebUIClick on DAGs link on the top of the page. This will show dags pre-created by the GitSync featureExecute the hello_world_scheduled_dag DAG by clicking on Play button (&gt;)Verify the DAG execution from Graph linkAll the Tasks will go green after few minutesClick on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3  Airflow to run Spark workloads with Karpenter 👈  Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h3"},{"title":"EMR on EKS with Flink Streaming","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink","content":"","keywords":"","version":"Next"},{"title":"Introduction to Apache Flink​","type":1,"pageTitle":"EMR on EKS with Flink Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink#introduction-to-apache-flink","content":" Apache Flink is an open-source, unified stream processing and batch processing framework that was designed to process large amounts of data. It provides fast, reliable, and scalable data processing with fault tolerance and exactly-once semantics. Some of the key features of Flink are:  Distributed Processing: Flink is designed to process large volumes of data in a distributed fashion, making it horizontally scalable and fault-tolerant.Stream Processing and Batch Processing: Flink provides APIs for both stream processing and batch processing. This means you can process data in real-time, as it's being generated, or process data in batches.Fault Tolerance: Flink has built-in mechanisms for handling node failures, network partitions, and other types of failures.Exactly-once Semantics: Flink supports exactly-once processing, which ensures that each record is processed exactly once, even in the presence of failures.Low Latency: Flink's streaming engine is optimized for low-latency processing, making it suitable for use cases that require real-time processing of data.Extensibility: Flink provides a rich set of APIs and libraries, making it easy to extend and customize to fit your specific use case.  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"EMR on EKS with Flink Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink#architecture","content":" Flink Architecture high level design with EKS.    ","version":"Next","tagName":"h2"},{"title":"EMR on EKS Flink Kubernetes Operator​","type":1,"pageTitle":"EMR on EKS with Flink Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink#emr-on-eks-flink-kubernetes-operator","content":" Amazon EMR releases 6.13.0 and higher support Amazon EMR on EKS with Apache Flink, or the , as a job submission model for Amazon EMR on EKS. With Amazon EMR on EKS with Apache Flink, you can deploy and manage Flink applications with the Amazon EMR release runtime on your own Amazon EKS clusters. Once you deploy the Flink Kubernetes operator in your Amazon EKS cluster, you can directly submit Flink applications with the operator. The operator manages the lifecycle of Flink applications.  Running, suspending and deleting applicationsStateful and stateless application upgradesTriggering and managing savepointsHandling errors, rolling-back broken upgrades  In addition to the above features, EMR Flink Kubernetes operator provides the following additional capabilities:  Launching Flink application using jars in Amazon S3Monitoring integration with Amazon S3 and Amazon CloudWatch and container log rotation.Automatically tunes Autoscaler configurations based on historical trends of observed metrics.Faster Flink Job Restart during scaling or Failure RecoveryIRSA (IAM Roles for Service Accounts) Native IntegrationPyflink support  Flink Operator defines two types of Custom Resources(CR) which are the extensions of the Kubernetes API.  FlinkDeploymentFlinkSessionJob FlinkDeployment FlinkDeployment CR defines Flink Application and Session Cluster deployments. Application deployments manage a single job deployment on a dedicated Flink cluster in Application mode. Session clusters allows you to run multiple Flink Jobs on an existing Session cluster. FlinkDeployment in Application modes, Click to toggle content! apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment metadata: namespace: default name: basic-example spec: image: flink:1.16 flinkVersion: v1_16 flinkConfiguration: taskmanager.numberOfTaskSlots: &quot;2&quot; serviceAccount: flink jobManager: resource: memory: &quot;2048m&quot; cpu: 1 taskManager: resource: memory: &quot;2048m&quot; cpu: 1 job: jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar parallelism: 2 upgradeMode: stateless state: running   info Session clusters use a similar spec to Application clusters with the only difference that job is not defined in the yaml spec.  info According to the Flink documentation, it is recommended to use FlinkDeployment in Application mode for production environments.  On top of the deployment types the Flink Kubernetes Operator also supports two modes of deployments: Native and Standalone.  NativeStandalone Native Native cluster deployment is the default deployment mode and uses Flink’s built in integration with Kubernetes when deploying the cluster.Flink cluster communicates directly with Kubernetes and allows it to manage Kubernetes resources, e.g. dynamically allocate and de-allocate TaskManager pods.Flink Native can be useful for advanced users who want to build their own cluster management system or integrate with existing management systems.Flink Native allows for more flexibility in terms of job scheduling and execution.For standard Operator use, running your own Flink Jobs in Native mode is recommended. apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment ... spec: ... mode: native   ","version":"Next","tagName":"h2"},{"title":"Best Practices for Running Flink Jobs on Kubernetes​","type":1,"pageTitle":"EMR on EKS with Flink Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink#best-practices-for-running-flink-jobs-on-kubernetes","content":" To get the most out of Flink on Kubernetes, here are some best practices to follow:  Use the Kubernetes Operator: Install and use the Flink Kubernetes Operator to automate the deployment and management of Flink clusters on Kubernetes.Deploy in dedicated namespaces: Create a separate namespace for the Flink Kubernetes Operator and another one for Flink jobs/workloads. This ensures that the Flink jobs are isolated and have their own resources.Use high-quality storage: Store Flink checkpoints and savepoints in high-quality storage such as Amazon S3 or another durable external storage. These storage options are reliable, scalable, and offer durability for large volumes of data.Optimize resource allocation: Allocate sufficient resources to Flink jobs to ensure optimal performance. This can be done by setting resource requests and limits for Flink containers.Proper network isolation: Use Kubernetes Network Policies to isolate Flink jobs from other workloads running on the same Kubernetes cluster. This ensures that Flink jobs have the required network access without being impacted by other workloads.Configure Flink optimally: Tune Flink settings according to your use case. For example, adjust Flink's parallelism settings to ensure that Flink jobs are scaled appropriately based on the size of the input data.Use checkpoints and savepoints: Use checkpoints for periodic snapshots of Flink application state and savepoints for more advanced use cases such as upgrading or downgrading the application.Store checkpoints and savepoints in the right places: Store checkpoints in distributed file systems or key-value stores like Amazon S3 or another durable external storage. Store savepoints in a durable external storage like Amazon S3.  ","version":"Next","tagName":"h2"},{"title":"Flink Upgrade​","type":1,"pageTitle":"EMR on EKS with Flink Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-flink#flink-upgrade","content":" Flink Operator provides three upgrade modes for Flink jobs. Checkout the Flink upgrade docs for up-to-date information.  stateless: Stateless application upgrades from empty statelast-state: Quick upgrades in any application state (even for failing jobs), does not require a healthy job as it always uses the latest checkpoint information. Manual recovery may be necessary if HA metadata is lost.savepoint: Use savepoint for upgrade, providing maximal safety and possibility to serve as backup/fork point. The savepoint will be created during the upgrade process. Note that the Flink job needs to be running to allow the savepoint to get created. If the job is in an unhealthy state, the last checkpoint will be used (unless kubernetes.operator.job.upgrade.last-state-fallback.enabled is set to false). If the last checkpoint is not available, the job upgrade will fail.  info last-state or savepoint are recommended modes for production  Deploying the Solution 👈  Execute Sample Flink job with Karpenter 👈  Autoscaler Example 👈  Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h2"},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/kafka","content":"","keywords":"","version":"Next"},{"title":"Why and what's KRaft?​","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/blueprints/streaming-platforms/kafka#why-and-whats-kraft","content":" KRaft mode simplifies Kafka deployments, enhances scalability, and improves overall system performance. By using a built-in consensus protocol, KRaft reduces operational complexity, potentially speeds up broker startup times, and allows for better handling of metadata operations. This architectural shift enables Kafka to manage larger clusters more efficiently, making it an attractive option for organizations looking to streamline their event streaming infrastructure and prepare for future scalability needs.  ","version":"Next","tagName":"h2"},{"title":"Strimzi for Apache Kafka​","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/blueprints/streaming-platforms/kafka#strimzi-for-apache-kafka","content":" Strimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. Strimzi combines security and simple configuration to deploy and manage Kafka on Kubernetes using kubectl and/or GitOps based on the Operator Pattern.  As of version 0.32.0, Strimzi provides full support for deploying Kafka clusters using KRaft, making it easier for organizations to leverage this new architecture. By using Strimzi, you can seamlessly deploy and manage Kafka clusters in KRaft mode on Kubernetes, taking advantage of its custom resource definitions (CRDs) and operators to handle the complexities of configuration and lifecycle management.  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/blueprints/streaming-platforms/kafka#architecture","content":" info Architecture diagram work in progress  Managed Alternatives 👈  Storage considerations when self-managing Kafka 👈  Deploying the Solution 👈  Create a Kafka cluster 👈  Create Kafka Topic and run Sample test 👈  Grafana Dashboard for Kafka 👈  Cleanup 👈 ","version":"Next","tagName":"h2"},{"title":"Flink Operator on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink","content":"","keywords":"","version":"Next"},{"title":"Introduction to Apache Flink​","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink#introduction-to-apache-flink","content":" Apache Flink is an open-source, unified stream processing and batch processing framework that was designed to process large amounts of data. It provides fast, reliable, and scalable data processing with fault tolerance and exactly-once semantics. Some of the key features of Flink are:  Distributed Processing: Flink is designed to process large volumes of data in a distributed fashion, making it horizontally scalable and fault-tolerant.Stream Processing and Batch Processing: Flink provides APIs for both stream processing and batch processing. This means you can process data in real-time, as it's being generated, or process data in batches.Fault Tolerance: Flink has built-in mechanisms for handling node failures, network partitions, and other types of failures.Exactly-once Semantics: Flink supports exactly-once processing, which ensures that each record is processed exactly once, even in the presence of failures.Low Latency: Flink's streaming engine is optimized for low-latency processing, making it suitable for use cases that require real-time processing of data.Extensibility: Flink provides a rich set of APIs and libraries, making it easy to extend and customize to fit your specific use case.  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink#architecture","content":" Flink Architecture high level design with EKS.    ","version":"Next","tagName":"h2"},{"title":"Flink Kubernetes Operator​","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink#flink-kubernetes-operator","content":" Flink Kubernetes Operator is a powerful tool for managing Flink clusters on Kubernetes. Flink Kubernetes Operator (Operator) acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. The Operator can be installed on a Kubernetes cluster using Helm. The core responsibility of the Flink operator is to manage the full production lifecycle of Flink applications.  Running, suspending and deleting applicationsStateful and stateless application upgradesTriggering and managing savepointsHandling errors, rolling-back broken upgrades  Flink Operator defines two types of Custom Resources(CR) which are the extensions of the Kubernetes API.  FlinkDeploymentFlinkSessionJob FlinkDeployment FlinkDeployment CR defines Flink Application and Session Cluster deployments. Application deployments manage a single job deployment on a dedicated Flink cluster in Application mode. Session clusters allows you to run multiple Flink Jobs on an existing Session cluster. FlinkDeployment in Application modes, Click to toggle content! apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment metadata: namespace: default name: basic-example spec: image: flink:1.16 flinkVersion: v1_16 flinkConfiguration: taskmanager.numberOfTaskSlots: &quot;2&quot; serviceAccount: flink jobManager: resource: memory: &quot;2048m&quot; cpu: 1 taskManager: resource: memory: &quot;2048m&quot; cpu: 1 job: jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar parallelism: 2 upgradeMode: stateless state: running   info Session clusters use a similar spec to Application clusters with the only difference that job is not defined in the yaml spec.  info According to the Flink documentation, it is recommended to use FlinkDeployment in Application mode for production environments.  On top of the deployment types the Flink Kubernetes Operator also supports two modes of deployments: Native and Standalone.  NativeStandalone Native Native cluster deployment is the default deployment mode and uses Flink’s built in integration with Kubernetes when deploying the cluster.Flink cluster communicates directly with Kubernetes and allows it to manage Kubernetes resources, e.g. dynamically allocate and de-allocate TaskManager pods.Flink Native can be useful for advanced users who want to build their own cluster management system or integrate with existing management systems.Flink Native allows for more flexibility in terms of job scheduling and execution.For standard Operator use, running your own Flink Jobs in Native mode is recommended. apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment ... spec: ... mode: native   ","version":"Next","tagName":"h2"},{"title":"Best Practices for Running Flink Jobs on Kubernetes​","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink#best-practices-for-running-flink-jobs-on-kubernetes","content":" To get the most out of Flink on Kubernetes, here are some best practices to follow:  Use the Kubernetes Operator: Install and use the Flink Kubernetes Operator to automate the deployment and management of Flink clusters on Kubernetes.Deploy in dedicated namespaces: Create a separate namespace for the Flink Kubernetes Operator and another one for Flink jobs/workloads. This ensures that the Flink jobs are isolated and have their own resources.Use high-quality storage: Store Flink checkpoints and savepoints in high-quality storage such as Amazon S3 or another durable external storage. These storage options are reliable, scalable, and offer durability for large volumes of data.Optimize resource allocation: Allocate sufficient resources to Flink jobs to ensure optimal performance. This can be done by setting resource requests and limits for Flink containers.Proper network isolation: Use Kubernetes Network Policies to isolate Flink jobs from other workloads running on the same Kubernetes cluster. This ensures that Flink jobs have the required network access without being impacted by other workloads.Configure Flink optimally: Tune Flink settings according to your use case. For example, adjust Flink's parallelism settings to ensure that Flink jobs are scaled appropriately based on the size of the input data.Use checkpoints and savepoints: Use checkpoints for periodic snapshots of Flink application state and savepoints for more advanced use cases such as upgrading or downgrading the application.Store checkpoints and savepoints in the right places: Store checkpoints in distributed file systems or key-value stores like Amazon S3 or another durable external storage. Store savepoints in a durable external storage like Amazon S3.  ","version":"Next","tagName":"h2"},{"title":"Flink Upgrade​","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/flink#flink-upgrade","content":" Flink Operator provides three upgrade modes for Flink jobs. Checkout the Flink upgrade docs for up-to-date information.  stateless: Stateless application upgrades from empty statelast-state: Quick upgrades in any application state (even for failing jobs), does not require a healthy job as it always uses the latest checkpoint information. Manual recovery may be necessary if HA metadata is lost.savepoint: Use savepoint for upgrade, providing maximal safety and possibility to serve as backup/fork point. The savepoint will be created during the upgrade process. Note that the Flink job needs to be running to allow the savepoint to get created. If the job is in an unhealthy state, the last checkpoint will be used (unless kubernetes.operator.job.upgrade.last-state-fallback.enabled is set to false). If the last checkpoint is not available, the job upgrade will fail.  info last-state or savepoint are recommended modes for production  Deploying the Solution 👈  Execute Sample Flink job with Karpenter 👈  Execute Sample Flink job with Managed Node Groups and Cluster Autoscaler 👈  Cleanup 👈  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h2"},{"title":"EMR on EKS with Spark Streaming","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream","content":"","keywords":"","version":"Next"},{"title":"Spark examples - read stream from MSK​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#spark-examples---read-stream-from-msk","content":" Spark consumer applications reading from Amazon MSK:  1. Run a job with EMR on EKS2. Same job with Fargate on EMR on EKS3. Same job with EMR on EC2  ","version":"Next","tagName":"h2"},{"title":"Spark examples - read stream from Kinesis​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#spark-examples---read-stream-from-kinesis","content":" 1. (Optional) Build a custom docker image2. Run a job with kinesis-sql connector3. Run a job with Spark's DStream  ","version":"Next","tagName":"h2"},{"title":"Deploy Infrastructure​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#deploy-infrastructure","content":" The provisioning takes about 30 minutes to complete. Two ways to deploy:  AWS CloudFormation template (CFN)AWS Cloud Development Kit (AWS CDK).  ","version":"Next","tagName":"h2"},{"title":"CloudFormation Deployment​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#cloudformation-deployment","content":" Region\tLaunch Template---------------------------\t----------------------- US East (N. Virginia)\t  To launch in a different AWS Region, check out the following customization section, or use the CDK deployment option.  ","version":"Next","tagName":"h3"},{"title":"Customization​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#customization","content":" You can customize the solution, such as set to a different region, then generate the CFN templates in your required region:  export BUCKET_NAME_PREFIX=&lt;my-bucket-name&gt; # bucket where customized code will reside export AWS_REGION=&lt;your-region&gt; export SOLUTION_NAME=emr-stream-demo export VERSION=v2.0.0 # version number for the customized code cd data-on-eks/analytics/cdk/stream-emr-on-eks ./deployment/build-s3-dist.sh $BUCKET_NAME_PREFIX $SOLUTION_NAME $VERSION # create the bucket where customized code will reside aws s3 mb s3://$BUCKET_NAME_PREFIX-$AWS_REGION --region $AWS_REGION # Upload deployment assets to the S3 bucket aws s3 cp ./deployment/global-s3-assets/ s3://$BUCKET_NAME_PREFIX-$AWS_REGION/$SOLUTION_NAME/$VERSION/ --recursive --acl bucket-owner-full-control aws s3 cp ./deployment/regional-s3-assets/ s3://$BUCKET_NAME_PREFIX-$AWS_REGION/$SOLUTION_NAME/$VERSION/ --recursive --acl bucket-owner-full-control echo -e &quot;\\nIn web browser, paste the URL to launch the template: https://console.aws.amazon.com/cloudformation/home?region=$AWS_REGION#/stacks/quickcreate?stackName=emr-stream-demo&amp;templateURL=https://$BUCKET_NAME_PREFIX-$AWS_REGION.s3.amazonaws.com/$SOLUTION_NAME/$VERSION/emr-stream-demo.template\\n&quot;   ","version":"Next","tagName":"h3"},{"title":"CDK Deployment​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#cdk-deployment","content":" Prerequisites​  Install the following tools:  Python 3.6 +.Node.js 10.3.0 +AWS CLI. Configure the CLI by aws configure.CDK toolkitOne-off CDK bootstrap for the first time deployment.  Deploy​  python3 -m venv .env source .env/bin/activate pip install -r requirements.txt cdk deploy   ","version":"Next","tagName":"h3"},{"title":"Post-deployment​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#post-deployment","content":" The following post-deployment.sh is executable in Linux, not for Mac OSX. Modify the script if needed.  Open the &quot;Kafka Client&quot; IDE in Cloud9 console. Create one if the Cloud9 IDE doesn't exist.  VPC prefix: 'emr-stream-demo' Instance Type: 't3.small'   Attach the IAM role that contains Cloud9Admin to your IDE. Turn off AWS managed temporary credentials in Cloud9:  curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; unzip awscliv2.zip sudo ./aws/install --update /usr/local/bin/aws cloud9 update-environment --environment-id $C9_PID --managed-credentials-action DISABLE rm -vf ${HOME}/.aws/credentials   Run the script to configure the cloud9 IDE environment:  curl https://raw.githubusercontent.com/aws-samples/stream-emr-on-eks/main/deployment/app_code/post-deployment.sh | bash   Wait for 5 mins, then check the MSK cluster status. Make sure it is active before sending data to the cluster.Launching a new terminal window in Cloud9, send the sample data to MSK:  wget https://github.com/xuite627/workshop_flink1015-1/raw/master/dataset/nycTaxiRides.gz zcat nycTaxiRides.gz | split -l 10000 --filter=&quot;kafka_2.12-2.8.1/bin/kafka-console-producer.sh --broker-list ${MSK_SERVER} --topic taxirides ; sleep 0.2&quot; &gt; /dev/null   Launching the 3rd terminal window and monitor the source MSK topic:  kafka_2.12-2.8.1/bin/kafka-console-consumer.sh \\ --bootstrap-server ${MSK_SERVER} \\ --topic taxirides \\ --from-beginning   ","version":"Next","tagName":"h2"},{"title":"MSK integration​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#msk-integration","content":" ","version":"Next","tagName":"h2"},{"title":"1. Submit a job with EMR on EKS​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#1-submit-a-job-with-emr-on-eks","content":" Sample job to consume data stream in MSKSubmit the job:  aws emr-containers start-job-run \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ --name msk_consumer \\ --execution-role-arn $EMR_ROLE_ARN \\ --release-label emr-5.33.0-latest \\ --job-driver '{ &quot;sparkSubmitJobDriver&quot;:{ &quot;entryPoint&quot;: &quot;s3://'$S3BUCKET'/app_code/job/msk_consumer.py&quot;, &quot;entryPointArguments&quot;:[&quot;'$MSK_SERVER'&quot;,&quot;s3://'$S3BUCKET'/stream/checkpoint/emreks&quot;,&quot;emreks_output&quot;], &quot;sparkSubmitParameters&quot;: &quot;--conf spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7 --conf spark.cleaner.referenceTracking.cleanCheckpoints=true --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2&quot;}}' \\ --configuration-overrides '{ &quot;applicationConfiguration&quot;: [ { &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.kubernetes.driver.podTemplateFile&quot;:&quot;s3://'$S3BUCKET'/app_code/job/driver_template.yaml&quot;,&quot;spark.kubernetes.executor.podTemplateFile&quot;:&quot;s3://'$S3BUCKET'/app_code/job/executor_template.yaml&quot; } } ], &quot;monitoringConfiguration&quot;: { &quot;s3MonitoringConfiguration&quot;: {&quot;logUri&quot;: &quot;s3://'${S3BUCKET}'/elasticmapreduce/emreks-log/&quot;}} }'   ","version":"Next","tagName":"h3"},{"title":"Verify the job is running:​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#verify-the-job-is-running","content":" # can see the job pod in EKS kubectl get po -n emr # verify in EMR console # in Cloud9, run the consumer tool to check if any data comeing through in the target Kafka topic kafka_2.12-2.8.1/bin/kafka-console-consumer.sh --bootstrap-server ${MSK_SERVER} --topic emreks_output --from-beginning   ","version":"Next","tagName":"h3"},{"title":"Cancel the long-running job (can get job id from the job submission output or in EMR console)​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#cancel-the-long-running-job-can-get-job-id-from-the-job-submission-output-or-in-emr-console","content":" aws emr-containers cancel-job-run --virtual-cluster-id $VIRTUAL_CLUSTER_ID --id &lt;YOUR_JOB_ID&gt;   ","version":"Next","tagName":"h3"},{"title":"2. EMR on EKS with Fargate​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#2-emr-on-eks-with-fargate","content":" Run the same job on the same EKS cluster, but with the serverless option - Fargate compute choice.  To ensure it is picked up by Fargate not by the managed nodegroup on EC2, we will tag the Spark job by a serverless label, which has setup in a Fargate profile previously:  --conf spark.kubernetes.driver.label.type=serverless --conf spark.kubernetes.executor.label.type=serverless   Submit the job to Fargate:  aws emr-containers start-job-run \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ --name msk_consumer_fg \\ --execution-role-arn $EMR_ROLE_ARN \\ --release-label emr-5.33.0-latest \\ --job-driver '{ &quot;sparkSubmitJobDriver&quot;:{ &quot;entryPoint&quot;: &quot;s3://'$S3BUCKET'/app_code/job/msk_consumer.py&quot;, &quot;entryPointArguments&quot;:[&quot;'$MSK_SERVER'&quot;,&quot;s3://'$S3BUCKET'/stream/checkpoint/emreksfg&quot;,&quot;emreksfg_output&quot;], &quot;sparkSubmitParameters&quot;: &quot;--conf spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7 --conf spark.cleaner.referenceTracking.cleanCheckpoints=true --conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=2 --conf spark.kubernetes.driver.label.type=serverless --conf spark.kubernetes.executor.label.type=serverless&quot;}}' \\ --configuration-overrides '{ &quot;monitoringConfiguration&quot;: { &quot;s3MonitoringConfiguration&quot;: {&quot;logUri&quot;: &quot;s3://'${S3BUCKET}'/elasticmapreduce/emreksfg-log/&quot;}}}'   ","version":"Next","tagName":"h3"},{"title":"Verify the job is running on EKS Fargate​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#verify-the-job-is-running-on-eks-fargate","content":" kubectl get po -n emr # verify in EMR console # in Cloud9, run the consumer tool to check if any data comeing through in the target Kafka topic kafka_2.12-2.8.1/bin/kafka-console-consumer.sh \\ --bootstrap-server ${MSK_SERVER} \\ --topic emreksfg_output \\ --from-beginning   ","version":"Next","tagName":"h3"},{"title":"3. (Optional) Submit step to EMR on EC2​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#3-optional-submit-step-to-emr-on-ec2","content":" cluster_id=$(aws emr list-clusters --cluster-states WAITING --query 'Clusters[?Name==`emr-stream-demo`].Id' --output text) MSK_SERVER=$(echo $MSK_SERVER | cut -d',' -f 2) aws emr add-steps \\ --cluster-id $cluster_id \\ --steps Type=spark,Name=emrec2_stream,Args=[--deploy-mode,cluster,--conf,spark.cleaner.referenceTracking.cleanCheckpoints=true,--conf,spark.executor.instances=2,--conf,spark.executor.memory=2G,--conf,spark.driver.memory=2G,--conf,spark.executor.cores=2,--packages,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,s3://$S3BUCKET/app_code/job/msk_consumer.py,$MSK_SERVER,s3://$S3BUCKET/stream/checkpoint/emrec2,emrec2_output],ActionOnFailure=CONTINUE   ","version":"Next","tagName":"h3"},{"title":"Verify​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#verify","content":" # verify in EMR console # in Cloud9, run the consumer tool to check if any data comeing through in the target Kafka topic kafka_2.12-2.8.1/bin/kafka-console-consumer.sh \\ --bootstrap-server ${MSK_SERVER} \\ --topic emrec2_output \\ --from-beginning   ","version":"Next","tagName":"h3"},{"title":"Kinesis integration​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#kinesis-integration","content":" ","version":"Next","tagName":"h2"},{"title":"1. (Optional) Build custom docker image​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#1-optional-build-custom-docker-image","content":" We will create &amp; delete a kinesis test stream on the fly via boto3, so a custom EMR on EKS docker image to include the Python library is needed. The custom docker image is not compulsory, if you don't need the boto3 and kinesis-sql connector.  Build a image based on EMR on EKS 6.5:  export AWS_REGION=$(aws configure list | grep region | awk '{print $2}') export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export ECR_URL=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 895885662937.dkr.ecr.us-west-2.amazonaws.com docker build -t emr6.5_custom . # create ECR repo in current account aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_URL aws ecr create-repository --repository-name emr6.5_custom_boto3 --image-scanning-configuration scanOnPush=true --region $AWS_REGION # push to ECR docker tag emr6.5_custom $ECR_URL/emr6.5_custom_boto3 docker push $ECR_URL/emr6.5_custom_boto3   ","version":"Next","tagName":"h3"},{"title":"2. Use kinesis-sql connector​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#2-use-kinesis-sql-connector","content":" This demo uses the com.qubole.spark/spark-sql-kinesis_2.12/1.2.0-spark_3.0 connector to interact with Kinesis.  To enable the job-level access control, ie. the IRSA feature, we have forked the kinesis-sql git repo and recompiled a new jar after upgraded the AWS java SDK. The custom docker build above will pick up the upgraded connector automatically.  Sample job to consume data stream in KinesisSubmit the job:  export AWS_REGION=$(aws configure list | grep region | awk '{print $2}') export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export ECR_URL=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com aws emr-containers start-job-run \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ --name kinesis-demo \\ --execution-role-arn $EMR_ROLE_ARN \\ --release-label emr-6.5.0-latest \\ --job-driver '{ &quot;sparkSubmitJobDriver&quot;:{ &quot;entryPoint&quot;: &quot;s3://'$S3BUCKET'/app_code/job/qubole-kinesis.py&quot;, &quot;entryPointArguments&quot;:[&quot;'${AWS_REGION}'&quot;,&quot;s3://'${S3BUCKET}'/qubolecheckpoint&quot;,&quot;s3://'${S3BUCKET}'/qubole-kinesis-output&quot;], &quot;sparkSubmitParameters&quot;: &quot;--conf spark.cleaner.referenceTracking.cleanCheckpoints=true&quot;}}' \\ --configuration-overrides '{ &quot;applicationConfiguration&quot;: [ { &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.kubernetes.container.image&quot;: &quot;'${ECR_URL}'/emr6.5_custom_boto3:latest&quot; } } ], &quot;monitoringConfiguration&quot;: { &quot;s3MonitoringConfiguration&quot;: {&quot;logUri&quot;: &quot;s3://'${S3BUCKET}'/elasticmapreduce/kinesis-fargate-log/&quot;} } }'   ","version":"Next","tagName":"h3"},{"title":"3. Use Spark's DStream​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#3-use-sparks-dstream","content":" This demo uses the spark-streaming-kinesis-asl_2.12 library to read from Kinesis. Check out the Spark's official document. The Spark syntax is slightly different from the spark-sql-kinesis approach. It operates at RDD level.  Sample job to consume data stream from KinesisSubmit the job:  export AWS_REGION=$(aws configure list | grep region | awk '{print $2}') export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export ECR_URL=$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com aws emr-containers start-job-run \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID \\ --name kinesis-demo \\ --execution-role-arn $EMR_ROLE_ARN \\ --release-label emr-6.5.0-latest \\ --job-driver '{ &quot;sparkSubmitJobDriver&quot;:{ &quot;entryPoint&quot;: &quot;s3://'$S3BUCKET'/app_code/job/pyspark-kinesis.py&quot;, &quot;entryPointArguments&quot;:[&quot;'${AWS_REGION}'&quot;,&quot;s3://'$S3BUCKET'/asloutput/&quot;], &quot;sparkSubmitParameters&quot;: &quot;--jars https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kinesis-asl_2.12/3.1.2/spark-streaming-kinesis-asl_2.12-3.1.2.jar,https://repo1.maven.org/maven2/com/amazonaws/amazon-kinesis-client/1.12.0/amazon-kinesis-client-1.12.0.jar&quot;}}' \\ --configuration-overrides '{ &quot;applicationConfiguration&quot;: [ { &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.kubernetes.container.image&quot;: &quot;'${ECR_URL}'/emr6.5_custom_boto3:latest&quot; } } ], &quot;monitoringConfiguration&quot;: { &quot;s3MonitoringConfiguration&quot;: {&quot;logUri&quot;: &quot;s3://'${S3BUCKET}'/elasticmapreduce/kinesis-fargate-log/&quot;} } }'   ","version":"Next","tagName":"h3"},{"title":"Useful commands​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#useful-commands","content":" kubectl get pod -n emr list running Spark jobskubectl delete pod --all -n emr delete all Spark jobskubectl logs &lt;pod name&gt; -n emr check logs against a pod in the emr namespacekubectl get node --label-columns=eks.amazonaws.com/capacityType,topology.kubernetes.io/zone check EKS compute capacity types and AZ distribution.  ","version":"Next","tagName":"h2"},{"title":"Clean up​","type":1,"pageTitle":"EMR on EKS with Spark Streaming","url":"/data-on-eks/docs/blueprints/streaming-platforms/emr-eks-stream#clean-up","content":" Run the clean-up script with:  curl https://raw.githubusercontent.com/aws-samples/stream-emr-on-eks/main/deployment/app_code/delete_all.sh | bash   Go to the CloudFormation console, manually delete the remaining resources if needed. ","version":"Next","tagName":"h2"},{"title":"Apache NiFi on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#introduction","content":" Apache NiFi is an open-source data integration and management system designed to automate and manage the flow of data between systems. It provides a web-based user interface for creating, monitoring, and managing data flows in real-time.  With its powerful and flexible architecture, Apache NiFi can handle a wide range of data sources, cloud platforms, and formats, including structured and unstructured data, and can be used for a variety of data integration scenarios, such as data ingest, data processing (low to medium level), data routing, data transformation, and data dissemination.  Apache NiFi provides a GUI based interface for building and managing data flows, making it easier for non-technical users. It also offers robust security features, including SSL, SSH, and fine-grained access control, to ensure the safe and secure transfer of sensitive data. Whether you are a data analyst, a data engineer, or a data scientist, Apache NiFi provides a comprehensive solution for managing and integrating your data on AWS and other platforms.  caution This blueprint should be considered as experimental and should only be used for proof of concept.  This example deploys an EKS Cluster running the Apache NiFi cluster. In the example, Apache NIfi is streaming data from the AWS Kinesis Data Stream to an Amazon DynamoDB table after some format transformation.  Creates a new sample VPC, 3 Private Subnets and 3 Public SubnetsCreates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with one managed node groupDeploys Apache NiFi, AWS Load Balancer Controller, Cert Manager and External DNS (optional) add-onsDeploys Apache NiFi cluster in the nifi namespace  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#prerequisites","content":" Ensure that you have installed the following tools on your machine.  aws clikubectlterraformjq  Additionally, for end-to-end configuration of Ingress, you will need to provide the following:  A Route53 Public Hosted Zone configured in the account where you are deploying this example. E.g. &quot;example.com&quot;An ACM Certificate in the account + region where you are deploying this example. A wildcard certificate is preferred, e.g. &quot;*.example.com&quot;  ","version":"Next","tagName":"h2"},{"title":"Deploy the EKS Cluster with Apache NiFi​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#deploy-the-eks-cluster-with-apache-nifi","content":" ","version":"Next","tagName":"h2"},{"title":"Clone the repository​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#clone-the-repository","content":" git clone https://github.com/awslabs/data-on-eks.git   ","version":"Next","tagName":"h3"},{"title":"Initialize Terraform​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#initialize-terraform","content":" Navigate into the example directory and run terraform init  cd data-on-eks/streaming/nifi/ terraform init   ","version":"Next","tagName":"h3"},{"title":"Terraform Plan​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#terraform-plan","content":" Run Terraform plan to verify the resources created by this execution.  Provide a Route53 Hosted Zone hostname and a corresponding ACM Certificate;  export TF_VAR_eks_cluster_domain=&quot;&lt;CHANGEME - example.com&gt;&quot; export TF_VAR_acm_certificate_domain=&quot;&lt;CHANGEME - *.example.com&gt;&quot; export TF_VAR_nifi_sub_domain=&quot;nifi&quot; export TF_VAR_nifi_username=&quot;admin&quot;   ","version":"Next","tagName":"h3"},{"title":"Deploy the pattern​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#deploy-the-pattern","content":" terraform plan terraform apply   Enter yes to apply.  Outputs: configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name nifi-on-eks&quot;   ","version":"Next","tagName":"h3"},{"title":"Verify Deployment​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#verify-deployment","content":" Update kubeconfig  aws eks --region us-west-2 update-kubeconfig --name nifi-on-eks   Verify all pods are running.  NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch aws-cloudwatch-metrics-7fbcq 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-82c9v 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-blrmt 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-dhpl7 1/1 Running 0 19h amazon-cloudwatch aws-cloudwatch-metrics-hpw5k 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-7d57b6576b-c52dw 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-cainjector-86f7f4749-hs7d9 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-webhook-66c85f8577-rxms8 1/1 Running 1 (43h ago) 2d external-dns external-dns-57bb948d75-g8kbs 1/1 Running 0 41h grafana grafana-7f5b7f5d4c-znrqk 1/1 Running 1 (43h ago) 2d kube-system aws-load-balancer-controller-7ff998fc9b-86gql 1/1 Running 1 (43h ago) 2d kube-system aws-load-balancer-controller-7ff998fc9b-hct9k 1/1 Running 1 (43h ago) 2d kube-system aws-node-4gcqk 1/1 Running 1 (43h ago) 2d kube-system aws-node-4sssk 1/1 Running 0 19h kube-system aws-node-4t62f 1/1 Running 1 (43h ago) 2d kube-system aws-node-g4ndt 1/1 Running 1 (43h ago) 2d kube-system aws-node-hlxmq 1/1 Running 1 (43h ago) 2d kube-system cluster-autoscaler-aws-cluster-autoscaler-7bd6f7b94b-j7td5 1/1 Running 1 (43h ago) 2d kube-system cluster-proportional-autoscaler-coredns-6ccfb4d9b5-27xsd 1/1 Running 1 (43h ago) 2d kube-system coredns-5c5677bc78-rhzkx 1/1 Running 1 (43h ago) 2d kube-system coredns-5c5677bc78-t7m5z 1/1 Running 1 (43h ago) 2d kube-system ebs-csi-controller-87c4ff9d4-ffmwh 6/6 Running 6 (43h ago) 2d kube-system ebs-csi-controller-87c4ff9d4-nfw28 6/6 Running 6 (43h ago) 2d kube-system ebs-csi-node-4mkc8 3/3 Running 0 19h kube-system ebs-csi-node-74xqs 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-8cw8t 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-cs9wp 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-ktdb7 3/3 Running 3 (43h ago) 2d kube-system kube-proxy-4s72m 1/1 Running 0 19h kube-system kube-proxy-95ptn 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-bhrdk 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-nzvb6 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-q9xkc 1/1 Running 1 (43h ago) 2d kube-system metrics-server-fc87d766-dd647 1/1 Running 1 (43h ago) 2d kube-system metrics-server-fc87d766-vv8z9 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-b5vqg 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-pklhr 1/1 Running 0 19h logging aws-for-fluent-bit-rq2nc 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-tnmtl 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-zzhfc 1/1 Running 1 (43h ago) 2d nifi nifi-0 5/5 Running 0 41h nifi nifi-1 5/5 Running 0 41h nifi nifi-2 5/5 Running 0 41h nifi nifi-registry-0 1/1 Running 0 41h nifi nifi-zookeeper-0 1/1 Running 0 41h nifi nifi-zookeeper-1 1/1 Running 0 41h nifi nifi-zookeeper-2 1/1 Running 0 18h prometheus prometheus-alertmanager-655fcb46df-2qh8h 2/2 Running 2 (43h ago) 2d prometheus prometheus-kube-state-metrics-549f6d74dd-wwhtr 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-5cpzk 1/1 Running 0 19h prometheus prometheus-node-exporter-8jhbk 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-nbd42 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-str6t 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-zkf5s 1/1 Running 1 (43h ago) 2d prometheus prometheus-pushgateway-677c6fdd5-9tqkl 1/1 Running 1 (43h ago) 2d prometheus prometheus-server-7bf9cbb9cf-b2zgl 2/2 Running 2 (43h ago) 2d vpa vpa-recommender-7c6bbb4f9b-rjhr7 1/1 Running 1 (43h ago) 2d vpa vpa-updater-7975b9dc55-g6zf6 1/1 Running 1 (43h ago) 2d   Apache NiFi UI​  The Apache NiFi Dashboard can be opened at the following url &quot;https://nifi.example.com/nifi&quot;    Run the command below to retrieve NiFi user's password and default username as admin  aws secretsmanager get-secret-value --secret-id &lt;nifi_login_password_secret_name from terraform outputs&gt; --region &lt;region&gt; | jq '.SecretString' --raw-output     ","version":"Next","tagName":"h3"},{"title":"Monitoring​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#monitoring","content":" Apache Nifi can be monitored using metrics reported by PrometheusReportingTask. JVM metrics are disabled by default, let's enable the JVM metrics by navigating to Controller Settings by the clicking on the hamburger icon (three horizontal bars) in the top right corner.    Next click on the REPORTING TASK tab and then click the + icon and search for PrometheusReportingTask in the filter. Select the PrometheusReportingTask and click ADD button.    The prometheus reporting task is stopped by default.    Click on the pencil icon to edit the task and click on the PROPERTIES tab. Set the Send JVM metrics to true and click on Apply. Start the task by clicking on the play icon and ensure it's in running state.    This blueprint uses the prometheus and grafana to create a monitoring stack for getting visibility into your Apache NiFi cluster.  aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name from terraform outputs&gt; --region &lt;region&gt; | jq '.SecretString' --raw-output   Run the command below and open the Grafana dashboard using the url &quot;http://localhost:8080&quot;.  kubectl port-forward svc/grafana -n grafana 8080:80   Import Apache NiFi Grafana dashboard    ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#example","content":" Create IAM policies for accessing Amazon DynamoDB and AWS Kinesis​  Create an AWS IAM role: Create an AWS IAM role with permissions to access the AWS Kinesis data stream and assign this role to the AWS EKS cluster hosting Apache NiFi. Attach the IAM policy: Attach a policy to the IAM role that limits access to the Kinesis data stream to read-only and IAM policy to enable EKS role to write Amazon DynamoDB table. Here's an example policy:  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;Nifi-access-to-Kinesis&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;kinesis:DescribeStream&quot;, &quot;kinesis:GetRecords&quot;, &quot;kinesis:GetShardIterator&quot;, &quot;kinesis:ListStreams&quot; ], &quot;Resource&quot;: &quot;arn:aws:kinesis:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:stream/kds-stream-nifi-on-EKS&quot; } ] }   { &quot;Sid&quot;: &quot;DynamoDBTableAccess&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;dynamodb:BatchGetItem&quot;, &quot;dynamodb:BatchWriteItem&quot;, &quot;dynamodb:ConditionCheckItem&quot;, &quot;dynamodb:PutItem&quot;, &quot;dynamodb:DescribeTable&quot;, &quot;dynamodb:DeleteItem&quot;, &quot;dynamodb:GetItem&quot;, &quot;dynamodb:Scan&quot;, &quot;dynamodb:Query&quot;, &quot;dynamodb:UpdateItem&quot; ], &quot;Resource&quot;: &quot;arn:aws:dynamodb:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:table/NifiStreamingTable&quot; }   Create AWS Kinesis Data Stream​  Create an AWS Kinesis data stream: Log in to the AWS Management Console, and create a Kinesis data stream in the region where you want to collect your data or use the below command line to create one.  aws kinesis create-stream --stream-name kds-stream-nifi-on-EKS   Create Amazon DynamoDB table​  Create a Amazon DynamoDB in the same AWS Account using the AWS console or the command line. Create a JSON file with Amazon DynamoDb table information called JSONSchemaDynamoDBTABLE.json   &quot;TableName&quot;: &quot;NifiStreamingTable&quot;, &quot;KeySchema&quot;: [ { &quot;AttributeName&quot;: &quot;Name&quot;, &quot;KeyType&quot;: &quot;HASH&quot; }, { &quot;AttributeName&quot;: &quot;Age&quot;, &quot;KeyType&quot;: &quot;RANGE&quot; }}, { &quot;AttributeName&quot;: &quot;Location&quot;, &quot;KeyType&quot;: &quot;RANGE&quot; } ], &quot;AttributeDefinitions&quot;: [ { &quot;AttributeName&quot;: &quot;Name&quot;, &quot;KeyType&quot;: &quot;S&quot; }, { &quot;AttributeName&quot;: &quot;Age&quot;, &quot;KeyType&quot;: &quot;S&quot; }}, { &quot;AttributeName&quot;: &quot;Location&quot;, &quot;KeyType&quot;: &quot;S&quot; } ], &quot;ProvisionedThroughput&quot;: { &quot;ReadCapacityUnits&quot;: 5, &quot;WriteCapacityUnits&quot;: 5 } }   Execute the command line to create the Amazon DynamoDB table from the JSON file.  aws dynamodb create-table --cli-input-json JSONSchemaDynamoDBTABLE.json   Open the Apache Nifi on the EKS UI using the endpoint, create a process group, and name it NifiStreamingExample.      Double-click on the Nifi-on-EKS-process-group and enter the process to create the data flow. Drag the processor icon from the top left, type Kinesis into the search window, and select the ConsumeKinesisStream processor. To create a Kinesis Consumer, click ADD.     Double click on the Kinesis processor, select the properties tab, and fill in the information for the configuration below. a. Amazon Kinesis Stream Name b. Application Name c. Region d. AWS Credentials Provider Service - Select AWSCredentialsProviderControllerService and create one.    Create AWS credential setup​  Setup the AWS credentials to access the AWS resource in the account using the AWS Credentials Provider Service. In this example, we are using the access key and secret key. Note : Other options are IAM role-based, assumed role options to authenticate an AWS resources.      Drag the processor icon from the top left, type &quot;dynamoDB&quot; into the search window, and select the &quot;PutDynamoDBRecord processor. Click on ADD to create an Amazon DynamoDB writer. Configure the processor using the fields below.  a. Record Reader - Change it to JSONTreeReader b. AWS Credentials Provider Service - select the previously created configuration c. Region b. Table Name d. Partition Key Field - select the partition field    Hover over the Kinesis consumer and drag it to the DynamoDB writer. The connection will be made, and the success queue will be created.    For the Kinesis Consumer and DynamoDB, create an error route to a funnel. This is to route the unprocessed, failed, and successful records for further processing. Note: Under the Relationship tab, you can see all the options for each processor. For the DynamoDB writer, success should always point to a funnel.    Check that none of the processors have any Hazard symbols. Right-click on the grid and click &quot;run the data flow.&quot; You can start seeing the data flowing in.  ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/nifi#cleanup","content":" To clean up your environment, destroy the Terraform modules in reverse order.  Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC  terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; --auto-approve terraform destroy -target=&quot;module.eks&quot; --auto-approve terraform destroy -target=&quot;module.vpc&quot; --auto-approve   Finally, destroy any additional resources that are not in the above modules  terraform destroy --auto-approve  ","version":"Next","tagName":"h2"},{"title":"Spark Streaming from Kafka in EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming","content":"","keywords":"","version":"Next"},{"title":"Deploy the EKS Cluster with all the add-ons and infrastructure needed to test this example​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#deploy-the-eks-cluster-with-all-the-add-ons-and-infrastructure-needed-to-test-this-example","content":" ","version":"Next","tagName":"h2"},{"title":"Clone the repository​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#clone-the-repository","content":" git clone https://github.com/awslabs/data-on-eks.git   ","version":"Next","tagName":"h3"},{"title":"Initialize Terraform​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#initialize-terraform","content":" Navigate into the example directory and run the initialization script install.sh.  cd data-on-eks/streaming/spark-streaming/terraform/ ./install.sh   ","version":"Next","tagName":"h3"},{"title":"Export Terraform Outputs​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#export-terraform-outputs","content":" After the Terraform script finishes, export the necessary variables to use them in the sed commands.  export CLUSTER_NAME=$(terraform output -raw cluster_name) export PRODUCER_ROLE_ARN=$(terraform output -raw producer_iam_role_arn) export CONSUMER_ROLE_ARN=$(terraform output -raw consumer_iam_role_arn) export MSK_BROKERS=$(terraform output -raw bootstrap_brokers) export REGION=$(terraform output -raw s3_bucket_region_spark_history_server) export ICEBERG_BUCKET=$(terraform output -raw s3_bucket_id_iceberg_bucket)   ","version":"Next","tagName":"h3"},{"title":"Update kubeconfig​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#update-kubeconfig","content":" Update the kubeconfig to verify the deployment.  aws eks --region $REGION update-kubeconfig --name $CLUSTER_NAME kubectl get nodes   ","version":"Next","tagName":"h3"},{"title":"Configuring Producer​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#configuring-producer","content":" In order to deploy the producer, update the examples/producer/00_deployment.yaml manifest with the variables exported from Terraform.  # Apply `sed` commands to replace placeholders in the producer manifest sed -i.bak -e &quot;s|__MY_PRODUCER_ROLE_ARN__|$PRODUCER_ROLE_ARN|g&quot; \\ -e &quot;s|__MY_AWS_REGION__|$REGION|g&quot; \\ -e &quot;s|__MY_KAFKA_BROKERS__|$MSK_BROKERS|g&quot; \\ ../examples/producer/00_deployment.yaml # Apply sed to delete topic manifest, this can be used to delete kafka topic and start the stack once again sed -i.bak -e &quot;s|__MY_KAFKA_BROKERS__|$MSK_BROKERS|g&quot; \\ ../examples/producer/01_delete_topic.yaml   ","version":"Next","tagName":"h3"},{"title":"Configuring Consumer​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#configuring-consumer","content":" In order to deploy the Spark consumer, update the examples/consumer/manifests/01_spark_application.yaml manifests with the variables exported from Terraform.  # Apply `sed` commands to replace placeholders in the consumer Spark application manifest sed -i.bak -e &quot;s|__MY_BUCKET_NAME__|$ICEBERG_BUCKET|g&quot; \\ -e &quot;s|__MY_KAFKA_BROKERS_ADRESS__|$MSK_BROKERS|g&quot; \\ ../examples/consumer/manifests/01_spark_application.yaml   ","version":"Next","tagName":"h3"},{"title":"Deploy Producer and Consumer​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#deploy-producer-and-consumer","content":" After configuring the producer and consumer manifests, deploy them using kubectl.  # Deploy Producer kubectl apply -f ../examples/producer/00_deployment.yaml # Deploy Consumer kubectl apply -f ../examples/consumer/manifests/   Checking Producer to MSK​  First, let's see the producer logs to verify data is being created and flowing into MSK:  kubectl logs $(kubectl get pods -l app=producer -oname) -f   Checking Spark Streaming application with Spark Operator​  For the consumer, we first need to get the SparkApplication that generates the spark-submit command to Spark Operator to create driver and executor pods based on the YAML configuration:  kubectl get SparkApplication -n spark-operator   You should see the STATUS equals RUNNING, now let's verify the driver and executors pods:  kubectl get pods -n spark-operator   You should see an output like below:  NAME READY STATUS RESTARTS AGE kafkatoiceberg-1e9a438f4eeedfbb-exec-1 1/1 Running 0 7m15s kafkatoiceberg-1e9a438f4eeedfbb-exec-2 1/1 Running 0 7m14s kafkatoiceberg-1e9a438f4eeedfbb-exec-3 1/1 Running 0 7m14s spark-consumer-driver 1/1 Running 0 9m spark-operator-9448b5c6d-d2ksp 1/1 Running 0 117m spark-operator-webhook-init-psm4x 0/1 Completed 0 117m   We have 1 driver and 3 executors pods. Now, let's check the driver logs:  kubectl logs pod/spark-consumer-driver -n spark-operator   You should see only INFO logs indicating that the job is running.  ","version":"Next","tagName":"h3"},{"title":"Verify Data Flow​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#verify-data-flow","content":" After deploying both the producer and consumer, verify the data flow by checking the consumer application's output in the S3 bucket. You can run the s3_automation script to get a live view of the data size in your S3 bucket.  Follow these steps:  Navigate to the s3_automation directory: cd ../examples/s3_automation/ Run the s3_automation script: python app.py This script will continuously monitor and display the total size of your S3 bucket, giving you a real-time view of data being ingested. You can choose to view the bucket size or delete specific directories as needed.  Using the s3_automation Script​  The s3_automation script offers two primary functions:  Check Bucket Size: Continuously monitor and display the total size of your S3 bucket.Delete Directory: Delete specific directories within your S3 bucket.  Here's how to use these functions:  Check Bucket Size: When prompted, enter size to get the current size of your bucket in megabytes (MB). Delete Directory: When prompted, enter delete and then provide the directory prefix you wish to delete (e.g., myfolder/).  ","version":"Next","tagName":"h3"},{"title":"Tuning the Producer and Consumer for Better Performance​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#tuning-the-producer-and-consumer-for-better-performance","content":" After deploying the producer and consumer, you can further optimize the data ingestion and processing by adjusting the number of replicas for the producer and the executor configuration for the Spark application. Here are some suggestions to get you started:  ","version":"Next","tagName":"h2"},{"title":"Adjusting the Number of Producer Replicas​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#adjusting-the-number-of-producer-replicas","content":" You can increase the number of replicas of the producer deployment to handle a higher rate of message production. By default, the producer deployment is configured with a single replica. Increasing this number allows more instances of the producer to run concurrently, increasing the overall throughput.  To change the number of replicas, update the replicas field in examples/producer/00_deployment.yaml:  spec: replicas: 200 # Increase this number to scale up the producer   You can also adjust the environment variables to control the rate and volume of messages produced:  env: - name: RATE_PER_SECOND value: &quot;200000&quot; # Increase this value to produce more messages per second - name: NUM_OF_MESSAGES value: &quot;20000000&quot; # Increase this value to produce more messages in total   Apply the updated deployment:  kubectl apply -f ../examples/producer/00_deployment.yaml   ","version":"Next","tagName":"h3"},{"title":"Tuning Spark Executors for Better Ingestion Performance​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#tuning-spark-executors-for-better-ingestion-performance","content":" To handle the increased data volume efficiently, you can add more executors to the Spark application or increase the resources allocated to each executor. This will allow the consumer to process data faster and reduce ingestion time.  To adjust the Spark executor configuration, update examples/consumer/manifests/01_spark_application.yaml:  spec: dynamicAllocation: enabled: true initialExecutors: 5 minExecutors: 5 maxExecutors: 50 # Increase this number to allow more executors executor: cores: 4 # Increase CPU allocation memory: &quot;8g&quot; # Increase memory allocation   Apply the updated Spark application:  kubectl apply -f ../examples/consumer/manifests/01_spark_application.yaml   ","version":"Next","tagName":"h3"},{"title":"Verify and Monitor​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#verify-and-monitor","content":" After making these changes, monitor the logs and metrics to ensure the system is performing as expected. You can check the producer logs to verify data production and the consumer logs to verify data ingestion and processing.  To check producer logs:  kubectl logs $(kubectl get pods -l app=producer -oname) -f   To check consumer logs:  kubectl logs pod/spark-consumer-driver -n spark-operator   Can use verify dataflow script again  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#summary","content":" By adjusting the number of producer replicas and tuning the Spark executor settings, you can optimize the performance of your data pipeline. This allows you to handle higher ingestion rates and process data more efficiently, ensuring that your Spark Streaming application can keep up with the increased data volume from Kafka.  Feel free to experiment with these settings to find the optimal configuration for your workload. Happy streaming!  ","version":"Next","tagName":"h3"},{"title":"Cleaning Up Producer and Consumer Resources​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#cleaning-up-producer-and-consumer-resources","content":" To clean up only the producer and consumer resources, use the following commands:  # Clean up Producer resources kubectl delete -f ../examples/producer/00_deployment.yaml # Clean up Consumer resources kubectl delete -f ../examples/consumer/manifests/   ","version":"Next","tagName":"h3"},{"title":"Restoring .yaml Files from .bak​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#restoring-yaml-files-from-bak","content":" If you need to reset the .yaml files to their original state with placeholders, move the .bak files back to .yaml.  # Restore Producer manifest mv ../examples/producer/00_deployment.yaml.bak ../examples/producer/00_deployment.yaml # Restore Consumer Spark application manifest mv ../examples/consumer/manifests/01_spark_application.yaml.bak ../examples/consumer/manifests/01_spark_application.yaml   ","version":"Next","tagName":"h3"},{"title":"Destroy the EKS Cluster and Resources​","type":1,"pageTitle":"Spark Streaming from Kafka in EKS","url":"/data-on-eks/docs/blueprints/streaming-platforms/spark-streaming#destroy-the-eks-cluster-and-resources","content":" To clean up the entire EKS cluster and associated resources:  cd data-on-eks/streaming/spark-streaming/terraform/ terraform destroy  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/data-on-eks/docs/blueprints/troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Error: local-exec provisioner error​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#error-local-exec-provisioner-error","content":" If you encounter the following error during the execution of the local-exec provisioner:  Error: local-exec provisioner error \\ with module.eks-blueprints.module.emr_on_eks[&quot;data_team_b&quot;].null_resource.update_trust_policy,\\ on .terraform/modules/eks-blueprints/modules/emr-on-eks/main.tf line 105, in resource &quot;null_resource&quot; \\ &quot;update_trust_policy&quot;:│ 105: provisioner &quot;local-exec&quot; {│ │ Error running command 'set -e│ │ aws emr-containers update-role-trust-policy \\ │ --cluster-name emr-on-eks \\│ --namespace emr-data-team-b \\│ --role-name emr-on-eks-emr-eks-data-team-b   ","version":"Next","tagName":"h2"},{"title":"Issue Description:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#issue-description","content":" The error message indicates that the emr-containers command is not present in the AWS CLI version being used. This issue has been addressed and fixed in AWS CLI version 2.0.54.  ","version":"Next","tagName":"h3"},{"title":"Solution​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#solution","content":" To resolve the issue, update your AWS CLI version to 2.0.54 or a later version by executing the following command:  pip install --upgrade awscliv2   By updating the AWS CLI version, you will ensure that the necessary emr-containers command is available and can be executed successfully during the provisioning process.  If you continue to experience any issues or require further assistance, please consult the AWS CLI GitHub issue for more details or contact our support team for additional guidance.  ","version":"Next","tagName":"h3"},{"title":"Timeouts during Terraform Destroy​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#timeouts-during-terraform-destroy","content":" ","version":"Next","tagName":"h2"},{"title":"Issue Description:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#issue-description-1","content":" Customers may experience timeouts during the deletion of their environments, specifically when VPCs are being deleted. This is a known issue related to the vpc-cni component.  ","version":"Next","tagName":"h3"},{"title":"Symptoms:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#symptoms","content":" ENIs (Elastic Network Interfaces) remain attached to subnets even after the environment is destroyed. The EKS managed security group associated with the ENI cannot be deleted by EKS.  ","version":"Next","tagName":"h3"},{"title":"Solution:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#solution-1","content":" To overcome this issue, follow the recommended solution below:  Utilize the provided cleanup.sh scripts to ensure a proper cleanup of resources. Run the `cleanup.sh`` script, which is included in the blueprint. This script will handle the removal of any lingering ENIs and associated security groups.  ","version":"Next","tagName":"h3"},{"title":"Error: could not download chart​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#error-could-not-download-chart","content":" If you encounter the following error while attempting to download a chart:  │ Error: could not download chart: failed to download &quot;oci://public.ecr.aws/karpenter/karpenter&quot; at version &quot;v0.18.1&quot; │ │ with module.eks_blueprints_kubernetes_addons.module.karpenter[0].module.helm_addon.helm_release.addon[0], │ on .terraform/modules/eks_blueprints_kubernetes_addons/modules/kubernetes-addons/helm-addon/main.tf line 1, in resource &quot;helm_release&quot; &quot;addon&quot;: │ 1: resource &quot;helm_release&quot; &quot;addon&quot; { │   Follow the steps below to resolve the issue:  ","version":"Next","tagName":"h2"},{"title":"Issue Description:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#issue-description-2","content":" The error message indicates that there was a failure in downloading the specified chart. This issue can occur due to a bug in Terraform during the installation of Karpenter.  ","version":"Next","tagName":"h3"},{"title":"Solution:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#solution-2","content":" To resolve the issue, you can try the following steps:  Authenticate with ECR: Run the following command to authenticate with the ECR (Elastic Container Registry) where the chart is located:  aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws   Re-run terraform apply: Execute the terraform apply command again with the --auto-approve flag to reapply the Terraform configuration:  terraform apply --auto-approve   By authenticating with ECR and re-running the terraform apply command, you will ensure that the necessary chart can be downloaded successfully during the installation process.  ","version":"Next","tagName":"h3"},{"title":"Terraform apply/destroy error to authenticate with EKS Cluster​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#terraform-applydestroy-error-to-authenticate-with-eks-cluster","content":" ERROR: ╷ │ Error: Get &quot;http://localhost/api/v1/namespaces/kube-system/configmaps/aws-auth&quot;: dial tcp [::1]:80: connect: connection refused │ │ with module.eks.kubernetes_config_map_v1_data.aws_auth[0], │ on .terraform/modules/eks/main.tf line 550, in resource &quot;kubernetes_config_map_v1_data&quot; &quot;aws_auth&quot;: │ 550: resource &quot;kubernetes_config_map_v1_data&quot; &quot;aws_auth&quot; { │ ╵   Solution:In this situation Terraform is unable to refresh the data resources and authenticate with EKS Cluster. See the discussion here  Try this approach first by using exec plugin.  provider &quot;kubernetes&quot; { host = module.eks_blueprints.eks_cluster_endpoint cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data) exec { api_version = &quot;client.authentication.k8s.io/v1beta1&quot; command = &quot;aws&quot; args = [&quot;eks&quot;, &quot;get-token&quot;, &quot;--cluster-name&quot;, module.eks_blueprints.eks_cluster_id] } }   If the issue still persists even after the above change then you can use alternative approach of using local kube config file. NOTE: This approach might not be ideal for production. It helps you to apply/destroy clusters with your local kube config.  Create a local kubeconfig for your cluster  aws eks update-kubeconfig --name &lt;EKS_CLUSTER_NAME&gt; --region &lt;CLUSTER_REGION&gt;   Update the providers.tf file with the below config by just using the config_path.  provider &quot;kubernetes&quot; { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; } provider &quot;helm&quot; { kubernetes { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; } } provider &quot;kubectl&quot; { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; }   ","version":"Next","tagName":"h2"},{"title":"EMR Containers Virtual Cluster (dhwtlq9yx34duzq5q3akjac00) delete: unexpected state 'ARRESTED'​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#emr-containers-virtual-cluster-dhwtlq9yx34duzq5q3akjac00-delete-unexpected-state-arrested","content":" If you encounter an error message stating &quot;waiting for EMR Containers Virtual Cluster (xwbc22787q6g1wscfawttzzgb) delete: unexpected state 'ARRESTED', wanted target ''. last error: %!s(nil)&quot;, you can follow the steps below to resolve the issue:  Note: Replace &lt;REGION&gt; with the appropriate AWS region where the virtual cluster is located.  Open a terminal or command prompt.Run the following command to list the virtual clusters in the &quot;ARRESTED&quot; state:  aws emr-containers list-virtual-clusters --region &lt;REGION&gt; --states ARRESTED \\ --query 'virtualClusters[0].id' --output text   This command retrieves the ID of the virtual cluster in the &quot;ARRESTED&quot; state.  Run the following command to delete the virtual cluster:  aws emr-containers list-virtual-clusters --region &lt;REGION&gt; --states ARRESTED \\ --query 'virtualClusters[0].id' --output text | xargs -I{} aws emr-containers delete-virtual-cluster \\ --region &lt;REGION&gt; --id {}   Replace &lt;VIRTUAL_CLUSTER_ID&gt; with the ID of the virtual cluster obtained from the previous step.  By executing these commands, you will be able to delete the virtual cluster that is in the &quot;ARRESTED&quot; state. This should resolve the unexpected state issue and allow you to proceed with further operations.  ","version":"Next","tagName":"h2"},{"title":"Terminating namespace issue​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#terminating-namespace-issue","content":" If you encounter the issue where a namespace is stuck in the &quot;Terminating&quot; state and cannot be deleted, you can use the following command to remove the finalizers on the namespace:  Note: Replace &lt;namespace&gt; with the name of the namespace you want to delete.  NAMESPACE=&lt;namespace&gt; kubectl get namespace $NAMESPACE -o json | sed 's/&quot;kubernetes&quot;//' | kubectl replace --raw &quot;/api/v1/namespaces/$NAMESPACE/finalize&quot; -f -   This command retrieves the namespace details in JSON format, removes the &quot;kubernetes&quot; finalizer, and performs a replace operation to remove the finalizer from the namespace. This should allow the namespace to complete the termination process and be successfully deleted.  Please ensure that you have the necessary permissions to perform this operation. If you continue to experience issues or require further assistance, please reach out to our support team for additional guidance and troubleshooting steps.  ","version":"Next","tagName":"h2"},{"title":"KMS Alias AlreadyExistsException​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#kms-alias-alreadyexistsexception","content":" During your Terraform installation or redeployment, you might encounter an error saying: AlreadyExistsException: An alias with the name ... already exists. This happens when the KMS alias you're trying to create already exists in your AWS account.  │ Error: creating KMS Alias (alias/eks/trainium-inferentia): AlreadyExistsException: An alias with the name arn:aws:kms:us-west-2:23423434:alias/eks/trainium-inferentia already exists │ │ with module.eks.module.kms.aws_kms_alias.this[&quot;cluster&quot;], │ on .terraform/modules/eks.kms/main.tf line 452, in resource &quot;aws_kms_alias&quot; &quot;this&quot;: │ 452: resource &quot;aws_kms_alias&quot; &quot;this&quot; { │   Solution:  To resolve this, delete the existing KMS alias using the aws kms delete-alias command. Remember to update the alias name and region in the command before running it.  aws kms delete-alias --alias-name &lt;KMS_ALIAS_NAME&gt; --region &lt;ENTER_REGION&gt;   ","version":"Next","tagName":"h2"},{"title":"Error: creating CloudWatch Logs Log Group​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#error-creating-cloudwatch-logs-log-group","content":" Terraform cannot create a CloudWatch Logs log group because it already exists in your AWS account.  ╷ │ Error: creating CloudWatch Logs Log Group (/aws/eks/trainium-inferentia/cluster): operation error CloudWatch Logs: CreateLogGroup, https response error StatusCode: 400, RequestID: 5c34c47a-72c6-44b2-a345-925824f24d38, ResourceAlreadyExistsException: The specified log group already exists │ │ with module.eks.aws_cloudwatch_log_group.this[0], │ on .terraform/modules/eks/main.tf line 106, in resource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot;: │ 106: resource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot; {   Solution:  Delete the existing log group by updating log group name and the region.  aws logs delete-log-group --log-group-name &lt;LOG_GROUP_NAME&gt; --region &lt;ENTER_REGION&gt;   ","version":"Next","tagName":"h2"},{"title":"Karpenter Error - Missing Service Linked Role​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#karpenter-error---missing-service-linked-role","content":" Karpenter throws below error while trying to create new instances.  &quot;error&quot;:&quot;launching nodeclaim, creating instance, with fleet error(s), AuthFailure.ServiceLinkedRoleCreationNotPermitted: The provided credentials do not have permission to create the service-linked role for EC2 Spot Instances.&quot;}   Solution:  You will need to create the service linked role in the AWS account you're using to avoid ServiceLinkedRoleCreationNotPermitted error.  aws iam create-service-linked-role --aws-service-name spot.amazonaws.com   ","version":"Next","tagName":"h2"},{"title":"Error: AmazonEKS_CNI_IPv6_Policy does not exist​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#error-amazoneks_cni_ipv6_policy-does-not-exist","content":" If you encounter the error below when deploying a solution that supports IPv6:  │ Error: attaching IAM Policy (arn:aws:iam::1234567890:policy/AmazonEKS_CNI_IPv6_Policy) to IAM Role (core-node-group-eks-node-group-20241111182906854800000003): operation error IAM: AttachRolePolicy, https response error StatusCode: 404, RequestID: 9c99395a-ce3d-4a05-b119-538470a3a9f7, NoSuchEntity: Policy arn:aws:iam::1234567890:policy/AmazonEKS_CNI_IPv6_Policy does not exist or is not attachable.   ","version":"Next","tagName":"h2"},{"title":"Issue Description:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#issue-description-3","content":" The Amazon VPC CNI plugin requires IAM permission to assign IPv6 addresses so you must create an IAM policy and associate it with the role that the CNI will use. However, each IAM policy name must be unique in the same AWS account. This causes a conflict if the policy is created as part of the terraform stack and it is deployed multiple times.  To resolve this error you will need to create the Policy with the commands below. You should only need to do this once per AWS account.  ","version":"Next","tagName":"h3"},{"title":"Solution:​","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/blueprints/troubleshooting#solution-3","content":" Copy the following text and save it to a file named vpc-cni-ipv6-policy.json.  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:AssignIpv6Addresses&quot;, &quot;ec2:DescribeInstances&quot;, &quot;ec2:DescribeTags&quot;, &quot;ec2:DescribeNetworkInterfaces&quot;, &quot;ec2:DescribeInstanceTypes&quot; ], &quot;Resource&quot;: &quot;&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:CreateTags&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:ec2::*:network-interface/*&quot; ] } ] }   Create the IAM policy.  aws iam create-policy --policy-name AmazonEKS_CNI_IPv6_Policy --policy-document file://vpc-cni-ipv6-policy.json   Re-run the install.sh script for the blueprint ","version":"Next","tagName":"h3"},{"title":"intro","type":0,"sectionRef":"#","url":"/data-on-eks/docs/intro","content":"intro","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/data-on-eks/docs/introduction/intro","content":"Introduction Data on Amazon EKS(DoEKS) - A tool for building aws managed and self-managed scalable data platforms on Amazon EKS. With DoEKS, You have access to: Robust Deployment Infrastructure as Code (IaC) Templates using Terraform and AWS CDK, among otherBest Practices for Deploying Data Solutions on Amazon EKSDetailed Performance Benchmark ReportsHands-on Samples of Apache Spark/ML Jobs and various other frameworksIn-depth Reference Architectures and Data Blogs to keep you ahead of the curve Architecture The diagram displays the open source data tools, k8s operators and frameworks that runs on Kubernetes covered in DoEKS. AWS Data Analytics managed services integration with Data on EKS OSS tools. Main Features 🚀 EMR on EKS 🚀 Open Source Spark on EKS 🚀 Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) 🚀 Job Schedulers (e.g., Apache Airflow, Argo Workflows) 🚀 AI/ML on Kubernetes (e.g., KubeFlow, MLFlow, Tensorflow, PyTorch, etc.) 🚀 Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) 🚀 Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) Getting Started Checkout the documentation for each section to deploy infrastructure and run sample Spark/ML jobs.","keywords":"","version":"Next"},{"title":"Introduction to Resources","type":0,"sectionRef":"#","url":"/data-on-eks/docs/resources/intro","content":"Introduction to Resources Welcome to the Resources section. This area is dedicated to providing you with a wealth of information, insights, and learning materials to enhance your understanding and skills in Data and AI workloads running on EKS. This area is a hub of valuable information, offering a range of materials from insightful mini blogs and AWS workshop links to engaging video content and reInvent talks.","keywords":"","version":"Next"},{"title":"Bin packing for Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#introduction","content":" In this post, we will show you how to enable a custom scheduler with Amazon EKS when running DoEKS especially for Spark on EKS, including OSS Spark and EMR on EKS. The custom scheduler is a custom Kubernetes scheduler with MostAllocated strategy running in data plane.  ","version":"Next","tagName":"h2"},{"title":"Why bin packing​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#why-bin-packing","content":" By default, the scheduling-plugin NodeResourcesFit use the LeastAllocated for score strategies. For the long running workloads, that is good because of high availability. But for batch jobs, like Spark workloads, this would lead high cost. By changing the from LeastAllocated to MostAllocated, it avoids spreading pods across all running nodes, leading to higher resource utilization and better cost efficiency.  Batch jobs like Spark are running on demand with limited or predicted time. With MostAllocated strategy, Spark executors are always bin packing into one node util the node can not host any pods. You can see the following picture shows the  MostAllocated in EMR on EKS.    LeastAllocated in EMR on EKS    ","version":"Next","tagName":"h3"},{"title":"Pros​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#pros","content":" Improve the node utilizationsSave the cost  ","version":"Next","tagName":"h3"},{"title":"Considerations​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#considerations","content":" Although we have provided upgrade guidance, support matrix and high availability design, but maintaining a custom scheduler in data plane needs effort including:  Upgrade operations. Plan the upgrading along with your batch jobs, make sure the scheduler are running as desired.Monitoring the scheduler. Monitoring and alerting are required for production purpose.Adjust the scheduler pod resource and other customizations regarding your requirements.  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#deploying-the-solution","content":" ","version":"Next","tagName":"h2"},{"title":"Clone the repo​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#clone-the-repo","content":" git clone https://github.com/aws-samples/custom-scheduler-eks cd custom-scheduler-eks   ","version":"Next","tagName":"h3"},{"title":"Manifests​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#manifests","content":" Amazon EKS 1.24  kubectl apply -f deploy/manifests/custom-scheduler/amazon-eks-1.24-custom-scheduler.yaml   Amazon EKS 1.29  kubectl apply -f deploy/manifests/custom-scheduler/amazon-eks-1.29-custom-scheduler.yaml   Other Amazon EKS versions  replace the related image URL(https://gallery.ecr.aws/eks-distro/kubernetes/kube-scheduler)  Please refer to custom-scheduler for more info.  ","version":"Next","tagName":"h3"},{"title":"Set up pod template to use the custom scheduler for Spark​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#set-up-pod-template-to-use-the-custom-scheduler-for-spark","content":" We should add custom scheduler name to the pod template as follows  kind: Pod spec: schedulerName: custom-k8s-scheduler volumes: - name: spark-local-dir-1 hostPath: path: /local1 initContainers: - name: volume-permission image: public.ecr.aws/docker/library/busybox # grant volume access to hadoop user command: ['sh', '-c', 'if [ ! -d /data1 ]; then mkdir /data1;fi; chown -R 999:1000 /data1'] volumeMounts: - name: spark-local-dir-1 mountPath: /data1 containers: - name: spark-kubernetes-executor volumeMounts: - name: spark-local-dir-1 mountPath: /data1   ","version":"Next","tagName":"h3"},{"title":"Verification and Monitor via eks-node-viewer​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#verification-and-monitor-via-eks-node-viewer","content":" Before apply the change in the pod template    After the change: Higher CPU usage at pod schedule time  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/data-on-eks/docs/resources/binpacking-custom-scheduler-eks#conclusion","content":" By using the custom scheduler, we can fully improve the node utilizations for the Spark workloads which will save the cost by triggering node scale in.  For the users that running Spark on EKS, we recommend you adopt this custom scheduler before Amazon EKS officially support the kube-scheduler customization. ","version":"Next","tagName":"h2"},{"title":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/resources/mountpoint-s3","content":"","keywords":"","version":"Next"},{"title":"What is Mountpoint-S3?​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#what-is-mountpoint-s3","content":" Mountpoint-S3 is an open-source file client developed by AWS that translates file operations into S3 API calls, enabling your applications to interact with Amazon S3 buckets as if they were local disks. Mountpoint for Amazon S3 is optimized for applications that need high read throughput to large objects, potentially from many clients at once, and to write new objects sequentially from a single client at a time. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads and AI/ML training.  A key feature of Mountpoint-S3 is its compatibility with both Amazon S3 Standrad and Amazon S3 Express One Zone. S3 Express One Zone is a high-performance storage class designed for single-Availability Zone deployment. It offers consistent single-digit millisecond data access, making it ideal for frequently accessed data and latency-sensitive applications. S3 Express One Zone is known for delivering data access speeds up to 10 times faster and at up to 50% lower request costs compared to S3 Standard. This storage class enables users to co-locate storage and compute resources in the same Availability Zone, optimizing performance and potentially reducing compute costs.  The integration with S3 Express One Zone enhances Mountpoint-S3's capabilities, particularly for machine learning and analytics workloads, as it can be used with services like Amazon EKS, Amazon SageMaker Model Training, Amazon Athena, Amazon EMR, and AWS Glue Data Catalog. The automatic scaling of storage based on consumption in S3 Express One Zone simplifies management for low-latency workloads, making Mountpoint-S3 a highly effective tool for a wide range of data-intensive tasks and AI/ML training environments.  warning Mountpoint-S3 does not support file renaming operations, which may limit its applicability in scenarios where such functionality is essential.  ","version":"Next","tagName":"h2"},{"title":"Deploying Mountpoint-S3 on Amazon EKS:​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#deploying-mountpoint-s3-on-amazon-eks","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Set Up IAM Role for S3 CSI Driver​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#step-1-set-up-iam-role-for-s3-csi-driver","content":" Create an IAM role using Terraform with the necessary permissions for the S3 CSI driver. This step is critical for ensuring secure and efficient communication between EKS and S3.   #--------------------------------------------------------------- # IRSA for Mountpoint for Amazon S3 CSI Driver #--------------------------------------------------------------- module &quot;s3_csi_driver_irsa&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks&quot; version = &quot;~&gt; 5.34&quot; role_name_prefix = format(&quot;%s-%s-&quot;, local.name, &quot;s3-csi-driver&quot;) role_policy_arns = { # WARNING: Demo purpose only. Bring your own IAM policy with least privileges s3_csi_driver = &quot;arn:aws:iam::aws:policy/AmazonS3FullAccess&quot; } oidc_providers = { main = { provider_arn = module.eks.oidc_provider_arn namespace_service_accounts = [&quot;kube-system:s3-csi-driver-sa&quot;] } } tags = local.tags }   ","version":"Next","tagName":"h3"},{"title":"Step 2: Configure EKS Blueprints Addons​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#step-2-configure-eks-blueprints-addons","content":" Configure EKS Blueprints Addons Terraform module to utilize this role for the Amazon EKS Add-on of S3 CSI driver.  #--------------------------------------------------------------- # EKS Blueprints Addons #--------------------------------------------------------------- module &quot;eks_blueprints_addons&quot; { source = &quot;aws-ia/eks-blueprints-addons/aws&quot; version = &quot;~&gt; 1.2&quot; cluster_name = module.eks.cluster_name cluster_endpoint = module.eks.cluster_endpoint cluster_version = module.eks.cluster_version oidc_provider_arn = module.eks.oidc_provider_arn #--------------------------------------- # Amazon EKS Managed Add-ons #--------------------------------------- eks_addons = { aws-mountpoint-s3-csi-driver = { service_account_role_arn = module.s3_csi_driver_irsa.iam_role_arn } } }   ","version":"Next","tagName":"h3"},{"title":"Step 3: Define a PersistentVolume​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#step-3-define-a-persistentvolume","content":" Specify the S3 bucket, region details and access modes in a PersistentVolume (PV) configuration. This step is crucial for defining how EKS interacts with the S3 bucket.  --- apiVersion: v1 kind: PersistentVolume metadata: name: s3-pv spec: capacity: storage: 1200Gi # ignored, required accessModes: - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany mountOptions: - uid=1000 - gid=2000 - allow-other - allow-delete - region &lt;ENTER_REGION&gt; csi: driver: s3.csi.aws.com # required volumeHandle: s3-csi-driver-volume volumeAttributes: bucketName: &lt;ENTER_S3_BUCKET_NAME&gt;   ","version":"Next","tagName":"h3"},{"title":"Step 4: Create a PersistentVolumeClaim​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#step-4-create-a-persistentvolumeclaim","content":" Establish a PersistentVolumeClaim (PVC) to utilize the defined PV, specifying access modes and static provisioning requirements.  --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: s3-claim namespace: spark-team-a spec: accessModes: - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany storageClassName: &quot;&quot; # required for static provisioning resources: requests: storage: 1200Gi # ignored, required volumeName: s3-pv   ","version":"Next","tagName":"h3"},{"title":"Step 5: Using PVC with Pod Definition​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#step-5-using-pvc-with-pod-definition","content":" After setting up the PersistentVolumeClaim (PVC), the next step is to utilize it within a Pod definition. This allows your applications running in Kubernetes to access the data stored in the S3 bucket. Below are examples demonstrating how to reference the PVC in a Pod definition for different scenarios.  Example 1: Basic Pod Using PVC for Storage​  This example shows a basic Pod definition that mounts the s3-claim PVC to a directory within the container.  In this example, the PVC s3-claim is mounted to the /data directory of the nginx container. This setup allows the application running within the container to read and write data to the S3 bucket as if it were a local directory.   apiVersion: v1 kind: Pod metadata: name: example-pod namespace: spark-team-a spec: containers: - name: app-container image: nginx # Example image volumeMounts: - name: s3-storage mountPath: &quot;/data&quot; # The path where the S3 bucket will be mounted volumes: - name: s3-storage persistentVolumeClaim: claimName: s3-claim   Example 2: AI/ML Training Job Using PVC​  In AI/ML training scenarios, data accessibility and throughput are critical. This example demonstrates a Pod configuration for a machine learning training job that accesses datasets stored in S3.   apiVersion: v1 kind: Pod metadata: name: ml-training-pod namespace: spark-team-a spec: containers: - name: training-container image: ml-training-image # Replace with your ML training image volumeMounts: - name: dataset-storage mountPath: &quot;/datasets&quot; # Mount path for training data volumes: - name: dataset-storage persistentVolumeClaim: claimName: s3-claim   warning Mountpoint S3, when used with Amazon S3 or S3 express, may not be suitable as a shuffle storage for Spark workloads. This limitation arises due to the nature of shuffle operations in Spark, which often involve multiple clients reading and writing to the same location simultaneously. Additionally, Mountpoint S3 does not support file renaming, a critical feature required for efficient shuffle operations in Spark. This lack of renaming capability can lead to operational challenges and potential performance bottlenecks in data processing tasks.  ","version":"Next","tagName":"h3"},{"title":"Next Steps:​","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/data-on-eks/docs/resources/mountpoint-s3#next-steps","content":" Explore the provided Terraform code snippet for detailed deployment instructions.Refer to the official Mountpoint-S3 documentation for further configuration options and limitations.Utilize Mountpoint-S3 to unlock high-performance, scalable S3 access within your EKS applications.  By understanding Mountpoint-S3's capabilities and limitations, you can make informed decisions to optimize your data-driven workloads on Amazon EKS. ","version":"Next","tagName":"h2"},{"title":"Mountpoint-S3 for Spark Workloads","type":0,"sectionRef":"#","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark","content":"","keywords":"","version":"Next"},{"title":"What is Mountpoint-S3?​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#what-is-mountpoint-s3","content":" Mountpoint-S3 is an open-source file client developed by AWS that translates file operations into S3 API calls, enabling your applications to interact with Amazon S3 buckets as if they were local disks. Mountpoint for Amazon S3 is optimized for applications that need high read throughput to large objects, potentially from many clients at once, and to write new objects sequentially from a single client at a time. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads or AI/ML training.  Mountpoint for Amazon S3 is optimized for high-throughput performance, largely due to its foundation on the AWS Common Runtime (CRT) library. The CRT library is a collection of libraries and modules designed to deliver high performance and low resource usage, specifically tailored for AWS services. Key features of the CRT library that enable high-throughput performance include:  Efficient I/O Management: The CRT library is optimized for non-blocking I/O operations, reducing latency and maximizing the utilization of network bandwidth.Lightweight and Modular Design: The library is designed to be lightweight, with minimal overhead, allowing it to perform efficiently even under high load. Its modular architecture ensures that only the necessary components are loaded, further enhancing performance.Advanced Memory Management: CRT employs advanced memory management techniques to minimize memory usage and reduce garbage collection overhead, leading to faster data processing and reduced latency.Optimized Network Protocols: The CRT library includes optimized implementations of network protocols, such as HTTP/2, that are specifically tuned for AWS environments. These optimizations ensure rapid data transfer between S3 and your compute instances, which is critical for large-scale Spark workloads.  ","version":"Next","tagName":"h2"},{"title":"Using Mountpoint-S3 with EKS​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#using-mountpoint-s3-with-eks","content":" For Spark workloads, we'll specifically focus on loading external JARs located in S3 for Spark Applications. We’ll examine two primary deployment strategies for Mountpoint-S3;  Leveraging the EKS Managed Addon CSI driver with Persistent Volumes (PV) and Persistent Volume Claims (PVC)Deploying Mountpoint-S3 at the Node level using either USERDATA scripts or DaemonSets.  The first approach is considered mounting at a Pod level because the PV created is available to individual pods. The second Approach is considered mounting at a Node level because the S3 is mounted on the host Node itself. Each approach is discussed in detail below, highlighting their respective strengths and considerations to help you determine the most effective solution for your specific use case.  Metric\tPod Level\tNode LevelAccess Control\tProvides fine-grained access control through service roles and RBAC, limiting PVC access to specific Pods. This is not possible with host-level mounts, where the mounted S3 bucket is accessible to all Pods on the Node.\tSimplifies configuration but lacks the granular control offered by Pod-level mounting. Scalabbility and Overhead\tInvolves managing individual PVCs, which can increase overhead in large-scale environments.\tReduces configuration complexity but provides less isolation between Pods. Performance Considerations\tOffers predictable and isolated performance for individual Pods.\tMay lead to contention if multiple Pods on the same Node access the same S3 bucket. Flexibility and Use Cases\tBest suited for use cases where different Pods require access to different datasets or where strict security and compliance controls are necessary.\tIdeal for environments where all Pods on a Node can share the same dataset, such as when running batch processing jobs or Spark jobs that require common dependencies.  ","version":"Next","tagName":"h2"},{"title":"Resource Allocation​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#resource-allocation","content":" Before being able to implement the Mountpoint-s3 solution provided, AWS cloud resources need to be allocated. To do deploy the Terraform stack following the instructions below. After allocating the resources and setting up the EKS environment, you can explore the two different approaches of utilizing Mountpoint-S3 in detail.  Deploy Solution Resources 👈  ","version":"Next","tagName":"h2"},{"title":"Approach 1: Deploy Mountpoint-S3 on EKS at Pod level​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#approach-1-deploy-mountpoint-s3-on-eks-at-pod-level","content":" Deploying Mountpoint-S3 at the Pod level involves using the EKS Managed Addon CSI driver with Persistent Volumes (PV) and Persistent Volume Claims (PVC) to mount an S3 bucket directly within a Pod. This method allows for fine-grained control over which Pods can access specific S3 buckets, ensuring that only the necessary workloads have access to the required data.  Once Mountpoint-S3 is enabled and the PV is created, the S3 bucket becomes a cluster-level resource, allowing any Pod to request access by creating a PVC that references the PV. To achieve fine-grained control over which Pods can access specific PVCs, you can use service roles within namespaces. By assigning specific service accounts to Pods and defining Role-Based Access Control (RBAC) policies, you can limit which Pods can bind to certain PVCs. This ensures that only authorized Pods can mount the S3 bucket, providing tighter security and access control compared to a host-level mount, where the hostPath is accessible to all Pods on the Node.  Using this approach can also be simplified using the EKS Managed Addon CSI driver. However, this does not support taints/tolerations and therefore cannot be used with GPUs. Additionally, because the Pods are not sharing the mount and therefore not sharing the cache it would lead to more S3 API calls.  For more information on how to deploy this approach refer to the deployment instructions  ","version":"Next","tagName":"h2"},{"title":"Approach 2: Deploy Mountpoint-S3 on EKS at Node level​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#approach-2--deploy-mountpoint-s3-on-eks-at-node-level","content":" Mounting a S3 Bucket at a Node level can streamline the management of dependency JAR files for SparkApplications by reducing build times and speeding up deployment. It can be implemented using either USERDATA or DaemonSet. USERDATA is the preferred method for implementing Mountpoint-S3. However, if you have static Nodes in your EKS cluster that you cannot bring down, the DaemonSet approach provides an alternative. Make sure to understand all of the security mechanisms that need to be enabled in order to utilize the DaemonSet approach before implementing it.  ","version":"Next","tagName":"h2"},{"title":"Approach 2.1: Using USERDATA​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#approach-21-using-userdata","content":" This approach is recommended for new clusters or where auto-scaling is customized to run workloads as the user-data script is run when a Node is initialized. Using the below script, the Node can be updated to have the S3 bucket mounted upon initialization in the EKS cluster that hosts the Pods. The below script outlines downloading, installing, and running the Mountpoint S3 package. There are a couple of arguments that are set for this application and defined below that can be altered depending on the use case. More information about these arguments and others can be found here  metadata-ttl: this is set to indefinite because the jar files are meant to be used as read only and will not change.allow-others: this is set so that the Node can have access to the mounted volume when using SSMcache: this is set to enable caching and limit the S3 API calls that need to be made by storing the files in cache for consecutive re-reads.  note These same arguments can also be used in the DaemonSet approach. In addition to these arguments that are set by this example, there are also a number of other options for additional logging and debugging  When autoscaling with Karpenter, this method allows for more flexibility and performance. For example when configuring Karpenter in the terraform code, the user data for different types of Nodes can be unique with different buckets depending on the workload so when Pods are scheduled and need a certain set of dependencies, Taints and Tolerations will allow Karpenter to allocate the specific instance type with the unique user data to ensure the correct bucket with the dependent files is mounted on the Node so that Pods can access is. Additionally, the user script will depend on the OS that the newly allocated Node is configured with.  USERDATA script:​  #!/bin/bash yum update -y yum install -y wget wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm yum install -y mount-s3.rpm mkdir -p /mnt/s3 /opt/aws/mountpoint-s3/bin/mount-s3 --metadata-ttl indefinite --allow-other --cache /tmp &lt;S3_BUCKET_NAME&gt; /mnt/s3   ","version":"Next","tagName":"h3"},{"title":"Approach 2.2: Using DaemonSet​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#approach-22-using-daemonset","content":" This approach is recommended for existing clusters. This approach is made up of 2 resources, a ConfigMap with a script that maintains the S3 Mount Point package onto the Node and a DaemonSet that runs a Pod on every Node in the cluster which will execute the script on the Node.  The ConfigMap script will run a loop to check the mountPoint every 60 seconds and remount it if there are any issues. There are multiple environment variables that can be altered for the mount location, cache location, S3 bucket name, log file location, and the URL of the package installation and the location of the of the installed package. These variables can be left as default as only the S3 bucket name is required to run.  The DaemonSet Pods will copy the script onto the Node, alter the permissions to allow execution, and then finally run the script. The Pod installs util-linux in order to have access to nsenter, which allows the Pod to execute the script in the Node space which allows the S3 Bucket to be mounted on to the Node directly by the Pod.  danger The DaemonSet Pod requires the securityContext to be privileged as well as hostPID, hostIPC, and hostNetwork to be set to true. Review below why these are required to be configured for this solution and their security implications.  securityContext: privileged Purpose: privileged mode gives the container full access to all host resources, similar to root access on the host.To install software packages, configure the system, and mount the S3 bucket onto the host, your container will likely need elevated permissions. Without privileged mode, the container might not have sufficient permissions to perform these actions on the host filesystem and network interfaces. hostPID Purpose: nsenter allows you to enter various namespaces, including the PID namespace of the host.When using nsenter to enter the host’s PID namespace, the container needs access to the host’s PID namespace. Thus, enabling hostPID: true is necessary to interact with processes on the host, which is crucial for operations like installing packages or running commands that require host-level process visibility like mountpoint-s3. hostIPC Purpose: hostIPC enables your container to share the host’s inter-process communication namespace, which includes shared memory.If nsenter commands or the script to run involves shared memory or other IPC mechanisms on the host, hostIPC: true will be necessary. While it’s less common than hostPID, it’s often enabled alongside it when nsenter is involved, especially if the script needs to interact with host processes that rely on IPC. hostNetwork Purpose: hostNetwork allows the container to use the host’s network namespace, giving the container access to the host’s IP address and network interfaces.During the installation process, the script will likely need to download packages from the internet (e.g., from repositories hosting the mountpoint-s3 package). By enabling hostNetwork with hostNetwork: true, you ensure that the download processes have direct access to the host’s network interface, avoiding issues with network isolation.  warning This sample code uses the spark-team-a namespace to run the job and host the DaemonSet. This is primarily because the Terraform stack already sets up IRSA for this namespace and allows the service account to access any S3 bucket. When using in production make sure create your own separate namespace, service account, and IAM role that follows the policy of least-privilege permissions and follows IAM role best practice   To view the DaemonSet, Click to toggle content! apiVersion: v1 kind: ConfigMap metadata: name: s3-mount-script namespace: spark-team-a data: monitor_s3_mount.sh: | #!/bin/bash set -e # Exit immediately if a command exits with a non-zero status # ENVIRONMENT VARIABLES LOG_FILE=&quot;/var/log/s3-mount.log&quot; S3_BUCKET_NAME=&quot;&lt;S3_BUCKET_NAME&gt;&quot; # Replace with your S3 Bucket Name before applying to EKS cluster MOUNT_POINT=&quot;/mnt/s3&quot; CACHE_DIR=&quot;/tmp&quot; MOUNT_S3_BIN=&quot;/usr/bin/mount-s3&quot; MOUNT_S3_URL=&quot;https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm&quot; # Function to install mount-s3 install_mount_s3() { echo &quot;$(date): Installing mount-s3&quot; | tee -a $LOG_FILE yum update -y | tee -a $LOG_FILE yum install -y wget util-linux | tee -a $LOG_FILE wget $MOUNT_S3_URL -O /tmp/mount-s3.rpm | tee -a $LOG_FILE yum install -y /tmp/mount-s3.rpm | tee -a $LOG_FILE } # Function to mount S3 bucket mount_s3_bucket() { echo &quot;$(date): Mounting S3 bucket: $S3_BUCKET_NAME to $MOUNT_POINT&quot; | tee -a $LOG_FILE $MOUNT_S3_BIN --metadata-ttl indefinite --allow-other --cache $CACHE_DIR $S3_BUCKET_NAME $MOUNT_POINT | tee -a $LOG_FILE if [ $? -ne 0 ]; then echo &quot;$(date): Failed to mount S3 bucket: $S3_BUCKET_NAME&quot; | tee -a $LOG_FILE exit 1 fi } # Ensure the mount point directory exists ensure_mount_point() { if [ ! -d $MOUNT_POINT ]; then echo &quot;$(date): Creating mount point directory: $MOUNT_POINT&quot; | tee -a $LOG_FILE mkdir -p $MOUNT_POINT fi } # Install mount-s3 install_mount_s3 # Continuous monitoring and remounting loop while true; do echo &quot;$(date): Checking if S3 bucket is mounted&quot; | tee -a $LOG_FILE ensure_mount_point if mount | grep $MOUNT_POINT &gt; /dev/null; then echo &quot;$(date): S3 bucket is already mounted&quot; | tee -a $LOG_FILE if ! ls $MOUNT_POINT &gt; /dev/null 2&gt;&amp;1; then echo &quot;$(date): Transport endpoint is not connected, remounting S3 bucket&quot; | tee -a $LOG_FILE fusermount -u $MOUNT_POINT || echo &quot;$(date): Failed to unmount S3 bucket&quot; | tee -a $LOG_FILE rm -rf $MOUNT_POINT || echo &quot;$(date): Failed to remove mount point directory&quot; | tee -a $LOG_FILE ensure_mount_point mount_s3_bucket fi else echo &quot;$(date): S3 bucket is not mounted, mounting now&quot; | tee -a $LOG_FILE mount_s3_bucket fi sleep 60 # Check every 60 seconds done --- apiVersion: apps/v1 kind: DaemonSet metadata: name: s3-mount-daemonset namespace: spark-team-a spec: selector: matchLabels: name: s3-mount-daemonset template: metadata: labels: name: s3-mount-daemonset spec: hostPID: true hostIPC: true hostNetwork: true volumes: - name: script configMap: name: s3-mount-script - name: host-root hostPath: path: / type: Directory restartPolicy: Always containers: - name: s3-mount image: amazonlinux:2 volumeMounts: - name: script mountPath: /config - name: host-root mountPath: /host mountPropagation: Bidirectional securityContext: privileged: true command: - /bin/bash - -c - | set -e echo &quot;Starting s3-mount&quot; yum install -y util-linux echo &quot;Copying script to /usr/bin&quot; cp /config/monitor_s3_mount.sh /host/usr/bin/monitor_s3_mount.sh chmod +x /host/usr/bin/monitor_s3_mount.sh echo &quot;Verifying the copied script&quot; ls -lha /host/usr/bin/monitor_s3_mount.sh echo &quot;Running the script in Host space&quot; nsenter --target 1 --mount --uts --ipc --net --pid ./usr/bin/monitor_s3_mount.sh echo &quot;Done&quot;   ","version":"Next","tagName":"h3"},{"title":"Executing Spark Job​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#executing-spark-job","content":" Here are the steps to test the scenario using Approach 2 with DaemonSet:  Deploy Spark Operator ResourcesPrepare the S3 Bucket cd ${DOEKS_HOME}/analytics/terraform/spark-k8s-operator/examples/mountpoint-s3-spark/chmod +x copy-jars-to-s3.sh./copy-jars-to-s3.sh Set-up Kubeconfig aws eks update-kubeconfig --name spark-operator-doeks Apply DaemonSet kubectl apply -f mountpoint-s3-daemonset.yaml Apply Spark Job sample kubectl apply -f mountpoint-s3-spark-job.yaml View Job Running There are a couple different resources of which we can view logs of as this SparkApplication CRD is running. Each of these logs should be in a separate terminal to view all of the logs simultaneously. spark operator kubectl -n spark-operator get podscopy the name of the spark operator pod kubectl -n spark-operator logs -f &lt;POD_NAME&gt; spark-team-a Pods In order to get the logs for the driver and exec Pods for the SparkApplication, we need to first verify that the Pods are running. Using wide output we should be able to see the Node that the Pods are running on and using -w we can see the status updates for each of the Pods.kubectl -n spark-team-a get pods -o wide -w driver Pod Once the driver Pod is in the running state, which will be visible in the previous terminal, we can get the logs for the driver Podkubectl -n spark-team-a logs -f taxi-trip exec Pod Once the exec Pod is in the running state which will be visible in the previous terminal, we can get the logs for the exec Pod. Make sure that the exec-1 is running before getting the logs, otherwise use another exec Pod that is in the running state. kubectl -n spark-team-a logs -f taxi-trip-exec-1  ","version":"Next","tagName":"h2"},{"title":"Verification​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#verification","content":" Once the job is done running you can see in the exec logs that the files are being copied from the local mountpoint-s3 location on the Node to the spark Pod in order to do the processing.  24/08/13 00:08:46 INFO Utils: Copying /mnt/s3/jars/hadoop-aws-3.3.1.jar to /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/-14716855631723507720806_cache 24/08/13 00:08:46 INFO Utils: Copying /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/-14716855631723507720806_cache to /opt/spark/work-dir/./hadoop-aws-3.3.1.jar 24/08/13 00:08:46 INFO Executor: Adding file:/opt/spark/work-dir/./hadoop-aws-3.3.1.jar to class loader 24/08/13 00:08:46 INFO Executor: Fetching file:/mnt/s3/jars/aws-java-sdk-bundle-1.12.647.jar with timestamp 1723507720806 24/08/13 00:08:46 INFO Utils: Copying /mnt/s3/jars/aws-java-sdk-bundle-1.12.647.jar to /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/14156613201723507720806_cache 24/08/13 00:08:47 INFO Utils: Copying /var/data/spark-5eae56b3-3999-4c2f-8004-afc46d1c82ba/spark-a433e7ce-db5d-4fd5-b344-abf751f43bd3/14156613201723507720806_cache to /opt/spark/work-dir/./aws-java-sdk-bundle-1.12.647.jar   Additionally, when viewing status of the spark-team-a Pods, you would notice that another Node comes online, this Node is is optimized to the run the SparkApplication and as soon as it comes online the DaemonSet Pod will also spin up and start running on the new Node so that any Pods that are run that new Node will also have access to the S3 Bucket. Using Systems Sessions Manager (SSM), you can connect any of the Nodes and verify the that the mountpoint-s3 package has been downloaded and installed by running:  mount-s3 --version  The largest advantage to using the mountpoint-S3 on the Node level for multiple Pods is that the data can be cached to allow other Pods to access the same data without having to make their own API calls. Once the karpenter-spark-compute-optimized optimized Node is allocated you can use Sessions Manager (SSM) to connect to the Node and verify that the files will be cached on the Node when the job is run and the volume is mounted. you can see the cache at:  sudo ls /tmp/mountpoint-cache/  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Mountpoint-S3 for Spark Workloads","url":"/data-on-eks/docs/resources/mountpoint-s3-for-spark#conclusion","content":" By leveraging the CRT library, Mountpoint for Amazon S3 can deliver the high throughput and low latency needed to efficiently manage and access large volumes of data stored in S3. This allows dependency JAR files to be stored and managed externally from the container image, decoupling them from the Spark jobs. Additionally, storing JARs in S3 enables multiple Pods to consume them, leading to cost savings as S3 provides a cost-effective storage solution compared to larger container images. S3 also offers virtually unlimited storage, making it easy to scale and manage dependencies.  Mountpoint-S3 offers a versatile and powerful way to integrate S3 storage with EKS for data and AI/ML workloads. Whether you choose to deploy it at the Pod level using PVs and PVCs, or at the Node level using USERDATA or DaemonSets, each approach has its own set of advantages and trade-offs. By understanding these options, you can make informed decisions to optimize your data and AI/ML workflows on EKS. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}