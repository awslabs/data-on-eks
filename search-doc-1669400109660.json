[{"title":"EMR on EKS Best Practices","type":0,"sectionRef":"#","url":"blog/EMR on EKS Best Practices","content":"","keywords":""},{"title":"EMR Containers Best Practices Guides‚Äã","type":1,"pageTitle":"EMR on EKS Best Practices","url":"blog/EMR on EKS Best Practices#emr-containers-best-practices-guides","content":"Amazon EMR on Amazon EKS enables you to submit Apache Spark jobs on demand on Amazon Elastic Kubernetes Service (EKS) without provisioning clusters. With EMR on EKS, you can consolidate analytical workloads with your other Kubernetes-based applications on the same Amazon EKS cluster to improve resource utilization and simplify infrastructure management. This link provides the best practices and templates to get started with Amazon EMR on EKS. We publish this guide on GitHub so we could iterate the content quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. Checkout the EMR on EKS Best practices GitHub docs here "},{"title":"Architecture‚Äã","type":1,"pageTitle":"EMR on EKS Best Practices","url":"blog/EMR on EKS Best Practices#architecture","content":"The following diagram illustrates the solution architecture Amazon EMR on EKS.  "},{"title":"Observability EMR on EKS","type":0,"sectionRef":"#","url":"blog/Observability EMR on EKS","content":"","keywords":""},{"title":"Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafana‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana","content":"In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases Checkout the full blog here "},{"title":"Architecture‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#architecture","content":"The following diagram illustrates the solution architecture for scraping Spark Driver and Executors‚Äô metrics, as well as writing to Amazon Managed Service for Prometheus.  "},{"title":"Grafana Dashboard for Spark‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"blog/Observability EMR on EKS#grafana-dashboard-for-spark","content":"The following Grafana dashboard displays the EMR on EKS Spark job metrics with Driver and Executor details.  "},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"","keywords":""},{"title":"Data on EKS Blogs & Benchmarks‚Äã","type":1,"pageTitle":"Welcome","url":"blog/welcome#data-on-eks-blogs--benchmarks","content":"In this section you will find Blogs and Benchmark reports for the following topics. AWS Data Analytics and ML blogs are featured a short blogs. üöÄ EMR on EKS üöÄ Spark on EKS üöÄ Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) üöÄ Job Schedulers (e.g., Apache Airflow, Argo Workflows) üöÄ Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) üöÄ Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) "},{"title":"AI/ML Platforms on EKS","type":0,"sectionRef":"#","url":"docs/ai-ml-eks","content":"AI/ML Platforms on EKS Build, deploy and scale ML platforms on Amazon EKS. This section also provides Sample ML scripts with various storage and compute types.","keywords":""},{"title":"Amazon EMR on Amazon EKS","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks","content":"","keywords":""},{"title":"Benefits of EMR on EKS‚Äã","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#benefits-of-emr-on-eks","content":""},{"title":"Simplify management‚Äã","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#simplify-management","content":"You get the same EMR benefits for Apache Spark on EKS that you get on EC2 today. This includes fully managed versions of Apache Spark 2.4 and 3.0, automatic provisioning, scaling, performance optimized runtime, and tools like EMR Studiofor authoring jobs and an Apache Spark UI for debugging. "},{"title":"Reduce Costs‚Äã","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#reduce-costs","content":"With EMR on EKS, your compute resources can be shared between your Apache Spark applications and your other Kubernetes applications. Resources are allocated and removed on demand to eliminate over-provisioning or under-utilization of these resources, enabling you to lower costs as you only pay for the resources you use. "},{"title":"Optimize Performance‚Äã","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#optimize-performance","content":"By running analytics applications on EKS, you can reuse existing EC2 instances in your shared Kubernetes cluster and avoid the startup time of creating a new cluster of EC2 instances dedicated for analytics. You can also get 3x faster performance running performance optimized Spark with EMR on EKS compared to standard Apache Spark on EKS. "},{"title":"EMR on EKS Deployment patterns with Terraform‚Äã","type":1,"pageTitle":"Amazon EMR on Amazon EKS","url":"docs/amazon-emr-on-eks#emr-on-eks-deployment-patterns-with-terraform","content":"The following Terraform templates are available to deploy. Scaling EMR on EKS Spark Jobs with Karpenter.Running EMR on EKS Spark Jobs with Apache YunikornRunning EMR on EKS Spark Jobs with FSx for Lustre as Shuffle StorageRunning ACK controller for EMR on EKS to run Spark Jobs "},{"title":"ACK Controller for EMR on EKS","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-ack","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#introduction","content":"In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases. "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS, as well as monitor spark job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Two managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc.Spark Node group with single AZ for running Spark jobs Enable EMR on EKS and creates two Data teams (emr-data-team-a, emr-data-team-b) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-aIAM policy for emr-data-team-aAmazon Managed Prometheus workspace to remote write metrics from Prometheus serverDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/terraform/emr-eks-ack terraform init  Set AWS_REGION and Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Change region according to your needs terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#verify-the-resources","content":"Let‚Äôs verify the resources created by terraform apply. Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus. aws eks describe-cluster --name emr-eks-ack aws amp list-workspaces --alias amp-ws-emr-eks-ack  Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region us-west-2 update-kubeconfig --name emr-eks-ack # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Setup Amazon Managed Grafana with SSO‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#setup-amazon-managed-grafana-with-sso","content":"Currently, this step is manual. Please follow the steps in this blog to create Amazon Managed Grafana with SSO enabled in your account. You can visualize the Spark jobs runs and metrics using Amazon Managed Prometheus and Amazon Managed Grafana. "},{"title":"Execute Sample Spark job on EMR Virtual Cluster‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#execute-sample-spark-job-on-emr-virtual-cluster","content":"Execute the Spark job using the below shell script. This script requires three input parameters in which EMR_VIRTUAL_CLUSTER_ID and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/terraform/emr-eks-ack/examples/spark/ ./emr-eks-spark-amp-amg.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"docs/amazon-emr-on-eks/emr-eks-ack#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Deploying EMR on EKS with CDK blueprint","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-cdk","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#introduction","content":"In this post, we will learn how to use EMR on EKS AddOn and Teams in the cdk-eks-blueprints to deploy a an infrasturcture on EKS to submit Spark Job. The cdk-eks-blueprints allows you deploy an EKS cluster and enable it to be used by EMR on EKS service with minimal setup. The architecture below shows a conceptual view of the infrastructure you will deploy through this blueprint.  "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#deploying-the-solution","content":"In this example, you will provision the following: Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Two managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Logging etc.Spark Node group with single AZ for running Spark jobs Enable EMR on EKS and create one Data teams (emr-data-team-a) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-aIAM policy for emr-data-team-aDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Metrics server with HA, Cluster Autoscaler, CertManager and AwsLoadBalancerController This blueprint can also take an EKS cluster that you defined using the cdk-blueprints-library. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlCDK NOTE: You need to have an AWS account and region that are bootstraped by AWS CDK. "},{"title":"Customize‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#customize","content":"The the entry point for this cdk blueprint is /bin/emr-eks.ts which instantiate a stack defined in lib/emr-eks-blueprint-stack.ts. This stack must be provided with a VPC and a list of EMR on EKS team defition and the role that will be admin of the EKS cluster. It can also take as options an EKS cluster defined through cdk-blueprints-library and the EKS cluster name. The properties that are passed to the emr on eks blueprint stack are defined as such: export interface EmrEksBlueprintProps extends StackProps { clusterVpc: IVpc, clusterAdminRoleArn: ArnPrincipal dataTeams: EmrEksTeamProps[], eksClusterName?: string, //Default eksBlueprintCluster eksCluster?: GenericClusterProvider, }  In this example we define a VPC in lib/vpc.ts and is instantiated in bin/emr-eks.ts. We also define a team called emr-data-team-a and which has an execution role called myBlueprintExecRole. The blueprint will deploy by default an EKS cluster with the managed nodegroups defined in the section Deploying the Solution. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#deploy","content":"Before you run the solution, you MUST change the clusterAdminRoleArn of the props object in lib/emr-eks.ts. This role allows you to interact manage EKS cluster and should have be allowed at least the IAM action eks:AccessKubernetesApi. Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run cdk synth cd analytics/cdk/emr-eks npm install cdk synth --profile YOUR-AWS-PROFILE  Deploy the pattern cdk deploy --all  Enter yes to deploy. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#verify-the-resources","content":"Let‚Äôs verify the resources created by cdk deploy. Verify the Amazon EKS Cluster aws eks describe-cluster --name eksBlueprintCluster # Update the name cluster name if you supplied your own  Verify EMR on EKS Namespaces batchjob and Pod status for Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name eksBlueprintCluster # Creates k8s config file to authenticate with EKS Cluster. Update the name cluster name if you supplied your own kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep batchjob # Output shows batchjob kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Execute Sample Spark job on EMR Virtual Cluster‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#execute-sample-spark-job-on-emr-virtual-cluster","content":"Execute the Spark job using the below shell script. Once you deploy the blueprint you will have as output the Virtual Cluster id. You can use the id and the execution role for which you supplied a policy to submit jobs. Below you can find an example of a job you can submit with AWS CLI.  export EMR_ROLE_ARN=arn:aws:iam::&lt;YOUR-ACCOUNT-ID&gt;:role/myBlueprintExecRole aws emr-containers start-job-run \\ --virtual-cluster-id=&lt;VIRTUAL-CLUSTER-ID-IN-CDK-OUTPUT&gt; \\ --name=pi-2 \\ --execution-role-arn=$EMR_ROLE_ARN \\ --release-label=emr-6.8.0-latest \\ --job-driver='{ &quot;sparkSubmitJobDriver&quot;: { &quot;entryPoint&quot;: &quot;local:///usr/lib/spark/examples/src/main/python/pi.py&quot;, &quot;sparkSubmitParameters&quot;: &quot;--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.kubernetes.node.selector.app=spark&quot; } }'  Verify the job execution kubectl get pods --namespace=batchjob -w  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Deploying EMR on EKS with CDK blueprint","url":"docs/amazon-emr-on-eks/emr-eks-cdk#cleanup","content":"To clean up your environment, you call the command below. This will destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC cdk destroy --all  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Ray on EKS","type":0,"sectionRef":"#","url":"docs/ai-ml-eks/ray","content":"","keywords":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraformpython3jq Additionally, for end-to-end configuration of Ingress, you can optionally provide the following: A Route53 Public Hosted Zone configured in the account where you are deploying this example. E.g. &quot;bar.com&quot;An ACM Certificate in the account + region where you are deploying this example. A wildcard certificate is preferred, e.g. &quot;*.bar.com&quot; "},{"title":"Deploy the EKS Cluster with Ray Operator‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#deploy-the-eks-cluster-with-ray-operator","content":""},{"title":"Clone the repository‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#clone-the-repository","content":"git clone https://github.com/awslabs/data-on-eks.git  "},{"title":"Build Docker Image for Ray Cluster‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#build-docker-image-for-ray-cluster","content":"First, in order to run the RayCluster, we need to push a container image to ECR repository that contains the all the dependencies. You can see the Dockerfile in the example with python dependencies packaged in. The next series of steps we will setup an ECR repository, build the docker image for our model and push it to the ECR repository. Create an ECR repository aws ecr create-repository --repository-name ray-demo  Login to the ECR repository export AWS_REGION=&lt;enter-your-region&gt; export ACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account') echo $ACCOUNT_ID aws ecr get-login-password \\ --region $AWS_REGION | docker login \\ --username AWS \\ --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/ray-demo  Build the docker image containing our model deployment. docker build sources -t $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/ray-demo  Push the docker image to the ECR repo docker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/ray-demo  "},{"title":"Initialize Terraform‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#initialize-terraform","content":"Navigate into the example directory and run terraform init cd data-on-eks/ai-ml/terraform/ray/ terraform init  "},{"title":"Terraform Plan‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#terraform-plan","content":"Run Terraform plan to verify the resources created by this execution. Optional - provide a Route53 Hosted Zone hostname and a corresponding ACM Certificate; export TF_VAR_eks_cluster_domain=&quot;bar.com&quot; export TF_VAR_acm_certificate_domain=&quot;*.bar.com&quot;  "},{"title":"Deploy the pattern‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#deploy-the-pattern","content":"terraform apply ... ... Outputs: configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name ray&quot; s3_bucket = &quot;ray-demo-models-20220719224423900800000001&quot;  Enter yes to apply. Export the s3_bucket to your local environment. export S3_BUCKET=&quot;s3://ray-demo-models-20220719224423900800000001&quot;  "},{"title":"Verify Deployment‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#verify-deployment","content":"Update kubeconfig aws eks --region us-west-2 update-kubeconfig --name ray  Verify all pods are running. NAMESPACE NAME READY STATUS RESTARTS AGE external-dns external-dns-99dd9564f-8fsvd 1/1 Running 2 (6h21m ago) 16d ingress-nginx ingress-nginx-controller-659678ccb9-dlc5r 1/1 Running 2 (6h20m ago) 16d kube-prometheus-stack alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 4 (6h21m ago) 16d kube-prometheus-stack kube-prometheus-stack-grafana-68d4b6d7f4-h97j4 3/3 Running 6 (6h21m ago) 15d kube-prometheus-stack kube-prometheus-stack-kube-state-metrics-6d6b967d6d-fc62f 1/1 Running 2 (6h21m ago) 16d kube-prometheus-stack kube-prometheus-stack-operator-7c8bffbd9b-5rv59 1/1 Running 2 (6h21m ago) 14d kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-4hj6g 1/1 Running 2 (6h20m ago) 16d kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-9pgqc 1/1 Running 2 (6h21m ago) 16d kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-v46pf 1/1 Running 2 (6h21m ago) 16d kube-prometheus-stack prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 4 (6h21m ago) 14d kube-system aws-load-balancer-controller-67b5dd7d69-jjq4r 1/1 Running 2 (6h21m ago) 16d kube-system aws-load-balancer-controller-67b5dd7d69-pthpn 1/1 Running 2 (6h20m ago) 16d kube-system aws-node-7bcvh 1/1 Running 3 (6h19m ago) 16d kube-system aws-node-csshj 1/1 Running 3 (6h19m ago) 16d kube-system aws-node-htcmn 1/1 Running 4 (6h19m ago) 16d kube-system coredns-7f5998f4c-m4lxr 1/1 Running 2 (6h21m ago) 16d kube-system coredns-7f5998f4c-t78m9 1/1 Running 2 (6h20m ago) 16d kube-system kube-proxy-4v4pr 1/1 Running 2 (6h21m ago) 16d kube-system kube-proxy-7rkb7 1/1 Running 2 (6h20m ago) 16d kube-system kube-proxy-v76mj 1/1 Running 2 (6h21m ago) 16d kuberay-operator kuberay-operator-7b88c9c4fb-kdhlz 1/1 Running 2 (6h21m ago) 16d ray-cluster raycluster-autoscaler-head-wxbgw 1/1 Running 0 10m ray-cluster raycluster-autoscaler-worker-large-group-twtld 1/1 Running 0 10m  Ray Dashboard‚Äã The Ray Dashboard can be opened at the following url &quot;https://ray-demo.bar.com/dashboard&quot;  "},{"title":"Examples‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#examples","content":"Hugging Face‚Äã Ray Serve‚Äã As a sample, we will use Ray Serve to deploy a sample machine learning model and expose it to the outside world via the Ingress configuration. The code for this deployment can be found sources/hface_t5_summarize_serve.py. We use the Hugging Face T5 model to serve an endpoint to summarize an arbitrary block of text. This model is deployed as Kubernetes job sample-jobs/summarize-serve-job.yaml. Create a Job to deploy the model to the Ray Cluster envsubst &lt; sample-jobs/summarize-serve-job.yaml | kubectl create -f - job.batch/ray-summarize-job-cjdd8 created  Tail the logs of the job to verify successful deployment of the job. kubectl logs -n ray-cluster ray-summarize-job-cjdd8-wmxm8 -f Caught schedule exception 2022-07-08 17:03:29,579 INFO common.py:220 -- Exception from actor creation is ignored in destructor. To receive this exception in application code, call a method on the actor reference before its destructor is run. (ServeController pid=458) INFO 2022-07-08 17:03:30,684 controller 458 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery. (ServeController pid=458) INFO 2022-07-08 17:03:30,788 controller 458 http_state.py:115 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-node:10.0.12.204-0' on node 'node:10.0.12.204-0' listening on '0.0.0.0:8000' (HTTPProxyActor pid=496) INFO: Started server process [496] (ServeController pid=458) INFO 2022-07-08 17:03:33,701 controller 458 deployment_state.py:1217 - Adding 1 replicas to deployment 'Summarizer'. Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.17k/1.17k [00:00&lt;00:00, 2.04MB/s] Downloading: 0%| | 0.00/231M [00:00&lt;?, ?B/s] Downloading: 2%|‚ñè | 5.68M/231M [00:00&lt;00:03, 59.5MB/s] Downloading: 5%|‚ñç | 11.4M/231M [00:00&lt;00:04, 47.3MB/s] Downloading: 7%|‚ñã | 16.0M/231M [00:00&lt;00:04, 45.2MB/s] Downloading: 9%|‚ñâ | 20.5M/231M [00:00&lt;00:04, 45.9MB/s] Downloading: 11%|‚ñà | 25.8M/231M [00:00&lt;00:04, 48.9MB/s] Downloading: 13%|‚ñà‚ñé | 31.1M/231M [00:00&lt;00:04, 51.2MB/s] Downloading: 16%|‚ñà‚ñå | 36.4M/231M [00:00&lt;00:03, 52.7MB/s] Downloading: 18%|‚ñà‚ñä | 41.5M/231M [00:00&lt;00:03, 52.7MB/s] Downloading: 20%|‚ñà‚ñà | 46.6M/231M [00:00&lt;00:03, 52.1MB/s] Downloading: 22%|‚ñà‚ñà‚ñè | 51.5M/231M [00:01&lt;00:03, 51.5MB/s] Downloading: 25%|‚ñà‚ñà‚ñç | 56.6M/231M [00:01&lt;00:03, 51.9MB/s] Downloading: 27%|‚ñà‚ñà‚ñã | 61.7M/231M [00:01&lt;00:03, 52.3MB/s] Downloading: 29%|‚ñà‚ñà‚ñâ | 66.9M/231M [00:01&lt;00:03, 53.2MB/s] Downloading: 31%|‚ñà‚ñà‚ñà | 72.0M/231M [00:01&lt;00:03, 51.1MB/s] Downloading: 33%|‚ñà‚ñà‚ñà‚ñé | 76.9M/231M [00:01&lt;00:03, 51.2MB/s] Downloading: 36%|‚ñà‚ñà‚ñà‚ñå | 82.2M/231M [00:01&lt;00:02, 52.5MB/s] Downloading: 38%|‚ñà‚ñà‚ñà‚ñä | 87.3M/231M [00:01&lt;00:02, 52.6MB/s] Downloading: 40%|‚ñà‚ñà‚ñà‚ñà | 92.6M/231M [00:01&lt;00:02, 53.5MB/s] Downloading: 42%|‚ñà‚ñà‚ñà‚ñà‚ñè | 97.7M/231M [00:01&lt;00:02, 53.5MB/s] Downloading: 45%|‚ñà‚ñà‚ñà‚ñà‚ñç | 103M/231M [00:02&lt;00:02, 54.2MB/s] Downloading: 47%|‚ñà‚ñà‚ñà‚ñà‚ñã | 108M/231M [00:02&lt;00:02, 53.5MB/s] Downloading: 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ | 114M/231M [00:02&lt;00:02, 54.3MB/s] Downloading: 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 119M/231M [00:02&lt;00:02, 54.6MB/s] Downloading: 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 124M/231M [00:02&lt;00:02, 54.8MB/s] Downloading: 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 129M/231M [00:02&lt;00:01, 54.2MB/s] Downloading: 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 135M/231M [00:02&lt;00:01, 54.7MB/s] Downloading: 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 140M/231M [00:02&lt;00:01, 54.2MB/s] Downloading: 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 145M/231M [00:02&lt;00:01, 53.2MB/s] Downloading: 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 150M/231M [00:02&lt;00:01, 53.9MB/s] Downloading: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 155M/231M [00:03&lt;00:01, 44.7MB/s] Downloading: 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 161M/231M [00:03&lt;00:01, 48.3MB/s] Downloading: 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 167M/231M [00:03&lt;00:01, 51.2MB/s] Downloading: 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 172M/231M [00:03&lt;00:01, 52.3MB/s] Downloading: 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 177M/231M [00:03&lt;00:01, 50.8MB/s] Downloading: 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 182M/231M [00:03&lt;00:00, 52.2MB/s] Downloading: 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 188M/231M [00:03&lt;00:00, 53.2MB/s] Downloading: 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 193M/231M [00:03&lt;00:00, 54.2MB/s] Downloading: 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 198M/231M [00:03&lt;00:00, 53.9MB/s] Downloading: 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 204M/231M [00:04&lt;00:00, 53.6MB/s] Downloading: 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 209M/231M [00:04&lt;00:00, 53.5MB/s] Downloading: 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 214M/231M [00:04&lt;00:00, 54.0MB/s] Downloading: 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 219M/231M [00:04&lt;00:00, 54.6MB/s] Downloading: 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 225M/231M [00:04&lt;00:00, 54.8MB/s] Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231M/231M [00:04&lt;00:00, 52.5MB/s] Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 773k/773k [00:00&lt;00:00, 29.1MB/s] Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.32M/1.32M [00:00&lt;00:00, 25.5MB/s]  Test Summarize Deployment‚Äã The client code uses python requests module to invoke the /summarize endpoint with a block of text. If all goes well, the endpoint should return summarized text of the block text submitted. python sources/summarize_client.py two astronauts steered their fragile lunar module safely and smoothly to the historic landing . the first men to reach the moon -- Armstrong and his co-pilot, col. Edwin E. Aldrin Jr. of the air force -- brought their ship to rest on a level, rock-strewn plain .  Pytorch HuggingFace Clothing‚Äã Ray Train‚Äã As a sample, we will use Ray Train to train a machine learning model using a sample dataset. See the code for model training sources/train_pytorch_huggingface_clothing.py Create a Job to submit the training job to the Ray Cluster envsubst &lt; sample-jobs/train-pytorch-huggingface-clothing.yaml | kubectl create -f - job.batch/ray-train-pytorch-huggingface-clothing-plkzf created  Tail the logs to see the training in progress: kubectl logs -n ray-cluster job.batch/ray-train-pytorch-huggingface-clothing-plkzf -f ... ... {'running_train_loss': tensor(1.1093, requires_grad=True), '_timestamp': 1660890352, '_time_this_iter_s': 32.8163115978241, '_training_iteration': 1, 'time_this_iter_s': 37.8390429019928, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': 'af450_00000', 'experiment_id': '5a41dbb6558648d29ff5070c391167ea', 'date': '2022-08-18_23-25-54', 'timestamp': 1660890354, 'time_total_s': 37.8390429019928, 'pid': 1469, 'hostname': 'raycluster-autoscaler-head-wxbgw', 'node_ip': '10.0.11.68', 'config': {}, 'time_since_restore': 37.8390429019928, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.002658843994140625, 'experiment_tag': '0'} s3://ray-demo-models-20220801234005040500000001/ray_output/TorchTrainer_2022-08-18_23-25-13/TorchTrainer_af450_00000_0_2022-08-18_23-25-14/checkpoint_000000/  As shown above a model checkpoint is written to an S3 location so that it can be retrived for model serving. The location of the checkpoint is stored in the SSM Parameter Store. Ray Serve‚Äã To create an inference endpoint which will be served using Ray Serve, create a Job to submit the Ray deployment sources/serve_pytorch_huggingface_clothing.py. envsubst &lt; sample-jobs/serve-pytorch-huggingface-clothing.yaml | kubectl create -f -  Test Deployment‚Äã A sample script sources/pytorch_huggingface_clothing_client.py is provided that uses the requests library to test the inference endpoint. python sources/pytorch_huggingface_clothing_client.py Positive  "},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#monitoring","content":"This blueprint uses the kube-prometheus-stack to create a monitoring stack for getting visibility into your RayCluster. Open the Grafana dashboard using the url &quot;https://ray-demo.bar.com/monitoring&quot;. The sample Ray dashboard can be accessed by browsing to the Ray grafana folder.  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Ray on EKS","url":"docs/ai-ml-eks/ray#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  "},{"title":"EMR on EKS with FSx for Lustre","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#introduction","content":"Amazon FSx for Lustre is a fully managed shared storage option built on the world‚Äôs most popular high-performance file system. It offers highly scalable, cost-effective storage, which provides sub-millisecond latencies, millions of IOPS, and throughput of hundreds of gigabytes per second. Its popular use cases include high-performance computing (HPC), financial modeling, video rendering, and machine learning. FSx for Lustre supports two types of deployments: For storage, EMR on EKS supports node ephemeral storage using hostPath where the storage is attached to individual nodes, and Amazon Elastic Block Store (Amazon EBS) volume per executor/driver pod using dynamic Persistent Volume Claims. However, some Spark users are looking for an HDFS-like shared file system to handle specific workloads like time-sensitive applications or streaming analytics. In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage for running Spark jobs with EMR on EKS. "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS with FSx for Lustre as shuffle storage, as well as monitor spark job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with two managed node groupsDeploys Metrics server with HA, Cluster Autoscaler, Prometheus, VPA, CoreDNS Autoscaler, FSx CSI driverEMR on EKS Teams and EMR Virtual cluster for emr-data-team-aCreates Amazon managed Prometheus Endpoint and configures Prometheus Server addon with remote write configuration to Amazon Managed PrometheusCreates PERSISTENT type FSx for Lustre filesystem, Static Persistent volume and Persistent volume claimCreates Scratch type FSx for Lustre filesystem with dynamic Persistent volume claimS3 bucket to sync FSx for Lustre filesystem data "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/emr-eks-fsx-lustre terraform init  Set AWS_REGION and Runterraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Change according to your need terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#verify-the-resources","content":"Let‚Äôs verify the resources created by terraform apply. Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus aws eks describe-cluster --name emr-eks-fsx-lustre aws amp list-workspaces --alias amp-ws-emr-eks-fsx-lustre  # Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region us-west-2 update-kubeconfig --name emr-eks-fsx-lustre # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod kubectl get pods -n kube-system | grep fsx # Output of the FSx controller and node pods kubectl get pvc -n emr-data-team-a # Output of persistent volume for static(`fsx-static-pvc`) and dynamic(`fsx-dynamic-pvc`) #FSx Storage Class kubectl get storageclasses | grep fsx emr-eks-fsx-lustre fsx.csi.aws.com Delete Immediate false 109s # Output of static persistent volume with name `fsx-static-pv` kubectl get pv | grep fsx fsx-static-pv 1000Gi RWX Recycle Bound emr-data-team-a/fsx-static-pvc fsx # Output of static persistent volume with name `fsx-static-pvc` and `fsx-dynamic-pvc` # Pending status means that the FSx for Lustre is still getting created. This will be changed to bound once the filesystem is created. Login to AWS console to verify. kubectl get pvc -n emr-data-team-a | grep fsx fsx-dynamic-pvc Pending fsx 4m56s fsx-static-pvc Bound fsx-static-pv 1000Gi RWX fsx 4m56s  "},{"title":"Spark Job Execution - FSx for Lustre Static Provisioning‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#spark-job-execution---fsx-for-lustre-static-provisioning","content":"Execute Spark Job by using FSx for Lustre with statically provisioned volume. Execute the Spark job using the below shell script. This script requires three input parameters which can be extracted from terraform apply output values. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/emr-eks-fsx-lustre/examples/spark-execute/ ./fsx-static-spark.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w  This will show the mounted /data directory with FSx DNS name kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /static  "},{"title":"Spark Job Execution - FSx for Lustre Dynamic Provisioning‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#spark-job-execution---fsx-for-lustre-dynamic-provisioning","content":"Execute Spark Job by using FSx for Lustre with dynamically provisioned volume and Fsx for Lustre file system This script requires three input parameters in which EMR_VIRTUAL_CLUSTER_ID and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_ID=$1 # Terraform output variable is emrcontainers_virtual_cluster_id S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/emr-eks-fsx-lustre/examples/spark-execute/ ./fsx-dynamic-spark.sh &quot;&lt;ENTER_EMR_VIRTUAL_CLUSTER_ID&gt;&quot; &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w  kubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti ny-taxi-trip-dyanmic-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /dyanmic  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS with FSx for Lustre","url":"docs/amazon-emr-on-eks/emr-eks-fsx-for-lustre#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"EMR on EKS with Apache YuniKorn","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#introduction","content":"In this post, we learn how to deploy highly scalable EMR on EKS Clusters with Apache YuniKorn batch scheduler and Cluster Autoscaler. Apache YuniKorn is designed to run Batch workloads on Kubernetes. Key features as follows... Application-aware schedulingFine-grained control over resources for different tenantsJob ordering and queueingResource allocation fairness with prioritiesAutomatic reservations for outstanding requestsAutoscalingGang Scheduling Spark jobs "},{"title":"Architecture‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#architecture","content":" "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS, as well as monitor spark job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Three managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc.Spark Driver Node group with ON_DEMAND instances for Spark Drivers with single AZSpark Executor Node group with SPOT instances for Spark Executors with single AZ Enable EMR on EKS and creates two Data teams (emr-data-team-a, emr-data-team-b) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster and IAM policy for emr-data-team-a and emr-data-team-bAmazon Managed Prometheus workspace to remote write metrics from Prometheus serverDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Apache YuniKorn, Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/terraform/emr-eks-yunikorn terraform init  Set AWS_REGION and Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Change region according to your needs. terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#verify-the-resources","content":"Let‚Äôs verify the resources created by terraform apply. Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus. aws eks describe-cluster --name emr-eks-yunikorn aws amp list-workspaces --alias amp-ws-emr-eks-yunikorn  Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name emr-eks-yunikorn # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Setup Amazon Managed Grafana with SSO‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#setup-amazon-managed-grafana-with-sso","content":"Currently, this step is manual. Please follow the steps in this blog to create Amazon Managed Grafana with SSO enabled in your account. You can visualize the Spark jobs runs and metrics using Amazon Managed Prometheus and Amazon Managed Grafana. "},{"title":"Execute EMR Spark Job with Apache YuniKorn Gang Scheduling‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#execute-emr-spark-job-with-apache-yunikorn-gang-scheduling","content":"Execute the Spark job using the below shell script. This script requires three input parameters in which EMR_VIRTUAL_CLUSTER_ID and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_NAME=$1 # Terraform output variable is `emrcontainers_virtual_cluster_name` S3_BUCKET=$2 # This script requires s3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is `emr_on_eks_role_arn`  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/terraform/emr-eks-yunikorn/examples/emr-yunikorn-gang-scheduling/ # Execute EMR Spark Job with Apache YuniKorn Gang Scheduling feature ./emr-eks-yunikorn-gang-scheduling.sh emr-eks-yunikorn-emr-data-team-a s3://&lt;S3_BUCKET_NAME&gt; arn:aws:iam::&lt;YOUR_ACCOUNT_ID&gt;:role/emr-eks-yunikorn-emr-eks-data-team-a  Verify the job execution kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS with Apache YuniKorn","url":"docs/amazon-emr-on-eks/emr-eks-yunikorn#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"EMR on EKS with Karpenter","type":0,"sectionRef":"#","url":"docs/amazon-emr-on-eks/emr-eks-karpenter","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#introduction","content":"In this pattern, you will learn how to deploy, configure and use multiple Karpenter provisioners for scaling Spark jobs with EMR on EKS. Multiple Data teams within the organization can run Spark jobs on the selected Karpenter provisioners using tolerations specified in the pod templates example. This pattern deploys three Karpenter provisioners. spark-compute-optimized provisioner to run spark jobs on c5d instances.spark-memory-optimized provisioner to run spark jobs on r5d instances.spark-graviton-memory-optimized provisioner to run spark jobs on r6gd Graviton instances(ARM64). Let's review the Karpenter provisioner for computed optimized instances deployed by this pattern. Karpenter provisioner for compute optimized instances. This template leverages the pre-created AWS Launch templates. apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: spark-compute-optimized namespace: karpenter # Same namespace as Karpenter add-on installed spec: kubeletConfiguration: containerRuntime: containerd # podsPerCore: 2 # maxPods: 20 requirements: - key: &quot;topology.kubernetes.io/zone&quot; operator: In values: [${azs}a] #Update the correct region and zones - key: &quot;karpenter.sh/capacity-type&quot; operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] - key: &quot;node.kubernetes.io/instance-type&quot; #If not included, all instance types are considered operator: In values: [&quot;c5d.large&quot;,&quot;c5d.xlarge&quot;,&quot;c5d.2xlarge&quot;,&quot;c5d.4xlarge&quot;,&quot;c5d.9xlarge&quot;] # 1 NVMe disk - key: &quot;kubernetes.io/arch&quot; operator: In values: [&quot;amd64&quot;] limits: resources: cpu: 1000 providerRef: name: spark-compute-optimized labels: type: karpenter provisioner: spark-compute-optimized NodeGroupType: SparkComputeOptimized taints: - key: spark-compute-optimized value: 'true' effect: NoSchedule ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: spark-compute-optimized namespace: karpenter spec: subnetSelector: Name: &quot;${eks_cluster_id}-private*&quot; # required launchTemplate: &quot;${launch_template_name}&quot; # optional, see Launch Template documentation tags: InstanceType: &quot;spark-compute-optimized&quot; # optional, add tags for your own use  Spark Jobs can use this provisioner to submit the jobs by adding tolerations to pod templates. e.g., spec: tolerations: - key: &quot;spark-compute-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;  Karpenter provisioner for memory optimized instances. This template uses the AWS Node template with Userdata. apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: spark-memory-optimized namespace: karpenter spec: kubeletConfiguration: containerRuntime: containerd # podsPerCore: 2 # maxPods: 20 requirements: - key: &quot;topology.kubernetes.io/zone&quot; operator: In values: [${azs}b] #Update the correct region and zone - key: &quot;karpenter.sh/capacity-type&quot; operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] - key: &quot;node.kubernetes.io/instance-type&quot; #If not included, all instance types are considered operator: In values: [&quot;r5d.4xlarge&quot;,&quot;r5d.8xlarge&quot;,&quot;r5d.8xlarge&quot;] # 2 NVMe disk - key: &quot;kubernetes.io/arch&quot; operator: In values: [&quot;amd64&quot;] limits: resources: cpu: 1000 providerRef: # optional, recommended to use instead of `provider` name: spark-memory-optimized labels: type: karpenter provisioner: spark-memory-optimized NodeGroupType: SparkMemoryOptimized taints: - key: spark-memory-optimized value: 'true' effect: NoSchedule ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: spark-memory-optimized namespace: karpenter spec: blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 200Gi volumeType: gp3 encrypted: true deleteOnTermination: true metadataOptions: httpEndpoint: enabled httpProtocolIPv6: disabled httpPutResponseHopLimit: 2 httpTokens: required subnetSelector: Name: &quot;${eks_cluster_id}-private*&quot; # Name of the Subnets to spin up the nodes securityGroupSelector: # required, when not using launchTemplate Name: &quot;${eks_cluster_id}-node*&quot; # name of the SecurityGroup to be used with Nodes instanceProfile: &quot;${instance_profile}&quot; # optional, if already set in controller args userData: | MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=&quot;BOUNDARY&quot; --BOUNDARY Content-Type: text/x-shellscript; charset=&quot;us-ascii&quot; #!/bin/bash echo &quot;Running a custom user data script&quot; set -ex IDX=1 DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}') for DEV in $DEVICES do mkfs.xfs /dev/$${DEV} mkdir -p /local$${IDX} echo /dev/$${DEV} /local$${IDX} xfs defaults,noatime 1 2 &gt;&gt; /etc/fstab IDX=$(($${IDX} + 1)) done mount -a /usr/bin/chown -hR +999:+1000 /local* --BOUNDARY-- tags: InstanceType: &quot;spark-memory-optimized&quot; # optional, add tags for your own use  Spark Jobs can use this provisioner to submit the jobs by adding tolerations to pod templates. e.g., spec: tolerations: - key: &quot;spark-memory-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;  "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs using EMR on EKS with Karpenter as Autoscaler, as well as monitor job metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Creates EKS Cluster Control plane with public endpoint (for demo purpose only)One managed node group Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc. Enables EMR on EKS and creates two Data teams (emr-data-team-a, emr-data-team-b) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-a and IAM policy for emr-data-team-aAmazon Managed Prometheus workspace to remotely write metrics from Prometheus serverDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Karpetner, Metrics server with HA, CoreDNS Cluster proportional Autoscaler, Cluster Autoscaler, Prometheus Server and Node Exporter, VPA for Prometheus, AWS for FluentBit, CloudWatchMetrics for EKS "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this userguide for supported regions. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/terraform/emr-eks-karpenter terraform init  Set AWS_REGION and Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; terraform plan  This command may take between 20 and 30 minutes to create all the resources. terraform apply  Enter yes to apply. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#verify-the-resources","content":"Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus aws eks describe-cluster --name emr-eks-karpenter aws amp list-workspaces --alias amp-ws-emr-eks-karpenter  Verify EMR on EKS Namespaces emr-data-team-a and emr-data-team-b and Pod status for Prometheus, Vertical Pod Autoscaler, Metrics Server and Cluster Autoscaler. aws eks --region us-west-2 update-kubeconfig --name emr-eks-karpenter # Creates k8s config file to authenticate with EKS Cluster kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams kubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods kubectl get pods --namespace=vpa # Output shows Vertical Pod Autoscaler pods kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Execute Sample Spark job‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-sample-spark-job","content":""},{"title":"Execute the sample PySpark Job to trigger compute optimized Karpenter provisioner‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-to-trigger-compute-optimized-karpenter-provisioner","content":"The following script requires three input parameters in which EMR_VIRTUAL_CLUSTER_NAME and EMR_JOB_EXECUTION_ROLE_ARN values can be extracted from terraform apply output values.For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket to store the scripts, input and output data required to run this sample job. EMR_VIRTUAL_CLUSTER_NAME=$1 # Terraform output variable is emrcontainers_virtual_cluster_name S3_BUCKET=$2 # This script requires S3 bucket as input parameter e.g., s3://&lt;bucket-name&gt; EMR_JOB_EXECUTION_ROLE_ARN=$3 # Terraform output variable is emr_on_eks_role_arn  caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/spark/ ./compute-nytaxi-pyspark-karpenter.sh &quot;&lt;EMR_VIRTUAL_CLUSTER_NAME&gt;&quot; \\ &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; \\ &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution‚Äã kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Execute the sample PySpark Job to trigger Memory optimized Karpenter provisioner‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-to-trigger-memory-optimized-karpenter-provisioner","content":"This pattern uses the Karpenter provisioner for memory optimized instances. This template leverages the Karpenter AWS Node template with Userdata. cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/spark/ ./memory-nytaxi-pyspark-karpenter.sh &quot;&lt;EMR_VIRTUAL_CLUSTER_NAME&gt;&quot; \\ &quot;s3://&lt;ENTER-YOUR-BUCKET-NAME&gt;&quot; \\ &quot;&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;&quot;  Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution‚Äã kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"docs/amazon-emr-on-eks/emr-eks-karpenter#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Distributed Databases on EKS","type":0,"sectionRef":"#","url":"docs/distributed-databases-eks","content":"Distributed Databases on EKS Build, deploy and scale Distributed Streaming platforms on Amazon EKS.","keywords":""},{"title":"Job Schedulers","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks","content":"Job Schedulers Job scheduling is the process where tasks are executed at a scheduled time or triggered by event driven solutions. The following are the most popular job scheduling tools used with data workloads. This section provides deployment patterns for the following tools and examples to trigger Spark/ML jobs using these schedulers. Apache AirflowAmazon Managed Workflows for Apache Airflow (MWAA)Argo WorkflowPrefect","keywords":""},{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/intro","content":"Introduction Data on Amazon EKS(DoEKS) is a tool for users to build aws managed and self-managed scalable data platforms on Amazon EKS. This repo provides the following tools. Scalable deployment Infrastructure as Code(IaC) templates(e.g., Terraform and AWS CDK etc.)Best Practices for deploying Data Solutions on Amazon EKSPerformance Benchmark reportsSample Apache Spark/ML jobs and various other frameworksReference Architectures and Data blogs Architecture The diagram displays the open source data tools, k8s operators and frameworks that runs on Kubernetes covered in DoEKS. AWS Data Analytics managed services integration with Data on EKS OSS tools. Main Features üöÄ EMR on EKS üöÄ Open Source Spark on EKS üöÄ Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) üöÄ Job Schedulers (e.g., Apache Airflow, Argo Workflows) üöÄ AI/ML on Kubernetes (e.g., KubeFlow, MLFlow, Tensorflow, PyTorch etc.) üöÄ Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) üöÄ Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) Getting Started Checkout the documentation for each section to deploy infrastructure and run sample Spark/ML jobs.","keywords":""},{"title":"Amazon Managed Workflows for Apache Airflow (MWAA)","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks/aws-managed-airflow","content":"","keywords":""},{"title":"Considerations‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#considerations","content":"Ideally we recommend adding the steps to sync requirements/sync dags to the MWAA S3 Bucket as part of a CI/CD pipeline. Generally Dags development have a different lifecycle than the Terraform code to provision infrastructure. For simplicity, we are providing steps for that using Terraform running AWS CLI commands on null_resource. "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#deploy","content":"To provision this example: git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/managed-airflow-mwaa terraform init terraform apply -var region=us-west-2 # Change according to your region  Enter yes at command prompt to apply Once done, you will see terraform output like below.  The following components are provisioned in your environment: A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server and cluster autoscalerA MWAA environment in version 2.2.2An EMR virtual cluster registered with the newly created EKSA S3 bucket with DAG code "},{"title":"Validate‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment. "},{"title":"Run update-kubeconfig command‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#run-update-kubeconfig-command","content":"aws eks --region us-west-2 update-kubeconfig --name managed-airflow-mwaa  "},{"title":"List the nodes‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#list-the-nodes","content":"kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-0-0-42.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-22-71.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-44-63.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326  "},{"title":"List the namespaces in EKS cluster‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#list-the-namespaces-in-eks-cluster","content":"kubectl get ns # Output should look like below default Active 4h38m emr-mwaa Active 4h34m kube-node-lease Active 4h39m kube-public Active 4h39m kube-system Active 4h39m mwaa Active 4h30m  namespace emr-mwaa will be used by EMR for running spark jobs. namespace mwaa will be used by MWAA directly. "},{"title":"Trigger jobs from MWAA‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#trigger-jobs-from-mwaa","content":""},{"title":"Log into Apache Airflow UI‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#log-into-apache-airflow-ui","content":"Open the Environments page on the Amazon MWAA consoleChoose an environmentUnder the Details section, click the link for the Airflow UI  Note: You will see red error message once login. That is because the EMR connection has not been setup. The message will be gone after following the steps below to set up the connection and login again. "},{"title":"Trigger the DAG workflow to execute job in EMR on EKS‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-emr-on-eks","content":"First, you need to set up the connection to EMR virtual cluster in MWAA  Click Add button, Make sure use emr_eks as Connection Id Amazon Web Services as Connection Type Replace the value in Extra based on your terraform output {&quot;virtual_cluster_id&quot;:&quot;&lt;emrcontainers_virtual_cluster_id in terraform output&gt;&quot;, &quot;job_role_arn&quot;:&quot;&lt;emr_on_eks_role_arn in terraform output&gt;&quot;}  Go back to Airflow UI main page, enable the example DAG emr_eks_pi_job and then trigger the job.  While it is running, use the following command to verify the spark jobs: kubectl get all -n emr-mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE pod/000000030tk2ihdmr8g-psstj 3/3 Running 0 90s pod/pythonpi-a8051f83b415c911-exec-1 2/2 Running 0 14s pod/pythonpi-a8051f83b415c911-exec-2 2/2 Running 0 14s pod/spark-000000030tk2ihdmr8g-driver 2/2 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/spark-000000030tk2ihdmr8g-ee64be83b4151dd5-driver-svc ClusterIP None &lt;none&gt; 7078/TCP,7079/TCP,4040/TCP 57s NAME COMPLETIONS DURATION AGE job.batch/000000030tk2ihdmr8g 0/1 92s 92s  You can also check the job status in Amazon EMR console. Under the Virtual clusters section, click on Virtual cluster  "},{"title":"Trigger the DAG workflow to execute job in EKS‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-eks","content":"In the Airflow UI, enable the example DAG kubernetes_pod_example and then trigger it.   Verify that the pod was executed successfully After it runs and completes successfully, use the following command to verify the pod: kubectl get pods -n mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE mwaa-pod-test.4bed823d645844bc8e6899fd858f119d 0/1 Completed 0 25s  "},{"title":"Destroy‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"docs/job-schedulers-eks/aws-managed-airflow#destroy","content":"To teardown and remove the resources created in this example: terraform destroy -auto-approve   "},{"title":"Self-managed Apache Airflow deployment for EKS","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks/self-managed-airflow","content":"","keywords":""},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/schedulers/terraform/self-managed-airflow terraform init  Set AWS_REGION and Run terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; terraform plan  Deploy the pattern terraform apply  Enter yes to apply. info Rerun terraform apply if your execution timed out. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-resources","content":""},{"title":"Create kubectl config‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#create-kubectl-config","content":"aws eks --region us-west-2 update-kubeconfig --name self-managed-airflow  "},{"title":"Describe the EKS Cluster‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#describe-the-eks-cluster","content":"aws eks describe-cluster --name self-managed-airflow  "},{"title":"Verify the EFS PV and PVC created by this deployment‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-efs-pv-and-pvc-created-by-this-deployment","content":"kubectl get pvc -n airflow NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE airflow-dags Bound pvc-157cc724-06d7-4171-a14d-something 10Gi RWX efs-sc 73m kubectl get pv -n airflow NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-157cc724-06d7-4171-a14d-something 10Gi RWX Delete Bound airflow/airflow-dags efs-sc 74m  "},{"title":"Verify the EFS Filesystem‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-efs-filesystem","content":"aws efs describe-file-systems --query &quot;FileSystems[*].FileSystemId&quot; --output text  "},{"title":"Verify S3 bucket created for Airflow logs‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-s3-bucket-created-for-airflow-logs","content":"aws s3 ls | grep airflow-logs-  "},{"title":"Verify the Airflow deployment‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#verify-the-airflow-deployment","content":"kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1/1 1 1 77m airflow-scheduler 2/2 2 2 77m airflow-statsd 1/1 1 1 77m airflow-triggerer 1/1 1 1 77m airflow-webserver 2/2 2 2 77m  "},{"title":"Fetch Postgres RDS password‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#fetch-postgres-rds-password","content":"Amazon Postgres RDS database password can be fetched from the Secrets manager Login to AWS console and open secrets managerClick on postgres secret nameClick on Retrieve secret value button to verify the Postgres DB master password "},{"title":"Login to Airflow Web UI‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#login-to-airflow-web-ui","content":"This deployment creates an Ingress object with public LoadBalancer(internet-facing) for demo purpose For production workloads, you can modify values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port. Execute the following command to get the ALB DNS name kubectl get ingress -n airflow NAME CLASS HOSTS ADDRESS PORTS AGE airflow-airflow-ingress alb * k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com 80 88m  The above ALB URL will be different for you deployment. So use your URL and open it in a brower e.g., Open URL http://k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com/ in a browser By default, Airflow creates a default user with admin and password as admin Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user "},{"title":"Create S3 Connection from Airflow Web UI‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#create-s3-connection-from-airflow-web-ui","content":"This step is critical for writing the Airflow logs to S3 bucket. Login to Airflow WebUI with admin and password as admin using ALB URLSelect Admin dropdown and Click on ConnectionsClick on &quot;+&quot; button to add a new recordEnter Connection Id as aws_s3_conn, Connection Type as Amazon Web Services and Extra as {&quot;region_name&quot;: &quot;&lt;ENTER_YOUR_REGION&gt;&quot;}Click on Save button  "},{"title":"Execute Sample Airflow Job‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#execute-sample-airflow-job","content":"Login to Airflow WebUIClick on DAGs link on the top of the page. This will show two dags pre-created by the GitSync featureExecute the first DAG by clicking on Play button (&gt;)Verify the DAG execution from Graph linkAll the Tasks will go green after few minutesClick on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3 "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"docs/job-schedulers-eks/self-managed-airflow#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.db&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  Make sure all the S3 buckets are empty and deleted once your test is finished  "},{"title":"Argo Workflows on EKS","type":0,"sectionRef":"#","url":"docs/job-schedulers-eks/argo-workflows-eks","content":"","keywords":""},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraformArgo WorkflowCLI "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#deploy","content":"To provision this example: git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/argo-workflow terraform init terraform apply -var region=us-west-2 # Change according to your need  Enter yes at command prompt to apply The following components are provisioned in your environment: A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server, cluster autoscaler, Spark Operator and yunikorn schedulerK8s roles and rolebindings for argo workflows and argo events  "},{"title":"Validate‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment. "},{"title":"Run update-kubeconfig command:‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#run-update-kubeconfig-command","content":"aws eks --region us-west-2 update-kubeconfig --name argoworkflows-eks  "},{"title":"List the nodes‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#list-the-nodes","content":"kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-1-131-99.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326 ip-10-1-16-117.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326 ip-10-1-80-41.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326  "},{"title":"List the namespaces in EKS cluster‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#list-the-namespaces-in-eks-cluster","content":"kubectl get ns # Output should look like below NAME STATUS AGE argo-events Active 28h argo-workflows Active 28h default Active 30h kube-node-lease Active 30h kube-public Active 30h kube-system Active 30h spark-operator Active 30h yunikorn Active 30h  "},{"title":"Access Argo Workflow WebUI‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#access-argo-workflow-webui","content":"kubectl -n argo-workflows port-forward deployment.apps/argo-workflows-server 2746:2746 argo auth token # get login token # result: Bearer k8s-aws-v1.aHR0cHM6Ly9zdHMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vP0FjdGlvbj1HZXRDYWxsZXJJZGVudGl0eSZWZXJzaW9uPTIwMTEtMDYtMTUmWC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNWNFhDV1dLUjZGVTRGMiUyRjIwMjIxMDEzJTJGdXMtd2VzdC0yJTJGc3RzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMjEwMTNUMDIyODAyWiZYLUFtei1FeHBpcmVzPTYwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCUzQngtazhzLWF3cy1pZCZYLUFtei1TaWduYXR1cmU9NmZiNmMxYmQ0MDQyMWIwNTI3NjY4MzZhMGJiNmUzNjg1MTk1YmM0NDQzMjIyMTg5ZDNmZmE1YzJjZmRiMjc4OA  Open browser and enter http://localhost:2746/ and paste the token  "},{"title":"Submit Spark Job with Argo Workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#submit-spark-job-with-argo-workflow","content":"Modify workflow-example/argo-spark.yaml with your eks api server url kubectl apply -f workflow-example/argo-spark.yaml kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Running 8s  You can also check the workflow status from Web UI  "},{"title":"Submit Spark Job with Spark Operator and Argo Workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#submit-spark-job-with-spark-operator-and-argo-workflow","content":"kubectl apply -f workflow-example/argo-spark-operator.yaml kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Succeeded 3m58s spark-operator Running 5s  The workflow status from web UI  "},{"title":"Trigger a workflow to create a spark job based on SQS message‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#trigger-a-workflow-to-create-a-spark-job-based-on-sqs-message","content":""},{"title":"Install argo events controllers‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#install-argo-events-controllers","content":"kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml  "},{"title":"Install eventbus which is for event transmission in argo events‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#install-eventbus-which-is-for-event-transmission-in-argo-events","content":"kubectl apply -f argo-events/eventbus.yaml  "},{"title":"Deploy eventsource-sqs.yaml to link with external SQS‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#deploy-eventsource-sqsyaml-to-link-with-external-sqs","content":"kubectl apply -f argo-events/eventsource-sqs.yaml  In this case, we configure a EventSource to license to the queue test1 in region us-east-1. Let's create that queue in your account if you don't have one. # create a queue aws sqs create-queue --queue-name test1 --region us-east-1 # get your queue arn aws sqs get-queue-attributes --queue-url &lt;your queue url&gt; --attribute-names QueueArn #Replace the following values in argo-events/sqs-accesspolicy.json #&lt;your queue arn&gt; #&lt;your event irsa arn&gt; (you can get from terraform output) aws sqs set-queue-attributes --queue-url &lt;your queue url&gt; --attributes file://argo-events/sqs-accesspolicy.json --region us-east-1  "},{"title":"Deploy sensor-rbac.yaml and sensor-sqs-spark-crossns.yaml for triggering workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#deploy-sensor-rbacyaml-and-sensor-sqs-spark-crossnsyaml-for-triggering-workflow","content":"kubectl apply -f argo-events/sensor-rbac.yaml kubectl apply -f argo-events/sensor-sqs-sparkjobs.yaml  "},{"title":"Verify argo-events namespace‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#verify-argo-events-namespace","content":"kubectl get all,eventbus,EventSource,sensor,sa,role,rolebinding -n argo-events   "},{"title":"Test from SQS‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#test-from-sqs","content":"Send a message from SQS: {&quot;message&quot;: &quot;hello&quot;}  Argo Events would capture the message and trigger Argo Workflows to create a workflow for spark jobs. kubectl get wf -A NAMESPACE NAME STATUS AGE MESSAGE argo-workflows aws-sqs-spark-workflow-p57qx Running 9s  "},{"title":"Destroy‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"docs/job-schedulers-eks/argo-workflows-eks#destroy","content":"To teardown and remove the resources created in this example: terraform destroy -auto-approve  "},{"title":"Apache Spark on EKS","type":0,"sectionRef":"#","url":"docs/spark-on-eks","content":"Apache Spark on EKS Build, deploy and scale Open source Spark on Amazon EKS. This section also provides Spark examples with various storage, compute types, Cluster Autoscalers and Batch Schedulers. Spark OperatorSpark SubmitKarpenterApache YuniKornVolcano","keywords":""},{"title":"Streaming Platforms on EKS","type":0,"sectionRef":"#","url":"docs/streaming-platforms-eks","content":"Streaming Platforms on EKS info Please note that Streaming platform work is currently in progress. Docs will be updated once the deployment example is added to this repo. The following streaming platforms can be deployed and scale on Amazon EKS. KafkaFlinkTrino info Presto SQL is now Trino","keywords":""},{"title":"Running Spark jobs with Spark Operator and YuniKorn","type":0,"sectionRef":"#","url":"docs/spark-on-eks/spark-operator-yunikorn","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#introduction","content":"In this post, we will learn to build, configure and deploy highly scalable EKS Cluster with Open source Spark Operator and Apache YuniKorn batch scheduler. "},{"title":"Spark Operator‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#spark-operator","content":"The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. a SparkApplication controller that watches events of creation, updates, and deletion of SparkApplication objects and acts on the watch events,a submission runner that runs spark-submit for submissions received from the controller,a Spark pod monitor that watches for Spark pods and sends pod status updates to the controller,a Mutating Admission Webhook that handles customizations for Spark driver and executor pods based on the annotations on the pods added by the controller,and also a command-line tool named sparkctl for working with the operator. The following diagram shows how different components of Spark Operator add-pn interact and work together.  "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#deploying-the-solution","content":"In this example, you will provision the following resources required to run Spark Jobs with open source Spark Operator and Apache YuniKorn. This example deploys an EKS Cluster running the Spark K8s Operator into a new VPC. Creates a new sample VPC, 3 Private Subnets and 3 Public SubnetsCreates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with one managed node groupDeploys Metrics server, Cluster Autoscaler, Spark-k8s-operator, Apache Yunikorn and Prometheus server.Spark Operator is a Kubernetes Operator for Apache Spark deployed to spark-operator namespace. The operator by default watches and handles SparkApplications in all namespaces. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/terraform/spark-k8s-operator terraform init  Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Change according to your needs terraform plan  Deploy the pattern terraform apply  Enter yes to apply. "},{"title":"Sample Spark Job with Spark Operator‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#sample-spark-job-with-spark-operator","content":"Execute sample PySpark Pi job.  cd analytics/terraform/spark-k8s-operator/examples kubectl apply -f pyspark-pi-job.yaml  Verify the Spark job status  kubectl get sparkapplications -n spark-team-a kubectl describe sparkapplication pyspark-pi -n spark-team-a  "},{"title":"NVMe Ephemeral SSD disk for Spark shuffle storage‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#nvme-ephemeral-ssd-disk-for-spark-shuffle-storage","content":"Example PySpark job that uses NVMe based ephemeral SSD disk for Driver and Executor shuffle storage  cd analytics/terraform/spark-k8s-operator/examples/nvme-ephemeral-storage  Update the variables in Shell script and execute  ./taxi-trip-execute.sh  Update YAML file and run the below command  kubectl apply -f nvme-ephemeral-storage.yaml  "},{"title":"EBS Dynamic PVC for shuffle storage‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#ebs-dynamic-pvc-for-shuffle-storage","content":"Example PySpark job that uses EBS ON_DEMAND volumes using Dynamic PVCs for Driver and Executor shuffle storage  cd analytics/terraform/spark-k8s-operator/examples/nvme-ephemeral-storage  Update the variables in Shell script and execute  ./taxi-trip-execute.sh  Update YAML file and run the below command  kubectl apply -f nvme-ephemeral-storage.yaml  "},{"title":"Apache YuniKorn Gang Scheduling with NVMe based SSD disk for shuffle storage‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#apache-yunikorn-gang-scheduling-with-nvme-based-ssd-disk-for-shuffle-storage","content":"Gang Scheduling Spark jobs using Apache YuniKorn and Spark Operator  cd analytics/terraform/spark-k8s-operator/examples/nvme-yunikorn-gang-scheduling  Update the variables in Shell script and execute  ./taxi-trip-execute.sh  Update YAML file and run the below command  kubectl apply -f nvme-yunikorn-gang-scheduling.yaml  "},{"title":"Example for TPCDS Benchmark test‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#example-for-tpcds-benchmark-test","content":"Check the pre-requisites in yaml file before running this job. cd analytics/terraform/spark-k8s-operator/examples/benchmark  Step1: Benchmark test data generation kubectl apply -f tpcds-benchmark-data-generation-1t  Step2: Execute Benchmark test  kubectl apply -f tpcds-benchmark-1t.yaml  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Running Spark jobs with Spark Operator and YuniKorn","url":"docs/spark-on-eks/spark-operator-yunikorn#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"docs/streaming-platforms-eks/kafka","content":"","keywords":""},{"title":"Strimzi for Apache Kafka‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#strimzi-for-apache-kafka","content":"Strimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. Strimzi combines security and simple configuration to deploy and manage Kafka on Kubernetes using kubectl and/or GitOps based on the Operator Pattern. "},{"title":"Architecture‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#architecture","content":"info Architecture diagram work in progress "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#deploying-the-solution","content":"In this example, you will provision the following resources to run Kafka Cluster on EKS. This example deploys an EKS Cluster with Kafka into a new VPC. Creates a new sample VPC, 3 Private Subnets and 3 Public Subnets.Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets.Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with two managed node groups.Deploys Metrics server, Cluster Autoscaler, self-managed ebs-csi-driver, Strimzi Kafka Operator, Grafana Operator.Strimzi Kafka Operator is a Kubernetes Operator for Apache Kafka deployed to strimzi-kafka-operator namespace. The operator by default watches and handles kafka in all namespaces. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/streaming/kafka terraform init  Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Select your own region terraform plan  Deploy the pattern terraform apply  Enter yes to apply. info This deployment may take between 20 to 30mins. "},{"title":"Verify the deployment‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#verify-the-deployment","content":""},{"title":"Create kube config‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#create-kube-config","content":"Create kube config file. aws eks --region us-west-2 update-kubeconfig --name kafka-on-eks  "},{"title":"Get nodes‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#get-nodes","content":"Check if the deployment has created 6 nodes. 3 nodes for Core Node group and 3 for Kafka brokers across 3 AZs. kubectl get nodes  Output NAME STATUS ROLES AGE VERSION ip-10-0-10-36.us-west-2.compute.internal Ready &lt;none&gt; 5h28m v1.24.7-eks-fb459a0 ip-10-0-10-47.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-11-218.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-11-223.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-12-202.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-12-50.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0  "},{"title":"Verify Kafka Brokers and Zookeeper‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#verify-kafka-brokers-and-zookeeper","content":"Verify the Kafka Broker and Zookeeper pods and the status created by the Strimzi Operator. kubectl get strimzipodsets.core.strimzi.io -n kafka  Output NAME PODS READY PODS CURRENT PODS AGE cluster-kafka 3 3 3 4h35m cluster-zookeeper 3 3 3 4h36m  kubectl get kafka.kafka.strimzi.io -n kafka  Output NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS READY WARNINGS cluster 3 3 True  kubectl get kafkatopic.kafka.strimzi.io -n kafka  Output NAME CLUSTER PARTITIONS REPLICATION FACTOR READY consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a cluster 50 3 True strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55 cluster 1 3 True strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b cluster 1 3 True strimzi.cruisecontrol.metrics cluster 1 3 True strimzi.cruisecontrol.modeltrainingsamples cluster 32 2 True strimzi.cruisecontrol.partitionmetricsamples cluster 32 2 True  "},{"title":"Verify the running Kafka pods‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#verify-the-running-kafka-pods","content":"kubectl get pods -n kafka  Output NAME READY STATUS RESTARTS AGE cluster-cruise-control-79f6457f8d-sm8c2 1/1 Running 0 4h40m cluster-entity-operator-5594c965ff-t9nl4 3/3 Running 0 4h40m cluster-kafka-0 1/1 Running 0 4h41m cluster-kafka-1 1/1 Running 0 4h41m cluster-kafka-2 1/1 Running 0 4h41m cluster-kafka-exporter-9dbfdff54-wx8vq 1/1 Running 0 4h39m cluster-zookeeper-0 1/1 Running 0 4h42m cluster-zookeeper-1 1/1 Running 0 4h42m cluster-zookeeper-2 1/1 Running 0 4h42m  "},{"title":"Create Kafka Topic and run Sample test‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#create-kafka-topic-and-run-sample-test","content":"We will create one kafka topic and run sample producer script to produce new messages to the kafka topic. We can then verify the data in the topic using sample consumer script. "},{"title":"Create a kafka Topic‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#create-a-kafka-topic","content":"Run this command to create a new topic called test-topic under kafka namespace cd streaming/kafka/examples/ kubectl apply -f kafka-topics.yaml  Verify the status of the test-topic topic. kubectl exec -it cluster-kafka-0 -c kafka -n kafka -- /bin/bash -c &quot;/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092&quot;  Output __consumer_offsets __strimzi-topic-operator-kstreams-topic-store-changelog __strimzi_store_topic strimzi.cruisecontrol.metrics strimzi.cruisecontrol.modeltrainingsamples strimzi.cruisecontrol.partitionmetricsamples test-topic  "},{"title":"Execute sample Kafka Producer‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#execute-sample-kafka-producer","content":"Open two terminals one for Kafka producer and one for Kafka Consumer. Execute the following command and press enter twice until you see the &gt; prompt. Start typing some random content. This data will be written to the test-topic. kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list cluster-kafka-bootstrap:9092 --topic test-topic  "},{"title":"Execute sample Kafka Consumer‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#execute-sample-kafka-consumer","content":"Now, you can verify the data written to test-topic by running Kafka consumer pod in another terminal kubectl -n kafka run kafka-consumer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server cluster-kafka-bootstrap:9092 --topic test-topic  "},{"title":"Kafka Producer and Consumer output‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#kafka-producer-and-consumer-output","content":" "},{"title":"Grafana Dashboard for Kafka‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#grafana-dashboard-for-kafka","content":""},{"title":"Login to Grafana‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#login-to-grafana","content":"Login to Grafana dashboard by running the following command. kubectl port-forward svc/grafana-service 3000:3000 -n grafana  Open browser with local Grafana Web UI Enter username as admin and password can be extracted from the below command. kubectl get secrets/grafana-admin-credentials --template={{.data.GF_SECURITY_ADMIN_PASSWORD}} -n grafana | base64 -D  "},{"title":"Open Strimzi Kafka Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#open-strimzi-kafka-dashboard","content":"The below are builtin Kafka dashboards which created during the deployment.  "},{"title":"Open Strimzi Zookeeper Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#open-strimzi-zookeeper-dashboard","content":" "},{"title":"Open Strimzi Zookeeper Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#open-strimzi-zookeeper-dashboard-1","content":"You can verify the test-topic with three partitions below.  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Apache Kafka","url":"docs/streaming-platforms-eks/kafka#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ex. Delete kafka-on-eks EBS volumes "}]