########################################
## CONFIG | Airflow Configs
########################################

# Default security context for airflow (deprecated, use `securityContexts` instead)
securityContext:
  fsGroup: 65534

###################################
# Ingress configuration
###################################
ingress:
  # Configs for the Ingress of the web Service
  web:
    # Enable web ingress resource
    enabled: true

    # Annotations for the web Ingress
    annotations:
      alb.ingress.kubernetes.io/group.name: dataengineering
      alb.ingress.kubernetes.io/target-type: instance
      alb.ingress.kubernetes.io/scheme: internal # Private Load Balancer can only be accessed within the VPC
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
      alb.ingress.kubernetes.io/healthcheck-path: '/health'
      # Enable the following if you have public/internal domain e.g., https://mycompany.com/
      # alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'
      # alb.ingress.kubernetes.io/certificate-arn: "arn:aws:acm:....................."

    # The path for the web Ingress
    path: "/"

    # The pathType for the above path (used only with Kubernetes v1.19 and above)
    pathType: "Prefix"

    # The Ingress Class for the web Ingress (used only with Kubernetes v1.19 and above)
    ingressClassName: "alb"

###################################
# Airflow executor
###################################
# One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor
executor: "KubernetesExecutor"

###################################
# Airflow database
###################################
data:
  # Otherwise pass connection values in
  metadataConnection:
    user: ${airflow_db_user}
    pass: ${airflow_db_pass}
    protocol: postgresql
    host: ${airflow_db_host}
    port: 5432
    db: ${airflow_db_name}
    sslmode: disable

###################################
# Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg
###################################
webserverSecretKeySecretName: ${webserver_secret_name}

###################################
# Airflow Worker Config
###################################
workers:
  persistence:
    # Enable persistent volumes
    enabled: false
  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${worker_service_account}

###################################
# Airflow scheduler settings
###################################
scheduler:
  # Airflow 2.0 allows users to run multiple schedulers,
  # However this feature is only recommended for MySQL 8+ and Postgres
  replicas: 2

  # Scheduler pod disruption budget
  podDisruptionBudget:
    enabled: true

    # PDB configuration
    config:
      # minAvailable and maxUnavailable are mutually exclusive
      maxUnavailable: 1

  resources:
    limits:
     cpu: 500m
     memory: 1024Mi
    requests:
     cpu: 500m
     memory: 1024Mi

  # This setting tells kubernetes that its ok to evict
  # when it wants to scale a node down.
  safeToEvict: false
  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${scheduler_service_account}

#extraVolumes is required for DAG GitSync #https://github.com/apache/airflow/issues/27476
  extraVolumes:
    - name: git-sync-ssh-key
      secret:
        secretName: airflow-ssh-secret
###################################
# Airflow webserver settings
###################################
webserver:
  # Number of webservers
  replicas: 2

  # Webserver pod disruption budget
  podDisruptionBudget:
    enabled: true

    # PDB configuration
    config:
      # minAvailable and maxUnavailable are mutually exclusive
      maxUnavailable: 1

  resources:
    limits:
      cpu: 1000m
      memory: 4Gi
    requests:
      cpu: 1000m
      memory: 4Gi

  # Create initial user.
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin

  service:
    #type: ClusterIP
    type: NodePort

  # Create ServiceAccount
  serviceAccount:
    create: false
    name: ${webserver_service_account}

###################################
# PgBouncer settings
###################################
pgbouncer:
  # Enable PgBouncer
  enabled: true

###################################
# Config settings to go into the mounted airflow.cfg
###################################
config:
  core:
    dags_folder: '{{ include "airflow_dags" . }}'
    # This is ignored when used with the official Docker image
    load_examples: 'True'
    executor: '{{ .Values.executor }}'
    remote_logging: 'True'
  logging:
    remote_logging: 'True'
    logging_level: 'INFO'
    colored_console_log: 'True'
    remote_base_log_folder: "s3://${s3_bucket_name}/airflow-logs"
    # aws_s3_conn is the name of the connection that needs to be created using Airflow admin UI once the deployment is complete
    # Steps can be seen in the docs link here -> https://github.com/apache/airflow/issues/25322
    remote_log_conn_id: 'aws_s3_conn'
    delete_worker_pods: 'False'
    encrypt_s3_logs: 'True'


###################################
# Git sync
###################################
dags:
  persistence:
    # Enable persistent volume for storing dags
    enabled: true
    # Volume size for dags
    size: 10Gi
    # If using a custom storageClass, pass name here
    storageClassName: efs-sc
    # access mode of the persistent volume
    accessMode: ReadWriteOnce
    ## the name of an existing PVC to use
    existingClaim: ${efs_pvc}

  gitSync:
    enabled: true

    # git repo clone url
    # Replace repo with a valid GitHub repository
    repo: git@github.com:xyz/airflow-dags.git
    branch: main
    rev: HEAD
    depth: 1

    # the number of consecutive failures allowed before aborting
    maxFailures: 5
    # subpath within the repo where dags are located
    # should be "" if dags are at repo root
    subPath: "dags"

    # If you are using an ssh clone url, you can specify the name of the secret below
    sshKeySecret: airflow-ssh-secret

    # If you are using an ssh private key, you can additionally
    # specify the content of your known_hosts file, example:
    #https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/githubs-ssh-key-fingerprints
    #https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#knownhosts
    # Replace knownHosts  with a valid values
    knownHosts: |
      github.com ssh-ed25519 AAAA
      github.com ecdsa-sha2-nistp256 AAAAE
      github.com ssh-rsa AAAAB

###################################
# Airflow Extra Secrets
###################################
# https://airflow.apache.org/docs/helm-chart/stable/manage-dags-files.html#mounting-dags-from-a-private-github-repo-using-git-sync-sidecar
# Replace gitSshKey: 'ABC' with a valid Private SSH Key
extraSecrets:
  airflow-ssh-secret:
    data: |
      gitSshKey: 'ABC'
