<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-gen-ai/training/Neuron/RayTrain-Llama2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">RayTrain-Llama2 | Data on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="RayTrain-Llama2 | Data on EKS"><meta data-rh="true" name="description" content="Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue."><meta data-rh="true" property="og:description" content="Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue."><link data-rh="true" rel="icon" href="/data-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2" hreflang="x-default"><link rel="stylesheet" href="/data-on-eks/assets/css/styles.74de95c8.css">
<script src="/data-on-eks/assets/js/runtime~main.d52a0fda.js" defer="defer"></script>
<script src="/data-on-eks/assets/js/main.cb7f2ea4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-on-eks/"><div class="navbar__logo"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/data-on-eks/docs/introduction/intro">Introduction</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-on-eks/docs/gen-ai">Gen AI</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/blueprints/amazon-emr-on-eks">Blueprints</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/bestpractices/intro">Best Practices</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/benchmarks/emr-on-eks">Benchmarks</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/resources/intro">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-on-eks/docs/gen-ai">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Expand sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/data-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Collapse sidebar category &#x27;Training on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/data-on-eks/docs/gen-ai/training/GPUs/bionemo">GPUs</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2">Neuron</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-on-eks/docs/gen-ai/training/Neuron/RayTrain-Llama2">Llama-2 with RayTrain on Trn1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/training/Neuron/Llama2">Llama-2 with Nemo-Megatron on Trn1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/training/Neuron/BERT-Large">BERT-Large on Trainium</a></li></ul></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/data-on-eks/docs/category/training-on-eks"><span itemprop="name">Training on EKS</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Neuron</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Llama-2 with RayTrain on Trn1</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_BuS1"><p>Note: Use of this Llama-2 model is governed by the Meta license.
In order to download the model weights and tokenizer, please visit the <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">website</a> and accept the license before requesting access.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects.</p></div></div>
<header><h1>Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay</h1></header>
<p>This comprehensive guide walks you through pre-training the <code>Llama2-7B</code> language model using AWS Trainium (Trn1) instances and the AWS Neuron SDK within a KubeRay cluster on Amazon EKS. This is a tailored version of the original <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tutorials/training_llama2_7b.html#llama2-7b-tp-zero1-tutorial" target="_blank" rel="noopener noreferrer">Llama2 pretraining tutorial</a> optimized for KubeRay&#x27;s distributed training capabilities.</p>
<p><img decoding="async" loading="lazy" alt="Llama2-RayTrain" src="/data-on-eks/assets/images/Llama2-RayTrain-Trn1-d93433eba0c8e8e7c617a9fa1b17f249.png" width="14330" height="7927" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-llama-2">What is Llama-2?<a href="#what-is-llama-2" class="hash-link" aria-label="Direct link to What is Llama-2?" title="Direct link to What is Llama-2?">​</a></h3>
<p>Llama-2 is a state-of-the-art large language model (LLM) designed for various natural language processing (NLP) tasks, including text generation, summarization, translation, question answering, and more. It&#x27;s a powerful tool that can be fine-tuned for specific use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-raytrain-and-kuberay-for-distributed-training">Why RayTrain and KubeRay for Distributed Training?<a href="#why-raytrain-and-kuberay-for-distributed-training" class="hash-link" aria-label="Direct link to Why RayTrain and KubeRay for Distributed Training?" title="Direct link to Why RayTrain and KubeRay for Distributed Training?">​</a></h3>
<p>Distributed training is essential for large models like Llama-2 due to their extensive computational and memory requirements. The combination of RayTrain and KubeRay, especially when leveraged with AWS Trainium, provides a robust framework for handling these demands efficiently and effectively:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="raytrain">RayTrain:<a href="#raytrain" class="hash-link" aria-label="Direct link to RayTrain:" title="Direct link to RayTrain:">​</a></h4>
<ul>
<li><strong>Simplified Distributed Training</strong>: RayTrain is a high-level library built on the Ray framework that abstracts the complexities of distributed training. It allows you to scale your Llama-2 training across multiple nodes with minimal code changes. Ray&#x27;s actor-based architecture and task-based parallelism enable efficient execution of distributed workloads.</li>
<li><strong>Flexible Strategies</strong>: RayTrain supports various distributed training strategies such as data parallelism and model parallelism. Data parallelism splits the dataset across multiple nodes, while model parallelism splits the model itself. This flexibility allows you to optimize training based on the specific needs of your model and the architecture of your training environment.</li>
<li><strong>Fault Tolerance</strong>: RayTrain includes built-in fault tolerance mechanisms. If a node fails, Ray can reschedule the tasks on other available nodes, ensuring that the training job continues without interruption. This feature is crucial for maintaining robustness in large-scale distributed training environments.</li>
<li><strong>Ease of Use</strong>: RayTrain offers intuitive APIs that simplify the setup and execution of distributed training jobs. Integrations with popular machine learning libraries like Hugging Face Transformers make it easier to incorporate RayTrain into your existing workflows without extensive modifications.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="kuberay">KubeRay:<a href="#kuberay" class="hash-link" aria-label="Direct link to KubeRay:" title="Direct link to KubeRay:">​</a></h4>
<ul>
<li><strong>Integration with Kubernetes</strong>: KubeRay leverages Kubernetes&#x27; native capabilities to deploy, manage, and scale Ray clusters. This integration allows you to use Kubernetes&#x27; robust orchestration features to handle Ray workloads effectively.</li>
<li><strong>Dynamic Scaling</strong>: KubeRay supports dynamic scaling of Ray clusters. Ray&#x27;s built-in autoscaler can request additional actor replicas based on workload demands, while Kubernetes tools like Karpenter or Cluster Autoscaler manage the creation of new nodes to meet these demands.</li>
<li><strong>Horizontal Scaling</strong>: Scale your Ray clusters horizontally by adding more worker nodes as the computational load increases. This allows efficient handling of large-scale distributed training and inference tasks.</li>
<li><strong>Custom Resource Definitions (CRDs)</strong>: KubeRay utilizes Kubernetes CRDs to define and manage Ray clusters and jobs. This provides a standardized way to handle Ray workloads within the Kubernetes ecosystem.</li>
<li><strong>Fault Tolerance</strong>: KubeRay takes advantage of Kubernetes&#x27; self-healing capabilities. If a Ray head node or worker node fails, Kubernetes automatically restarts the failed components, ensuring minimal downtime and continuous operation.</li>
<li><strong>Distributed Scheduling</strong>: Ray&#x27;s actor-based model and distributed task scheduling, combined with Kubernetes&#x27; orchestration, ensure high availability and efficient task execution even in the event of node failures.</li>
<li><strong>Declarative Configuration</strong>: KubeRay allows you to define Ray clusters and jobs using declarative YAML configurations. This simplifies the deployment and management process, making it easier to set up and maintain Ray clusters.</li>
<li><strong>Integrated Logging and Monitoring</strong>: KubeRay integrates with Kubernetes&#x27; logging and monitoring tools, such as Prometheus and Grafana. This provides comprehensive insights into the performance and health of Ray clusters, facilitating easier debugging and optimization.</li>
<li><strong>Spot Instances</strong>: Use Kubernetes&#x27; support for spot instances to run Ray clusters cost-effectively. This allows you to take advantage of lower-cost compute resources while maintaining the ability to scale as needed.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="aws-trainium">AWS Trainium:<a href="#aws-trainium" class="hash-link" aria-label="Direct link to AWS Trainium:" title="Direct link to AWS Trainium:">​</a></h4>
<ul>
<li><strong>Optimized for Deep Learning</strong>: AWS Trainium-based Trn1 instances are specifically designed for deep learning workloads. They offer high throughput and low latency, making them ideal for training large-scale models like Llama-2. Trainium chips provide significant performance improvements over traditional processors, accelerating training times.</li>
<li><strong>Neuron SDK</strong>: The AWS Neuron SDK is tailored to optimize your deep learning models for Trainium. It includes features like advanced compiler optimizations and support for mixed precision training, which can further accelerate your training workloads while maintaining accuracy.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-this-combination-is-powerful">Why This Combination is Powerful<a href="#why-this-combination-is-powerful" class="hash-link" aria-label="Direct link to Why This Combination is Powerful" title="Direct link to Why This Combination is Powerful">​</a></h3>
<ul>
<li><strong>Simplified Scaling</strong>: RayTrain and KubeRay simplify the process of scaling Llama-2 training across multiple nodes. With Ray&#x27;s efficient distributed execution and KubeRay&#x27;s Kubernetes-native orchestration, you can easily scale your training workloads to leverage the full power of AWS Trainium on Trn1 instances.</li>
<li><strong>Optimized Performance</strong>: The Neuron SDK enhances the performance of your training jobs by optimizing them specifically for Trainium&#x27;s architecture. Combined with Ray&#x27;s ability to efficiently manage distributed tasks and KubeRay&#x27;s resource orchestration, this setup ensures optimal training performance.</li>
<li><strong>Cost-Effective</strong>: Ray&#x27;s autoscaling capabilities and Kubernetes&#x27; resource management help you optimize costs by efficiently allocating resources and scaling your cluster based on demand. This ensures you only use the resources you need, reducing unnecessary expenditure.</li>
</ul>
<p>By using this combination of technologies, you can take advantage of the latest advancements in distributed training and hardware to pre-train Llama-2 efficiently and effectively.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-volcano">What is Volcano?<a href="#what-is-volcano" class="hash-link" aria-label="Direct link to What is Volcano?" title="Direct link to What is Volcano?">​</a></h3>
<p>Volcano is an open-source batch scheduling system built on Kubernetes, specifically designed to manage high-performance computing (HPC) and machine learning workloads. It provides advanced scheduling capabilities such as gang scheduling, fair sharing, and preemption, which are essential for efficiently running large-scale, distributed training jobs in a Kubernetes environment.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-volcano-works-with-gang-scheduling">How Volcano Works with Gang Scheduling<a href="#how-volcano-works-with-gang-scheduling" class="hash-link" aria-label="Direct link to How Volcano Works with Gang Scheduling" title="Direct link to How Volcano Works with Gang Scheduling">​</a></h3>
<p>Volcano&#x27;s gang scheduling ensures that all pods in a job (or &quot;gang&quot;) are scheduled simultaneously. This is critical for distributed training workloads where multiple pods need to start together to function correctly. If even one pod in the gang cannot be scheduled due to resource constraints, none of the pods in the gang will start. This prevents partial execution and ensures that all resources required for the job are available before execution begins.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-deploying-the-solution">1. Deploying the Solution<a href="#1-deploying-the-solution" class="hash-link" aria-label="Direct link to 1. Deploying the Solution" title="Direct link to 1. Deploying the Solution">​</a></h2>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-build-the-docker-image-optional-step">2. Build the Docker Image (Optional Step)<a href="#2-build-the-docker-image-optional-step" class="hash-link" aria-label="Direct link to 2. Build the Docker Image (Optional Step)" title="Direct link to 2. Build the Docker Image (Optional Step)">​</a></h2>
<p>To simplify the blueprint deployment, we have already built the Docker image and made it available under the public ECR.
If you want to customize the Docker image, you can update the <code>Dockerfile</code> and follow the optional step to build the Docker image.
Please note that you will also need to modify the RayCluster YAML file, <code>llama2-pretrain-trn1-raycluster.yaml</code>, with the newly created image using your own private ECR.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd gen-ai/training/raytrain-llama2-pretrain-trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./kuberay-trn1-llama2-pretrain-build-image.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>After running this script, note the Docker image URL and tag that are produced.
You will need this information for the next step.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-launch-the-ray-cluster-with-kuberay-operator">3. Launch the Ray Cluster with KubeRay Operator<a href="#3-launch-the-ray-cluster-with-kuberay-operator" class="hash-link" aria-label="Direct link to 3. Launch the Ray Cluster with KubeRay Operator" title="Direct link to 3. Launch the Ray Cluster with KubeRay Operator">​</a></h2>
<p>If you skip step 2, you don&#x27;t need to modify the YAML file.
You can simply run the <code>kubectl apply</code> command on the file, and it will use the public ECR image that we published.</p>
<p>If you built a custom Docker image in <strong>Step 2</strong>, update the <code>gen-ai/training/raytrain-llama2-pretrain-trn1/llama2-pretrain-trn1-raycluster.yaml</code> file with the Docker image URL and tag obtained from the previous step.</p>
<p>Once you have updated the YAML file (if needed), run the following command to launch the KubeRay cluster pods in your EKS cluster:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f llama2-pretrain-trn1-raycluster.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Verify the Pod Status:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods -l &quot;ray.io/cluster=kuberay-trn1&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gang-scheduling-of-ray-head-and-worker-pods-with-volcano">Gang Scheduling of Ray Head and Worker Pods with Volcano<a href="#gang-scheduling-of-ray-head-and-worker-pods-with-volcano" class="hash-link" aria-label="Direct link to Gang Scheduling of Ray Head and Worker Pods with Volcano" title="Direct link to Gang Scheduling of Ray Head and Worker Pods with Volcano">​</a></h3>
<p>In the context of deploying a Ray cluster for training Llama2, Volcano is crucial for ensuring that the Ray head and worker pods are scheduled together efficiently.
The Ray head pod, typically running on an x86 instance, coordinates the distributed training, while the worker pods, running on AWS Trainium (Trn1) instances, perform the computationally intensive tasks.
By leveraging <strong>Volcano&#x27;s gang scheduling</strong>, we can ensure that the head and all worker pods are allocated the necessary resources simultaneously, enabling the distributed training job to start without delays.</p>
<p>Here&#x27;s an example configuration for integrating Volcano with a RayCluster for Llama2 training:</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>We are using the default namespace for this deployment because the <code>fsx-claim</code> <strong>PVC</strong> is created under the <code>default</code> namespace by the Terraform blueprint.</p><p>If you want to deploy the cluster in a dedicated namespace, ensure that the FSX for Lustre file system is also created in the same namespace since PVCs are namespace-bound.</p></div></div>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Docs for Volcano with KubeRay: https://docs.ray.io/en/master/cluster/kubernetes/k8s-ecosystem/volcano.html</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">---</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> scheduling.volcano.sh/v1beta1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Queue</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">training</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">queue</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">weight</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">capability</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">cpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;500&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">memory</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> 1500Gi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">---</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ray.io/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> RayCluster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> kuberay</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">labels</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">ray.io/scheduler-name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> volcano</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">volcano.sh/queue-name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">training</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">queue</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">rayVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> 2.22.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">headGroupSpec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">...</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">...</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should see one ray-head pod and two ray-worker pods in the Running state:</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Please be aware that it may take up to 10 minutes for the image to be pulled and the pod to become ready.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                    READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kuberay-trn1-head-67t46                 0/1     Pending   0          2m50s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kuberay-trn1-worker-workergroup-fz8bs   0/1     Pending   0          2m50s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kuberay-trn1-worker-workergroup-gpnxh   0/1     Pending   0          2m50s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Check the logs of the head pod:</p>
<p>Look for messages indicating that the Ray head has started and the cluster is operational.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs kuberay-trn1-head-xxxxx</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accessing-the-ray-dashboard-port-forwarding">Accessing the Ray Dashboard (Port Forwarding):<a href="#accessing-the-ray-dashboard-port-forwarding" class="hash-link" aria-label="Direct link to Accessing the Ray Dashboard (Port Forwarding):" title="Direct link to Accessing the Ray Dashboard (Port Forwarding):">​</a></h3>
<p>The Ray dashboard provides valuable insights into your cluster&#x27;s status and job progress. To access it:</p>
<p><strong>Forward the Port:</strong></p>
<p>This forwards the Ray dashboard port (8265) from your local machine to the head pod within the cluster.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/kuberay-trn1-head-svc 8265:8265</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Open Browser and navigate to <a href="http://localhost:8265" target="_blank" rel="noopener noreferrer">http://localhost:8265</a> in your web browser to view the dashboard.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-generate-pre-training-data-on-fsx-shared-filesystem">4. Generate Pre-training Data on FSx Shared Filesystem<a href="#4-generate-pre-training-data-on-fsx-shared-filesystem" class="hash-link" aria-label="Direct link to 4. Generate Pre-training Data on FSx Shared Filesystem" title="Direct link to 4. Generate Pre-training Data on FSx Shared Filesystem">​</a></h2>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Data generation step can take up to 20 minutes to create all the data in FSx for Lustre.</p></div></div>
<p>In this step, we&#x27;ll leverage KubeRay&#x27;s Job specification to kickstart the data generation process. We&#x27;ll submit a job directly to the Ray head pod. This job plays a key role in preparing your model for training.</p>
<p>Check out the <code>RayJob</code> definition spec below to leverage the existing RayCluster using <code>clusterSelector</code> to submit the jobs to RayCluster.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># RayJob: llama2-generate-pretraining-test-data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Description:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># This RayJob is responsible for generating pre-training test data required for</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># the Llama2 model training. It sources data from the specified dataset, processes</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># it, and prepares it for use in subsequent training stages. The job runs a Python</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># script (`get_dataset.py`) that performs these data preparation steps.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Usage:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Apply this configuration to your Kubernetes cluster using `kubectl apply -f 1-llama2-pretrain-trn1-rayjob-create-test-data.yaml`.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ray.io/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> RayJob</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">generate</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">pretraining</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">test</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">submissionMode</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> K8sJobMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">entrypoint</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;python3 get_dataset.py&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">runtimeEnvYAML</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">|</span><span class="token scalar string" style="color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    working_dir: /llama2_pretrain</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    env_vars:</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">      PYTHONUNBUFFERED: &#x27;0&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">      requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">        cpu: &quot;6&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">        memory: &quot;30Gi&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">clusterSelector</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">ray.io/cluster</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> kuberay</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">rayClusterNamespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default  </span><span class="token comment" style="color:#999988;font-style:italic"># Replace with the namespace where your RayCluster is deployed</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">ttlSecondsAfterFinished</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">60</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Time to live for the pod after completion (in seconds)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Execute the following command to run the Test Data creation Ray job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f 1-llama2-pretrain-trn1-rayjob-create-test-data.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>What Happens Behind the Scenes:</strong></p>
<p><strong>Job Launch:</strong> You&#x27;ll use kubectl to submit the KubeRay job specification. The Ray head pod in your <code>kuberay-trn1</code> cluster receives and executes this job.</p>
<p><strong>Data Generation:</strong> The job runs the <code>gen-ai/training/raytrain-llama2-pretrain-trn1/llama2_pretrain/get_dataset.py</code> script, which harnesses the power of the Hugging Face datasets library to fetch and process the raw English Wikipedia dataset (&quot;wikicorpus&quot;).</p>
<p><strong>Tokenization:</strong> The script tokenizes the text using a pre-trained tokenizer from Hugging Face transformers. Tokenization breaks down the text into smaller units (words or subwords) for the model to understand.</p>
<p><strong>Data Storage:</strong> The tokenized data is neatly organized and saved to a specific directory (<code>/shared/wikicorpus_llama2_7B_tokenized_4k/</code>) within your FSx for Lustre shared filesystem. This ensures all worker nodes in your cluster can readily access this standardized data during pre-training.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-the-job">Monitoring the Job:<a href="#monitoring-the-job" class="hash-link" aria-label="Direct link to Monitoring the Job:" title="Direct link to Monitoring the Job:">​</a></h3>
<p>To keep tabs on the job&#x27;s progress:</p>
<p><strong>Ray Dashboard</strong>: Head over to the Ray dashboard, accessible via your Ray head pod&#x27;s IP address and port 8265. You&#x27;ll see real-time updates on the job&#x27;s status.</p>
<p><img decoding="async" loading="lazy" alt="Prepare the Dataset" src="/data-on-eks/assets/images/raytrain-testdata-raydash1-b4670a064fa2d403d5710ccbaf639df7.png" width="3022" height="934" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="Prepare the Dataset" src="/data-on-eks/assets/images/raytrain-testdata-raydash2-d587f6edbefc961341e179b801414f5d.png" width="3022" height="934" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="Prepare the Dataset" src="/data-on-eks/assets/images/raytrain-testdata-raydash3-09f961034e80ed2f517469333f748079.png" width="3022" height="1642" class="img_ev3q"></p>
<p>Alternatively, you can use the following command in your terminal:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods | grep llama2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Output:</strong></p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama2-generate-pretraining-test-data-g6ccl   1/1     Running   0             5m5s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The following screenshot taken from Lens K8s IDE to show the logs of the pod.</p>
<p><img decoding="async" loading="lazy" alt="Prepare the Dataset" src="/data-on-eks/assets/images/raytrain-testdata-lens-7a27be9b9fca7ff7389068a5e8e1f06b.png" width="3022" height="1628" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-run-pre-compilation-job-optimization-step">5. Run Pre-compilation Job (Optimization Step)<a href="#5-run-pre-compilation-job-optimization-step" class="hash-link" aria-label="Direct link to 5. Run Pre-compilation Job (Optimization Step)" title="Direct link to 5. Run Pre-compilation Job (Optimization Step)">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Pre-compilation job can take upto 6 min</p></div></div>
<p>Before starting the actual training, we&#x27;ll perform a pre-compilation step to optimize the model for the Neuron SDK. This helps the model run more efficiently on the <code>Trn1</code> instances. This script will use the Neuron SDK to compile and optimize the model&#x27;s computational graph, making it ready for efficient training on the Trn1 processors.</p>
<p>In this step, you will run a pre-compilation job where the Neuron SDK will identify, compile, and cache the compute graphs associated with <code>Llama2</code> pretraining.</p>
<p>Check out the <code>RayJob</code> definition spec below to run the pre-compilation job:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># RayJob: llama2-precompilation-job</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Description:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># This RayJob is responsible for the pre-compilation step required for the Llama2 model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># training. It runs a Python script (`ray_train_llama2.py`) with the `--neuron_parallel_compile`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># option to compile the model in parallel using AWS Neuron devices. This step is crucial for</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># optimizing the model for efficient training on AWS infrastructure.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Usage:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Apply this configuration to your Kubernetes cluster using `kubectl apply -f 2-llama2-pretrain-trn1-rayjob-precompilation.yaml`.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">---</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ray.io/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> RayJob</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">precompilation</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">job</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">submissionMode</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> K8sJobMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">entrypoint</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;NEURON_NUM_DEVICES=32 python3 /llama2_pretrain/ray_train_llama2.py --neuron_parallel_compile&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">runtimeEnvYAML</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">|</span><span class="token scalar string" style="color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    working_dir: /llama2_pretrain</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    env_vars:</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">      PYTHONUNBUFFERED: &#x27;0&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">clusterSelector</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">ray.io/cluster</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> kuberay</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">rayClusterNamespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default  </span><span class="token comment" style="color:#999988;font-style:italic"># Replace with the namespace where your RayCluster is deployed</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">ttlSecondsAfterFinished</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">60</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Time to live for the pod after completion (in seconds)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Execute the following command to run the pre-compilation job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f 2-llama2-pretrain-trn1-rayjob-precompilation.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Verification Steps:</strong></p>
<p>To monitor the job&#x27;s progress and verify that it is running correctly, use the following commands and tools:</p>
<p><strong>Ray Dashboard:</strong> Access the Ray dashboard via your Ray head pod&#x27;s IP address and port <code>8265</code> to see real-time updates on the job&#x27;s status.</p>
<p><img decoding="async" loading="lazy" alt="Precompilation progress" src="/data-on-eks/assets/images/raytrain-precomplilation1-777f1caf778704cc0021889c8968ec58.png" width="3814" height="1818" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="Precompilation progress" src="/data-on-eks/assets/images/raytrain-precomplilation2-705da082daec14cb61c6b27f566e0dab.png" width="3814" height="1818" class="img_ev3q"></p>
<p>The following screenshot taken from Lens K8s IDE to show the logs of the pod.</p>
<p><img decoding="async" loading="lazy" alt="Precompilation progress" src="/data-on-eks/assets/images/raytrain-precomplilation3-fe1f06d03638e88e965c106514105218.png" width="1922" height="907" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-run-distributed-pre-training-job">6. Run Distributed Pre-training Job<a href="#6-run-distributed-pre-training-job" class="hash-link" aria-label="Direct link to 6. Run Distributed Pre-training Job" title="Direct link to 6. Run Distributed Pre-training Job">​</a></h2>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>This job can run for many hours so feel free to cancel the job using Ctrl+C once you are convinced that the loss is decreasing and the model is learning.</p></div></div>
<p>Now, you&#x27;re ready to begin the actual training of the Llama 2 model! This step involves running the distributed pre-training job using a RayJob. The job will utilize AWS Neuron devices to efficiently train the model with the prepared dataset.</p>
<p>Check out the <code>RayJob</code> definition spec below to run the pretraining job:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># RayJob: llama2-pretraining-job</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">#</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Description:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># This RayJob is responsible for the main pretraining step of the Llama2 model.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># It runs a Python script (`ray_train_llama2.py`) to perform the pretraining using AWS Neuron devices.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># This step is critical for training the language model with the prepared dataset.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Usage:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Apply this configuration to your Kubernetes cluster using `kubectl apply -f 3-llama2-pretrain-trn1-rayjob.yaml`.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># ----------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">---</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ray.io/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> RayJob</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">pretraining</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">job</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">submissionMode</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> K8sJobMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">entrypoint</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;NEURON_NUM_DEVICES=32 python3 ray_train_llama2.py&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">runtimeEnvYAML</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">|</span><span class="token scalar string" style="color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    working_dir: /llama2_pretrain</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">    env_vars:</span><br></span><span class="token-line" style="color:#393A34"><span class="token scalar string" style="color:#e3116c">      PYTHONUNBUFFERED: &#x27;0&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">clusterSelector</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">ray.io/cluster</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> kuberay</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">rayClusterNamespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> default  </span><span class="token comment" style="color:#999988;font-style:italic"># Replace with the namespace where your RayCluster is deployed</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">shutdownAfterJobFinishes</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean important" style="color:#36acaa">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">activeDeadlineSeconds</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">600</span><span class="token plain">   </span><span class="token comment" style="color:#999988;font-style:italic"># The job will be terminated if it runs longer than 600 seconds (10 minutes)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">ttlSecondsAfterFinished</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">60</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Time to live for the pod after completion (in seconds)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Execute the following command to run the pretraining job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f 3-llama2-pretrain-trn1-rayjob.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Monitor Progress:</strong></p>
<p>You can monitor the progress of the training job using the Ray Dashboard or by observing the logs output to your terminal. Look for information like the training loss, learning rate, and other metrics to assess how well the model is learning.</p>
<p><img decoding="async" loading="lazy" alt="Training Progress" src="/data-on-eks/assets/images/raytrain-training-progress1-a0075f05772879597fe736982d834b18.png" width="3814" height="1818" class="img_ev3q"></p>
<p><strong>Ray Dashboard:</strong> Access the Ray dashboard via your Ray head pod&#x27;s IP address and port 8265 to see real-time updates on the job&#x27;s status.</p>
<p><img decoding="async" loading="lazy" alt="Training Progress Ray Dashboard" src="/data-on-eks/assets/images/raytrain-training-progress2-52bb1ff83523781d2048dbf00670e901.png" width="3814" height="1818" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="Training Progress Ray Dashboard" src="/data-on-eks/assets/images/raytrain-training-progress3-b614686ce3efb30717a77a20ddfe39a6.png" width="3814" height="1818" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cleaning-up">Cleaning up<a href="#cleaning-up" class="hash-link" aria-label="Direct link to Cleaning up" title="Direct link to Cleaning up">​</a></h3>
<p>To remove the resources created using this solution, run the cleanup script:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Delete the RayCluster Resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd gen-ai/training/raytrain-llama2-pretrain-trn1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete -f llama2-pretrain-trn1-raycluster.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Clean Up the EKS Cluster and Associated Resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/ai-ml/trainium-inferentia</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/training/Neuron/RayTrain-Llama2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-on-eks/docs/gen-ai/training/GPUs/bionemo"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">BioNeMo on EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-on-eks/docs/gen-ai/training/Neuron/Llama2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Llama-2 with Nemo-Megatron on Trn1</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-llama-2" class="table-of-contents__link toc-highlight">What is Llama-2?</a></li><li><a href="#why-raytrain-and-kuberay-for-distributed-training" class="table-of-contents__link toc-highlight">Why RayTrain and KubeRay for Distributed Training?</a></li><li><a href="#why-this-combination-is-powerful" class="table-of-contents__link toc-highlight">Why This Combination is Powerful</a></li><li><a href="#what-is-volcano" class="table-of-contents__link toc-highlight">What is Volcano?</a></li><li><a href="#how-volcano-works-with-gang-scheduling" class="table-of-contents__link toc-highlight">How Volcano Works with Gang Scheduling</a></li><li><a href="#1-deploying-the-solution" class="table-of-contents__link toc-highlight">1. Deploying the Solution</a><ul><li><a href="#verify-the-resources" class="table-of-contents__link toc-highlight">Verify the resources</a></li></ul></li><li><a href="#2-build-the-docker-image-optional-step" class="table-of-contents__link toc-highlight">2. Build the Docker Image (Optional Step)</a></li><li><a href="#3-launch-the-ray-cluster-with-kuberay-operator" class="table-of-contents__link toc-highlight">3. Launch the Ray Cluster with KubeRay Operator</a><ul><li><a href="#gang-scheduling-of-ray-head-and-worker-pods-with-volcano" class="table-of-contents__link toc-highlight">Gang Scheduling of Ray Head and Worker Pods with Volcano</a></li><li><a href="#accessing-the-ray-dashboard-port-forwarding" class="table-of-contents__link toc-highlight">Accessing the Ray Dashboard (Port Forwarding):</a></li></ul></li><li><a href="#4-generate-pre-training-data-on-fsx-shared-filesystem" class="table-of-contents__link toc-highlight">4. Generate Pre-training Data on FSx Shared Filesystem</a><ul><li><a href="#monitoring-the-job" class="table-of-contents__link toc-highlight">Monitoring the Job:</a></li></ul></li><li><a href="#5-run-pre-compilation-job-optimization-step" class="table-of-contents__link toc-highlight">5. Run Pre-compilation Job (Optimization Step)</a></li><li><a href="#6-run-distributed-pre-training-job" class="table-of-contents__link toc-highlight">6. Run Distributed Pre-training Job</a><ul><li><a href="#cleaning-up" class="table-of-contents__link toc-highlight">Cleaning up</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Started</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-on-eks/docs/introduction/intro">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>