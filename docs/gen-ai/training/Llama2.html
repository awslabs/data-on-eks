<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-gen-ai/training/Llama2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Llama-2 on Trainium | Data on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Llama2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama-2 on Trainium | Data on EKS"><meta data-rh="true" name="description" content="Note: Use of this Llama-2 model is governed by the Meta license."><meta data-rh="true" property="og:description" content="Note: Use of this Llama-2 model is governed by the Meta license."><link data-rh="true" rel="icon" href="/data-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Llama2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Llama2" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/Llama2" hreflang="x-default"><link rel="stylesheet" href="/data-on-eks/assets/css/styles.7e16996c.css">
<script src="/data-on-eks/assets/js/runtime~main.72da48df.js" defer="defer"></script>
<script src="/data-on-eks/assets/js/main.66a309a9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_gu5v" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-on-eks/"><div class="navbar__logo"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_ZRzL themedComponent--light_dGsa"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_ZRzL themedComponent--dark_pzCA"></div></a><a class="navbar__item navbar__link" href="/data-on-eks/docs/introduction/intro">Introduction</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-on-eks/docs/gen-ai">Gen AI</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/blueprints/amazon-emr-on-eks">Blueprints</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/bestpractices/intro">Best Practices</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/benchmarks/emr-on-eks">Benchmarks</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/resources/intro">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_kWbt colorModeToggle_GwZs"><button class="clean-btn toggleButton_fOL9 toggleButtonDisabled_STpu" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_DCeJ"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_DFgp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_IP3a"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_IbdI"><div class="docsWrapper_JGIH"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_SdI4" type="button"></button><div class="docRoot_eRbX"><aside class="theme-doc-sidebar-container docSidebarContainer_Ta75"><div class="sidebarViewport_fgog"><div class="sidebar_oDHW"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_vPEQ"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-on-eks/docs/gen-ai">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/data-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Expand sidebar category &#x27;Inference on EKS&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/data-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Collapse sidebar category &#x27;Training on EKS&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/training/BERT-Large">BERT-Large on Trainium</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-on-eks/docs/gen-ai/training/Llama2">Llama-2 on Trainium</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/training/bionemo">BioNeMo on EKS</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_Cq4q"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_eHqP"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_lg0V"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_nDJs"><div class="docItemContainer_OGiL"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_k3Z9" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_JACu"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/data-on-eks/docs/category/training-on-eks"><span itemprop="name">Training on EKS</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Llama-2 on Trainium</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_QCOD theme-doc-toc-mobile tocMobile_N0YI"><button type="button" class="clean-btn tocCollapsibleButton_pHwF">On this page</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-danger admonition_LMjb alert alert--danger"><div class="admonitionHeading_GGQ4"><span class="admonitionIcon_ifdW"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_pGk6"><p>Note: Use of this Llama-2 model is governed by the Meta license.
In order to download the model weights and tokenizer, please visit the <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">website</a> and accept the license before requesting access.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_LMjb alert alert--info"><div class="admonitionHeading_GGQ4"><span class="admonitionIcon_ifdW"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_pGk6"><p>We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects.</p></div></div>
<h1>Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator</h1>
<p>Welcome to the comprehensive guide on training the <a href="https://ai.meta.com/llama/#inside-the-model" target="_blank" rel="noopener noreferrer">Meta Llama-2-7b </a> model on Amazon Elastic Kubernetes Service (EKS) using AWS Trainium, Neuronx-Nemo-Megatron, and the MPI Operator.</p>
<p>In this tutorial you will learn how to run multi-node training jobs using <a href="https://aws.amazon.com/machine-learning/trainium/" target="_blank" rel="noopener noreferrer">AWS Trainium</a> accelerators in Amazon EKS. Specifically, you will pretrain Llama-2-7b on 4 AWS EC2 trn1.32xlarge instances using a <a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample" target="_blank" rel="noopener noreferrer">subset of the RedPajama dataset</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="what-is-llama-2">What is Llama-2?<a href="#what-is-llama-2" class="hash-link" aria-label="Direct link to What is Llama-2?" title="Direct link to What is Llama-2?">â€‹</a></h3>
<p>Llama-2 is a large language model (LLM) trained on 2 trillion tokens of text and code. It is one of the largest and most powerful LLMs available today. Llama-2 can be used for a variety of tasks, including natural language processing, text generation, and translation.</p>
<p>Although Llama-2 is available as a pretrained model, in this tutorial we will show how to pretrain the model from scratch.</p>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="llama-2-chat">Llama-2-chat<a href="#llama-2-chat" class="hash-link" aria-label="Direct link to Llama-2-chat" title="Direct link to Llama-2-chat">â€‹</a></h4>
<p>Llama-2 is a remarkable language model that has undergone a rigorous training process. It starts with pretraining using publicly available online data.</p>
<p>Llama-2 is available in three different model sizes:</p>
<ul>
<li><strong>Llama-2-70b:</strong> This is the largest Llama-2 model, with 70 billion parameters. It is the most powerful Llama-2 model and can be used for the most demanding tasks.</li>
<li><strong>Llama-2-13b:</strong> This is a medium-sized Llama-2 model, with 13 billion parameters. It is a good balance between performance and efficiency, and can be used for a variety of tasks.</li>
<li><strong>Llama-2-7b:</strong> This is the smallest Llama-2 model, with 7 billion parameters. It is the most efficient Llama-2 model and can be used for tasks that do not require the highest level of performance.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="which-llama-2-model-size-should-i-use"><strong>Which Llama-2 model size should I use?</strong><a href="#which-llama-2-model-size-should-i-use" class="hash-link" aria-label="Direct link to which-llama-2-model-size-should-i-use" title="Direct link to which-llama-2-model-size-should-i-use">â€‹</a></h3>
<p>The best Llama-2 model size for you will depend on your specific needs. and it may not always be the largest model for achieving the highest performance. It&#x27;s advisable to evaluate your needs and consider factors such as computational resources, response time, and cost-efficiency when selecting the appropriate Llama-2 model size. The decision should be based on a comprehensive assessment of your application&#x27;s goals and constraints.</p>
<p><strong>Performance Boost</strong>
While Llama-2 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-2&#x27;s inference speeds. This translates to faster response times and improved user experiences when deploying Llama-2 on Trn1/Inf2 instances.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="solution-architecture">Solution Architecture<a href="#solution-architecture" class="hash-link" aria-label="Direct link to Solution Architecture" title="Direct link to Solution Architecture">â€‹</a></h2>
<p>In this section, we will delve into the architecture of our solution.</p>
<p><strong>Trn1.32xl Instance:</strong> This is an EC2 accelerated instance type that is part of the EC2 Trn1 (Trainium) instance family, optimized for machine learning training workloads</p>
<p><strong>MPI Worker Pods:</strong> These are Kubernetes pods configured for running MPI (Message Passing Interface) tasks. MPI is a standard for distributed memory parallel computing. Each worker pod runs on a trn1.32xlarge instance which is equipped with 16 Trainium accelerators and 8 Elastic Fabric Adapters (EFAs). EFAs are network devices that support high-performance computing applications running on Amazon EC2 instances.</p>
<p><strong>MPI Launcher Pod:</strong> This pod is responsible for coordinating the MPI job across the worker pods. When a training job is first submitted to the cluster, an MPI launcher pod is created which waits for the workers to come online, connects to each worker, and invokes the training script.</p>
<p><strong>MPI Operator:</strong> An operator in Kubernetes is a method of packaging, deploying, and managing a Kubernetes application. The MPI Operator automates the deployment and management of MPI workloads.</p>
<p><strong>FSx for Lustre:</strong> A shared, high-performance filesystem which is well suited for workloads such as machine learning, high performance computing (HPC), video processing, and financial modeling. The FSx for Lustre filesystem will be shared across worker pods in the training job, providing a central repository to access the training data and to store model artifacts and logs.</p>
<p><img loading="lazy" alt="Llama-2-trn1" src="/data-on-eks/assets/images/llama2-trainium-076066f4fecbd832a016e76f75269187.png" width="885" height="614" class="img_wQsy"></p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="deploying-the-solution">Deploying the Solution<a href="#deploying-the-solution" class="hash-link" aria-label="Direct link to Deploying the Solution" title="Direct link to Deploying the Solution">â€‹</a></h2>
<p><strong>Steps to train Llama-2 using AWS Trainium on Amazon EKS</strong></p>
<p>Note: This post makes use of Metaâ€™s Llama tokenizer, which is protected by a user license that must be accepted before the tokenizer files can be downloaded. Please ensure that you have access to the Llama files by requesting access here.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites</span></h2><span class="icon_PckA">ðŸ‘ˆ</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="distributed-training">Distributed training<a href="#distributed-training" class="hash-link" aria-label="Direct link to Distributed training" title="Direct link to Distributed training">â€‹</a></h2>
<p>Once the EKS Cluster is deployed, you can proceed with the next steps of building neuronx-nemo-megatron container image and pushing the image to ECR.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="build-the-neuronx-nemo-megatron-container-image">Build the neuronx-nemo-megatron container image<a href="#build-the-neuronx-nemo-megatron-container-image" class="hash-link" aria-label="Direct link to Build the neuronx-nemo-megatron container image" title="Direct link to Build the neuronx-nemo-megatron container image">â€‹</a></h3>
<p>Navigate to examples/llama2 directory</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">cd examples/llama2/</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Run the <code>1-llama2-neuronx-pretrain-build-image.sh</code> script to build the neuronx-nemo-megatron container image and push the image into ECR.</p>
<p>When prompted for a region, enter the region in which you launched your EKS cluster, above.</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">./1-llama2-neuronx-pretrain-build-image.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Note: The image building and pushing to ECR will take ~10 minutes</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="launch-and-connect-to-a-cli-pod">Launch and connect to a CLI pod<a href="#launch-and-connect-to-a-cli-pod" class="hash-link" aria-label="Direct link to Launch and connect to a CLI pod" title="Direct link to Launch and connect to a CLI pod">â€‹</a></h3>
<p>In this step we need access to the shared FSx storage. To copy files to this storage, weâ€™ll first launch and connect to a CLI pod running the neuronx-nemo-megatron docker image that you created above.</p>
<p>Run the following script to launch the CLI pod:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">./2-launch-cmd-shell-pod.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Next, periodically run the following command until you see the CLI pod go into â€˜Runningâ€™ state:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pod -w</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once the CLI pod is â€˜Runningâ€™, connect to it using the following command:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec -it cli-cmd-shell -- /bin/bash</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="download-the-llama-tokenizer-and-redpajama-dataset-to-fsx">Download the Llama tokenizer and Redpajama dataset to FSx<a href="#download-the-llama-tokenizer-and-redpajama-dataset-to-fsx" class="hash-link" aria-label="Direct link to Download the Llama tokenizer and Redpajama dataset to FSx" title="Direct link to Download the Llama tokenizer and Redpajama dataset to FSx">â€‹</a></h3>
<p>From within the CLI pod, weâ€™ll download the Llama tokenizer files. These files are protected by Meta&#x27;s Llama license, so you will need to run the <code>huggingface-cli login</code> command to login to Hugging Face using your access token. The access token is found under Settings â†’ Access Tokens on the Hugging Face website.</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">huggingface-cli login</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>When prompted for your token, paste-in the access token and hit <code>ENTER</code>.</p>
<p>Next, you download the llama7-7b tokenizer files to /shared/llama7b_tokenizer by running the following python code:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">python3 &lt;&lt;EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tok = transformers.AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tok.save_pretrained(&quot;/shared/llama7b_tokenizer&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EOF</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Next, download the RedPajama-Data-1T-Sample dataset (a small subset of the full RedPajama dataset that contains 1B tokens).</p>
<p>While still connected to the CLI pod, use git to download the dataset</p>
<div class="codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-text codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">cd /shared</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    data/RedPajama-Data-1T-Sample</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="tokenize-the-dataset">Tokenize the dataset<a href="#tokenize-the-dataset" class="hash-link" aria-label="Direct link to Tokenize the dataset" title="Direct link to Tokenize the dataset">â€‹</a></h3>
<p>Tokenize the dataset using the preprocessing script included with neuronx-nemo-megatron. This preprocessing step will take ~60 minutes to run on a trn1.32xl instance.</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">cd /shared</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Clone the neuronx-nemo-megatron repo, which includes the required scripts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/aws-neuron/neuronx-nemo-megatron.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Combine the separate redpajama files to a single jsonl file</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cat /shared/data/RedPajama-Data-1T-Sample/*.jsonl &gt; /shared/redpajama_sample.jsonl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Run preprocessing script using llama tokenizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 neuronx-nemo-megatron/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --input=/shared/redpajama_sample.jsonl \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --json-keys=text \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --tokenizer-library=huggingface \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --tokenizer-type=/shared/llama7b_tokenizer \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --dataset-impl=mmap \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --output-prefix=/shared/data/redpajama_sample \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --append-eod \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --need-pad-id \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --workers=32</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="modify-dataset-and-tokenizer-paths-in-the-training-script">Modify dataset and tokenizer paths in the training script<a href="#modify-dataset-and-tokenizer-paths-in-the-training-script" class="hash-link" aria-label="Direct link to Modify dataset and tokenizer paths in the training script" title="Direct link to Modify dataset and tokenizer paths in the training script">â€‹</a></h3>
<p>Note: When we later launch our training jobs in EKS, the training pods will run the training script from within neuronx-nemo-megatron/nemo/examples directory on FSx. This is convenient, because it will let you modify your training script directly on FSx without requiring that you rebuild the neuronx-nemo-megatron container for every change.</p>
<p>Modify the test_llama.sh script <code>/shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</code> to update the following two lines. These lines tell the training pod workers where to find the Llama tokenizer and the dataset on the FSx filesystem.</p>
<p>Run:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">sed -i &#x27;s#^\(: ${TOKENIZER_PATH=\).*#\1/shared/llama7b_tokenizer}#&#x27; /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sed -i &#x27;s#^\(: ${DATASET_PATH=\).*#\1/shared/data/redpajama_sample_text_document}#&#x27; /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Before changes:</p>
<div class="codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-text codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">: ${TOKENIZER_PATH=$HOME/llamav2_weights/7b-hf}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">: ${DATASET_PATH=$HOME/examples_datasets/llama_7b/book.jsonl-processed_text_document}</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>After changes:</p>
<div class="codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-text codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">: ${TOKENIZER_PATH=/shared/llama7b_tokenizer}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">: ${DATASET_PATH=/shared/data/redpajama_sample_text_document}</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can save your changes in nano by pressing <code>CTRL-X</code>, then <code>y</code>, then <code>ENTER</code>.</p>
<p>When you are finished, type <code>exit</code> or press <code>CTRL-d</code> to exit the CLI pod.</p>
<p>If you no longer need the CLI pod you can remove it by running:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete pod cli-cmd-shell</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>We are finally ready to launch our pre-compilation and training jobs!</p>
<p>First, let&#x27;s check to make sure the MPI operator is functional by running this command:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all -n mpi-operator</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If the MPI Operator is not installed, please follow the <a href="https://github.com/kubeflow/mpi-operator#installation" target="_blank" rel="noopener noreferrer">MPI Operator installation instructions</a> before proceeding.</p>
<p>Before we can run the training job, we first run a pre-compilation job in order to prepare the model artifacts. This step extracts and compiles the underlying compute graphs for the Llama-2-7b model and generates Neuron executable files (NEFFs) that can run on the Trainium accelerators. These NEFFs are stored in a persistent Neuron cache on FSx so that the training job can later access them.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="run-pre-compilation-job">Run pre-compilation job<a href="#run-pre-compilation-job" class="hash-link" aria-label="Direct link to Run pre-compilation job" title="Direct link to Run pre-compilation job">â€‹</a></h3>
<p>Run the pre-compilation script</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">./3-llama2-neuronx-mpi-compile.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Pre-compilation will take ~10 minutes when using 4 trn1.32xlarge nodes.</p>
<p>Periodically run <code>kubectl get pods | grep compile</code> and wait until you see that the compile job shows â€˜Completedâ€™.</p>
<p>When pre-compilation is complete, you can then launch the pre-training job on 4 trn1.32xl nodes by running the following script:</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="run-training-job">Run training job<a href="#run-training-job" class="hash-link" aria-label="Direct link to Run training job" title="Direct link to Run training job">â€‹</a></h3>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">./4-llama2-neuronx-mpi-train.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="view-training-job-output">View training job output<a href="#view-training-job-output" class="hash-link" aria-label="Direct link to View training job output" title="Direct link to View training job output">â€‹</a></h3>
<p>To monitor the training job output - first, find the name of the launcher pod associated with your training job:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods | grep launcher</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once you have identified the name of the launcher pod and see that it is â€˜Runningâ€™, the next step is to determine its UID. Replace test-mpi-train-launcher-xxx with your launcher pod name in the following command and it will output the UID:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pod test-mpi-train-launcher-xxx -o json | jq -r &quot;.metadata.uid&quot;</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Use the UID to determine the log path so you can tail the training logs. Replace <code>UID</code> in the following command with the above value.</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec -it test-mpi-train-worker-0 -- tail -f /shared/nemo_experiments/UID/0/log</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>When you are done viewing the logs, you can press <code>CTRL-C</code> to quit the tail command.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="monitor-trainium-accelerator-utilization">Monitor Trainium accelerator utilization<a href="#monitor-trainium-accelerator-utilization" class="hash-link" aria-label="Direct link to Monitor Trainium accelerator utilization" title="Direct link to Monitor Trainium accelerator utilization">â€‹</a></h3>
<p>To monitor Trainium accelerator utilization you can use the neuron-top command. Neuron-top is a console-based tool for monitoring Neuron and system-related performance metrics on trn1/inf2/inf1 instances. You can launch neuron-top on one of the worker pods as follows:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec -it test-mpi-train-worker-0 -- /bin/bash -l neuron-top</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="view-training-job-metrics-in-tensorboard">View training job metrics in TensorBoard<a href="#view-training-job-metrics-in-tensorboard" class="hash-link" aria-label="Direct link to View training job metrics in TensorBoard" title="Direct link to View training job metrics in TensorBoard">â€‹</a></h3>
<p><a href="https://www.tensorflow.org/tensorboard" target="_blank" rel="noopener noreferrer">TensorBoard</a> is a web-based visualization tool that is commonly used to monitor and explore training jobs. It allows you to quickly monitor training metrics, and you can also easily compare metrics across different training runs.</p>
<p>TensorBoard logs available in the /shared/nemo_experiments/ directory on the FSx for Lustre filesystem.</p>
<p>Run the following script to create a TensorBoard deployment so you can visualize your Llama-2 training job progress:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">./5-deploy-tensorboard.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once the deployment is ready the script will output a password-protected URL for your new TensorBoard deployment.</p>
<p>Launch the URL to view your training progress.</p>
<p>When you have opened the TensorBoard interface, choose your training job UID from the left-hand menu, and then explore the various training metrics (ex: reduced-train-loss, throughput, and grad-norm) from the main application window.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="stopping-the-training-job">Stopping the training job<a href="#stopping-the-training-job" class="hash-link" aria-label="Direct link to Stopping the training job" title="Direct link to Stopping the training job">â€‹</a></h3>
<p>To stop your training job and remove the launcher/worker pods, run the following command:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete mpijob test-mpi-train</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can then run <code>kubectl get pods</code> to confirm that the launcher/worker pods have been removed.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="cleaning-up">Cleaning up<a href="#cleaning-up" class="hash-link" aria-label="Direct link to Cleaning up" title="Direct link to Cleaning up">â€‹</a></h3>
<p>To remove the resources created using this solution, run the cleanup script:</p>
<div class="language-bash codeBlockContainer_aalF theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_MHx8"><pre tabindex="0" class="prism-code language-bash codeBlock_zHgq thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_RjmQ"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/ai-ml/trainium-inferentia</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre><div class="buttonGroup_Sd8_"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_LnQD" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_t3l1"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_IiZV"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/training/Llama2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NulP" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated__GQF"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-on-eks/docs/gen-ai/training/BERT-Large"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">BERT-Large on Trainium</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-on-eks/docs/gen-ai/training/bionemo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">BioNeMo on EKS</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_IS5x thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-llama-2" class="table-of-contents__link toc-highlight">What is Llama-2?</a></li><li><a href="#which-llama-2-model-size-should-i-use" class="table-of-contents__link toc-highlight"><strong>Which Llama-2 model size should I use?</strong></a></li><li><a href="#solution-architecture" class="table-of-contents__link toc-highlight">Solution Architecture</a></li><li><a href="#deploying-the-solution" class="table-of-contents__link toc-highlight">Deploying the Solution</a><ul><li><a href="#verify-the-resources" class="table-of-contents__link toc-highlight">Verify the resources</a></li></ul></li><li><a href="#distributed-training" class="table-of-contents__link toc-highlight">Distributed training</a><ul><li><a href="#build-the-neuronx-nemo-megatron-container-image" class="table-of-contents__link toc-highlight">Build the neuronx-nemo-megatron container image</a></li><li><a href="#launch-and-connect-to-a-cli-pod" class="table-of-contents__link toc-highlight">Launch and connect to a CLI pod</a></li><li><a href="#download-the-llama-tokenizer-and-redpajama-dataset-to-fsx" class="table-of-contents__link toc-highlight">Download the Llama tokenizer and Redpajama dataset to FSx</a></li><li><a href="#tokenize-the-dataset" class="table-of-contents__link toc-highlight">Tokenize the dataset</a></li><li><a href="#modify-dataset-and-tokenizer-paths-in-the-training-script" class="table-of-contents__link toc-highlight">Modify dataset and tokenizer paths in the training script</a></li><li><a href="#run-pre-compilation-job" class="table-of-contents__link toc-highlight">Run pre-compilation job</a></li><li><a href="#run-training-job" class="table-of-contents__link toc-highlight">Run training job</a></li><li><a href="#view-training-job-output" class="table-of-contents__link toc-highlight">View training job output</a></li><li><a href="#monitor-trainium-accelerator-utilization" class="table-of-contents__link toc-highlight">Monitor Trainium accelerator utilization</a></li><li><a href="#view-training-job-metrics-in-tensorboard" class="table-of-contents__link toc-highlight">View training job metrics in TensorBoard</a></li><li><a href="#stopping-the-training-job" class="table-of-contents__link toc-highlight">Stopping the training job</a></li><li><a href="#cleaning-up" class="table-of-contents__link toc-highlight">Cleaning up</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Started</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-on-eks/docs/introduction/intro">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with â¤ï¸ at AWS  <br> Â© 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
</body>
</html>