<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-gen-ai/inference/GPUs/vLLM-NVIDIATritonServer" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">NVIDIA Triton Server with vLLM | Data on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NVIDIA Triton Server with vLLM | Data on EKS"><meta data-rh="true" name="description" content="Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue."><meta data-rh="true" property="og:description" content="Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue."><link data-rh="true" rel="icon" href="/data-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer" hreflang="x-default"><link rel="stylesheet" href="/data-on-eks/assets/css/styles.74de95c8.css">
<script src="/data-on-eks/assets/js/runtime~main.f1c02848.js" defer="defer"></script>
<script src="/data-on-eks/assets/js/main.2a3ae0b2.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-on-eks/"><div class="navbar__logo"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/data-on-eks/docs/introduction/intro">Introduction</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-on-eks/docs/gen-ai">Gen AI</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/blueprints/amazon-emr-on-eks">Blueprints</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/bestpractices/intro">Best Practices</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/benchmarks/emr-on-eks">Benchmarks</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/resources/intro">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-on-eks/docs/gen-ai">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/data-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Collapse sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve">GPUs</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve">RayServe with vLLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer">NVIDIA Triton Server with vLLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/stablediffusion-gpus">Stable Diffusion on GPU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/nvidia-nim-llama3">NVIDIA NIM LLM on Amazon EKS</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2">Neuron</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Expand sidebar category &#x27;Training on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/data-on-eks/docs/category/inference-on-eks"><span itemprop="name">Inference on EKS</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">GPUs</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">NVIDIA Triton Server with vLLM</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue.</p></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_BuS1"><p>The use of <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noopener noreferrer">Meta-llama/Llama-2-7b-chat-hf</a> and <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" target="_blank" rel="noopener noreferrer">Mistralai/Mistral-7B-Instruct-v0.2</a> models requires access through a Hugging Face account.</p></div></div>
<header><h1>Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM</h1></header>
<p>In this pattern, we&#x27;ll explore how to deploy multiple large language models (LLMs) using the <a href="https://github.com/triton-inference-server/server" target="_blank" rel="noopener noreferrer">Triton Inference Server</a> and the <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> backend/engine. We&#x27;ll demonstrate this process with two specific models: <code>mistralai/Mistral-7B-Instruct-v0.2</code> and <code>meta-llama/Llama-2-7b-chat-hf</code>. These models will be hosted on a <strong>g5.24xlarge</strong> multi-GPU instance, equipped with <strong>4 GPUs</strong>, with each model utilizing up to one GPU.</p>
<p>NVIDIA Triton Inference Server, when combined with the vLLM backend, offers a robust framework for deploying multiple large language models (LLMs). User applications interact with the inference service via REST API or gRPC, which is managed by NGINX and a Network Load Balancer (NLB) to efficiently distribute incoming requests to the Triton K8s Service. The Triton K8s Service is the core of our deployment, where the Triton Server processes inference requests. For this deployment, we use g5.24xlarge instances, each equipped with 4 GPUs, to run multiple models like Llama2-7b and Mistral7b. The Horizontal Pod Autoscaler (HPA) monitors custom metrics and dynamically scales Triton pods based on demand, ensuring efficient handling of varying loads. Prometheus and Grafana are used to collect and visualize metrics, providing insights into performance and aiding in autoscaling decisions.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA Triton Server" src="/data-on-eks/assets/images/triton-architecture-26f45e98081552c17f8381dbb7dd5f61.png" width="1920" height="1080" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-to-expect">What to Expect<a href="#what-to-expect" class="hash-link" aria-label="Direct link to What to Expect" title="Direct link to What to Expect">​</a></h2>
<p>When you deploy everything as described, you can expect quick response times for your inference requests. Below is an example output from running the <code>triton-client.py</code> script with the <code>Llama-2-7b-chat-hf</code> and <code>Mistral-7B-Instruct-v0.2</code> models:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand comparison results</summary><div><div class="collapsibleContent_i85q"><table><thead><tr><th><strong>Run 1: Llama2</strong></th><th><strong>Run 2: Mistral7b</strong></th></tr></thead><tbody><tr><td>python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt</td><td>python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt</td></tr><tr><td>Loading inputs from <code>prompts.txt</code>...</td><td>Loading inputs from <code>prompts.txt</code>...</td></tr><tr><td>Model llama2 - Request 11: 0.00 ms</td><td>Model mistral7b - Request 3: 0.00 ms</td></tr><tr><td>Model llama2 - Request 15: 0.02 ms</td><td>Model mistral7b - Request 14: 0.00 ms</td></tr><tr><td>Model llama2 - Request 3: 0.00 ms</td><td>Model mistral7b - Request 11: 0.00 ms</td></tr><tr><td>Model llama2 - Request 8: 0.01 ms</td><td>Model mistral7b - Request 15: 0.00 ms</td></tr><tr><td>Model llama2 - Request 0: 0.01 ms</td><td>Model mistral7b - Request 5: 0.00 ms</td></tr><tr><td>Model llama2 - Request 9: 0.01 ms</td><td>Model mistral7b - Request 0: 0.01 ms</td></tr><tr><td>Model llama2 - Request 14: 0.01 ms</td><td>Model mistral7b - Request 7: 0.01 ms</td></tr><tr><td>Model llama2 - Request 16: 0.00 ms</td><td>Model mistral7b - Request 13: 0.00 ms</td></tr><tr><td>Model llama2 - Request 19: 0.02 ms</td><td>Model mistral7b - Request 9: 0.00 ms</td></tr><tr><td>Model llama2 - Request 4: 0.02 ms</td><td>Model mistral7b - Request 16: 0.01 ms</td></tr><tr><td>Model llama2 - Request 10: 0.02 ms</td><td>Model mistral7b - Request 18: 0.01 ms</td></tr><tr><td>Model llama2 - Request 6: 0.01 ms</td><td>Model mistral7b - Request 4: 0.01 ms</td></tr><tr><td>Model llama2 - Request 1: 0.02 ms</td><td>Model mistral7b - Request 8: 0.01 ms</td></tr><tr><td>Model llama2 - Request 7: 0.02 ms</td><td>Model mistral7b - Request 1: 0.01 ms</td></tr><tr><td>Model llama2 - Request 18: 0.01 ms</td><td>Model mistral7b - Request 6: 0.00 ms</td></tr><tr><td>Model llama2 - Request 12: 0.01 ms</td><td>Model mistral7b - Request 12: 0.00 ms</td></tr><tr><td>Model llama2 - Request 2: 0.01 ms</td><td>Model mistral7b - Request 17: 0.00 ms</td></tr><tr><td>Model llama2 - Request 17: 0.02 ms</td><td>Model mistral7b - Request 2: 0.01 ms</td></tr><tr><td>Model llama2 - Request 13: 0.01 ms</td><td>Model mistral7b - Request 19: 0.01 ms</td></tr><tr><td>Model llama2 - Request 5: 0.02 ms</td><td>Model mistral7b - Request 10: 0.02 ms</td></tr><tr><td>Storing results into <code>llama2_results.txt</code>...</td><td>Storing results into <code>mistral_results.txt</code>...</td></tr><tr><td>Total time for all requests: 0.00 seconds (0.18 milliseconds)</td><td>Total time for all requests: 0.00 seconds (0.11 milliseconds)</td></tr><tr><td>PASS: vLLM example</td><td>PASS: vLLM example</td></tr></tbody></table></div></div></details>
<h1>Triton Server Internals and Backend Integration</h1>
<p>NVIDIA Triton Inference Server is engineered for high-performance inference across a wide range of model types and deployment scenarios. The core strength of Triton lies in its support for various backends, which provide the flexibility and power needed to handle different types of models and workloads effectively.</p>
<p>Once a request reaches the Triton K8s Service, it is processed by the Triton Server. The server supports dynamic batching, allowing multiple inference requests to be grouped together to optimize processing. This is particularly useful in scenarios with high throughput requirements, as it helps reduce latency and improve overall performance.</p>
<p>Requests are then managed by scheduled queues, ensuring that each model&#x27;s inference requests are processed in an orderly manner. The Triton Server supports selective and compute model loading, which means it can dynamically load models based on the current workload and resource availability. This feature is crucial for efficiently managing the resources in a multi-model deployment.</p>
<p>The backbone of Triton’s inference capabilities are its various backends, including TensorRT-LLM and vLLM:</p>
<p><strong><a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener noreferrer">TensorRT-LLM</a></strong>: TensorRT-LLM backend optimizes large language model (LLM) inference on NVIDIA GPUs. Leveraging TensorRT&#x27;s high-performance capabilities, it accelerates inference, providing low-latency and high-throughput performance. TensorRT is particularly well-suited for deep learning models that require intensive computational resources, making it ideal for real-time AI applications.</p>
<p><strong><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a></strong>: vLLM backend is specifically designed to handle various LLM workloads. It offers efficient memory management and execution pipelines tailored for large models. This backend ensures that memory resources are used optimally, allowing for the deployment of very large models without running into memory bottlenecks. vLLM is crucial for applications that need to serve multiple large models simultaneously, providing a robust and scalable solution.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA Triton Server" src="/data-on-eks/assets/images/triton-internals-ffbbc8d44314b0020343b41b63593ffa.png" width="1920" height="1080" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mistralaimistral-7b-instruct-v02">Mistralai/Mistral-7B-Instruct-v0.2<a href="#mistralaimistral-7b-instruct-v02" class="hash-link" aria-label="Direct link to Mistralai/Mistral-7B-Instruct-v0.2" title="Direct link to Mistralai/Mistral-7B-Instruct-v0.2">​</a></h3>
<p>Mistralai/Mistral-7B-Instruct-v0.2 is a state-of-the-art large language model designed to provide high-quality, instructive responses. Trained on a diverse dataset, it excels in understanding and generating human-like text across a variety of topics. Its capabilities make it suitable for applications requiring detailed explanations, complex queries, and natural language understanding.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="meta-llamallama-2-7b-chat-hf">Meta-llama/Llama-2-7b-chat-hf<a href="#meta-llamallama-2-7b-chat-hf" class="hash-link" aria-label="Direct link to Meta-llama/Llama-2-7b-chat-hf" title="Direct link to Meta-llama/Llama-2-7b-chat-hf">​</a></h3>
<p>Meta-llama/Llama-2-7b-chat-hf is an advanced conversational AI model developed by Meta. It is optimized for chat applications, delivering coherent and contextually relevant responses. With its robust training on extensive dialogue datasets, this model excels in maintaining engaging and dynamic conversations, making it ideal for customer service bots, interactive agents, and other chat-based applications.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-solution">Deploying the Solution<a href="#deploying-the-solution" class="hash-link" aria-label="Direct link to Deploying the Solution" title="Direct link to Deploying the Solution">​</a></h2>
<p>To get started with deploying both <code>mistralai/Mistral-7B-Instruct-v0.2</code> and <code>meta-llama/Llama-2-7b-chat-hf</code> on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>, we will cover the necessary prerequisites and guide you through the deployment process step by step. This process includes setting up the infrastructure, deploying the NVIDIA Triton Inference Server, and creating the Triton client Python application that sends gRPC requests to the Triton server for inferencing.</p>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_BuS1"><p>Important: Deploying on <code>g5.24xlarge</code> instances, which are equipped with multiple GPUs, can be expensive. Ensure you carefully monitor and manage your usage to avoid unexpected costs. Consider setting budget alerts and usage limits to keep track of your expenditures.</p></div></div>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites</span></h2><span class="icon_PckA">👈</span></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nvidia-triton-server-with-vllm-backend">NVIDIA Triton Server with vLLM Backend<a href="#nvidia-triton-server-with-vllm-backend" class="hash-link" aria-label="Direct link to NVIDIA Triton Server with vLLM Backend" title="Direct link to NVIDIA Triton Server with vLLM Backend">​</a></h3>
<p>This blueprint uses <a href="https://github.com/aws-ia/terraform-aws-eks-data-addons/tree/main/helm-charts/nvidia-triton-server" target="_blank" rel="noopener noreferrer">Triton helm chart</a> to install and configure the Triton server on Amazon EKS. The deployment is configured using the following Terraform code in the blueprint.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand the deployment code</summary><div><div class="collapsibleContent_i85q"><div class="language-hcl codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-hcl codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">module &quot;triton_server_vllm&quot; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  depends_on = [module.eks_blueprints_addons.kube_prometheus_stack]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  source     = &quot;aws-ia/eks-data-addons/aws&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  version    = &quot;~&gt; 1.32.0&quot; # ensure to update this to the latest/desired version</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  oidc_provider_arn = module.eks.oidc_provider_arn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  enable_nvidia_triton_server = false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  nvidia_triton_server_helm_config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    version   = &quot;1.0.0&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    timeout   = 120</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    wait      = false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    namespace = kubernetes_namespace_v1.triton.metadata[0].name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    values = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;&lt;-EOT</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      replicaCount: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      image:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        repository: nvcr.io/nvidia/tritonserver</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        tag: &quot;24.06-vllm-python-py3&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      serviceAccount:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        create: false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        name: ${kubernetes_service_account_v1.triton.metadata[0].name}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      modelRepositoryPath: s3://${module.s3_bucket.s3_bucket_id}/model_repository</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      environment:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: model_name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: ${local.default_model_name}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: &quot;LD_PRELOAD&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: &quot;TRANSFORMERS_CACHE&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;/home/triton-server/.cache&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: &quot;shm-size&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;5g&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: &quot;NCCL_IGNORE_DISABLED_P2P&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: tensor_parallel_size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: gpu_memory_utilization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;0.9&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: dtype</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          value: &quot;auto&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      secretEnvironment:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - name: &quot;HUGGING_FACE_TOKEN&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          secretName: ${kubernetes_secret_v1.huggingface_token.metadata[0].name}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          key: &quot;HF_TOKEN&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          cpu: 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          memory: 25Gi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          nvidia.com/gpu: 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          cpu: 6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          memory: 25Gi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          nvidia.com/gpu: 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      nodeSelector:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        NodeGroupType: g5-gpu-karpenter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type: karpenter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      tolerations:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        - key: &quot;nvidia.com/gpu&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          operator: &quot;Exists&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          effect: &quot;NoSchedule&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      EOT</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<p><strong>Note:</strong> The container image that&#x27;s being used for Triton server is <code>nvcr.io/nvidia/tritonserver:24.02-vllm-python-py3</code> and is vLLM backend enabled. You can choose appropriate tags in the <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags" target="_blank" rel="noopener noreferrer">NGC Catalog</a>.</p>
<p><strong>Model Repository</strong>:
The Triton Inference Server serves models from one or more model repositories specified at server startup. Triton can access models from locally accessible file paths and cloud storage locations like Amazon S3.</p>
<p>The directories and files that compose a model repository must follow a required layout. The repository layout should be structured as follows:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand the model directory hierarchy</summary><div><div class="collapsibleContent_i85q"><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;model-repository-path&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;model-name&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [config.pbtxt]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&lt;output-labels-file&gt; ...]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;model-definition-file&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;model-definition-file&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;model-name&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [config.pbtxt]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    [&lt;output-labels-file&gt; ...]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;model-definition-file&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;model-definition-file&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Example:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model-repository/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mistral-7b/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config.pbtxt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    1/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      model.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  llama-2/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config.pbtxt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    1/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      model.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<p>For vLLM enabled Triton model, the model_repository can be found at <code>gen-ai/inference/vllm-nvidia-triton-server-gpu/model_repository</code> location. During the deployment, the blueprint creates an S3 bucket and syncs the local <code>model_repository</code> contents to the S3 bucket.</p>
<p><strong>model.py</strong>: This script uses vLLM library as Triton backend framework and initializes a <code>TritonPythonModel</code> class by loading the model configuration and configuring vLLM engine. The <code>huggingface_hub</code> library&#x27;s login function is used to establish access to the hugging face repository for model access. It then starts an asyncio event loop to process the received requests asynchronously. The script has several functions that processes the inference requests, issues the requests to vLLM backend and return the response.</p>
<p><strong>config.pbtxt</strong>: This is a model configuration file that specifies parameters such as</p>
<ul>
<li>Name - The name of the model must match the <code>name</code> of the model repository directory containing the model.</li>
<li>max_batch_size - The <code>max_batch_size</code> value indicates the maximum batch size that the model supports for the type of batching that can be exploited by Triton</li>
<li>Inputs and Outputs - Each model input and output must specify a name, datatype, and shape. An input shape indicates the shape of an input tensor expected by the model and by Triton in inference requests. An output shape indicates the shape of an output tensor produced by the model and returned by Triton in response to an inference request. Input and output shapes are specified by a combination of <code>max_batch_size</code> and the dimensions specified by <code>input dims</code> or <code>output dims</code>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="verify-deployment">Verify Deployment<a href="#verify-deployment" class="hash-link" aria-label="Direct link to Verify Deployment" title="Direct link to Verify Deployment">​</a></h3>
<p>To verify that the Triton Inference Server has been deployed successfully, run the following command:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all -n triton-vllm</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Output below shows that there is one pod running the Triton server, which is hosting two models.
There is one service to interact with the models, and one ReplicaSet for Triton servers.
Deployment will be horizontally scaled based on custom metrics and the HPA object.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                               READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/nvidia-triton-server-triton-inference-server-c49bd559d-szlpf   1/1     Running   0          13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/nvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   &lt;none&gt;        8000/TCP,8001/TCP,8002/TCP   13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/nvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    &lt;none&gt;        8080/TCP                     13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                           READY   UP-TO-DATE   AVAILABLE   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deployment.apps/nvidia-triton-server-triton-inference-server   1/1     1            1           13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                                     DESIRED   CURRENT   READY   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">replicaset.apps/nvidia-triton-server-triton-inference-server-c49bd559d   1         1         1       13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                                               REFERENCE                                                 TARGETS                        MINPODS   MAXPODS   REPLICAS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">horizontalpodautoscaler.autoscaling/nvidia-triton-server-triton-inference-server   Deployment/nvidia-triton-server-triton-inference-server   &lt;unknown&gt;/80%, &lt;unknown&gt;/80%   1         5         1          13m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This output indicates that the Triton server pods are running, the services are correctly set up, and the deployment is functioning as expected. The Horizontal Pod Autoscaler is also active, ensuring that the number of pods scales based on the specified metrics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="testing-llama-2-7b-chat-and-mistral-7b-chat-models">Testing Llama-2-7b Chat and Mistral-7b Chat Models<a href="#testing-llama-2-7b-chat-and-mistral-7b-chat-models" class="hash-link" aria-label="Direct link to Testing Llama-2-7b Chat and Mistral-7b Chat Models" title="Direct link to Testing Llama-2-7b Chat and Mistral-7b Chat Models">​</a></h3>
<p>It&#x27;s time to test both the Llama-2-7b chat and Mistral-7b chat models. We will run the following commands with the same prompts to verify the outputs generated by both models.</p>
<p>First, execute a port forward to the Triton-inference-server Service using kubectl:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl -n triton-vllm port-forward svc/nvidia-triton-server-triton-inference-server 8001:8001</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Next, run the Triton client for each model using the same prompts:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/gen-ai/inference/vllm-nvidia-triton-server-gpu/triton-client</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m venv .venv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source .venv/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install tritonclient[all]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You will see an output something like below:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Loading inputs from `prompts.txt`...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 3: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 14: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 11: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 15: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 5: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 0: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 7: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 13: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 9: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 16: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 18: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 4: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 8: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 1: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 6: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 12: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 17: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 2: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 19: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model mistral7b - Request 10: 0.02 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Storing results into `mistral_results.txt`...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Total time for all requests: 0.00 seconds (0.11 milliseconds)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PASS: vLLM example</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Output for <code>mistral_results.txt</code> should like below:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand Mistral results partial output</summary><div><div class="collapsibleContent_i85q"><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Keep short answers of no more than 100 sentences.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;&lt;/SYS&gt;&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">What are the key differences between traditional machine learning models and very large language models (vLLM)?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[/INST] Traditional machine learning models (MLMs) are trained on specific datasets and features to learn patterns and make predictions based on that data. They require labeled data for training and are limited by the size and diversity of the training data. MLMs can be effective for solving structured problems, such as image recognition or speech recognition.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Very Large Language Models (vLLMs), on the other hand, are trained on vast amounts of text data using deep learning techniques. They learn to generate human-like text based on the input they receive. vLLMs can understand and generate text in a more contextually aware and nuanced way than MLMs. They can also perform a wider range of tasks, such as text summarization, translation, and question answering. However, vLLMs can be more computationally expensive and require large amounts of data and power to train. They also have the potential to generate inaccurate or biased responses if not properly managed.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">=========</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Keep short answers of no more than 100 sentences.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;&lt;/SYS&gt;&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Can you explain how TensorRT optimizes LLM inference on NVIDIA hardware?</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[/INST] TensorRT is a deep learning inference optimization tool from NVIDIA. It utilizes dynamic and static analysis to optimize deep learning models for inference on NVIDIA GPUs. For Maximum Likelihood Modeling (LLM) inference, TensorRT applies the following optimizations:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. Model Optimization: TensorRT converts the LLM model into an optimized format, such as INT8 or FP16, which reduces memory usage and increases inference speed.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Engine Generation: TensorRT generates a custom engine for the optimized model, which includes kernel optimizations for specific NVIDIA GPUs.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Memory Optimization: TensorRT minimizes memory usage by using data layout optimizations, memory pooling, and other techniques.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Execution Optimization: TensorRT optimizes the execution of the engine on the GPU by scheduling and managing thread execution, reducing latency and increasing throughput.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5. I/O Optimization: TensorRT optimizes input and output data transfer between the host and the GPU, reducing the time spent on data transfer and increasing overall inference speed.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6. Dynamic Batching: TensorRT dynamically batches input data to maximize GPU utilization and reduce latency.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">7. Multi-Streaming: TensorRT supports multi-streaming, allowing multiple inference requests to be processed concurrently, increasing overall throughput.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">8. Profiling and Monitoring: TensorRT provides profiling and monitoring tools to help developers identify performance bottlenecks and optimize their models further.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Overall, TensorRT optimizes LLM inference on NVIDIA hardware by applying a combination of model, engine, memory, execution, I/O, dynamic batching, multi-streaming, and profiling optimizations.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<p>Now, try to run the inference on the Llama-2-7b-chat model with the same prompts and observe the output under a new file called <code>llama2_results.txt</code>.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Output should look like:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Loading inputs from `prompts.txt`...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 11: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 15: 0.02 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 3: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 8: 0.03 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 5: 0.02 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 0: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 14: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 16: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 19: 0.02 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 4: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 1: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 10: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 9: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 7: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 18: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 12: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 2: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 6: 0.00 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 17: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model llama2 - Request 13: 0.01 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Storing results into `llama2_results.txt`...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Total time for all requests: 0.00 seconds (0.18 milliseconds)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PASS: vLLM example</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="observability">Observability<a href="#observability" class="hash-link" aria-label="Direct link to Observability" title="Direct link to Observability">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-with-aws-cloudwatch-and-neuron-monitor">Observability with AWS CloudWatch and Neuron Monitor<a href="#observability-with-aws-cloudwatch-and-neuron-monitor" class="hash-link" aria-label="Direct link to Observability with AWS CloudWatch and Neuron Monitor" title="Direct link to Observability with AWS CloudWatch and Neuron Monitor">​</a></h3>
<p>This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the blueprint integrates GPU metrics using NVIDIA&#x27;s DCGM plugin, which is essential for monitoring high-performance GPU workloads. For machine learning models running on AWS Inferentia or Trainium, the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide" target="_blank" rel="noopener noreferrer">Neuron Monitor plugin</a> is added to capture and report Neuron-specific metrics.</p>
<p>All metrics, including container insights, GPU performance, and Neuron metrics, are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively.</p>
<p>In addition to deploying CloudWatch EKS addon, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability.</p>
<p>First, let&#x27;s verify the services deployed by the Kube Prometheus stack:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc -n kube-prometheus-stack</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should see output similar to this:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc -n kube-prometheus-stack</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kube-prometheus-stack-grafana                    ClusterIP   172.20.252.10    &lt;none&gt;        80/TCP              11d</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kube-prometheus-stack-kube-state-metrics         ClusterIP   172.20.34.181    &lt;none&gt;        8080/TCP            11d</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kube-prometheus-stack-operator                   ClusterIP   172.20.186.93    &lt;none&gt;        443/TCP             11d</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kube-prometheus-stack-prometheus                 ClusterIP   172.20.147.64    &lt;none&gt;        9090/TCP,8080/TCP   11d</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kube-prometheus-stack-prometheus-node-exporter   ClusterIP   172.20.171.165   &lt;none&gt;        9100/TCP            11d</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prometheus-operated                              ClusterIP   None             &lt;none&gt;        9090/TCP            11d</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>To expose the NVIDIA Triton server metrics, we have deployed a metrics service(<code>nvidia-triton-server-triton-inference-server-metrics</code>) on port <code>8080</code>. Verify it by running</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc -n triton-vllm</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The output should be:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc -n triton-vllm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nvidia-triton-server-triton-inference-server           ClusterIP   172.20.193.97   &lt;none&gt;        8000/TCP,8001/TCP,8002/TCP   34m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nvidia-triton-server-triton-inference-server-metrics   ClusterIP   172.20.5.247    &lt;none&gt;        8080/TCP                     34m</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This confirms that the NVIDIA Triton server metrics are being scraped by the Prometheus server. You can visualize these metrics using the Grafana dashboard.</p>
<p>In the Grafana dashboard below, you can see several important metrics:</p>
<ul>
<li><strong>Average GPU Power Usage</strong>: This gauge shows the current power usage of the GPU, which is crucial for monitoring the efficiency and performance of your inference tasks.</li>
<li><strong>Compute Time (milliseconds)</strong>: This bar graph displays the time taken to compute inference requests, helping identify any latency issues.</li>
<li><strong>Cumulative Inference Requests</strong>: This graph shows the total number of inference requests processed over time, providing insights into the workload and performance trends.</li>
<li><strong>Queue Time (milliseconds)</strong>: This line graph indicates the time requests spend in the queue before being processed, highlighting potential bottlenecks in the system.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="NVIDIA Triton Server" src="/data-on-eks/assets/images/triton-observability-c1fbf7456677ce1c8ed6f423f0cce09e.png" width="1920" height="1080" class="img_ev3q"></p>
<p>To create a new Grafana dashboard to monitor these metrics, follow the steps below:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">- Port-forward Grafana service:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n kube-prometheus-stack</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Grafana Admin user</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">admin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Get secret name from Terraform output</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform output grafana_secret_name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Get admin user password</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Login to Grafana:</strong></p>
<ul>
<li>Open your web browser and navigate to <a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a>.</li>
<li>Login with the username <code>admin</code> and the password retrieved from AWS Secrets Manager.</li>
</ul>
<p><strong>Import an Open Source Grafana Dashboard:</strong></p>
<ul>
<li>Once logged in, click on the &quot;+&quot; icon on the left sidebar and select &quot;Import&quot;.</li>
<li>Enter the following URL to import the dashboard JSON: <a href="https://github.com/triton-inference-server/server/blob/main/deploy/k8s-onprem/dashboard.json" target="_blank" rel="noopener noreferrer">Triton Server Grafana Dashboard</a></li>
<li>Follow the prompts to complete the import process.</li>
</ul>
<p>You should now see the metrics displayed on your new Grafana dashboard, allowing you to monitor the performance and health of your NVIDIA Triton Inference Server deployment.</p>
<p><img decoding="async" loading="lazy" alt="triton-grafana-dash2" src="/data-on-eks/assets/images/triton-grafana-dash2-6bc3f332abbb3cd2dc6da59d2ecd46b7.png" width="3836" height="1938" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Deploying and managing multiple large language models on Amazon EKS with NVIDIA Triton Inference Server and vLLM backend offers a powerful and scalable solution for modern AI applications. By following this blueprint, you have set up the necessary infrastructure, deployed the Triton server, and configured robust observability using the Kube Prometheus stack and Grafana.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Direct link to Cleanup" title="Direct link to Cleanup">​</a></h2>
<p>Finally, we&#x27;ll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.</p>
<p><strong>Cleanup the EKS Cluster:</strong>
This script will cleanup the environment using <code>-target</code> option to ensure all the resources are deleted in correct order.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export AWS_DEAFULT_REGION=&quot;DEPLOYED_EKS_CLUSTER_REGION&gt;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/ai-ml/nvidia-triton-server/ &amp;&amp; chmod +x cleanup.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/GPUs/vLLM-NVIDIATritonServer.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-on-eks/docs/gen-ai/inference/GPUs/vLLM-rayserve"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">RayServe with vLLM</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-on-eks/docs/gen-ai/inference/GPUs/stablediffusion-gpus"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Stable Diffusion on GPU</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-to-expect" class="table-of-contents__link toc-highlight">What to Expect</a><ul><li><a href="#mistralaimistral-7b-instruct-v02" class="table-of-contents__link toc-highlight">Mistralai/Mistral-7B-Instruct-v0.2</a></li><li><a href="#meta-llamallama-2-7b-chat-hf" class="table-of-contents__link toc-highlight">Meta-llama/Llama-2-7b-chat-hf</a></li></ul></li><li><a href="#deploying-the-solution" class="table-of-contents__link toc-highlight">Deploying the Solution</a><ul><li><a href="#deploy" class="table-of-contents__link toc-highlight">Deploy</a></li><li><a href="#verify-the-resources" class="table-of-contents__link toc-highlight">Verify the resources</a></li><li><a href="#nvidia-triton-server-with-vllm-backend" class="table-of-contents__link toc-highlight">NVIDIA Triton Server with vLLM Backend</a></li><li><a href="#verify-deployment" class="table-of-contents__link toc-highlight">Verify Deployment</a></li><li><a href="#testing-llama-2-7b-chat-and-mistral-7b-chat-models" class="table-of-contents__link toc-highlight">Testing Llama-2-7b Chat and Mistral-7b Chat Models</a></li></ul></li><li><a href="#observability" class="table-of-contents__link toc-highlight">Observability</a><ul><li><a href="#observability-with-aws-cloudwatch-and-neuron-monitor" class="table-of-contents__link toc-highlight">Observability with AWS CloudWatch and Neuron Monitor</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Started</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-on-eks/docs/introduction/intro">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>