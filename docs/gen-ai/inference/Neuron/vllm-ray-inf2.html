<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-gen-ai/inference/Neuron/vllm-ray-inf2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">Llama-3-8B with vLLM on Inferentia2 | Data on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama-3-8B with vLLM on Inferentia2 | Data on EKS"><meta data-rh="true" name="description" content="Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance."><meta data-rh="true" property="og:description" content="Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance."><link data-rh="true" rel="icon" href="/data-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2" hreflang="x-default"><link rel="stylesheet" href="/data-on-eks/assets/css/styles.74de95c8.css">
<script src="/data-on-eks/assets/js/runtime~main.cf694a6c.js" defer="defer"></script>
<script src="/data-on-eks/assets/js/main.2631361c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-on-eks/"><div class="navbar__logo"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/data-on-eks/docs/introduction/intro">Introduction</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-on-eks/docs/gen-ai">Gen AI</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/blueprints/amazon-emr-on-eks">Blueprints</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/bestpractices/intro">Best Practices</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/benchmarks/emr-on-eks">Benchmarks</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/resources/intro">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-on-eks/docs/gen-ai">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/data-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Collapse sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/GPUs/ray-vllm-deepseek">GPUs</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2">Neuron</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/vllm-ray-inf2">Llama-3-8B with vLLM on Inferentia2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/Mistral-7b-inf2">Mistral-7B on Inferentia2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/llama3-inf2">Llama-3-8B on Inferentia2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/llama2-inf2">Llama-2 on Inferentia2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/stablediffusion-inf2">Stable Diffusion on Inferentia2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Neuron/rayserve-ha">Ray Serve High Availability</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/data-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Expand sidebar category &#x27;Training on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/data-on-eks/docs/category/inference-on-eks"><span itemprop="name">Inference on EKS</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Neuron</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Llama-3-8B with vLLM on Inferentia2</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn&#x27;t working, it’s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren&#x27;t initializing, check the logs for Karpenter or Node groups to resolve the issue.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_BuS1"><p>Note: Use of this Llama-3 Instruct model is governed by the Meta license.
In order to download the model weights and tokenizer, please visit the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noopener noreferrer">website</a> and accept the license before requesting access.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects.</p></div></div>
<header><h1>Serving LLMs with RayServe and vLLM on AWS Neuron</h1></header>
<p>Welcome to the comprehensive guide on deploying LLMs on Amazon Elastic Kubernetes Service (EKS) using <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> and AWS Neuron.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-aws-neuron">What is AWS Neuron?<a href="#what-is-aws-neuron" class="hash-link" aria-label="Direct link to What is AWS Neuron?" title="Direct link to What is AWS Neuron?">​</a></h3>
<p>In this tutorial, you&#x27;ll leverage <a href="https://aws.amazon.com/machine-learning/neuron/" target="_blank" rel="noopener noreferrer">AWS Neuron</a>, a powerful SDK that optimizes deep learning performance on AWS Inferentia and Trainium accelerators. Neuron seamlessly integrates with frameworks like PyTorch and TensorFlow, providing a comprehensive toolkit for developing, profiling, and deploying high-performance machine learning models on specialized EC2 instances such as Inf1, Inf2, Trn1, and Trn1n.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-vllm">What is vLLM?<a href="#what-is-vllm" class="hash-link" aria-label="Direct link to What is vLLM?" title="Direct link to What is vLLM?">​</a></h3>
<p><a href="https://docs.vllm.ai/en/latest/" target="_blank" rel="noopener noreferrer">vLLM</a> is a high-performance library for LLM inference and serving, designed to maximize throughput and minimize latency. At its core, vLLM utilizes <a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html" target="_blank" rel="noopener noreferrer">PagedAttention</a>, an innovative attention algorithm that dramatically improves memory efficiency, allowing for optimal utilization of GPU resources. This open-source solution offers seamless integration through its Python API and OpenAI-compatible server, enabling developers to deploy and scale large language models like Llama 3 with unprecedented efficiency in production environments.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-rayserve">What is RayServe?<a href="#what-is-rayserve" class="hash-link" aria-label="Direct link to What is RayServe?" title="Direct link to What is RayServe?">​</a></h3>
<p>Ray Serve is a scalable model serving library built on top of Ray, designed for deploying machine learning models and AI applications with features like framework-agnostic deployment, model composition, and built-in scaling. You will also encounter RayService, which is a Kubernetes custom resource that&#x27;s part of the KubeRay project, used to deploy and manage Ray Serve applications on Kubernetes clusters.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-llama-3-8b-instruct">What is Llama-3-8B Instruct?<a href="#what-is-llama-3-8b-instruct" class="hash-link" aria-label="Direct link to What is Llama-3-8B Instruct?" title="Direct link to What is Llama-3-8B Instruct?">​</a></h3>
<p>Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.</p>
<p>More information on Llama3 sizes and model architecture can be found <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-aws-accelerators">Why AWS Accelerators?<a href="#why-aws-accelerators" class="hash-link" aria-label="Direct link to Why AWS Accelerators?" title="Direct link to Why AWS Accelerators?">​</a></h3>
<p><strong>Scalability and Availability</strong></p>
<p>One of the key challenges in deploying large language models (<code>LLMs</code>) like Llama-3 is the scalability and availability of suitable hardware. Traditional <code>GPU</code> instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively.</p>
<p>In contrast, <code>Trn1/Inf2</code> instances, such as <code>trn1.32xlarge</code>, <code>trn1n.32xlarge</code>, <code>inf2.24xlarge</code> and <code>inf2.48xlarge</code>, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your <code>Llama-3</code> models as needed, without resource bottlenecks or delays.</p>
<p><strong>Cost Optimization</strong></p>
<p>Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing. <strong>Trn1/Inf2</strong> instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.</p>
<p><strong>Performance Boost</strong></p>
<p>While Llama-3 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-3&#x27;s inference speeds. This translates to faster response times and improved user experiences when deploying Llama-3 on Trn1/Inf2 instances.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="solution-architecture">Solution Architecture<a href="#solution-architecture" class="hash-link" aria-label="Direct link to Solution Architecture" title="Direct link to Solution Architecture">​</a></h2>
<p>In this section, we will delve into the architecture of our solution, which combines Llama-3 model, <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> and <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">Inferentia2</a> on Amazon EKS.</p>
<p><img decoding="async" loading="lazy" alt="Llama-3-inf2" src="/data-on-eks/assets/images/ray-vllm-inf2-f4743ae82b32bc6cfd823bcd97a29144.png" width="1642" height="834" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-solution">Deploying the Solution<a href="#deploying-the-solution" class="hash-link" aria-label="Direct link to Deploying the Solution" title="Direct link to Deploying the Solution">​</a></h2>
<p>To get started with deploying <code>Llama-3-8B-instruct</code> on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>, we will cover the necessary prerequisites and guide you through the deployment process step by step.</p>
<p>This includes setting up the infrastructure using AWS Inferentia instances and deploying the <strong>Ray cluster</strong>.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-ray-cluster-with-llama3-model">Deploying the Ray Cluster with Llama3 Model<a href="#deploying-the-ray-cluster-with-llama3-model" class="hash-link" aria-label="Direct link to Deploying the Ray Cluster with Llama3 Model" title="Direct link to Deploying the Ray Cluster with Llama3 Model">​</a></h2>
<p>In this tutorial, we leverage the KubeRay operator, which extends Kubernetes with custom resource definitions for Ray-specific constructs like RayCluster, RayJob, and RayService. The operator watches for user events related to these resources, automatically creates necessary Kubernetes artifacts to form Ray clusters, and continuously monitors cluster state to ensure the desired configuration matches the actual state. It handles lifecycle management including setup, dynamic scaling of worker groups, and teardown, abstracting away the complexity of managing Ray applications on Kubernetes.</p>
<p>Each Ray cluster consists of a head node pod and a collection of worker node pods, with optional autoscaling support to size clusters according to workload requirements. KubeRay supports heterogeneous compute nodes (including GPUs) and running multiple Ray clusters with different Ray versions in the same Kubernetes cluster. Additionally, KubeRay can integrate with AWS Inferentia accelerators, enabling efficient deployment of large language models like Llama 3 on specialized hardware, potentially improving performance and cost-effectiveness for machine learning inference tasks.</p>
<p>Having deployed the EKS cluster with all the necessary components, we can now proceed with the steps to deploy <code>NousResearch/Meta-Llama-3-8B-Instruct</code> using <code>RayServe</code> and <code>vLLM</code> on AWS Accelerators.</p>
<p><strong>Step 1:</strong> To deploy the RayService cluster, navigate to the directory containing the <code>vllm-rayserve-deployment.yaml</code> file and execute the <code>kubectl apply</code> command in your terminal.
This will apply the RayService configuration and deploy the cluster on your EKS setup.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f vllm-rayserve-deployment.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Optional Configuration</strong></p>
<p>By default, an <code>inf2.8xlarge</code> instance will be provisioned. If you would like to use <code>inf2.48xlarge</code>, modify the file <code>vllm-rayserve-deployment.yaml</code> to change <code>resources</code> section under <code>worker</code> container.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: &quot;30&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: &quot;110G&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: &quot;1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: &quot;30&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: &quot;110G&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: &quot;1&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>to the following:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: &quot;90&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: &quot;360G&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: &quot;12&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: &quot;90&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: &quot;360G&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: &quot;12&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Step 2:</strong> Verify the deployment by running the following commands</p>
<p>To ensure that the deployment has been successfully completed, run the following commands:</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Deployment process may take up to <strong>10 minutes</strong>. The Head Pod is expected to be ready within 5 to 6 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.</p></div></div>
<p>According to the RayServe configuration, you will have one Ray head pod running on an <code>x86</code> instance and one worker pod running on a <code>inf2</code> instance. You can modify the RayServe YAML file to run multiple replicas; however, be aware that each additional replica can potentially create new instances.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods -n vllm</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                      READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">lm-llama3-inf2-raycluster-ksh7w-worker-inf2-group-dcs5n   1/1     Running   0          2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-raycluster-ksh7w-head-4ck8f              2/2     Running   0          2d4h</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This deployment also configures a service with multiple ports. Port <strong>8265</strong> is designated for the Ray dashboard, and port <strong>8000</strong> is for the vLLM inference server endpoint.</p>
<p>Run the following command to verify the services:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc -n vllm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm                         ClusterIP   172.20.23.54    &lt;none&gt;        8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP   2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-head-svc    ClusterIP   172.20.18.130   &lt;none&gt;        6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-serve-svc   ClusterIP   172.20.153.10   &lt;none&gt;        8000/TCP                                        2d4h</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>To access the Ray dashboard, you can port-forward the relevant port to your local machine:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl -n vllm port-forward svc/vllm 8265:8265</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can then access the web UI at <a href="http://localhost:8265" target="_blank" rel="noopener noreferrer">http://localhost:8265</a>, which displays the deployment of jobs and actors within the Ray ecosystem.</p>
<p><img decoding="async" loading="lazy" alt="RayServe Deployment" src="/data-on-eks/assets/images/ray-dashboard-vllm-llama3-inf2-1ec06c016b0ffdf955c973406854f2f0.png" width="3456" height="1202" class="img_ev3q"></p>
<p>Once the deployment is complete, the Controller and Proxy status should be <code>HEALTHY</code> and Application status should be <code>RUNNING</code></p>
<p><img decoding="async" loading="lazy" alt="RayServe Deployment Logs" src="/data-on-eks/assets/images/ray-logs-vllm-llama3-inf2-5c5963506ac6563dae4bd04a4f6a16bf.png" width="3450" height="1224" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="to-test-the-llama3-model">To Test the Llama3 Model<a href="#to-test-the-llama3-model" class="hash-link" aria-label="Direct link to To Test the Llama3 Model" title="Direct link to To Test the Llama3 Model">​</a></h3>
<p>Now it&#x27;s time to test the <code>Meta-Llama-3-8B-Instruct</code> chat model. We&#x27;ll use a Python client script to send prompts to the RayServe inference endpoint and verify the outputs generated by the model.</p>
<p>First, execute a port forward to the <code>vllm-llama3-inf2-serve-svc</code> Service using kubectl:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><code>openai-client.py</code> uses the HTTP POST method to send a list of prompts to the inference endpoint for text completion and Q&amp;A, targeting the vllm server.</p>
<p>To run the Python client application in a virtual environment, follow these steps:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m venv .venv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source .venv/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install openai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 openai-client.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You will see an output something like below in the terminal:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand Python Client Terminal output</summary><div><div class="collapsibleContent_i85q"><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Example 1 - Simple chat completion:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The capital of India is New Delhi.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Example 2 - Chat completion with different parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The twin suns of Tatooine set slowly in the horizon, casting a warm orange glow over the bustling spaceport of Anchorhead. Amidst the hustle and bustle, a young farm boy named Anakin Skywalker sat atop a dusty speeder, his eyes fixed on the horizon as he dreamed of adventure beyond the desert planet.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">As the suns dipped below the dunes, Anakin&#x27;s uncle, Owen Lars, called out to him from the doorway of their humble moisture farm. &quot;Anakin, it&#x27;s time to head back! Your aunt and I have prepared a special dinner in your honor.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">But Anakin was torn. He had received a strange message from an unknown sender, hinting at a great destiny waiting for him. Against his uncle&#x27;s warnings, Anakin decided to investigate further, sneaking away into the night to follow the mysterious clues.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">As he rode his speeder through the desert, the darkness seemed to grow thicker, and the silence was broken only by the distant</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Example 3 - Streaming chat completion:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">I&#x27;d be happy to help you with that. Here we go:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">7...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">8...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">9...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">10!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Let me know if you have any other requests!</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="observability">Observability<a href="#observability" class="hash-link" aria-label="Direct link to Observability" title="Direct link to Observability">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-with-aws-cloudwatch-and-neuron-monitor">Observability with AWS CloudWatch and Neuron Monitor<a href="#observability-with-aws-cloudwatch-and-neuron-monitor" class="hash-link" aria-label="Direct link to Observability with AWS CloudWatch and Neuron Monitor" title="Direct link to Observability with AWS CloudWatch and Neuron Monitor">​</a></h3>
<p>This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the addon utilizes the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide" target="_blank" rel="noopener noreferrer">Neuron Monitor plugin</a> to capture and report Neuron-specific metrics.</p>
<p>All metrics, including container insights and Neuron metrics such as Neuron Core utilization, NeuronCore memory usage are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively.</p>
<p><img decoding="async" loading="lazy" alt="CloudWatch-neuron-monitor" src="/data-on-eks/assets/images/neuron-monitor-cwci-4ed152729b7e072f5b1d2f79c25d75b3.png" width="3070" height="1916" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-webui-deployment">Open WebUI Deployment<a href="#open-webui-deployment" class="hash-link" aria-label="Direct link to Open WebUI Deployment" title="Direct link to Open WebUI Deployment">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> is compatible only with models that work with the OpenAI API server and Ollama.</p></div></div>
<p><strong>1. Deploy the WebUI</strong></p>
<p>Deploy the <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> by running the following command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f openai-webui-deployment.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>2. Port Forward to Access WebUI</strong></p>
<p><strong>Note</strong> If you&#x27;re running a port forward already to test the inference with python client, then press <code>ctrl+c</code> to interrupt that.</p>
<p>Use kubectl port-forward to access the WebUI locally:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/open-webui 8081:80 -n openai-webui</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>3. Access the WebUI</strong></p>
<p>Open your browser and go to <a href="http://localhost:8081" target="_blank" rel="noopener noreferrer">http://localhost:8081</a></p>
<p><strong>4. Sign Up</strong></p>
<p>Sign up using your name, email, and a dummy password.</p>
<p><strong>5. Start a New Chat</strong></p>
<p>Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/data-on-eks/assets/images/openweb-ui-ray-vllm-inf2-1-ca4458ab24555d56156a7a393d81b5eb.png" width="2848" height="1442" class="img_ev3q"></p>
<p><strong>6. Enter Test Prompt</strong></p>
<p>Enter your prompt, and you will see the streaming results, as shown below:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/data-on-eks/assets/images/openweb-ui-ray-vllm-inf2-2-bd168c90b7849105772e8332dcad5f86.png" width="1436" height="611" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-benchmarking-with-llmperf-tool">Performance Benchmarking with LLMPerf Tool<a href="#performance-benchmarking-with-llmperf-tool" class="hash-link" aria-label="Direct link to Performance Benchmarking with LLMPerf Tool" title="Direct link to Performance Benchmarking with LLMPerf Tool">​</a></h2>
<p><a href="https://github.com/ray-project/llmperf/blob/main/README.md" target="_blank" rel="noopener noreferrer">LLMPerf</a> is an open-source tool designed for benchmarking the performance of large language models (LLMs).</p>
<p>LLMPerf tool connects to the vllm service via port 8000 using the port forwarding setup above done using the command <code>kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000</code>.</p>
<p>Execute the commands below in your terminal.</p>
<p>Clone the LLMPerf repository:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/ray-project/llmperf.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd llmperf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install -e .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install pandas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install ray</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Create the <code>vllm_benchmark.sh</code> file using the command below:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cat &lt;&lt; &#x27;EOF&#x27; &gt; vllm_benchmark.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model=${1:-NousResearch/Meta-Llama-3-8B-Instruct}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vu=${2:-1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export OPENAI_API_KEY=EMPTY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export OPENAI_API_BASE=&quot;http://localhost:8000/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export TOKENIZERS_PARALLELISM=true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#if you have more vllm servers, append the below line to the above</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#;http://localhost:8001/v1;http://localhost:8002/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max_requests=$(expr ${vu} \* 8 )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">date_str=$(date &#x27;+%Y-%m-%d-%H-%M-%S&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python ./token_benchmark_ray.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --model ${model} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --mean-input-tokens 512 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --stddev-input-tokens 20 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --mean-output-tokens 245 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --stddev-output-tokens 20 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --max-num-completed-requests ${max_requests} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --timeout 7200 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --num-concurrent-requests ${vu} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --results-dir &quot;vllm_bench_results/${date_str}&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --llm-api openai \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       --additional-sampling-params &#x27;{}&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><code>--mean-input-tokens</code>: specifies the average number of tokens in the input prompts</p>
<p><code>--stddev-input-tokens</code>: specifies the variability in input token lengths for creating a more realistic testing environment</p>
<p><code>--mean-output-tokens</code>: specifies the average number of tokens expected in the model&#x27;s output to simulate realistic response lengths</p>
<p><code>--stddev-output-tokens</code>: specifies the variability in output token lengths introducing diversity in response sizes</p>
<p><code>--max-num-completed-requests</code>: sets the maximum number of requests to process</p>
<p><code>--num-concurrent-requests</code>: specifies the number of simultaneous requests to simulate parallel workload</p>
<p>The command below executes the benchmarking script with the specified model, <code>NousResearch/Meta-Llama-3-8B-Instruct</code>, and sets the number of virtual users to 2. This results in the benchmark testing the model&#x27;s performance with 2 concurrent requests, calculating a maximum of 16 requests to be processed.</p>
<p>Execute the command below:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should see similar output like the following:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won&#x27;t be available and only tokenizers, configuration and file/data utilities can be used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">You are using the default legacy behaviour of the &lt;class &#x27;transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast&#x27;&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2024-09-03 09:54:45,976	INFO worker.py:1783 -- Started a local Ray instance.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  0%|                                                                                                                                                                                                                                                    | 0/16 [00:00&lt;?, ?it/s]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 12%|█████████████████████████████▌                                                                                                                                                                                                              | 2/16 [00:17&lt;02:00,  8.58s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 25%|███████████████████████████████████████████████████████████                                                                                                                                                                                 | 4/16 [00:33&lt;01:38,  8.20s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 38%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                   | 6/16 [00:47&lt;01:17,  7.75s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                      | 8/16 [01:00&lt;00:58,  7.36s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 62%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                        | 10/16 [01:15&lt;00:43,  7.31s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ █████████████████████████████████████████████████████████▎                                                          | 12/16 [01:29&lt;00:28,  7.20s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                             | 14/16 [01:45&lt;00:15,  7.52s/it]Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [02:01&lt;00:00,  7.58s/it]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">\Results for token benchmark for NousResearch/Meta-Llama-3-8B-Instruct queried with the openai api.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inter_token_latency_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 0.051964785839225695</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 0.053331799814278796</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 0.05520852723583741</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 0.05562424625711179</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 0.05629651696856784</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 0.057518213120178636</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 0.053548951905597324</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 0.0499955879607504</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 0.05782363715808134</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 0.002070751885022901</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ttft_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 1.5284210312238429</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 1.7579061459982768</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 1.8209733433031943</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 1.842437624989543</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 1.852818323241081</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 1.8528624982456676</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 1.5821313202395686</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 0.928935999982059</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 1.8528735419968143</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 0.37523908630204694</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">end_to_end_latency_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 13.74749460403109</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 14.441407957987394</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 15.53337344751344</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 16.104882833489683</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 16.366086292022374</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 16.395070491998922</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 14.528114874927269</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 10.75658329098951</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 16.40231654199306</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 1.4182672949824733</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">request_output_throughput_token_per_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 18.111220396798153</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 18.703139371912407</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 19.243016652511997</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 19.37836414194298</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 19.571455249271224</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 19.915057038539217</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 18.682678715983627</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 17.198769813363445</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 20.000957485856215</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 0.725563381521316</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">number_input_tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 502.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 509.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 516.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 546.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 569.25</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 574.65</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 515.25</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 485</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 576</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 24.054105678657024</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">number_output_tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 = 259.75</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 = 279.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p75 = 291.75</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p90 = 297.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p95 = 300.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p99 = 301.7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean = 271.625</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min = 185</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max = 302</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    stddev = 29.257192847799555</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Number Of Errored Requests: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Overall Output Throughput: 35.827933968528434</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Number Of Completed Requests: 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Completed Requests Per Minute: 7.914131755588426</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can try generating benchmarking results with multiple concurrent requests to understand how performance varies with increasing number of concurrent requests:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-benchmarking-metrics">Performance Benchmarking Metrics<a href="#performance-benchmarking-metrics" class="hash-link" aria-label="Direct link to Performance Benchmarking Metrics" title="Direct link to Performance Benchmarking Metrics">​</a></h3>
<p>You can find the results of the benchmarking script  under <code>vllm_bench_results</code> directory in the <code>llmperf</code> directory. The results are stored in folders following a date-time naming convention. New folders are created every time the benchmarking script is executed.</p>
<p>You will find that the results for every execution of the benchmarking script comprise of 2 files in the format below:</p>
<p><code>NousResearch-Meta-Llama-3-8B-Instruct_512_245_summary_32.json</code> - contains the summary of performance metrics across all the request/response pairs.</p>
<p><code>NousResearch-Meta-Llama-3-8B-Instruct_512_245_individual_responses.json</code> - contains performance metrics for each and every request / response pair.</p>
<p>Each of these files contain the following Performance Benchmarking Metrics:</p>
<p><code>results_inter_token_latency_s_*</code>: Also referred to as Token generation latency (TPOT).Inter-Token latency refers to the average time elapsed between generating consecutive output tokens by a large language model (LLM) during the decoding or generation phase</p>
<p><code>results_ttft_s_*</code>: Time taken to generate the first token (TTFT)</p>
<p><code>results_end_to_end_s_*</code>: End-to-End latency - total time taken from when a user submits an input prompt to when the complete output response is generated by the LLM</p>
<p><code>results_request_output_throughput_token_per_s_*</code>: Number of output tokens generated by a large language model (LLM) per second across all user requests or queries</p>
<p><code>results_number_input_tokens_*</code>: Number of input tokens in the requests (Input length)</p>
<p><code>results_number_output_tokens_*</code>: Number of output tokens in the requests (Output length)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In summary, when it comes to deploying and scaling Llama-3, AWS Trn1/Inf2 instances offer a compelling advantage.
They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you&#x27;re building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-3 on the AWS cloud.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Direct link to Cleanup" title="Direct link to Cleanup">​</a></h2>
<p>Finally, we&#x27;ll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.</p>
<p>Delete the RayCluster</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/gen-ai/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete -f vllm-rayserve-deployment.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Destroy the EKS Cluster and resources</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd data-on-eks/ai-ml/trainium-inferentia/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/Neuron/vllm-ray-inf2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-on-eks/docs/gen-ai/inference/GPUs/nvidia-nim-llama3"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">NVIDIA NIM LLM on Amazon EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-on-eks/docs/gen-ai/inference/Neuron/Mistral-7b-inf2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Mistral-7B on Inferentia2</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-aws-neuron" class="table-of-contents__link toc-highlight">What is AWS Neuron?</a></li><li><a href="#what-is-vllm" class="table-of-contents__link toc-highlight">What is vLLM?</a></li><li><a href="#what-is-rayserve" class="table-of-contents__link toc-highlight">What is RayServe?</a></li><li><a href="#what-is-llama-3-8b-instruct" class="table-of-contents__link toc-highlight">What is Llama-3-8B Instruct?</a></li><li><a href="#why-aws-accelerators" class="table-of-contents__link toc-highlight">Why AWS Accelerators?</a></li><li><a href="#solution-architecture" class="table-of-contents__link toc-highlight">Solution Architecture</a></li><li><a href="#deploying-the-solution" class="table-of-contents__link toc-highlight">Deploying the Solution</a><ul><li><a href="#deploy" class="table-of-contents__link toc-highlight">Deploy</a></li><li><a href="#verify-the-resources" class="table-of-contents__link toc-highlight">Verify the resources</a></li><li><a href="#verify-neuron-plugin" class="table-of-contents__link toc-highlight">Verify Neuron Plugin</a></li><li><a href="#verify-neuron-scheduler" class="table-of-contents__link toc-highlight">Verify Neuron Scheduler</a></li></ul></li><li><a href="#deploying-the-ray-cluster-with-llama3-model" class="table-of-contents__link toc-highlight">Deploying the Ray Cluster with Llama3 Model</a><ul><li><a href="#to-test-the-llama3-model" class="table-of-contents__link toc-highlight">To Test the Llama3 Model</a></li></ul></li><li><a href="#observability" class="table-of-contents__link toc-highlight">Observability</a><ul><li><a href="#observability-with-aws-cloudwatch-and-neuron-monitor" class="table-of-contents__link toc-highlight">Observability with AWS CloudWatch and Neuron Monitor</a></li></ul></li><li><a href="#open-webui-deployment" class="table-of-contents__link toc-highlight">Open WebUI Deployment</a></li><li><a href="#performance-benchmarking-with-llmperf-tool" class="table-of-contents__link toc-highlight">Performance Benchmarking with LLMPerf Tool</a><ul><li><a href="#performance-benchmarking-metrics" class="table-of-contents__link toc-highlight">Performance Benchmarking Metrics</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Started</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-on-eks/docs/introduction/intro">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>