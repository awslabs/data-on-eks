<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-gen-ai/inference/Llama2">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Llama-2 on Inferentia | Data on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Llama2"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama-2 on Inferentia | Data on EKS"><meta data-rh="true" name="description" content="Note: Use of this Llama-2 model is governed by the Meta license."><meta data-rh="true" property="og:description" content="Note: Use of this Llama-2 model is governed by the Meta license."><link data-rh="true" rel="icon" href="/data-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Llama2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Llama2" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/Llama2" hreflang="x-default"><link rel="stylesheet" href="/data-on-eks/assets/css/styles.a6bd98bc.css">
<link rel="preload" href="/data-on-eks/assets/js/runtime~main.a405c129.js" as="script">
<link rel="preload" href="/data-on-eks/assets/js/main.e8012fb7.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-on-eks/"><div class="navbar__logo"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/data-on-eks/img/header-icon.png" alt="DoEKS Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a class="navbar__item navbar__link" href="/data-on-eks/docs/introduction/intro">Introduction</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-on-eks/docs/gen-ai">Gen AI</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/blueprints/amazon-emr-on-eks">Blueprints</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/bestpractices/intro">Best Practices</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/benchmarks/emr-on-eks">Benchmarks</a><a class="navbar__item navbar__link" href="/data-on-eks/docs/resources/intro">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-on-eks/docs/gen-ai">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/data-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Toggle the collapsible sidebar category &#x27;Inference on EKS&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/Llama2">Llama-2 on Inferentia</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-on-eks/docs/gen-ai/inference/StableDiffusion">Stable Diffusion on GPUs</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/data-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Toggle the collapsible sidebar category &#x27;Training on EKS&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/data-on-eks/docs/category/inference-on-eks"><span itemprop="name">Inference on EKS</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Llama-2 on Inferentia</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Llama-2 on Inferentia</h1></header><div class="theme-admonition theme-admonition-danger alert alert--danger admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_S0QG"><p>Note: Use of this Llama-2 model is governed by the Meta license.
In order to download the model weights and tokenizer, please visit the <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">website</a> and accept the license before requesting access.</p></div></div><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>We are actively enhancing this blueprint to incorporate improvements in observability, logging, and scalability aspects.</p></div></div><h1>Deploying Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio</h1><p>Welcome to the comprehensive guide on deploying the <a href="https://ai.meta.com/llama/#inside-the-model" target="_blank" rel="noopener noreferrer">Meta Llama-2-13b chat</a> model on Amazon Elastic Kubernetes Service (EKS) using <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a>.
In this tutorial, you will not only learn how to harness the power of Llama-2, but also gain insights into the intricacies of deploying large language models (LLMs) efficiently, particularly on <a href="https://aws.amazon.com/machine-learning/neuron/" target="_blank" rel="noopener noreferrer">trn1/inf2</a> (powered by AWS Trainium and Inferentia) instances, such as <code>inf2.24xlarge</code> and <code>inf2.48xlarge</code>,
which are optimized for deploying and scaling large language models.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-llama-2">What is Llama-2?<a href="#what-is-llama-2" class="hash-link" aria-label="Direct link to What is Llama-2?" title="Direct link to What is Llama-2?">â€‹</a></h3><p>Llama-2 is a pretrained large language model (LLM) trained on 2 trillion tokens of text and code. It is one of the largest and most powerful LLMs available today. Llama-2 can be used for a variety of tasks, including natural language processing, text generation, and translation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2-chat">Llama-2-chat<a href="#llama-2-chat" class="hash-link" aria-label="Direct link to Llama-2-chat" title="Direct link to Llama-2-chat">â€‹</a></h4><p>Llama-2 is a remarkable language model that has undergone a rigorous training process. It starts with pretraining using publicly available online data. An initial version of Llama-2-chat is then created through supervised fine-tuning.
Following that, <code>Llama-2-chat</code> undergoes iterative refinement using Reinforcement Learning from Human Feedback (<code>RLHF</code>), which includes techniques like rejection sampling and proximal policy optimization (<code>PPO</code>).
This process results in a highly capable and fine-tuned language model that we will guide you to deploy and utilize effectively on <strong>Amazon EKS</strong> with <strong>Ray Serve</strong>.</p><p>Llama-2 is available in three different model sizes:</p><ul><li><strong>Llama-2-70b:</strong> This is the largest Llama-2 model, with 70 billion parameters. It is the most powerful Llama-2 model and can be used for the most demanding tasks.</li><li><strong>Llama-2-13b:</strong> This is a medium-sized Llama-2 model, with 13 billion parameters. It is a good balance between performance and efficiency, and can be used for a variety of tasks.</li><li><strong>Llama-2-7b:</strong> This is the smallest Llama-2 model, with 7 billion parameters. It is the most efficient Llama-2 model and can be used for tasks that do not require the highest level of performance.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="which-llama-2-model-size-should-i-use"><strong>Which Llama-2 model size should I use?</strong><a href="#which-llama-2-model-size-should-i-use" class="hash-link" aria-label="Direct link to which-llama-2-model-size-should-i-use" title="Direct link to which-llama-2-model-size-should-i-use">â€‹</a></h3><p>The best Llama-2 model size for you will depend on your specific needs. and it may not always be the largest model for achieving the highest performance. It&#x27;s advisable to evaluate your needs and consider factors such as computational resources, response time, and cost-efficiency when selecting the appropriate Llama-2 model size. The decision should be based on a comprehensive assessment of your application&#x27;s goals and constraints.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference-on-trn1inf2-instances-unlocking-the-full-potential-of-llama-2">Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Llama-2<a href="#inference-on-trn1inf2-instances-unlocking-the-full-potential-of-llama-2" class="hash-link" aria-label="Direct link to Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Llama-2" title="Direct link to Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Llama-2">â€‹</a></h2><p><strong>Llama-2</strong> can be deployed on a variety of hardware platforms, each with its own set of advantages. However, when it comes to maximizing the efficiency, scalability, and cost-effectiveness of Llama-2, <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">AWS Trn1/Inf2 instances</a> shine as the optimal choice.</p><p><strong>Scalability and Availability</strong>
One of the key challenges in deploying large language models (<code>LLMs</code>) like Llama-2 is the scalability and availability of suitable hardware. Traditional <code>GPU</code> instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively.
In contrast, <code>Trn1/Inf2</code> instances, such as <code>trn1.32xlarge</code>, <code>trn1n.32xlarge</code>, <code>inf2.24xlarge</code> and <code>inf2.48xlarge</code>, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your <code>Llama-2</code> models as needed, without resource bottlenecks or delays.</p><p><strong>Cost Optimization:</strong>
Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing.
<strong>Trn1/Inf2</strong> instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost.
This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.</p><p><strong>Performance Boost</strong>
While Llama-2 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-2&#x27;s inference speeds. This translates to faster response times and improved user experiences when deploying Llama-2 on Trn1/Inf2 instances.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-specification">Model Specification<a href="#model-specification" class="hash-link" aria-label="Direct link to Model Specification" title="Direct link to Model Specification">â€‹</a></h3><p>The table provides information about the different sizes of Llama-2 models, their weights, and the hardware requirements for deploying them. This information can be used to design the infrastructure required to deploy any size of Llama-2 model. For example, if you want to deploy the <code>Llama-2-13b-chat</code> model, you will need to use an instance type with at least <code>26 GB</code> of total accelerator memory.</p><table><thead><tr><th>Model</th><th>Weights</th><th>Bytes</th><th>Parameter Size (Billions)</th><th>Total Accelerator Memory (GB)</th><th>Accelerator Memory Size for NeuronCore (GB)</th><th>Required Neuron Cores</th><th>Required Neuron Accelerators</th><th>Instance Type</th><th>tp_degree</th></tr></thead><tbody><tr><td>Meta/Llama-2-70b</td><td>float16</td><td>2</td><td>70</td><td>140</td><td>16</td><td>9</td><td>5</td><td>inf2.48x</td><td>24</td></tr><tr><td>Meta/Llama-2-13b</td><td>float16</td><td>2</td><td>13</td><td>26</td><td>16</td><td>2</td><td>1</td><td>inf2.24x</td><td>12</td></tr><tr><td>Meta/Llama-2-7b</td><td>float16</td><td>2</td><td>7</td><td>14</td><td>16</td><td>1</td><td>1</td><td>inf2.24x</td><td>12</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-usecase">Example usecase<a href="#example-usecase" class="hash-link" aria-label="Direct link to Example usecase" title="Direct link to Example usecase">â€‹</a></h3><p>A company wants to deploy a Llama-2 chatbot to provide customer support. The company has a large customer base and expects to receive a high volume of chat requests at peak times. The company needs to design an infrastructure that can handle the high volume of requests and provide a fast response time.</p><p>The company can use Inferentia2 instances to scale its Llama-2 chatbot efficiently. Inferentia2 instances are specialized hardware accelerators for machine learning tasks. They can provide up to 20x better performance and up to 7x lower cost than GPUs for machine learning workloads.</p><p>The company can also use Ray Serve to horizontally scale its Llama-2 chatbot. Ray Serve is a distributed framework for serving machine learning models. It can automatically scale your models up or down based on demand.</p><p>To scale its Llama-2 chatbot, the company can deploy multiple Inferentia2 instances and use Ray Serve to distribute the traffic across the instances. This will allow the company to handle a high volume of requests and provide a fast response time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="solution-architecture">Solution Architecture<a href="#solution-architecture" class="hash-link" aria-label="Direct link to Solution Architecture" title="Direct link to Solution Architecture">â€‹</a></h2><p>In this section, we will delve into the architecture of our solution, which combines Llama-2 model, <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> and <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">Inferentia2</a> on Amazon EKS.</p><p><img loading="lazy" alt="Llama-2-inf2" src="/data-on-eks/assets/images/llama2-inf2-6145720e940706806709b8e483dd9a8f.png" width="11358" height="7442" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-solution">Deploying the Solution<a href="#deploying-the-solution" class="hash-link" aria-label="Direct link to Deploying the Solution" title="Direct link to Deploying the Solution">â€‹</a></h2><p>To get started with deploying <code>Llama-2-13b chat</code> on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>, we will cover the necessary prerequisites and guide you through the deployment process step by step.
This includes setting up the infrastructure, deploying the <strong>Ray cluster</strong>, and creating the <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a> WebUI app.</p><div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites</span></h2><span class="icon_PckA">ðŸ‘ˆ</span></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-ray-cluster-with-llama-2-chat-model">Deploying the Ray Cluster with Llama-2-Chat Model<a href="#deploying-the-ray-cluster-with-llama-2-chat-model" class="hash-link" aria-label="Direct link to Deploying the Ray Cluster with Llama-2-Chat Model" title="Direct link to Deploying the Ray Cluster with Llama-2-Chat Model">â€‹</a></h2><p>Once the <code>Trainium on EKS</code> Cluster is deployed, you can proceed to use <code>kubectl</code> to deploy the <code>ray-service-Llama-2.yaml</code>.</p><p>In this step, we will deploy the Ray Serve cluster, which comprises one <code>Head Pod</code> on <code>x86 CPU</code> instances using Karpenter autoscaling, as well as <code>Ray workers</code> on <code>Inf2.48xlarge</code> instances, autoscaled by <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter</a>.</p><p>Let&#x27;s take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:</p><ul><li><p><strong>ray_serve_Llama-2.py:</strong>
This script uses FastAPI, Ray Serve, and PyTorch-based Hugging Face Transformers to create an efficient API for text generation using the <a href="https://huggingface.co/NousResearch/Llama-2-13b-chat-hf" target="_blank" rel="noopener noreferrer">NousResearch/Llama-2-13b-chat-hf</a> language model.
Alternatively, users have the flexibility to switch to the <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" target="_blank" rel="noopener noreferrer">meta-llama/Llama-2-13b-chat-hf</a> model. The script establishes an endpoint that accepts input sentences and efficiently generates text outputs, benefiting from Neuron acceleration for enhanced performance. With its high configurability, users can fine-tune model parameters to suit a wide range of natural language processing applications, including chatbots and text generation tasks.</p></li><li><p><strong>ray-service-Llama-2.yaml:</strong>
This Ray Serve YAML file serves as a Kubernetes configuration for deploying the Ray Serve service, facilitating efficient text generation using the <code>Llama-2-13b-chat</code> model.
It defines a Kubernetes namespace named <code>Llama-2</code> to isolate resources. Within the configuration, the <code>RayService</code> specification, named <code>Llama-2-service</code>, is created and hosted within the <code>Llama-2</code> namespace. The <code>RayService</code> specification leverages the Python script <code>ray_serve_Llama-2.py</code> (copied into the Dockerfile located within the same folder) to create the Ray Serve service.
The Docker image used in this example is publicly available on Amazon Elastic Container Registry (ECR) for ease of deployment.
Users can also modify the Dockerfile to suit their specific requirements and push it to their own ECR repository, referencing it in the YAML file.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-the-llama-2-chat-model">Deploy the Llama-2-Chat Model<a href="#deploy-the-llama-2-chat-model" class="hash-link" aria-label="Direct link to Deploy the Llama-2-Chat Model" title="Direct link to Deploy the Llama-2-Chat Model">â€‹</a></h3><p><strong>Ensure the cluster is configured locally</strong></p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws eks --region us-west-2 update-kubeconfig --name trainium</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Deploy RayServe Cluster</strong></p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-ml/trainium-inferentia/examples/ray-serve/Llama-2-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f ray-service-Llama-2.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Verify the deployment by running the following commands</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.</p></div></div><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ kubectl get all -n Llama-2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                          READY   STATUS              RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/Llama-2-service-raycluster-bt7bs-head-nhdct                0/1     ContainerCreating   0          68s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/service-raycluster-bt7bs-worker-inf2-worker-group-wtv47   0/1     Pending             0          68s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                     TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                       AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/Llama-2-service   NodePort   172.20.123.199   &lt;none&gt;        6379:31306/TCP,8265:30765/TCP,10001:32101/TCP,8000:30807/TCP,52365:31237/TCP,8080:31221/TCP   69s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ kubectl get ingress -n Llama-2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME             CLASS   HOSTS   ADDRESS                                                                         PORTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Llama-2-ingress   nginx   *       k8s-ingressn-ingressn-randomid-randomid.elb.us-west-2.amazonaws.com   80      2m4s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, you can access the Ray Dashboard from the Load balancer URL below.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http://&lt;NLB_DNS_NAME&gt;/dashboard/#/serve</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>If you don&#x27;t have access to a public Load Balancer, you can use port-forwarding and browse the Ray Dashboard using localhost with the following command:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/Llama-2-service </span><span class="token number" style="color:#36acaa">8265</span><span class="token plain">:8265 -n Llama-2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Open the link in the browser</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">http://localhost:8265/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>From this webpage, you will be able to monitor the progress of Model deployment, as shown in the image below:</p><p><img loading="lazy" alt="Ray Dashboard" src="/data-on-eks/assets/images/ray-dashboard-f6cb3c6c4a023ceb750a02ccc787b6d0.png" width="1345" height="886" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="to-test-the-llama-2-chat-model">To Test the Llama-2-Chat Model<a href="#to-test-the-llama-2-chat-model" class="hash-link" aria-label="Direct link to To Test the Llama-2-Chat Model" title="Direct link to To Test the Llama-2-Chat Model">â€‹</a></h3><p>Once you see the status of the model deployment is in <code>running</code> state then you can start using Llama-2-chat.</p><p>You can use the following URL with a query added at the end of the URL.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http://&lt;NLB_DNS_NAME&gt;/serve/infer?sentence=what is data parallelism and tensor parallelisma and the diffrences</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You will see an output like this in your browser:</p><p><img loading="lazy" alt="Chat Output" src="/data-on-eks/assets/images/llama-2-chat-ouput-e291e1c9481febc387222da8c06c1170.png" width="2432" height="1308" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-gradio-webui-app">Deploying the Gradio WebUI App<a href="#deploying-the-gradio-webui-app" class="hash-link" aria-label="Direct link to Deploying the Gradio WebUI App" title="Direct link to Deploying the Gradio WebUI App">â€‹</a></h2><p>Discover how to create a user-friendly chat interface using <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a> that integrates seamlessly with deployed models.</p><p>Let&#x27;s deploy Gradio app locally on your machine to interact with the LLama-2-Chat model deployed using RayServe.</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>The Gradio app interacts with the locally exposed service created solely for the demonstration. Alternatively, you can deploy the Gradio app on EKS as a Pod with Ingress and Load Balancer for wider accessibility.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="execute-port-forward-to-the-llama-2-ray-service">Execute Port Forward to the Llama-2 Ray Service<a href="#execute-port-forward-to-the-llama-2-ray-service" class="hash-link" aria-label="Direct link to Execute Port Forward to the Llama-2 Ray Service" title="Direct link to Execute Port Forward to the Llama-2 Ray Service">â€‹</a></h3><p>First, execute a port forward to the Llama-2 Ray Service using kubectl:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/Llama-2-service </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000 -n Llama-2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-gradio-webui-locally">Deploy Gradio WebUI Locally<a href="#deploy-gradio-webui-locally" class="hash-link" aria-label="Direct link to Deploy Gradio WebUI Locally" title="Direct link to Deploy Gradio WebUI Locally">â€‹</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="create-a-virtual-environment">Create a Virtual Environment<a href="#create-a-virtual-environment" class="hash-link" aria-label="Direct link to Create a Virtual Environment" title="Direct link to Create a Virtual Environment">â€‹</a></h4><p>Create a Python virtual environment in your machine for the Gradio application:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-ml/trainium-inferentia/examples/gradio-ui</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m venv .venv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">source</span><span class="token plain"> .venv/bin/activate</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="install-gradio-chatbot-app">Install Gradio ChatBot app<a href="#install-gradio-chatbot-app" class="hash-link" aria-label="Direct link to Install Gradio ChatBot app" title="Direct link to Install Gradio ChatBot app">â€‹</a></h4><p>Install all the Gradio WebUI app dependencies with pip</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> gradio requests</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="invoke-the-webui">Invoke the WebUI<a href="#invoke-the-webui" class="hash-link" aria-label="Direct link to Invoke the WebUI" title="Direct link to Invoke the WebUI">â€‹</a></h4><p>Run the Gradio WebUI using the following command:</p><p>NOTE: <code>gradio-app.py</code> refers to the port forward url. e.g., <code>service_name = &quot;http://localhost:8000&quot; </code></p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python gradio-app.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You should see output similar to the following:</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Using cache from ~/data-on-eks/ai-ml/trainium-inferentia/examples/gradio-ui/gradio_cached_examples/16&#x27; directory. If method or examples have changed since last caching, delete this folder to clear cache.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Running on local URL:  http://127.0.0.1:7860</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">To create a public link, set `share=True` in `launch()`.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="24-access-the-webui-from-your-browser">2.4. Access the WebUI from Your Browser<a href="#24-access-the-webui-from-your-browser" class="hash-link" aria-label="Direct link to 2.4. Access the WebUI from Your Browser" title="Direct link to 2.4. Access the WebUI from Your Browser">â€‹</a></h4><p>Open your web browser and access the Gradio WebUI by navigating to the following URL:</p><p><a href="http://127.0.0.1:7860" target="_blank" rel="noopener noreferrer">http://127.0.0.1:7860</a></p><p>You should now be able to interact with the Gradio application from your local machine.</p><p><img loading="lazy" alt="Gradio Llama-2 AI Chat" src="/data-on-eks/assets/images/gradio-llama-ai-chat-d9cebff0013257e5eaed696ca4a99456.png" width="1345" height="927" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">â€‹</a></h2><p>In conclusion, you will have successfully deployed the <strong>Llama-2-13b chat</strong> model on EKS with Ray Serve and created a chatGPT-style chat web UI using Gradio.
This opens up exciting possibilities for natural language processing and chatbot development.</p><p>In summary, when it comes to deploying and scaling Llama-2, AWS Trn1/Inf2 instances offer a compelling advantage.
They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs.
Whether you&#x27;re building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-2 on the AWS cloud.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Direct link to Cleanup" title="Direct link to Cleanup">â€‹</a></h2><p>Finally, we&#x27;ll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.</p><p><strong>Step1:</strong> Cancel the execution of the <code>python gradio-app.py</code></p><p><strong>Step2:</strong> Delete Ray Cluster</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-ml/trainium-inferentia/examples/ray-serve/Llama-2-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete -f ray-service-Llama-2.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Step3:</strong> Cleanup the EKS Cluster
This script will cleanup the environment using <code>-target</code> option to ensure all the resources are deleted in correct order.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">AWS_DEAFULT_REGION</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;DEPLOYED_EKS_CLUSTER_REGION&gt;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> data-on-eks/ai-ml/trainium/ </span><span class="token operator" style="color:#393A34">&amp;&amp;</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">chmod</span><span class="token plain"> +x cleanup.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/awslabs/data-on-eks/blob/main/website/docs/gen-ai/inference/Llama2.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-on-eks/docs/category/inference-on-eks"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Inference on EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-on-eks/docs/gen-ai/inference/StableDiffusion"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Stable Diffusion on GPUs</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-llama-2" class="table-of-contents__link toc-highlight">What is Llama-2?</a></li><li><a href="#which-llama-2-model-size-should-i-use" class="table-of-contents__link toc-highlight"><strong>Which Llama-2 model size should I use?</strong></a></li><li><a href="#inference-on-trn1inf2-instances-unlocking-the-full-potential-of-llama-2" class="table-of-contents__link toc-highlight">Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Llama-2</a><ul><li><a href="#model-specification" class="table-of-contents__link toc-highlight">Model Specification</a></li><li><a href="#example-usecase" class="table-of-contents__link toc-highlight">Example usecase</a></li></ul></li><li><a href="#solution-architecture" class="table-of-contents__link toc-highlight">Solution Architecture</a></li><li><a href="#deploying-the-solution" class="table-of-contents__link toc-highlight">Deploying the Solution</a><ul><li><a href="#deploy" class="table-of-contents__link toc-highlight">Deploy</a></li><li><a href="#verify-the-resources" class="table-of-contents__link toc-highlight">Verify the resources</a></li></ul></li><li><a href="#deploying-the-ray-cluster-with-llama-2-chat-model" class="table-of-contents__link toc-highlight">Deploying the Ray Cluster with Llama-2-Chat Model</a><ul><li><a href="#deploy-the-llama-2-chat-model" class="table-of-contents__link toc-highlight">Deploy the Llama-2-Chat Model</a></li><li><a href="#to-test-the-llama-2-chat-model" class="table-of-contents__link toc-highlight">To Test the Llama-2-Chat Model</a></li></ul></li><li><a href="#deploying-the-gradio-webui-app" class="table-of-contents__link toc-highlight">Deploying the Gradio WebUI App</a><ul><li><a href="#execute-port-forward-to-the-llama-2-ray-service" class="table-of-contents__link toc-highlight">Execute Port Forward to the Llama-2 Ray Service</a></li><li><a href="#deploy-gradio-webui-locally" class="table-of-contents__link toc-highlight">Deploy Gradio WebUI Locally</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Get Started</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-on-eks/docs/introduction/intro">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/data-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with â¤ï¸ at AWS  <br> Â© 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer></div>
<script src="/data-on-eks/assets/js/runtime~main.a405c129.js"></script>
<script src="/data-on-eks/assets/js/main.e8012fb7.js"></script>
</body>
</html>