<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bestpractices/analytics/spark-best-practices" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>Spark on EKS Best Practices | Data on EKS</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://awslabs.github.io/data-on-eks/docs/bestpractices/analytics/spark-best-practices><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="Spark on EKS Best Practices | Data on EKS"><meta data-rh=true name=description content="This page aims to provide comprehensive best practices and guidelines for deploying, managing, and optimizing Apache Spark workloads on Amazon Elastic Kubernetes Service (EKS). This helps organizations to successfully run and scale their Spark Applications at scale in a containerised environment on Amazon EKS."><meta data-rh=true property=og:description content="This page aims to provide comprehensive best practices and guidelines for deploying, managing, and optimizing Apache Spark workloads on Amazon Elastic Kubernetes Service (EKS). This helps organizations to successfully run and scale their Spark Applications at scale in a containerised environment on Amazon EKS."><link data-rh=true rel=icon href=/data-on-eks/img/header-icon.png><link data-rh=true rel=canonical href=https://awslabs.github.io/data-on-eks/docs/bestpractices/analytics/spark-best-practices><link data-rh=true rel=alternate href=https://awslabs.github.io/data-on-eks/docs/bestpractices/analytics/spark-best-practices hreflang=en><link data-rh=true rel=alternate href=https://awslabs.github.io/data-on-eks/docs/bestpractices/analytics/spark-best-practices hreflang=x-default><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://awslabs.github.io/data-on-eks/docs/category/data-analytics","name":"Data Analytics","position":1},{"@type":"ListItem","item":"https://awslabs.github.io/data-on-eks/docs/bestpractices/analytics/spark-best-practices","name":"Spark on EKS Best Practices","position":2}]}</script><link rel=stylesheet href=/data-on-eks/assets/css/styles.3153bbff.css><script src=/data-on-eks/assets/js/runtime~main.f5414970.js defer></script><script src=/data-on-eks/assets/js/main.141b9732.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/data-on-eks/><div class=navbar__logo><img src=/data-on-eks/img/header-icon.png alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/data-on-eks/img/header-icon.png alt="DoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href=/data-on-eks/docs/blueprints/data-analytics>Blueprints</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/data-on-eks/docs/bestpractices/intro>Best Practices</a><a class="navbar__item navbar__link" href=/data-on-eks/docs/benchmarks/emr-on-eks>Benchmarks</a><a class="navbar__item navbar__link" href=/data-on-eks/docs/resources/intro>Resources</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/awslabs/data-on-eks target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><div class=navbar__search><span aria-label="expand searchbar" role=button class=search-icon tabindex=0></span><input id=search_input_react type=search placeholder=Loading... aria-label=Search class="navbar__search-input search-bar" disabled></div></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/data-on-eks/docs/bestpractices/intro>Introduction</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/data-on-eks/docs/bestpractices/eks-best-practices>EKS Best Practices</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/data-on-eks/docs/category/data-analytics>Data Analytics</a><button aria-label="Collapse sidebar category 'Data Analytics'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/bestpractices/analytics/trino-best-practices>Trino on EKS Best Practices</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/data-on-eks/docs/bestpractices/analytics/emr-on-eks>EMR on EKS Best Practices</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/data-on-eks/docs/bestpractices/analytics/spark-best-practices>Spark on EKS Best Practices</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/data-on-eks/docs/category/networking>Networking</a><button aria-label="Expand sidebar category 'Networking'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/data-on-eks/docs/category/scalability>Scalability</a><button aria-label="Expand sidebar category 'Scalability'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/data-on-eks/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/data-on-eks/docs/category/data-analytics><span>Data Analytics</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>Spark on EKS Best Practices</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Spark on EKS Best Practices</h1></header>
<p>This page aims to provide comprehensive best practices and guidelines for deploying, managing, and optimizing Apache Spark workloads on Amazon Elastic Kubernetes Service (EKS). This helps organizations to successfully run and scale their Spark Applications at scale in a containerised environment on Amazon EKS.</p>
<p>For deploying Spark on EKS you can leverage the <a href=https://awslabs.github.io/data-on-eks/docs/blueprints/data-analytics/spark-operator-yunikorn target=_blank rel="noopener noreferrer">blueprints</a>, which readily incorporates most of the best practices. You can further customize this blueprint, to tweak the configurations to match your specific application requirements and environment constraints, as outlined in this guide.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=eks-networking>EKS Networking<a href=#eks-networking class=hash-link aria-label="Direct link to EKS Networking" title="Direct link to EKS Networking">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=vpc-and-subnets-sizing>VPC and Subnets Sizing<a href=#vpc-and-subnets-sizing class=hash-link aria-label="Direct link to VPC and Subnets Sizing" title="Direct link to VPC and Subnets Sizing">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=vpc-ip-address-exhaustion>VPC IP address exhaustion<a href=#vpc-ip-address-exhaustion class=hash-link aria-label="Direct link to VPC IP address exhaustion" title="Direct link to VPC IP address exhaustion">​</a></h4>
<p>As EKS clusters scale up with additional Spark workloads, the number of pods managed by a cluster can easily grow into the thousands, each consuming an IP address. This creates challenges, since IP addresses within a VPC are limited, and it's not always feasible to recreate a larger VPC or extend the current VPC's CIDR blocks.</p>
<p>Worker nodes and pods both consume IP addresses. By default, VPC CNI has <code>WARM_ENI_TARGET=1</code> means that <code>ipamd</code> should keep "a full ENI" of available IPs around in the <code>ipamd</code> warm pool for the Pod IP assignment.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=remediation-for-ip-address-exhaustion>Remediation for IP Address exhaustion<a href=#remediation-for-ip-address-exhaustion class=hash-link aria-label="Direct link to Remediation for IP Address exhaustion" title="Direct link to Remediation for IP Address exhaustion">​</a></h4>
<p>While IP exhaustion remediation methods exist for VPCs, they introduce additional operational complexity and have significant implications to consider. Hence, for new EKS clusters, it is recommended to over-provision the subnets you will use for Pod networking for growth.</p>
<p>For addressing IP address exhaustion, consider adding secondary CIDR blocks to your VPC and creating new subnets from these additional address ranges, then deploying worker nodes in these expanded subnets.</p>
<p>If adding more subnets, is not an option, then you will have to work on optimising the IP address assignment by tweaking CNI Configuration Variables. Refer to <a href=/data-on-eks/docs/bestpractices/networking#avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn>configure MINIMUM_IP_TARGET</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=coredns-recommendations>CoreDNS Recommendations<a href=#coredns-recommendations class=hash-link aria-label="Direct link to CoreDNS Recommendations" title="Direct link to CoreDNS Recommendations">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=dns-lookup-throttling>DNS Lookup Throttling<a href=#dns-lookup-throttling class=hash-link aria-label="Direct link to DNS Lookup Throttling" title="Direct link to DNS Lookup Throttling">​</a></h4>
<p>Spark applications running on Kubernetes generate high volumes of DNS lookups when executors communicate with external services.</p>
<p>This occurs because Kubernetes' DNS resolution model requires each pod to query the cluster's DNS service (kube-dns or CoreDNS) for every new connection, and during task executions Spark executors frequently create new connections for communicating with external services. By default, Kubernetes does not cache DNS results at the pod level, meaning each executor pod must perform a new DNS lookup even for previously resolved hostnames.</p>
<p>This behavior is amplified in Spark applications due to their distributed nature, where multiple executor pods simultaneously attempt to resolve the same external service endpoints.This occurs during data ingestion, processing, and when connecting to external databases or shuffle services.</p>
<p>When DNS traffic exceeds 1024 packets per second for a CoreDNS replica, DNS requests will be throttled, resulting in <code>unknownHostException</code> errors.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=remediation>Remediation<a href=#remediation class=hash-link aria-label="Direct link to Remediation" title="Direct link to Remediation">​</a></h4>
<p>It is recommended to scale CoreDNS, as your workload scales. Refer to <a href=/data-on-eks/docs/bestpractices/networking#scaling-coredns>Scaling CoreDNS</a> for more details on implementation choices.</p>
<p>It is also recommended to continuously monitor CoreDNS metrics. Refer to <a href=https://docs.aws.amazon.com/eks/latest/best-practices/monitoring_eks_workloads_for_network_performance_issues.html#_monitoring_coredns_traffic_for_dns_throttling_issues target=_blank rel="noopener noreferrer">EKS Networking Best Practices</a> for detailed information.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=reduce-inter-az-traffic>Reduce Inter AZ Traffic<a href=#reduce-inter-az-traffic class=hash-link aria-label="Direct link to Reduce Inter AZ Traffic" title="Direct link to Reduce Inter AZ Traffic">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=inter-az-costs>Inter AZ Costs<a href=#inter-az-costs class=hash-link aria-label="Direct link to Inter AZ Costs" title="Direct link to Inter AZ Costs">​</a></h4>
<p>During the shuffle stage, Spark executors may need to exchange data between them. If the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front, which will be charged as Inter-AZ Traffic costs.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=remediation-1>Remediation<a href=#remediation-1 class=hash-link aria-label="Direct link to Remediation" title="Direct link to Remediation">​</a></h4>
<p>For Spark workloads, it is recommended to colocate executor pods and worker nodes in the same AZ. Colocating workloads in the same AZ serves two main purposes:</p>
<ul>
<li>Reduce inter-AZ traffic costs</li>
<li>Reduce network latency between executors/Pods</li>
</ul>
<p>Refer to <a href=/data-on-eks/docs/bestpractices/networking#inter-az-network-optimization>Inter AZ Network Optimization</a> for having pods co-locate on the same AZ.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=karpenter-recommendations>Karpenter Recommendations<a href=#karpenter-recommendations class=hash-link aria-label="Direct link to Karpenter Recommendations" title="Direct link to Karpenter Recommendations">​</a></h2>
<p><a href=https://karpenter.sh/docs/ target=_blank rel="noopener noreferrer">Karpenter</a> enhances Spark on EKS deployments by providing rapid node provisioning capability that aligns with Spark's dynamic resource scaling needs. This automated scaling solution improves resource utilization and cost-efficiency by bringing in right-sized nodes as needed. This also allows Spark jobs to scale seamlessly without the need for pre-configured node groups or manual intervention, there by simplifying operational management.</p>
<p>Here are the Karpenter recommendations for scaling compute nodes while running Spark workloads. For complete Karpenter configuration details, refer <a href=https://karpenter.sh/docs/ target=_blank rel="noopener noreferrer">Karpenter documentation</a>.</p>
<p>Consider creating separate NodePools for driver and executor pods.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=driver-nodepool>Driver Nodepool<a href=#driver-nodepool class=hash-link aria-label="Direct link to Driver Nodepool" title="Direct link to Driver Nodepool">​</a></h3>
<p>The Spark driver is a single pod and manages the entire lifecycle of the Spark application. Terminating Spark driver pod, effectively means terminating the entire Spark job.</p>
<ul>
<li>Configure Driver Nodepool to always use <code>on-demand</code> nodes only. When Spark driver pods run on spot instances, they are vulnerable to unexpected terminations due to spot instance reclamation, resulting in computation loss and interrupted processing that requires manual intervention to restart.</li>
<li>Disable <a href=https://karpenter.sh/docs/concepts/disruption/#consolidation target=_blank rel="noopener noreferrer"><code>consolidation</code></a> on Driver Nodepool.</li>
<li>Use <code>node selectors</code> or <code>taints/tolerations</code> for placing driver pods on this designated Driver NodePool.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=executor-nodepool>Executor Nodepool<a href=#executor-nodepool class=hash-link aria-label="Direct link to Executor Nodepool" title="Direct link to Executor Nodepool">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=configure-spot-instances>Configure Spot instances<a href=#configure-spot-instances class=hash-link aria-label="Direct link to Configure Spot instances" title="Direct link to Configure Spot instances">​</a></h4>
<p>In the absence of <a href=https://aws.amazon.com/ec2/pricing/reserved-instances/ target=_blank rel="noopener noreferrer">Amazon EC2 Reserved Instances</a> or <a href=https://aws.amazon.com/savingsplans/ target=_blank rel="noopener noreferrer">Savings Plans</a>, consider using <a href=https://aws.amazon.com/ec2/spot/ target=_blank rel="noopener noreferrer">Amazon EC2 Spot Instances</a> for executors to reduce dataplane costs.</p>
<p>When spot instances are interrupted, executors will be terminated and rescheduled on available nodes. For details on interruption behaviour and node termination management, refer to the <code>Handling Interruptions</code> section.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=instance-and-capacity-type-selection>Instance and Capacity type selection<a href=#instance-and-capacity-type-selection class=hash-link aria-label="Direct link to Instance and Capacity type selection" title="Direct link to Instance and Capacity type selection">​</a></h4>
<p>Using multiple instance types in the node pool enables access to various spot instance pools, increasing capacity availability and optimizing for both price and capacity across the available instance options.</p>
<p>With <code>Weighted Nodepools</code>, node selection can be optimized using weighted nodepools arranged in priority order. By assigning different weights to each nodepool, you can establish a selection hierarchy, such as: Spot (highest weight), followed by Graviton, AMD, and Intel (lowest weight).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=consolidation-configuration>Consolidation Configuration<a href=#consolidation-configuration class=hash-link aria-label="Direct link to Consolidation Configuration" title="Direct link to Consolidation Configuration">​</a></h4>
<p>While enabling <code>consolidation</code> for Spark executor pods can lead to better cluster resource utilization, it's crucial to strike a balance with job performance. Frequent consolidation events can result in slower execution times for Spark jobs, as executors are forced to recompute the shuffle data and RDD blocks.</p>
<p>This impact is particularly noticeable in long-running Spark jobs. To mitigate this, it's essential to carefully tune the consolidation interval.</p>
<p>Enable graceful executor pods shutdown:</p>
<ul>
<li>
<p><code>spark.executor.decommission.enabled=true</code>: Enables graceful decommissioning of executors, allowing them to complete their current tasks and transfer their cached data before shutting down. This is particularly useful when using spot instances for executors.</p>
</li>
<li>
<p><code>spark.storage.decommission.enabled=true</code>: Enables the migration of cached RDD blocks from the decommissioning executor to other active executors before shutdown, preventing data loss and the need for recomputation.</p>
</li>
</ul>
<p>To explore other means to save intermediate data computed in Spark Executors, refer to <a href=#storage-best-practices>Storage Best Practices</a>.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=handling-interruptions-during-karpenter-consolidationspot-termination>Handling interruptions during Karpenter Consolidation/Spot Termination<a href=#handling-interruptions-during-karpenter-consolidationspot-termination class=hash-link aria-label="Direct link to Handling interruptions during Karpenter Consolidation/Spot Termination" title="Direct link to Handling interruptions during Karpenter Consolidation/Spot Termination">​</a></h4>
<p>Perform controlled decommissioning instead of abruptly killing executors when nodes are scheduled for termination. To achieve this:</p>
<ul>
<li>Configure appropriate TerminationGracePeriod values for Spark workloads.</li>
<li>Implement executor-aware termination handling.</li>
<li>Ensure shuffle data is saved before nodes are decommissioned.</li>
</ul>
<p>Spark provides native configurations to control termination behavior:</p>
<p><strong>Controlling executor interruptions</strong></p>
<ul>
<li><strong>Configs</strong>:</li>
<li><code>spark.executor.decommission.enabled</code></li>
<li><code>spark.executor.decommission.forceKillTimeout</code>
These configurations are particularly useful in scenarios where executors might be terminated due to spot instance interruptions or Karpenter consolidation events. When enabled, executors will gracefully shutdown by stopping task acceptance and notifying the driver about their decommissioning state.</li>
</ul>
<p><strong>Controlling executor's BlockManager behavior</strong></p>
<ul>
<li><strong>Configs</strong>:</li>
<li><code>spark.storage.decommission.enabled</code></li>
<li><code>spark.storage.decommission.shuffleBlocks.enabled</code></li>
<li><code>spark.storage.decommission.rddBlocks.enabled</code></li>
<li><code>spark.storage.decommission.fallbackStorage.path</code>
These settings enable the migration of shuffle and RDD blocks from decommissioning executors to other available executors or to a fallback storage location. This approach helps in dynamic environments by reducing the need to recompute shuffle data or RDD blocks, thereby improving job completion times and resource efficiency.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=advanced-scheduling-considerations>Advanced Scheduling Considerations<a href=#advanced-scheduling-considerations class=hash-link aria-label="Direct link to Advanced Scheduling Considerations" title="Direct link to Advanced Scheduling Considerations">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=default-kubernetes-scheduler-behaviour>Default Kubernetes Scheduler behaviour.<a href=#default-kubernetes-scheduler-behaviour class=hash-link aria-label="Direct link to Default Kubernetes Scheduler behaviour." title="Direct link to Default Kubernetes Scheduler behaviour.">​</a></h3>
<p>Default Kubernetes scheduler uses <code>least allocated</code> approach. This strategy aims to distribute pods evenly across cluster, which helps in maintaining availability and a balanced resource utilization across all nodes, rather than packing more pods in fewer nodes.</p>
<p><code>Most allocated</code> approach on the other hand, aims to favor nodes with most amount of allocated resources, which leads to packing more pods onto nodes that are already heavily allocated. This approach is favourable for Spark jobs, as it aims for high utilization on select nodes at pod scheduling time, leading to better consolidation of nodes. You will have to leverage a custom kube-scheduler with this option enabled, or leverage Custom Schedulers purpose built for more advanced orchestration.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=custom-schedulers>Custom Schedulers<a href=#custom-schedulers class=hash-link aria-label="Direct link to Custom Schedulers" title="Direct link to Custom Schedulers">​</a></h3>
<p>Custom schedulers enhance Kubernetes’ native scheduling capabilities by providing advanced features tailored for batch and high-performance computing workloads. Custom schedulers enhance resource allocation by optimizing bin-packing and offering scheduling tailored to specific application needs. Here are popular custom schedulers for running Spark workloads on Kubernetes.</p>
<ul>
<li><a href=https://yunikorn.apache.org/ target=_blank rel="noopener noreferrer">Apache Yunikorn</a></li>
<li><a href=https://volcano.sh/en/ target=_blank rel="noopener noreferrer">Volcano</a></li>
</ul>
<p>Advantages of leveraging custom schedulers like Yunikorn.</p>
<ul>
<li>Hierarchical queue system and configurable policies allowing for complex resource management.</li>
<li>Gang scheduling, which ensures all related pods (like Spark executors) start together, preventing resource wastage.</li>
<li>Resource fairness across different tenants and workloads.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=how-will-yunikorn-and-karpenter-work-together>How will Yunikorn and Karpenter work together?<a href=#how-will-yunikorn-and-karpenter-work-together class=hash-link aria-label="Direct link to How will Yunikorn and Karpenter work together?" title="Direct link to How will Yunikorn and Karpenter work together?">​</a></h3>
<p>Karpenter and Yunikorn complement each other by handling different aspects of workload management in Kubernetes:</p>
<ul>
<li>
<p><strong>Karpenter</strong> focuses on node provisioning and scaling, determining when to add or remove nodes based on resource demands.</p>
</li>
<li>
<p><strong>Yunikorn</strong> brings application awareness to scheduling through advanced features like queue management, resource fairness, and gang scheduling.</p>
</li>
</ul>
<p>In a typical workflow, Yunikorn first schedules pods based on application-aware policies and queue priorities. When these pods remain pending due to insufficient cluster resources, Karpenter detects these pending pods and provisions appropriate nodes to accommodate them. This integration ensures both efficient pod placement (Yunikorn) and optimal cluster scaling (Karpenter).</p>
<p>For Spark workloads, this combination is particularly effective: Yunikorn ensures executors are scheduled according to application SLAs and dependencies, while Karpenter ensures the right node types are available to meet those specific requirements.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=storage-best-practices>Storage Best Practices<a href=#storage-best-practices class=hash-link aria-label="Direct link to Storage Best Practices" title="Direct link to Storage Best Practices">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=node-storage>Node Storage<a href=#node-storage class=hash-link aria-label="Direct link to Node Storage" title="Direct link to Node Storage">​</a></h3>
<p>By default, the EBS root volumes of worker nodes are set to 20GB. Spark Executors use local storage for temporary data like shuffle data, intermediate results, and temporary files. This default storage of 20GB root volume attached to worker nodes can be limiting in both size and performance. Consider the following options to address your performance and storage size requirements:</p>
<ul>
<li>Expand the root volume capacity to provide ample space for intermediate Spark data. You will have to arrive at optimal capacity based on average size of the dataset that each executor will be processing and complexity of Spark job.</li>
<li>Configure high-performance storage with better I/O and latency.</li>
<li>Mount additional volumes on worker nodes for temporary data storage.</li>
<li>Leverage dynamically provisioned PVCs that can be attached directly to executor pods.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=reuse-pvc>Reuse PVC<a href=#reuse-pvc class=hash-link aria-label="Direct link to Reuse PVC" title="Direct link to Reuse PVC">​</a></h3>
<p>This option allows reusing PVCs associated with Spark executors even after the executors are terminated (either due to consolidation activity or preemption in case of Spot instances).</p>
<p>This allows for preserving the intermediate shuffle data and cached data on the PVC. When Spark requests new executor pod to replace the terminated one, the system attempts to reuse an existing PVC that belonged to terminated executor. This option can be enabled by the following configuration:</p>
<p><code>spark.kubernetes.executor.reusePersistentVolume=true</code></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=external-shuffle-services>External Shuffle services<a href=#external-shuffle-services class=hash-link aria-label="Direct link to External Shuffle services" title="Direct link to External Shuffle services">​</a></h3>
<p>Leverage external shuffle services like Apache Celeborn to decouple compute and storage, allowing Spark executors to write data to an external shuffle service instead of local disks. This reduces the risk of data loss and data re-computation due to executor termination or consolidation.</p>
<p>This also allows for better resource management, especially when <code>Spark Dynamic Resource Allocation</code> is enabled. External shuffle service allows Spark to preserve shuffle data even after executors are removed during dynamic resource allocation, preventing the need for recomputation of shuffle data when new executors are added. This enables more efficient scale-down of resources when they're not needed.</p>
<p>Also consider the performance implications of external shuffle services. For smaller datasets or applications with low shuffle data volues, the overhead of setting up and managing external shuffle service might outweigh its benefits.</p>
<p>External shuffle services is recommended when dealing with either shuffle data volumes exceeding 500GB to 1TB per job or long running Spark applications that run for several hours to multiple days.</p>
<p>Refer to this <a href=https://celeborn.apache.org/docs/latest/deploy_on_k8s/ target=_blank rel="noopener noreferrer">Celeborn Documentation</a> for deployment on Kubernetes and integration configuration with Apache Spark.</div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col><a href=https://github.com/awslabs/data-on-eks/blob/main/website/docs/bestpractices/analytics/spark-best-practices.md target=_blank rel="noopener noreferrer" class=theme-edit-this-page><svg fill=currentColor height=20 width=20 viewBox="0 0 40 40" class=iconEdit_Z9Sw aria-hidden=true><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"/></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/data-on-eks/docs/bestpractices/analytics/emr-on-eks><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>EMR on EKS Best Practices</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/data-on-eks/docs/category/networking><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Networking</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#eks-networking class="table-of-contents__link toc-highlight">EKS Networking</a><ul><li><a href=#vpc-and-subnets-sizing class="table-of-contents__link toc-highlight">VPC and Subnets Sizing</a><li><a href=#coredns-recommendations class="table-of-contents__link toc-highlight">CoreDNS Recommendations</a><li><a href=#reduce-inter-az-traffic class="table-of-contents__link toc-highlight">Reduce Inter AZ Traffic</a></ul><li><a href=#karpenter-recommendations class="table-of-contents__link toc-highlight">Karpenter Recommendations</a><ul><li><a href=#driver-nodepool class="table-of-contents__link toc-highlight">Driver Nodepool</a><li><a href=#executor-nodepool class="table-of-contents__link toc-highlight">Executor Nodepool</a></ul><li><a href=#advanced-scheduling-considerations class="table-of-contents__link toc-highlight">Advanced Scheduling Considerations</a><ul><li><a href=#default-kubernetes-scheduler-behaviour class="table-of-contents__link toc-highlight">Default Kubernetes Scheduler behaviour.</a><li><a href=#custom-schedulers class="table-of-contents__link toc-highlight">Custom Schedulers</a><li><a href=#how-will-yunikorn-and-karpenter-work-together class="table-of-contents__link toc-highlight">How will Yunikorn and Karpenter work together?</a></ul><li><a href=#storage-best-practices class="table-of-contents__link toc-highlight">Storage Best Practices</a><ul><li><a href=#node-storage class="table-of-contents__link toc-highlight">Node Storage</a><li><a href=#reuse-pvc class="table-of-contents__link toc-highlight">Reuse PVC</a><li><a href=#external-shuffle-services class="table-of-contents__link toc-highlight">External Shuffle services</a></ul></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Get Started</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/data-on-eks/docs/introduction/intro>Docs</a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Get Involved</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://github.com/awslabs/data-on-eks target=_blank rel="noopener noreferrer" class=footer__link-item>Github<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "7fbc7ab02fae4767b1af2588eba0cdf2"}'></script></div>