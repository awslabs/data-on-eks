---
apiVersion: v1
kind: Namespace
metadata:
  name: llama2

---
apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: llama2-service
  namespace: llama2
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfig:
    importPath: ray_serve_llama2:entrypoint  # Specify the correct path to your Python script
    runtimeEnv: |
      env_vars: {"MODEL_ID": "NousResearch/Llama-2-13b-chat-hf"}  # Replace with the appropriate model ID

  rayClusterConfig:
    rayVersion: '2.7.1'
    headGroupSpec:
      serviceType: NodePort
      headService:
        metadata:
          name: llama2-service
          namespace: llama2
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: ray-head
              image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama2:latest # Image created using the Dockerfile attached in the folder
              imagePullPolicy: Always # Ensure the image is always pulled when updated
              lifecycle:
                preStop:
                  exec:
                    command: [ "/bin/sh","-c","ray stop" ]
              ports:
                - containerPort: 6379
                  name: gcs
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
              resources:
                limits:
                  cpu: 4
                  memory: 20Gi
                requests:
                  cpu: 4
                  memory: 20Gi
          nodeSelector: # This is using Karpenter Nodes with the provisioner label
            instanceType: mixed-x86
            provisionerType: Karperter
            workload: rayhead
          volumes:
            - name: ray-logs
              emptyDir: {}

    workerGroupSpecs:
      - groupName: inf2-worker-group
        replicas: 1
        minReplicas: 1
        maxReplicas: 1
        rayStartParams: {}
        template:
          spec:
            containers:
              - name: ray-worker
                image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama2:latest
                imagePullPolicy: Always # Ensure the image is always pulled when updated
                lifecycle:
                  preStop:
                    exec:
                      command: [ "/bin/sh","-c","ray stop" ]
                resources:
                  limits:
                    cpu: "180"
                    memory: "700G"
                    aws.amazon.com/neuron: "12"
                  requests:
                    cpu: "180"
                    memory: "700G"
                    aws.amazon.com/neuron: "12"
            nodeSelector:
              instanceType: inferentia-inf2
              provisionerType: Karperter
            tolerations:
              - key: "aws.amazon.com/neuroncore"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "aws.amazon.com/neuron"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "hub.jupyter.org/dedicated"
                operator: "Equal"
                value: "user"
                effect: "NoSchedule"


---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama2-ingress
  namespace: llama2
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          # Ray Dashboard
          - path: /dashboard/(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: llama2-service
                port:
                  number: 8265
          # Ray Serve
          - path: /serve/(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: llama2-service
                port:
                  number: 8000
