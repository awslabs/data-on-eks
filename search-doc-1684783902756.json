[{"title":"EMR on EKS Best Practices","type":0,"sectionRef":"#","url":"/data-on-eks/blog/EMR on EKS Best Practices","content":"","keywords":""},{"title":"EMR Containers Best Practices Guides‚Äã","type":1,"pageTitle":"EMR on EKS Best Practices","url":"/data-on-eks/blog/EMR on EKS Best Practices#emr-containers-best-practices-guides","content":"Amazon EMR on Amazon EKS enables you to submit Apache Spark jobs on demand on Amazon Elastic Kubernetes Service (EKS) without provisioning clusters. With EMR on EKS, you can consolidate analytical workloads with your other Kubernetes-based applications on the same Amazon EKS cluster to improve resource utilization and simplify infrastructure management. This link provides the best practices and templates to get started with Amazon EMR on EKS. We publish this guide on GitHub so we could iterate the content quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community. Checkout the EMR on EKS Best practices GitHub docs here "},{"title":"Architecture‚Äã","type":1,"pageTitle":"EMR on EKS Best Practices","url":"/data-on-eks/blog/EMR on EKS Best Practices#architecture","content":"The following diagram illustrates the solution architecture Amazon EMR on EKS.  "},{"title":"EMR on EKS Data Platform with AWS CDK","type":0,"sectionRef":"#","url":"/data-on-eks/blog/Data Platform with AWS CDK","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#introduction","content":"In this blog we will show you how you can use AWS CDK and the Analytics Reference Architecture (ARA) library to deploy an end to end data analytics platform. This platform will allow you to run Spark interactive Session in Jupyter notebook with EMR Studio supported by EMR on EKS and run Spark jobs with EMR on EKS. The architecture below shows the infrasturcture you will deploy using the CDK and ARA library.  "},{"title":"Analytics Reference Architecture‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#analytics-reference-architecture","content":"AWS Analytics Reference Architecture (ARA) exposes set fo reusable core components in an AWS CDK library, currently available in Typescript and Python. This library contains AWS CDK constructs (L3) that can be used to quickly provision analytics solutions in demos, prototypes, proofs of concept, and end-to-end reference architectures. The API of ARA Library is defined here. In our case the library help you deploy an infrastructure optimised for Apache Spark running on EKS leveraging EMR on EKS. The infrastructure will out of the box provide you with pod collocation to reduce network traffic, deploy nodegroup in a single AZ to reduce cross AZ traffic during shuffle, use dedicated instances for EMR on EKS, use optimized instances for memory intensive jobs, use spot and on-demand instances for non-critical job and for critical jobs. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlCDK "},{"title":"Solution‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#solution","content":"To deploy the data platform we will use an example in the Analytics Reference Architecture. The example is in the directory examples/emr-eks-app that you will find in the repository you will clone below. Clone the repository git clone https://github.com/aws-samples/aws-analytics-reference-architecture.git  This solution will deploy the following: EKS cluster and a set of Nodegroups: Managed Nodegroup called tooling for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, EBS CSI Driver..Three Managed Nodegroup called critical for critical jobs, each in one AZ, this nodegroup use on-demand instancesThree Managed Nodegroup called non-critical for non-critical jobs, each in one AZ, this nodegroup use spot instancesThree Managed Nodegroup called notebook-driver for non-critical jobs, each in one AZ, this nodegroup use on-demand instances to have a stable driver.Three Managed Nodegroup called notebook-executor for non-critical jobs, each in one AZ, this nodegroup use spot instances instances for executors. Enable EKS Cluster to be with with EMR on EKS service EMR Virtual Cluster called batchjob, used to submitted jobs EMR Virtual Cluster called emrvcplatform, used to submitted jobs EMR Studio called platform A managed endpoint, called platform-myendpoint , to use with Jupyter notebooks you will create in the EMR Studio Execution role to use when submiting jobs with EMR on EKS start-job-run Execution role to use with managed endpoint. pod templates stored in an S3 bucket called &quot;EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION&quot; "},{"title":"Customize‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#customize","content":"The infrastructure described above is defined in emr-eks-app/lib/emr-eks-app-stack.ts. If you want to customize it you can change the values in it. For example, you can chose not to create the default nodegroup to use for jobs, in this case you can set the defaultNodeGroups parameter to false in the EmrEksCluster. You can also call the addEmrEksNodegroup method to define your own nodegroups with specific labels, instances or taints. The addEmrEksNodegroup method is defined here. You can also create your own execution role through the createExecutionRole method or create a managed endpoint to attach it to an EMR Studio you deployed outside of the ARA library. In order to simplify this example we use IAM authentication with IAM user for EMR Studio. If you would like to use a user in the AWS IAM Identity Center you can change studioAuthMode in the NotebookPlatform construct. Below you will can see the code snipet that you need to change. const notebookPlatform = new ara.NotebookPlatform(this, 'platform-notebook', { emrEks: emrEks, eksNamespace: 'dataanalysis', studioName: 'platform', studioAuthMode: ara.StudioAuthMode.IAM, });  "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#deploy","content":"Before you run the solution, you MUST change the eksAdminRoleArn of the props object of EmrEksCluster in lib/emr-eks-app-stack.ts. This role allows you to interact manage EKS cluster and should have be allowed at least the IAM action eks:AccessKubernetesApi. You need to also change the identityName in the addUser method of the NotebookPlatform construct. The identityName MUST BE a valid IAM username that you use. Below you will can see the code snipet that you need to change. notebookPlatform.addUser([{ identityName:'', notebookManagedEndpoints: [{ emrOnEksVersion: 'emr-6.8.0-latest', executionPolicy: emrEksPolicy, managedEndpointName: 'myendpoint' }], }]);  Last you shold also update the IAM policies passed to the createExecutionRole, if you want to process data that is in S3 buckets that you own. Navigate into one of the example directories and run cdk synth --profile YOUR-AWS-PROFILE cd examples/emr-eks-app npm install cdk synth --profile YOUR-AWS-PROFILE  Once the synth is completed you can deploy the infrastructrue with the following command: cdk deploy  At the end of the deployment you will see output like follow:  In the output you will find job sample configurations with the best practices for Spark on Kubernetes like dynamicAllocation and pod collocation. "},{"title":"Job submission‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#job-submission","content":"In this example we will use the crittical-job job configuration to submit a job using that will compute pi using that is part of Spark distribution. To submit a job we will use Below you use start-job-run command with AWS CLI. Before you run the command below, make sure to change update the following parameters with the on created by your own deployment. - &lt;CLUSTER-ID&gt; ‚Äì The EMR virtual cluster ID, which you get from the AWS CDK output - &lt;SPARK-JOB-NAME&gt; ‚Äì The name of your Spark job - &lt;ROLE-ARN&gt; ‚Äì The execution role you created, which you get from the AWS CDK output - &lt;S3URI-CRITICAL-DRIVER&gt; ‚Äì The Amazon S3 URI of the driver pod template, which you get from the AWS CDK output - &lt;S3URI-CRITICAL-EXECUTOR&gt; ‚Äì The Amazon S3 URI of the executor pod template, which you get from the AWS CDK output - &lt;Log_Group_Name&gt; ‚Äì Your CloudWatch log group name - &lt;Log_Stream_Prefix&gt; ‚Äì Your CloudWatch log stream prefix  AWS CLI for start-job-run command aws emr-containers start-job-run \\ --virtual-cluster-id CLUSTER-ID\\ --name=SPARK-JOB-NAME\\ --execution-role-arn ROLE-ARN \\ --release-label emr-6.8.0-latest \\ --job-driver '{ &quot;sparkSubmitJobDriver&quot;:{ &quot;entryPoint&quot;: &quot;local:///usr/lib/spark/examples/src/main/python/pi.py&quot; } }' \\ --configuration-overrides '{ &quot;applicationConfiguration&quot;: [ { &quot;classification&quot;: &quot;spark-defaults&quot;, &quot;properties&quot;: { &quot;spark.hadoop.hive.metastore.client.factory.class&quot;: &quot;com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory&quot;, &quot;spark.sql.catalogImplementation&quot;: &quot;hive&quot;, &quot;spark.dynamicAllocation.enabled&quot;:&quot;true&quot;, &quot;spark.dynamicAllocation.minExecutors&quot;: &quot;8&quot;, &quot;spark.dynamicAllocation.maxExecutors&quot;: &quot;40&quot;, &quot;spark.kubernetes.allocation.batch.size&quot;: &quot;8&quot;, &quot;spark.executor.cores&quot;: &quot;8&quot;, &quot;spark.kubernetes.executor.request.cores&quot;: &quot;7&quot;, &quot;spark.executor.memory&quot;: &quot;28G&quot;, &quot;spark.driver.cores&quot;: &quot;2&quot;, &quot;spark.kubernetes.driver.request.cores&quot;: &quot;2&quot;, &quot;spark.driver.memory&quot;: &quot;6G&quot;, &quot;spark.dynamicAllocation.executorAllocationRatio&quot;: &quot;1&quot;, &quot;spark.dynamicAllocation.shuffleTracking.enabled&quot;: &quot;true&quot;, &quot;spark.dynamicAllocation.shuffleTracking.timeout&quot;: &quot;300s&quot;, &quot;spark.kubernetes.driver.podTemplateFile&quot;: &quot;s3://EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION/EKS-CLUSTER-NAME/pod-template/critical-driver.yaml&quot;, &quot;spark.kubernetes.executor.podTemplateFile&quot;: &quot;s3://EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION/EKS-CLUSTER-NAME/pod-template/critical-executor.yaml&quot; } } ], &quot;monitoringConfiguration&quot;: { &quot;cloudWatchMonitoringConfiguration&quot;: { &quot;logGroupName&quot;: &quot;Log_Group_Name&quot;, &quot;logStreamNamePrefix&quot;: &quot;Log_Stream_Prefix&quot; } } }'  Verify the job execution kubectl get pods --namespace=batchjob -w  "},{"title":"Interactive session‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#interactive-session","content":"To use an interactive session, you should log in to the EMR Studio instance with the URL provided to you at the end of cdk deploy. This link will be in the form of https://es-xxxxx/emrstudio-prod-REGION.amazonaws.com. Once you click on the link you will be see a log in page where you MUST sign-in with the username provided to the addUser method. When you sign in you should follow these steps. Create workspace, this will start for a Jupyter notebookConnect to the Jupter notebookAttach to a Virtual cluster, this would be have the following name &quot;emrvcplatform&quot; and chose an endpoint called &quot;platform-myendpoint&quot;Open a notebook and select the PySpark kernelYou are now ready to perform analyse your data with Spark running on EMR on EKS. "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS Data Platform with AWS CDK","url":"/data-on-eks/blog/Data Platform with AWS CDK#cleanup","content":"To clean up your environment, you call the command below. This will destroy the EKS cluster with Node groups and VPC cdk destroy  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Observability EMR on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/blog/Observability EMR on EKS","content":"","keywords":""},{"title":"Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafana‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"/data-on-eks/blog/Observability EMR on EKS#monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana","content":"In this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases Checkout the full blog here "},{"title":"Architecture‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"/data-on-eks/blog/Observability EMR on EKS#architecture","content":"The following diagram illustrates the solution architecture for scraping Spark Driver and Executors‚Äô metrics, as well as writing to Amazon Managed Service for Prometheus.  "},{"title":"Grafana Dashboard for Spark‚Äã","type":1,"pageTitle":"Observability EMR on EKS","url":"/data-on-eks/blog/Observability EMR on EKS#grafana-dashboard-for-spark","content":"The following Grafana dashboard displays the EMR on EKS Spark job metrics with Driver and Executor details.  "},{"title":"AI/ML Platforms on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/ai-ml","content":"AI/ML Platforms on EKS Running AI/ML platforms on Kubernetes can greatly simplify and automate the deployment, scaling, and management of these complex applications. There are a number of popular tools and technologies that have emerged to support this use case, including TensorFlow, PyTorch and KubeFlow. These tools make it easy to deploy AI/ML models in a containerized environment, and provide features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability. By leveraging the power of Kubernetes, organizations can focus on building and training their AI/ML models, rather than worrying about the underlying infrastructure. With its robust ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running AI/ML platforms in production. The following Terraform templates are available to deploy. Ray on EKS: This template deploys RayCluster on EKS.Kubeflow on AWS: This template deploys the Kubeflow on AWS distribution on EKS.","keywords":""},{"title":"Kubeflow on AWS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/ai-ml/kubeflow","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#introduction","content":"Kubeflow on AWS is an open source distribution of Kubeflow that allows customers to build machine learning systems with ready-made AWS service integrations. Use Kubeflow on AWS to streamline data science tasks and build highly reliable, secure, and scalable machine learning systems with reduced operational overheads. The open source repository for the Kubeflow on AWS distribution is available under awslabs GitHub organization. "},{"title":"Kubeflow‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#kubeflow","content":"Kubeflow is the machine learning toolkit for Kubernetes. It provides a set of tools that enable developers to build, deploy, and manage machine learning workflows at scale. The following diagram shows Kubeflow as a platform for arranging the components of your ML system on top of Kubernetes:  Source: https://www.kubeflow.org/docs/started/architecture/ "},{"title":"AWS Features for Kubeflow‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#aws-features-for-kubeflow","content":""},{"title":"Architecture‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#architecture","content":" Source: https://aws.amazon.com/blogs/machine-learning/build-and-deploy-a-scalable-machine-learning-system-on-kubernetes-with-kubeflow-on-aws/ Running Kubeflow on AWS gives you the following feature benefits and configuration options: "},{"title":"Manage AWS compute environments‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#manage-aws-compute-environments","content":"Provision and manage your Amazon Elastic Kubernetes Service (EKS) clusters with eksctl and easily configure multiple compute and GPU node configurations.Use AWS-optimized container images, based on AWS Deep Learning Containers, with Kubeflow Notebooks. "},{"title":"CloudWatch Logs and Metrics‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#cloudwatch-logs-and-metrics","content":"Integrate Kubeflow on AWS with Amazon CloudWatch for persistent logging and metrics on EKS clusters and Kubeflow pods.Use AWS Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. "},{"title":"Load balancing, certificates, and identity management‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#load-balancing-certificates-and-identity-management","content":"Manage external traffic with AWS Application Load Balancer.Get started with TLS authentication using AWS Certificate Manager and AWS Cognito. "},{"title":"AWS database and storage solutions‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#aws-database-and-storage-solutions","content":"Integrate Kubeflow with Amazon Relational Database Service (RDS) for a highly scalable pipelines and metadata store.Deploy Kubeflow with integrations for Amazon S3 for an easy-to-use pipeline artifacts store.Use Kubeflow with Amazon Elastic File System (EFS) for a simple, scalabale, and serverless storage solution.Leverage the Amazon FSx CSI driver to manage Lustre file systems which are optimized for compute-intensive workloads, such as high-performance computing and machine learning. Amazon FSx for Lustre can scale to hundreds of GBps of throughput and millions of IOPS. "},{"title":"Integrate with Amazon SageMaker‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#integrate-with-amazon-sagemaker","content":"Use Kubeflow on AWS with Amazon SageMaker to create hybrid machine learning workflows.Train, tune, and deploy machine learning models in Amazon SageMaker without logging into the SageMaker console using SageMaker Operators for Kubernetes (ACK).Create a Kubeflow Pipeline built entirely using SageMaker Components for Kubeflow Pipelines, or integrate individual components into your workflow as needed. "},{"title":"Deployment‚Äã","type":1,"pageTitle":"Kubeflow on AWS","url":"/data-on-eks/docs/ai-ml/kubeflow#deployment","content":"caution Terraform deployment options mentioned below are still in preview. caution Please make sure to visit the version compability page to ensure the Kubeflow version you are planning to run is compatible with the EKS version. Kubeflow on AWS can be deployed on an existing EKS cluster using Kustomize or Helm. Additionally, terraform templates are also made available if an EKS cluster is not available and needs to be created. AWS provides various Kubeflow deployment options: Vanilla deploymentDeployment with Amazon RDS and Amazon S3Deployment with Amazon CognitoDeployment with Amazon Cognito, Amazon RDS, and Amazon S3 Please visit the deployment documentation on the Kubeflow on AWS website for the deployment options available and steps for each of those options. "},{"title":"Ray on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/ai-ml/ray","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Ray on EKS","url":"/data-on-eks/docs/ai-ml/ray#introduction","content":"Ray is an open-source framework for building scalable and distributed applications. It is designed to make it easy to write parallel and distributed Python applications by providing a simple and intuitive API for distributed computing. It has a growing community of users and contributors, and is actively maintained and developed by the Ray team at Anyscale, Inc. To deploy Ray in production across multiple machines users must first deploy Ray Cluster. A Ray Cluster consists of head nodes and worker nodes which can be autoscaled using the built-in Ray Autoscaler.  Source: https://docs.ray.io/en/latest/cluster/key-concepts.html "},{"title":"Ray on Kubernetes‚Äã","type":1,"pageTitle":"Ray on EKS","url":"/data-on-eks/docs/ai-ml/ray#ray-on-kubernetes","content":"Deploying Ray Cluster on Kubernetes including on Amazon EKS is supported via the KubeRay Operator. The operator provides a Kubernetes-native way to manage Ray clusters. The installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob and RayService as documented here. Deploying Ray on Kubernetes can provide several benefits: Scalability: Kubernetes allows you to scale your Ray cluster up or down based on your workload requirements, making it easy to manage large-scale distributed applications. Fault tolerance: Kubernetes provides built-in mechanisms for handling node failures and ensuring high availability of your Ray cluster. Resource allocation: With Kubernetes, you can easily allocate and manage resources for your Ray workloads, ensuring that they have access to the necessary resources for optimal performance. Portability: By deploying Ray on Kubernetes, you can run your workloads across multiple clouds and on-premises data centers, making it easy to move your applications as needed. Monitoring: Kubernetes provides rich monitoring capabilities, including metrics and logging, making it easy to troubleshoot issues and optimize performance. Overall, deploying Ray on Kubernetes can simplify the deployment and management of distributed applications, making it a popular choice for many organizations that need to run large-scale machine learning workloads. Before moving forward with the deployment please make sure you have read the pertinent sections of the official documentation.  Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html "},{"title":"Deploying the Example‚Äã","type":1,"pageTitle":"Ray on EKS","url":"/data-on-eks/docs/ai-ml/ray#deploying-the-example","content":"In this example, you will provision Ray Cluster on Amazon EKS using the KubeRay Operator. The example also demonstrates the use of Karpenter of autoscaling of worker nodes for job specific Ray Clusters.  Pre-requisites Deploy the EKS Cluster with KubeRay Operator Verify Deployment Deploy Ray Clusters and Workloads Teardown "},{"title":"Amazon EMR on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/amazon-emr-on-eks","content":"","keywords":""},{"title":"Benefits of EMR on EKS‚Äã","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks#benefits-of-emr-on-eks","content":""},{"title":"Simplify management‚Äã","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks#simplify-management","content":"You get the same EMR benefits for Apache Spark on EKS that you get on EC2 today. This includes fully managed versions of Apache Spark 2.4 and 3.0, automatic provisioning, scaling, performance optimized runtime, and tools like EMR Studio for authoring jobs and an Apache Spark UI for debugging. "},{"title":"Reduce Costs‚Äã","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks#reduce-costs","content":"With EMR on EKS, your compute resources can be shared between your Apache Spark applications and your other Kubernetes applications. Resources are allocated and removed on-demand to eliminate over-provisioning or under-utilization of these resources, enabling you to lower costs as you only pay for the resources you use. "},{"title":"Optimize Performance‚Äã","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks#optimize-performance","content":"By running analytics applications on EKS, you can reuse existing EC2 instances in your shared Kubernetes cluster and avoid the startup time of creating a new cluster of EC2 instances dedicated for analytics. You can also get 3x faster performance running performance optimized Spark with EMR on EKS compared to standard Apache Spark on EKS. "},{"title":"EMR on EKS Deployment patterns with Terraform‚Äã","type":1,"pageTitle":"Amazon EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks#emr-on-eks-deployment-patterns-with-terraform","content":"The following Terraform templates are available to deploy. EMR on EKS with Karpenter: üëàüèº Start Here if you are new to EMR on EKS. This template deploys EMR on EKS cluster and uses Karpenter to scale Spark jobs.ACK controller for EMR on EKS: This template deploys EMR on EKS cluster and uses ACK controller to manage Spark jobs "},{"title":"ACK Controller for EMR on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-ack","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-ack#introduction","content":"In this post, we will learn to build EMR on EKS Spark workloads by using AWS Controllers for Kubernetes (ACK). We will also build a end-to-end observability for Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications and then use Amazon Managed Grafana to build dashboards for monitoring use cases. Deploying the Solution "},{"title":"Setup Amazon Managed Grafana with SSO‚Äã","type":1,"pageTitle":"ACK Controller for EMR on EKS","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-ack#setup-amazon-managed-grafana-with-sso","content":"Currently, this step is manual. Please follow the steps in this blog to create Amazon Managed Grafana with SSO enabled in your account. You can visualize the Spark jobs runs and metrics using Amazon Managed Prometheus and Amazon Managed Grafana. Execute Sample Spark job - EMR Virtual Cluster Cleanup caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"EMR on EKS with CDK blueprint","type":0,"sectionRef":"#","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#introduction","content":"In this post, we will learn how to use EMR on EKS AddOn and Teams in the cdk-eks-blueprints to deploy a an infrasturcture on EKS to submit Spark Job. The cdk-eks-blueprints allows you deploy an EKS cluster and enable it to be used by EMR on EKS service with minimal setup. The architecture below shows a conceptual view of the infrastructure you will deploy through this blueprint.  "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#deploying-the-solution","content":"In this example, you will provision the following: Creates EKS Cluster Control plane with public endpoint (for demo purpose only)Two managed node groups Core Node group with 3 AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Logging etc.Spark Node group with single AZ for running Spark jobs Enable EMR on EKS and create one Data teams (emr-data-team-a) Creates new namespace for each teamCreates Kubernetes role and role binding(emr-containers user) for the above namespaceNew IAM role for the team execution roleUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service account EMR Virtual Cluster for emr-data-team-aIAM policy for emr-data-team-aDeploys the following Kubernetes Add-ons Managed Add-ons VPC CNI, CoreDNS, KubeProxy, AWS EBS CSi Driver Self Managed Add-ons Metrics server with HA, Cluster Autoscaler, CertManager and AwsLoadBalancerController This blueprint can also take an EKS cluster that you defined using the cdk-blueprints-library. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlCDK NOTE: You need to have an AWS account and region that are bootstraped by AWS CDK. "},{"title":"Customize‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#customize","content":"The the entry point for this cdk blueprint is /bin/emr-eks.ts which instantiate a stack defined in lib/emr-eks-blueprint-stack.ts. This stack must be provided with a VPC and a list of EMR on EKS team defition and the role that will be admin of the EKS cluster. It can also take as options an EKS cluster defined through cdk-blueprints-library and the EKS cluster name. The properties that are passed to the emr on eks blueprint stack are defined as such: export interface EmrEksBlueprintProps extends StackProps { clusterVpc: IVpc, clusterAdminRoleArn: ArnPrincipal dataTeams: EmrEksTeamProps[], eksClusterName?: string, //Default eksBlueprintCluster eksCluster?: GenericClusterProvider, }  In this example we define a VPC in lib/vpc.ts and is instantiated in bin/emr-eks.ts. We also define a team called emr-data-team-a and which has an execution role called myBlueprintExecRole. The blueprint will deploy by default an EKS cluster with the managed nodegroups defined in the section Deploying the Solution. "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#deploy","content":"Before you run the solution, you MUST change the clusterAdminRoleArn of the props object in lib/emr-eks.ts. This role allows you to interact manage EKS cluster and should have be allowed at least the IAM action eks:AccessKubernetesApi. Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run cdk synth cd analytics/cdk/emr-eks npm install cdk synth --profile YOUR-AWS-PROFILE  Deploy the pattern cdk deploy --all  Enter yes to deploy. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#verify-the-resources","content":"Let‚Äôs verify the resources created by cdk deploy. Verify the Amazon EKS Cluster aws eks describe-cluster --name eksBlueprintCluster # Update the name cluster name if you supplied your own  Verify EMR on EKS Namespaces batchjob and Pod status for Metrics Server and Cluster Autoscaler. aws eks --region &lt;ENTER_YOUR_REGION&gt; update-kubeconfig --name eksBlueprintCluster # Creates k8s config file to authenticate with EKS Cluster. Update the name cluster name if you supplied your own kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | grep batchjob # Output shows batchjob kubectl get pods --namespace=kube-system | grep metrics-server # Output shows Metric Server pod kubectl get pods --namespace=kube-system | grep cluster-autoscaler # Output shows Cluster Autoscaler pod  "},{"title":"Execute Sample Spark job on EMR Virtual Cluster‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#execute-sample-spark-job-on-emr-virtual-cluster","content":"Execute the Spark job using the below shell script. Once you deploy the blueprint you will have as output the Virtual Cluster id. You can use the id and the execution role for which you supplied a policy to submit jobs. Below you can find an example of a job you can submit with AWS CLI.  export EMR_ROLE_ARN=arn:aws:iam::&lt;YOUR-ACCOUNT-ID&gt;:role/myBlueprintExecRole aws emr-containers start-job-run \\ --virtual-cluster-id=&lt;VIRTUAL-CLUSTER-ID-IN-CDK-OUTPUT&gt; \\ --name=pi-2 \\ --execution-role-arn=$EMR_ROLE_ARN \\ --release-label=emr-6.8.0-latest \\ --job-driver='{ &quot;sparkSubmitJobDriver&quot;: { &quot;entryPoint&quot;: &quot;local:///usr/lib/spark/examples/src/main/python/pi.py&quot;, &quot;sparkSubmitParameters&quot;: &quot;--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.kubernetes.node.selector.app=spark&quot; } }'  Verify the job execution kubectl get pods --namespace=batchjob -w  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS with CDK blueprint","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-cdk#cleanup","content":"To clean up your environment, you call the command below. This will destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC cdk destroy --all  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow","content":"","keywords":""},{"title":"Architecture‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#architecture","content":"The following diagram shows the complete setup that we will walk through in this walk through:  "},{"title":"Solution Walkthrough‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#solution-walkthrough","content":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#prerequisites","content":"You will need the following to complete the steps in this post: An Ubuntu development environment with access to an AWS environmentInstall awscurl which is a curl-like tool with AWS Signature Version 4 request signing on your environment First, Let‚Äôs start by setting a few environment variables: export KFL_EKS_CLUSTER=KFL-EKS-CLUSTER export KFL_EKS_CLUSTER_V=1.25 export KFL_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text) export KFL_AWS_REGION=us-west-2 # Your AWS Region export AWS_REGION=us-west-2 # Your AWS Region export KFL_AMP_WORKSPACE_NAME=kubeflow-amp-workshop export CLUSTER_NAME=KFL-EKS-CLUSTER export CLUSTER_REGION=us-west-2 export KUBEFLOW_RELEASE_VERSION=v1.7.0 export AWS_RELEASE_VERSION=v1.7.0-aws-b1.0.1  Next, let's start with installing prerequisites such as AWS CLI version 2, eksctl, kubectl, python3.8, yq, jq, awscurl,kustomize version 5+ required to run the demonstration. Clone the awslabs/kubeflow-manifests repo and checkout a release. Substitute the value for AWS_RELEASE_VERSION with v1.7.0-aws-b1.0.1 and run the following command. Read more about releases and versioning policy to determine the right version for you for installing Kubeflow. git clone https://github.com/awslabs/kubeflow-manifests.git &amp;&amp; cd kubeflow-manifests git checkout ${AWS_RELEASE_VERSION} git clone --branch ${KUBEFLOW_RELEASE_VERSION} https://github.com/kubeflow/manifests.git upstream `make install``-``tools`  "},{"title":"Create an EKS Cluster‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#create-an-eks-cluster","content":"Let‚Äôs create an Amazon EKS cluster using eksctl: ## eksctl Cluster creation command for EKS cluster. eksctl create cluster \\ --name $KFL_EKS_CLUSTER \\ --version $KFL_EKS_CLUSTER_V \\ --region $KFL_AWS_REGION \\ --nodegroup-name linux-nodes \\ --node-type m5.xlarge \\ --nodes 5 \\ --nodes-min 1 \\ --nodes-max 10 \\ --managed \\ --with-oidc  "},{"title":"Installing Amazon Elastic Block Store (EBS) Container Storage Interface Driver‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#installing-amazon-elastic-block-store-ebs-container-storage-interface-driver","content":"A Container Storage Interface (CSI) driver is needed in order to get your PersisentVolumeClaims served by a PersistentVolume. Please run the following commands to create Amazon EBS CSI driver IAM role and add EBS CSI add-on : eksctl create iamserviceaccount \\ --name ebs-csi-controller-sa \\ --namespace kube-system \\ --cluster $KFL_EKS_CLUSTER \\ --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\ --approve \\ --role-only \\ --role-name AmazonEKS_EBS_CSI_DriverRole eksctl create addon \\ --name aws-ebs-csi-driver \\ --cluster $KFL_EKS_CLUSTER \\ --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\ --force  "},{"title":"Installing Kubeflow‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#installing-kubeflow","content":"You can install all Kubeflow official components (residing under apps) and all common services (residing under common) using the following command: make deploy-kubeflow INSTALLATION_OPTION=kustomize DEPLOYMENT_OPTION=vanilla  It takes around 5 minutes for all components to get installed. Once everything is installed successfully, you can access the Kubeflow Central Dashboard. Kubeflow on AWS page has more information for learning open source distribution of Kubeflow on AWS. After installation, it will take some time for all Pods to become ready. Make sure all Pods are ready before trying to connect, otherwise you might get unexpected errors. To check that all Kubeflow-related Pods are ready, use the following command: kubectl get pods -A -o json | jq -r '.items[] | select(.metadata.namespace==&quot;cert-manager&quot; or &quot;istio-system&quot; or &quot;auth&quot; or &quot;knative-eventing&quot; or &quot;knative-serving&quot; or &quot;kubeflow&quot; or &quot;kubeflow-user-example-com&quot;) | .metadata.namespace + &quot;|&quot; + .metadata.name + &quot;|&quot; + .status.phase'  "},{"title":"Accessing Kubeflow Central Dashboard‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#accessing-kubeflow-central-dashboard","content":"Kubeflow can be accessed via port-forward and this enables you to get started quickly without imposing any requirements on your environment. Run the following to port-forward Istio's Ingress-Gateway to local port 8080: kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80  After running the command, you can access the Kubeflow Central Dashboard by doing the following: Dex is an OpenID Connect Identity (OIDC) with multiple authentication backends. Open your browser and visit http://localhost:8080 and You should get the Dex login screen.Login with the default user's credential. The default email address is user@example.com and the default password is 12341234.  "},{"title":"Setup Amazon Managed Service for Prometheus‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#setup-amazon-managed-service-for-prometheus","content":"A workspace in Amazon Managed Service for Prometheus is a logical and isolated Prometheus server dedicated to Prometheus resources such as metrics. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, delete, and the ingestion and querying of metrics. Please open a new terminal window and setup all environment variables as you did in start of the demonstration. Please use the below command to create an Amazon Managed Service for Prometheus workspace. aws amp create-workspace \\ --alias $KFL_AMP_WORKSPACE_NAME \\ --region $KFL_AWS_REGION  The Amazon Managed Service for Prometheus workspace should be created in just a few seconds. As a best practice, create a VPC endpointfor Amazon Managed Service for Prometheus in VPC running your Amazon EKS cluster. Please visit Using Amazon Managed Service for Prometheus with interface VPC endpoints for more information. "},{"title":"Setting up the AWS Distro for OpenTelemetry (ADOT) Collector to Ingest Metrics‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#setting-up-the-aws-distro-for-opentelemetry-adot-collector-to-ingest-metrics","content":"Amazon Managed Service for Prometheus does not directly scrape operational metrics from containerized workloads in a Kubernetes or ECS cluster. It requires users to deploy a collection agent such as Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector in their cluster to perform this task. One of the easiest ways to collect Prometheus metrics from Amazon EKS workloads is by using the AWS Distro for OpenTelemetry (ADOT) collector. Customers can deploy the ADOT Collector in a variety of deployment models and easily manage configuration using the ADOT Operator. The ADOT Operator is also available as an EKS Add-Onfor easier deployment and management. Read our launch blogto learn about this feature. The best way to provision permissions for resources running on EKS clusters is through IRSA. The command below will use AWS CloudFormation to create a K8s namespace called prometheus, create a K8s Service Account called amp-iamproxy-ingest-role, create a new IAM Role with the AmazonPrometheusRemoteWriteAccess policy attached to it. It will also create a trust policy between the EKS cluster's IAM OpenID Connect Provider (OIDC) and the created Service Account. See this link to learn more about this command. kubectl create namespace prometheus eksctl create iamserviceaccount \\ --name amp-iamproxy-ingest-role \\ --namespace prometheus \\ --cluster $KFL_EKS_CLUSTER \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\ --approve \\--override-existing-serviceaccounts  Next, we will grant permissions to Amazon EKS add-ons to install ADOT and then we will installing the ADOT Add-on : kubectl apply -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml aws eks create-addon \\ --addon-name adot \\ --cluster-name $KFL_EKS_CLUSTER  Now, wait for 30 seconds and execute the following command. You should see &quot;ACTIVE&quot; as result indicating that the add-on is installed successfully. aws eks describe-addon \\ --addon-name adot \\ --cluster-name $KFL_EKS_CLUSTER | jq .addon.status  Next, we will Install the OTel Collector Custom Resource Definition(CRD) and then we will configure the ADOT collector to push metrics to Amazon Managed Service for Prometheus endpoint. KFL_WORKSPACE_ID=$(aws amp list-workspaces \\ --alias $KFL_AMP_WORKSPACE_NAME \\ --region=${KFL_AWS_REGION} \\ --query 'workspaces[0].[workspaceId]' \\ --output text) KFL_AMP_ENDPOINT_URL=$(aws amp describe-workspace \\ --workspace-id $KFL_WORKSPACE_ID | jq .workspace.prometheusEndpoint -r) KFL_AMP_REMOTE_WRITE_URL=${KFL_AMP_ENDPOINT_URL}api/v1/remote_write curl -O https://raw.githubusercontent.com/aws-samples/one-observability-demo/main/PetAdoptions/cdk/pet_stack/resources/otel-collector-prometheus.yaml sed -i -e s/AWS_REGION/$KFL_AWS_REGION/g otel-collector-prometheus.yaml sed -i -e s^AMP_WORKSPACE_URL^$KFL_AMP_REMOTE_WRITE_URL^g otel-collector-prometheus.yaml kubectl apply -f ./otel-collector-prometheus.yaml  Now, lets verify that the ADOT collector is running and you should see a result like the one below showing that the collector has been successfully installed and being ready. kubectl get all -n prometheus  NAME READY STATUS RESTARTS AGEpod/observability-collector-5774bbc68d-7nj54 1/1 Running 0 59s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/observability-collector-monitoring ClusterIP 10.100.114.1 &lt;none&gt; 8888/TCP 59s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/observability-collector 1/1 1 1 59s NAME DESIRED CURRENT READY AGE replicaset.apps/observability-collector-5774bbc68d 1 1 1 59s  Now you have successfully deployed the ADOT Collector to collect metrics from the EKS cluster and send it to the Amazon Managed Service for Prometheus workspace you created. To test whether Amazon Managed Service for Prometheus received the metrics, use awscurl. This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from Amazon Managed Service for Prometheus. For instructions on installing awscurl, see awscurl. awscurl --service=&quot;aps&quot; \\ --region=&quot;$KFL_AWS_REGION&quot; &quot;https://aps-workspaces.$KFL_AWS_REGION.amazonaws.com/workspaces/$KFL_WORKSPACE_ID/api/v1/query?query=istio_requests_total&quot;  Your results should look similar to shown below: { &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: { &quot;resultType&quot;: &quot;vector&quot;, &quot;result&quot;: [ { &quot;metric&quot;: { &quot;__name__&quot;: &quot;istio_requests_total&quot;, &quot;app&quot;: &quot;istio-ingressgateway&quot;, &quot;chart&quot;: &quot;gateways&quot;, .................................... .................................... &quot;version&quot;: &quot;v1&quot; }, &quot;value&quot;: [ 1647974689.212, &quot;1&quot; ] } ] } }  "},{"title":"Amazon Managed Grafana Setup‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#amazon-managed-grafana-setup","content":"Two steps are necessary for setting up AWS IAM Identity Center, setting up and logging in to Amazon Managed Grafana, and querying metrics from Amazon Managed Service for Prometheus workspace from the post. To set up Authentication and Authorization, follow the instructions in the Amazon Managed Grafana User Guide for enabling AWS IAM Identity Center. Second, setup the data source for Amazon Managed Service for Prometheus. You may also reference Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana blog, starting from the AWS Single Sign-On (SSO) section for Amazon Managed Grafana setup. "},{"title":"Query Kubeflow Metrics‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#query-kubeflow-metrics","content":"Next lets navigate to Amazon Managed Grafana console and import Grafana dashboards which allows us to visualize metrics from Istio environment. Go to the plus sign on the left navigation bar and select Import as shown below:  In the Import screen, type 7630 (Istio Workload Dashboard) in Import via grafana.com textbox and click Load. Select the Prometheus data source in the drop down at the bottom and click on Import. Once complete, you will be able to see the Grafana dashboard showing metrics from the Istio Workload Dashboard through Prometheus data source as shown below:  "},{"title":"Creating a sample Machine Learning pipeline in Kubeflow‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#creating-a-sample-machine-learning-pipeline-in-kubeflow","content":"Now that we have configured Amazon Managed Grafana with the Prometheus data source within our cluster, we can initiate a Machine Learning pipeline in Kubeflow, and be able to display metrics on the Grafana dashboards. Before we create the notebook to use Kubeflow Pipelines SDK, we have to supply a token so that the notebook can authenticate with the Kubeflow Pipelines API. To do so, run the following command to create a Pod to mount a token volume: cat &lt;&lt;EOF | kubectl apply -f - apiVersion: kubeflow.org/v1alpha1 kind: PodDefault metadata: name: access-ml-pipeline namespace: kubeflow-user-example-com spec: desc: Allow access to Kubeflow Pipelines selector: matchLabels: access-ml-pipeline: &quot;true&quot; volumes: - name: volume-kf-pipeline-token projected: sources: - serviceAccountToken: path: token expirationSeconds: 7200 audience: pipelines.kubeflow.org volumeMounts: - mountPath: /var/run/secrets/kubeflow/pipelines name: volume-kf-pipeline-token readOnly: true env: - name: KF_PIPELINES_SA_TOKEN_PATH value: /var/run/secrets/kubeflow/pipelines/token EOF  Now, access Kubeflow dashboard as described in the previous sections, via port-forwarding. Select Notebooks, and create a new CPU-based notebook, using the following configurations: name: ml-training-notebookdocker image: kubeflow-on-aws/notebook-servers/jupyter-pytorch:2.0.0-cpu-py310-ubuntu20.04-ec2-v1.0Requested CPUs: 1Requested memory in Gi: 5Advanced Options ‚Üí Configurations: Allow access to Kubeflow Pipelines (This configuration is the token we have generated above) With all other configurations as defaults, you should be able to see the notebook generate successfully and show up on the Kubeflow dashboard.  You can also verify that the notebook is created by verifying the Kubernetes resources being created: kubectl get pods -n kubeflow-user-example-com --field-selector=status.phase==Running  NAME READY STATUS RESTARTS AGE ml-pipeline-ui-artifact-5b7794c7b5-5hkqf 2/2 Running 0 100m ml-pipeline-visualizationserver-85c6d6cc9f-vs24x 2/2 Running 0 100m ml-training-notebook-0 2/2 Running 0 11m  You will be able to access the JupyterLab notebook by clicking CONNECT. This will open up a new JupyterLab window:  We will run a simple pipeline training notebook that uses Kubeflow Pipelines, from an existing AWS Deep Learning sample repository. This is a simple model training to predict the taxi fare of Chicago cabs, and demonstrates continuous training using a recursive loop. It triggers a Kubeflow pipeline to train the initial model and then gradually trains the model until evaluation metrics are good enough. Lets run the Kubeflow pipeline using the following steps on the console : Clone the following repo by selecting Git -&gt; Clone a Repository from the top navigation bar and paste https://github.com/aws-samples/aws-deeplearning-labs and press enter.Open the following notebook from the directory view in the left pane: aws-deeplearning-labs/workshop/pytorch-distributed-training/STEP2_simple_xgboost_training_pipeline.ipynb.Run all the cells of the model by selecting Kernel -&gt; Restart Kernel and Run All Cells from the top menu  "},{"title":"Visualizing Machine Learning pipeline metrics on Amazon Managed Grafana‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#visualizing-machine-learning-pipeline-metrics-on-amazon-managed-grafana","content":"Using the Amazon Managed Grafana, we can show the resource utilization from our Machine Learning Pipelines with the same method we used to look above: using the Istio Workload Dashboard (7630). Select the following to monitor your resources for this particular ML pipeline: Datasource: your prometheus workspace nameNamespace: kubeflow-user-example-comWorkload: ml-training-notebook  "},{"title":"Alerting Kubeflow workflows with Amazon Managed Grafana‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#alerting-kubeflow-workflows-with-amazon-managed-grafana","content":"As we configure workflows with Kubeflow, alerting is a mechanism we can employ to alert on specific situations. By quickly identifying unintended changes in your workflow and notifying the same using alerts, you can take actions to minimize disruptions to your services. Amazon Managed Grafana supports multiple notification channels such as SNS, Slack, PagerDuty etc to which you can send alerts notifications. Alerts page will show you more information on how to setup alerts in Amazon Managed Grafana. You learn about setting up alerts from Amazon Managed Grafana to Slack from our Monitoring hybrid environments using Amazon Managed Grafana blog. Also check our Blog on Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana which will show you on triggering Amazon Managed Grafana alerts to PagerDuty. "},{"title":"Clean-up‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#clean-up","content":"Use the following commands to clean up the created AWS resources for this demonstration: # Clean up ADOT Collector and Prometheus. kubectl delete -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml kubectl delete -f ./otel-collector-prometheus.yaml rm -rf ./otel-collector-prometheus.yaml aws eks delete-addon \\ --addon-name adot \\ --cluster-name $KFL_EKS_CLUSTER aws amp delete-workspace \\ --workspace-id $KFL_WORKSPACE_ID \\ --region $KFL_AWS_REGION eksctl delete iamserviceaccount \\ --name amp-iamproxy-ingest-role \\ --namespace prometheus \\ --cluster $KFL_EKS_CLUSTER kubectl delete namespace prometheus # Cleaning up kubeflow installation components make delete-kubeflow INSTALLATION_OPTION=kustomize DEPLOYMENT_OPTION=vanilla cd .. rm -rf kubeflow-manifests eksctl delete iamserviceaccount \\ --name ebs-csi-controller-sa \\ --namespace kube-system \\ --cluster $KFL_EKS_CLUSTER aws eks delete-addon \\ --addon-name aws-ebs-csi-driver \\ --cluster-name $KFL_EKS_CLUSTER # Cleaning up Amazon EKS Cluster. eksctl delete cluster --region $AWS_REGION --name $KFL_EKS_CLUSTER  Finally navigate to Amazon Managed Grafana console to delete the created Grafana workspace. "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Monitor Machine Learning workflows with Kubeflow on Amazon EKS","url":"/data-on-eks/docs/ai-ml/monitoring-kubeflow#conclusion","content":"This post demonstrated the detailed steps on how you can setup Amazon EKS cluster with Kubeflow, Amazon Managed Service for Prometheus and Amazon Managed Grafana to monitor your Kubeflow machine learning workflows. It is also important to have a centralized incident management process to keep systems running smoothly. You can view more details on alerting in and various supported providers at alert notifications for Amazon Managed Grafana. You can also check out previous blogs posts Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty and how to integrate Amazon Managed Service for Prometheus with Slack to see how you can setup alerting with Amazon Managed Service for Prometheus. For further reading on Kubeflow deployment and monitoring on Amazon EKS, check out Build and deploy a scalable machine learning system on Kubernetes with Kubeflow on AWS and CloudWatch add-on for Kubeflow.. "},{"title":"EMR Virtual Cluster on EKS Fargate","type":0,"sectionRef":"#","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate","content":"","keywords":""},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"EMR Virtual Cluster on EKS Fargate","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"EMR Virtual Cluster on EKS Fargate","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/analytics/emr-eks-fargate terraform init  Set AWS_REGION and Runterraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Change according to your need terraform plan  Deploy the pattern terraform apply  Enter yes at command prompt to apply "},{"title":"Validate‚Äã","type":1,"pageTitle":"EMR Virtual Cluster on EKS Fargate","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl. Run update-kubeconfig command: aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;  Test by listing all the pods running currently. Note: the EMR on EKS virtual cluster(s) will create pods as needed to execute jobs and the pods shown will vary depending on how long after deploying the example you run the kubectl get pods -A command: kubectl get pods -A # Output should look like below NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cluster-proportional-autoscaler-coredns-6ccfb4d9b5-sjb8m 1/1 Running 0 8m27s kube-system coredns-7c8d74d658-9cmn2 1/1 Running 0 8m27s kube-system coredns-7c8d74d658-pmf5l 1/1 Running 0 7m38s  Execute the sample EMR on EKS job. This will calculate the value of Pi using sample PySpark job. cd analytics/terraform/emr-eks-fargate/examples ./basic-pyspark-job '&lt;ENTER_EMR_EMR_VIRTUAL_CLUSTER_ID&gt;' '&lt;EMR_JOB_EXECUTION_ROLE_ARN&gt;'  Once the job is complete, navigate to the CloudWatch log console and find the log group created by this example /emr-on-eks-logs/emr-workload/emr-workload. Click Search Log Group and enter roughly into the search field. You should see a log entry that has the returned results from the job. { &quot;message&quot;: &quot;Pi is roughly 3.146360&quot;, &quot;time&quot;: &quot;2022-11-20T16:46:59+00:00&quot; }  "},{"title":"Destroy‚Äã","type":1,"pageTitle":"EMR Virtual Cluster on EKS Fargate","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-fargate#destroy","content":"To teardown and remove the resources created in this example: kubectl delete all --all -n emr-workload -n emr-custom # ensure all jobs resources are cleaned up first terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks&quot; -auto-approve terraform destroy -auto-approve  If the EMR virtual cluster fails to delete and the following error is shown: Error: waiting for EMR Containers Virtual Cluster (xwbc22787q6g1wscfawttzzgb) delete: unexpected state 'ARRESTED', wanted target ''. last error: %!s(&lt;nil&gt;)  You can clean up any of the clusters in the ARRESTED state with the following: aws emr-containers list-virtual-clusters --region us-west-2 --states ARRESTED \\ --query 'virtualClusters[0].id' --output text | xargs -I{} aws emr-containers delete-virtual-cluster \\ --region us-west-2 --id {}  "},{"title":"Data Analytics on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/data-analytics","content":"Data Analytics on EKS Running data analytics tools on Kubernetes can provide a number of benefits for organizations looking to extract insights from large and complex data sets. Tools such as Apache Spark and DASK are designed to run on a cluster of machines, making them well-suited for deployment on Kubernetes. The Spark Operator for Kubernetes is a popular Kubernetes operator that simplifies the deployment and management of Apache Spark on Kubernetes. By using the Spark Operator, organizations can take advantage of features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of their data analytics pipelines. This can greatly simplify and automate the deployment, scaling, and management of these complex applications, freeing up data scientists and engineers to focus on the analysis and interpretation of the data. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running data analytics platforms in production. Spark OperatorSpark SubmitKarpenterApache YuniKornVolcano","keywords":""},{"title":"Observability Spark on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#introduction","content":"In this post, we will learn the Observability for Spark on EKS. We will use Spark History Server to watch Spark Applications logs and check the Spark job progress via the Spark Web UI. Amazon Managed Service for Prometheus is used to collect and store the metrics generated by Spark Applications and Grafana is used to build dashboards for monitoring use cases. "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#deploying-the-solution","content":"We will reuse the previous Spark on Operator example. Please follow this link to provision resources "},{"title":"Set up data and py script‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#set-up-data-and-py-script","content":"let's navigate to one example folder under spark-k8s-operator and run the shell script to upload data and py script to the S3 bucket created by terraform above. cd data-on-eks/analytics/terraform/spark-k8s-operator/examples/nvme-ephemeral-storage # replace &lt;S3_BUCKET&gt; with your S3 bucket and &lt;REGION&gt; with your region, then run ./taxi-trip-execute.sh  "},{"title":"Spark Web UI‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#spark-web-ui","content":"When you submit a Spark application, Spark context is created which ideally gives you Spark Web UI to monitor the execution of the application. Monitoring includes the following. Spark configurations usedSpark Jobs, stages, and tasks detailsDAG executionDriver and Executor resource utilizationApplication logs and many more  When your application is done with the processing, Spark context will be terminated so your Web UI as well. and if you wanted to see the monitoring for already finished application, we cannot do it. To try Spark web UI, let's update &lt;S3_BUCKET&gt; with your bucket name and &lt;JOB_NAME&gt; with &quot;nvme-taxi-trip&quot; in nvme-ephemeral-storage.yaml  kubectl apply -f nvme-ephemeral-storage.yaml  Then run port forward command to expose spark web service. kubectl port-forward -n=spark &amp;lt;SPARK_DRIVER_NAME&amp;gt; 4040:4040  Then open browser and enter localhost:4040. You can view your spark application like below.  "},{"title":"Spark History Server‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#spark-history-server","content":"As mentioned above, spark web UI will be terminated once the spark job is done. This is where Spark history Server comes into the picture, where it keeps the history (event logs) of all completed applications and its runtime information which allows you to review metrics and monitor the application later in time. In this example, we installed Spark history Server to read logs from S3 bucket. In your spark application yaml file, make sure you have the following setting: sparkConf: &quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;: &quot;com.amazonaws.auth.InstanceProfileCredentialsProvider&quot; &quot;spark.hadoop.fs.s3a.impl&quot;: &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot; &quot;spark.eventLog.enabled&quot;: &quot;true&quot; &quot;spark.eventLog.dir&quot;: &quot;s3a://&lt;your bucket&gt;/logs/&quot;  Run port forward command to expose spark-history-server service. kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server  Then open browser and enter localhost:18085. You can view your spark history server like below. "},{"title":"Prometheus‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#prometheus","content":"Spark users must add the following config to spark application yaml file to extract the metrics from Spark Driver and Executors. In the example, they are added into nvme-ephemeral-storage.yaml already. &quot;spark.ui.prometheus.enabled&quot;: &quot;true&quot; &quot;spark.executor.processTreeMetrics.enabled&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/scrape&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/path&quot;: &quot;/metrics/executors/prometheus/&quot; &quot;spark.kubernetes.driver.annotation.prometheus.io/port&quot;: &quot;4040&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/scrape&quot;: &quot;true&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/path&quot;: &quot;/metrics/driver/prometheus/&quot; &quot;spark.kubernetes.driver.service.annotation.prometheus.io/port&quot;: &quot;4040&quot; &quot;spark.metrics.conf.*.sink.prometheusServlet.class&quot;: &quot;org.apache.spark.metrics.sink.PrometheusServlet&quot; &quot;spark.metrics.conf.*.sink.prometheusServlet.path&quot;: &quot;/metrics/driver/prometheus/&quot; &quot;spark.metrics.conf.master.sink.prometheusServlet.path&quot;: &quot;/metrics/master/prometheus/&quot; &quot;spark.metrics.conf.applications.sink.prometheusServlet.path&quot;: &quot;/metrics/applications/prometheus/&quot;  Run port forward command to expose prometheus service. kubectl port-forward service/prometheus-server 8080:80 -n prometheus  Then open browser and enter localhost:8080. You can view your prometheus server like below. "},{"title":"Grafana‚Äã","type":1,"pageTitle":"Observability Spark on EKS","url":"/data-on-eks/docs/data-analytics/observability-spark-on-eks#grafana","content":"Grafana has been installed. Use the command below to access with port forward. get grafana password kubectl port-forward service/grafana 8080:80 -n grafana  login username is admin and password can get from secrets manager. You can import dashboard with ID: 7890.  "},{"title":"Spark Operator with YuniKorn","type":0,"sectionRef":"#","url":"/data-on-eks/docs/data-analytics/spark-operator-yunikorn","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Spark Operator with YuniKorn","url":"/data-on-eks/docs/data-analytics/spark-operator-yunikorn#introduction","content":"The EKS Cluster design for the Data on EKS blueprint is optimized for running Spark applications with Spark Operator and Apache YuniKorn as the batch scheduler. This blueprint shows both options of leveraging Cluster Autoscaler and Karpenter for Spark Workloads. AWS for FluentBit is employed for logging, and a combination of Prometheus, Amazon Managed Prometheus, and open source Grafana are used for observability. Additionally, the Spark History Server Live UI is configured for monitoring running Spark jobs through an NLB and NGINX ingress controller. Spark workloads with Karpenter Spark workloads with ClusterAutoscaler and Managed NodeGroups NVMe SSD Instance Storage for Spark Shuffle data Spark Operator Deploying the Solution Execute Sample Spark job with Karpenter Execute Sample Spark job with Cluster Autoscaler and Managed Node groups Example for TPCDS Benchmark test Cleanup caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Distributed Databases on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/distributed-databases","content":"Distributed Databases on EKS info Note: The blueprints for distributed databases and query engines are currently in the process of development. Documentation will be updated once a deployment example has been added to the repository. Running distributed databases and query engines on Kubernetes can provide a number of benefits for organizations looking to manage and process large amounts of data in real-time. Kubernetes provides features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of these systems. There are a number of popular distributed databases and query engines that have emerged to support this use case, including Apache Cassandra, Amazon DynamoDB, and Apache Presto. These systems make it easy to manage and process large amounts of data in real-time, and provide features such as scalability, high availability, and real-time data processing. By leveraging the power of Kubernetes, organizations can simplify and automate the deployment, scaling, and management of these complex systems, freeing up resources to focus on other areas of the business. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running distributed databases and query engines in production.","keywords":""},{"title":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","type":0,"sectionRef":"#","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#introduction","content":"CloudNativePG is an open sourceoperatordesigned to manage PostgreSQL workloads Kubernetes. It defines a new Kubernetes resource called Cluster representing a PostgreSQL cluster made up of a single primary and an optional number of replicas that co-exist in a chosen Kubernetes namespace for High Availability and offloading of read-only queries. Applications that reside in the same Kubernetes cluster can access the PostgreSQL database using a service which is solely managed by the operator, without having to worry about changes of the primary role following a failover or a switchover. Applications that reside outside the Kubernetes cluster, need to configure a Service or Ingress object to expose the Postgres via TCP. Web applications can take advantage of the native connection pooler based on PgBouncer. CloudNativePG was originally built by EDB, then released open source under Apache License 2.0 and submitted for CNCF Sandbox in April 2022. The source code repository is in Github. More details about the project will be found on this link "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#deploying-the-solution","content":"Let's go through the deployment steps "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraformpsql "},{"title":"Deploy the EKS Cluster with CloudNativePG Operator‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#deploy-the-eks-cluster-with-cloudnativepg-operator","content":"First, clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into cloudnative-postgres folder and run install.sh script. By default the script deploys EKS cluster to us-west-2 region. Update variables.tf to change the region. This is also the time to update any other input variables or make any other changes to the terraform template. cd data-on-eks/distributed-databases/cloudnative-postgres ./install .sh  "},{"title":"Verify Deployment‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#verify-deployment","content":"Verify the Amazon EKS Cluster aws eks describe-cluster --name cnpg-on-eks  Update local kubeconfig so we can access kubernetes cluster aws eks update-kubeconfig --name cnpg-on-eks --region us-west-2  First, lets verify that we have worker nodes running in the cluster. kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-1-10-192.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-10-249.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-11-38.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af ip-10-1-12-195.us-west-2.compute.internal Ready &lt;none&gt; 4d17h v1.25.6-eks-48e63af  Next, lets verify all the pods are running. kubectl get pods --namespace=monitoring NAME READY STATUS RESTARTS AGE alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 1 (4d17h ago) 4d17h kube-prometheus-stack-grafana-7f8b9dc64b-sb27n 3/3 Running 0 4d17h kube-prometheus-stack-kube-state-metrics-5979d9d98c-r9fxn 1/1 Running 0 60m kube-prometheus-stack-operator-554b6f9965-zqszr 1/1 Running 0 60m prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 0 4d17h kubectl get pods --namespace=cnpg-system NAME READY STATUS RESTARTS AGE cnpg-on-eks-cloudnative-pg-587d5d8fc5-65z9j 1/1 Running 0 4d17h  "},{"title":"Deploy a PostgreSQL cluster‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#deploy-a-postgresql-cluster","content":"First of all, we need to create a storageclass using the ebs-csi-driver, a demo namespace and kubernetes secrets for login/password for database authentication app-auth. Check examples folder for all kubernetes manifests. Storage‚Äã For running a highly scalable and durable self-managed PostgreSQL database on Kubernetes with Amazon EKS and EC2, it is recommended to use Amazon Elastic Block Store (EBS) volumes that provide high performance and fault tolerance. The preferred EBS volume types for this use case are: 1.Provisioned IOPS SSD (io2 or io1): Designed for I/O-intensive workloads such as databases.Offers consistent and low-latency performance.Allows you to provision a specific number of IOPS (input/output operations per second) according to your requirements.Provides up to 64,000 IOPS per volume and 1,000 MB/s throughput, making it suitable for demanding database workloads. 2.General Purpose SSD (gp3 or gp2): Suitable for most workloads and offers a balance between performance and cost.Provides a baseline performance of 3,000 IOPS and 125 MB/s throughput per volume, which can be increased if needed (up to 16,000 IOPS and 1,000 MB/s for gp3).Recommended for less I/O-intensive database workloads or when cost is a primary concern. You can find both storageclass template in examples folder. kubectl create -f examples/storageclass.yaml kubectl create -f examples/auth-prod.yaml  As with any other deployment in Kubernetes, to deploy a PostgreSQL cluster you need to apply a configuration file that defines your desired Cluster. CloudNativePG operator offers two type of Bootstrapping a new database: Bootstrap an empty clusterBootstrap From another cluster. In this first example, we are going to create a new empty database cluster using initdbflags. We are going to use the template below by modifying the IAM role for IRSA configuration 1 and S3 bucket for backup restore process and WAL archiving 2. The Terraform could already created this use terraform output to extract these parameters: cd data-on-eks/distributed-databases/cloudnative-postgres terraform output barman_backup_irsa = &quot;arn:aws:iam::&lt;your_account_id&gt;:role/cnpg-on-eks-prod-irsa&quot; barman_s3_bucket = &quot;XXXX-cnpg-barman-bucket&quot; configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name cnpg-on-eks&quot;  --- apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: prod namespace: demo spec: description: &quot;Cluster Demo for DoEKS&quot; # Choose your PostGres Database Version imageName: ghcr.io/cloudnative-pg/postgresql:15.2 # Number of Replicas instances: 3 startDelay: 300 stopDelay: 300 replicationSlots: highAvailability: enabled: true updateInterval: 300 primaryUpdateStrategy: unsupervised serviceAccountTemplate: # For backup and restore, we use IRSA for barman tool. # You will find this IAM role on terraform outputs. metadata: annotations: eks.amazonaws.com/role-arn: arn:aws:iam::&lt;&lt;account_id&gt;&gt;:role/cnpg-on-eks-prod-irsa #1 postgresql: parameters: shared_buffers: 256MB pg_stat_statements.max: '10000' pg_stat_statements.track: all auto_explain.log_min_duration: '10s' pg_hba: # - hostssl app all all cert - host app app all password logLevel: debug storage: storageClass: ebs-sc size: 1Gi walStorage: storageClass: ebs-sc size: 1Gi monitoring: enablePodMonitor: true bootstrap: initdb: # Deploying a new cluster database: WorldDB owner: app secret: name: app-auth backup: barmanObjectStore: # For backup, we S3 bucket to store data. # On this Blueprint, we create an S3 check the terraform output for it. destinationPath: s3://&lt;your-s3-barman-bucket&gt; #2 s3Credentials: inheritFromIAMRole: true wal: compression: gzip maxParallel: 8 retentionPolicy: &quot;30d&quot; resources: # m5large: m5xlarge 2vCPU, 8GI RAM requests: memory: &quot;512Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;1Gi&quot; cpu: &quot;2&quot; affinity: enablePodAntiAffinity: true topologyKey: failure-domain.beta.kubernetes.io/zone nodeMaintenanceWindow: inProgress: false reusePVC: false  Once updated, you can apply your template. kubectl create -f examples/prod-cluster.yaml  Verify that CloudNatvicePG operator has created three pods: one primary and two standby.  kubectl get pods,svc -n demo NAME READY STATUS RESTARTS AGE pod/prod-1 1/1 Running 0 4m36s pod/prod-2 1/1 Running 0 3m45s pod/prod-3 1/1 Running 0 3m9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prod-any ClusterIP 172.20.230.153 &lt;none&gt; 5432/TCP 4m54s service/prod-r ClusterIP 172.20.33.61 &lt;none&gt; 5432/TCP 4m54s service/prod-ro ClusterIP 172.20.96.16 &lt;none&gt; 5432/TCP 4m53s service/prod-rw ClusterIP 172.20.236.1 &lt;none&gt; 5432/TCP 4m53s  The operator created also three services: -rw: points only to the primary instances of cluster database-ropoints only to hot standby replicas for read-only-workloads-rpoints to any of the instances for read-only workloads Note that -any points on all the instances. Another way to check Cluster status is by using cloudnative-pg kubectl plugin offered by the CloudNativePG community, kubectl cnpg status prod Cluster Summary Name: prod Namespace: demo System ID: 7214866198623563798 PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:15.2 Primary instance: prod-1 Status: Cluster in healthy state Instances: 3 Ready instances: 3 Current Write LSN: 0/6000000 (Timeline: 1 - WAL File: 000000010000000000000005) Certificates Status Certificate Name Expiration Date Days Left Until Expiration ---------------- --------------- -------------------------- prod-ca 2023-06-24 14:40:27 +0000 UTC 89.96 prod-replication 2023-06-24 14:40:27 +0000 UTC 89.96 prod-server 2023-06-24 14:40:27 +0000 UTC 89.96 Continuous Backup status First Point of Recoverability: Not Available Working WAL archiving: OK WALs waiting to be archived: 0 Last Archived WAL: 000000010000000000000005 @ 2023-03-26T14:52:09.24307Z Last Failed WAL: - Streaming Replication status Replication Slots Enabled Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority Replication Slot ---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- ---------------- prod-2 0/6000000 0/6000000 0/6000000 0/6000000 00:00:00 00:00:00 00:00:00 streaming async 0 active prod-3 0/6000000 0/6000000 0/6000000 0/6000000 00:00:00 00:00:00 00:00:00 streaming async 0 active Unmanaged Replication Slot Status No unmanaged replication slots found Instances status Name Database Size Current LSN Replication role Status QoS Manager Version Node ---- ------------- ----------- ---------------- ------ --- --------------- ---- prod-1 29 MB 0/6000000 Primary OK BestEffort 1.19.0 ip-10-1-10-192.us-west-2.compute.internal prod-2 29 MB 0/6000000 Standby (async) OK BestEffort 1.19.0 ip-10-1-12-195.us-west-2.compute.internal prod-3 29 MB 0/6000000 Standby (async) OK BestEffort 1.19.0 ip-10-1-11-38.us-west-2.compute.internal  "},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#monitoring","content":"In this example, we deployed a Prometheus and Grafana addons to monitor all database clusters created by CloudNativePG. Let's check Grafana dashboard. kubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80   "},{"title":"Import database sample‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#import-database-sample","content":"You can expose your database outside the cluster using ingress-controller or kubernetes service type LoadBalancer. However, for internal usage inside your EKS cluster, you can use kubernetes service prod-rw and prod-ro. In this section, we are going to expose read-write service -rwusing kubectl port-forward.  kubectl port-forward svc/prod-rw 5432:5432 -n demo  Now, we use psql cli to import world.sql into our database instance WorldDB using credentials from app-auth secrets.  psql -h localhost --port 5432 -U app -d WorldDB &lt; world.sql # Quick check on db tables. psql -h localhost --port 5432 -U app -d WorldDB -c '\\dt' Password for user app: List of relations Schema | Name | Type | Owner --------+-----------------+-------+------- public | city | table | app public | country | table | app public | countrylanguage | table | app (3 rows)  "},{"title":"Create Backup to S3‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#create-backup-to-s3","content":"Now that we had a running database with data, CloudNativePG operator offers backup-restore feature using barman tool. CloudNativePG allows database admin to create on-demand database or Scheduled backups and for more details on documentations. In this example, we will create a Backup object to start a backup process immediately. apiVersion: postgresql.cnpg.io/v1 kind: Backup metadata: name: ondemand spec: cluster: name: prod   kubectl create -f examples/backup-od.yaml  It will take couple minutes to run, then, check the backup process kubectl describe backup ondemand Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 60s cloudnative-pg-backup Starting backup for cluster prod Normal Starting 60s instance-manager Backup started Normal Completed 56s instance-manager Backup completed  "},{"title":"Restore‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#restore","content":"For restore, we use bootstrap a new cluster using backup file on S3. The backup tool barman manages restore process, but, it doesn't support backup and restore for kubernetes secrets. This must be managed separately, like using csi-secrets-driver with AWS SecretsManager. First let's delete prod database. kubectl delete cluster prod -n demo  Then, update your template examples/cluster-restore.yaml with your S3 bucket and IAM role. Note that on restore template, CloudNativePG use externalClusters to point on the database.  kubectl create -f examples/cluster-restore.yaml Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreatingPodDisruptionBudget 7m12s cloudnative-pg Creating PodDisruptionBudget prod-primary Normal CreatingPodDisruptionBudget 7m12s cloudnative-pg Creating PodDisruptionBudget prod Normal CreatingServiceAccount 7m12s cloudnative-pg Creating ServiceAccount Normal CreatingRole 7m12s cloudnative-pg Creating Cluster Role Normal CreatingInstance 7m12s cloudnative-pg Primary instance (from backup) Normal CreatingInstance 6m33s cloudnative-pg Creating instance prod-2 Normal CreatingInstance 5m51s cloudnative-pg Creating instance prod-3  When creating a new cluster, the operator will create a ServiceAccount with IRSA configuration as described on Cluster resources. Make sure the trust policy points the right ServiceAccount. Let's check if the data were covered as expected.  psql -h localhost --port 5432 -U app -d WorldDB -c '\\dt' Password for user app: List of relations Schema | Name | Type | Owner --------+-----------------+-------+------- public | city | table | app public | country | table | app public | countrylanguage | table | app (3 rows) psql -h localhost --port 5432 -U app -d WorldDB -c 'SELECT CURRENT_TIME;'  "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Deploying PostgreSQL Database on EKS using CloudNativePG Operator","url":"/data-on-eks/docs/distributed-databases/cloudnative-postgres#conclusion","content":"CloudNativePG operator provides Level 5 from Operator Capability Levels. In this example, we share a blueprint that deploy the operator as an addon along with its monitoring stack (Prometheus and grafana). Among many features, we highlighted couple of examples on creating cluster, importing data and restoring database in case of disaster (or cluster deletion). More features are available on this documentation "},{"title":"EMR on EKS with Karpenter","type":0,"sectionRef":"#","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#introduction","content":"In this pattern, you will deploy an EMR on EKS cluster and use Karpenter provisioners for scaling Spark jobs. Architecture This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults if you are new to EMR on EKS and only customize if you have viable alternative option available for replacement. In terms of infrastructure, here are the resources that are created by this pattern Creates an EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)One managed node group Core Node group with 3 instances spanning multi-AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc. Enables EMR on EKS Creates two namespaces (emr-data-team-a, emr-data-team-b) for data teamsCreates Kubernetes role and role binding(emr-containers user) for both namespacesIAM roles for both teams needed for job executionUpdate AWS_AUTH config map with emr-containers user and AWSServiceRoleForAmazonEMRContainers roleCreate a trust relationship between the job execution role and the identity of the EMR managed service accountCreate EMR Virtual Cluster for emr-data-team-a &amp; emr-data-team-b and IAM policies for both You can see the list of add-ons available below. tip We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as core-node-group as provided by this pattern. danger We don't recommend removing critical add-ons (Amazon VPC CNI, CoreDNS, Kube-proxy). Add-on\tEnabled by default?\tBenefits\tLinkAmazon VPC CNI\tYes\tVPC CNI is available as an EKS add-on and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods\tVPC CNI Documentation CoreDNS\tYes\tCoreDNS is available as an EKS add-on and is responsible for resolving DNS queries for spark application and for Kubernetes cluster\tEKS CoreDNS Documentation Kube-proxy\tYes\tKube-proxy is available as an EKS add-on and it maintains network rules on your nodes and enables network communication to your spark application pods\tEKS kube-proxy Documentation Amazon EBS CSI driver\tYes\tEBS CSI driver is available as an EKS add-on and it allows EKS clusters to manage the lifecycle of EBS volumes\tEBS CSI Driver Documentation Karpenter\tYes\tKarpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters\tKarpenter Documentation Cluster Autoscaler\tYes\tKubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as core-node-group) in the cluster\tCluster Autoscaler Documentation Cluster proportional autoscaler\tYes\tThis is responsible for scaling CoreDNS pods in your Kubernetes cluster\tCluster Proportional Autoscaler Documentation Metrics server\tYes\tKubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster\tEKS Metrics Server Documentation Prometheus\tYes\tPrometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost\tPrometheus Documentation Amazon Managed Prometheus\tYes\tThis is responsible for storing and scaling of EKS cluster and spark application metrics\tAmazon Managed Prometheus Documentation Kubecost\tYes\tKubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels\tEKS Kubecost Documentation CloudWatch metrics\tNo\tCloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard\tCloudWatch Container Insights Documentation AWS for Fluent-bit\tNo\tThis can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system\tAWS For Fluent-bit Documentation FSx for Lustre CSI driver\tNo\tThis can be used for running Spark application using FSx for Lustre\tFSx for Lustre CSI Driver Documentation Customizing Add-ons Deploying the Solution "},{"title":"Run Sample Spark job‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#run-sample-spark-job","content":"The pattern shows how to run spark jobs in a multi-tenant EKS cluster. The examples showcases two data teams using namespaces emr-data-team-a and emr-data-team-b mapped to their EMR virtual clusters. You can use different Karpenter provisioners for each team so that they can submit jobs that are unique to their workload. Teams can also use different storage requirements to run their Spark jobs. For example, you can use compute optimized provisioner that has taints and specify tolerations using pod templates so that you can run spark on compute optimized EC2 instances. In terms of storage, you can decide whether to use EC2 instance-store or EBS or FSx for lustre volumes for data processing. The default storage that is used in these examples is EC2 instance store because of performance benefit spark-compute-optimized provisioner to run spark jobs on c5d instances.spark-memory-optimized provisioner to run spark jobs on r5d instances.spark-graviton-memory-optimized provisioner to run spark jobs on r6gd Graviton instances(ARM64). spark-compute-optimizedspark-memory-optimizedspark-graviton-memory-optimized In this tutorial, you will use Karpenter provisioner that uses compute optimized instances. This template leverages the Karpenter AWSNodeTemplates. To view Karpenter provisioner for compute optimized instances, Click to toggle content! apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: spark-compute-optimized namespace: karpenter # Same namespace as Karpenter add-on installed spec: kubeletConfiguration: containerRuntime: containerd # podsPerCore: 2 # maxPods: 20 requirements: - key: &quot;topology.kubernetes.io/zone&quot; operator: In values: [${azs}a] #Update the correct region and zones - key: &quot;karpenter.sh/capacity-type&quot; operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] - key: &quot;node.kubernetes.io/instance-type&quot; #If not included, all instance types are considered operator: In values: [&quot;c5d.large&quot;,&quot;c5d.xlarge&quot;,&quot;c5d.2xlarge&quot;,&quot;c5d.4xlarge&quot;,&quot;c5d.9xlarge&quot;] # 1 NVMe disk - key: &quot;kubernetes.io/arch&quot; operator: In values: [&quot;amd64&quot;] limits: resources: cpu: 1000 providerRef: name: spark-compute-optimized labels: type: karpenter provisioner: spark-compute-optimized NodeGroupType: SparkComputeOptimized taints: - key: spark-compute-optimized value: 'true' effect: NoSchedule ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: spark-compute-optimized namespace: karpenter spec: blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 100Gi volumeType: gp3 encrypted: true deleteOnTermination: true metadataOptions: httpEndpoint: enabled httpProtocolIPv6: disabled httpPutResponseHopLimit: 2 httpTokens: required subnetSelector: Name: &quot;${eks_cluster_id}-private*&quot; # Name of the Subnets to spin up the nodes securityGroupSelector: # required, when not using launchTemplate Name: &quot;${eks_cluster_id}-node*&quot; # name of the SecurityGroup to be used with Nodes # instanceProfile: &quot;&quot; # optional, if already set in controller args userData: | MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=&quot;BOUNDARY&quot; --BOUNDARY Content-Type: text/x-shellscript; charset=&quot;us-ascii&quot; #!/bin/bash echo &quot;Running a custom user data script&quot; set -ex yum install mdadm -y DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}') DISK_ARRAY=() for DEV in $DEVICES do DISK_ARRAY+=(&quot;/dev/$${DEV}&quot;) done DISK_COUNT=$${#DISK_ARRAY[@]} if [ $${DISK_COUNT} -eq 0 ]; then echo &quot;No SSD disks available. No further action needed.&quot; else if [ $${DISK_COUNT} -eq 1 ]; then TARGET_DEV=$${DISK_ARRAY[0]} mkfs.xfs $${TARGET_DEV} else mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]} mkfs.xfs /dev/md0 TARGET_DEV=/dev/md0 fi mkdir -p /local1 echo $${TARGET_DEV} /local1 xfs defaults,noatime 1 2 &gt;&gt; /etc/fstab mount -a /usr/bin/chown -hR +999:+1000 /local1 fi --BOUNDARY-- tags: InstanceType: &quot;spark-compute-optimized&quot; To run Spark Jobs that can use this provisioner, you need to submit your jobs by adding tolerations to your pod templates For example, spec: tolerations: - key: &quot;spark-compute-optimized&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; Execute the sample PySpark Job to trigger compute optimized Karpenter provisioner The following script requires four input parameters virtual_cluster_id, job_execution_role_arn, cloudwatch_log_group_name &amp; S3_Bucket to store PySpark scripts, Pod templates and Input data. You can get these values terraform apply output values or by running terraform output. For S3_BUCKET, Either create a new S3 bucket or use an existing S3 bucket. caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-compute-provisioner/ ./execute_emr_eks_job.sh Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3 Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name&gt;: s3://example-bucket Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution kubectl get pods --namespace=emr-data-team-a -w  "},{"title":"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter provisioner‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-provisioner","content":"This pattern uses EBS volumes for data processing and compute optimized provisioner. You can modify the provisioner by changing nodeselector in driver and executor pod templates. In order to change provisioners, simply update your pod templates to desired provisioner  nodeSelector: NodeGroupType: &quot;SparkComputeOptimized&quot;  You can also update EC2 instances that doesn't include instance store volumes (for example c5.xlarge) and remove c5d's if needed for this exercise We will create Storageclass that will be used by drivers and executors. We'll create static Persistant Volume Claim (PVC) for the driver pod but we'll use dynamically created ebs volumes for executors. Create StorageClass and PVC using example provided cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/ kubectl apply -f emr-eks-karpenter-ebs.yaml  Let's run the job cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/ ./execute_emr_eks_job.sh Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3 Enter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://&lt;bucket-name&gt;: s3://example-bucket  You'll notice the PVC spark-driver-pvc will be used by driver pod but Spark will create multiple ebs volumes for executors mapped to Storageclass emr-eks-karpenter-ebs-sc. All dynamically created ebs volumes will be deleted once the job completes "},{"title":"Running Sample Spark job using FSx for Lustre‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#running-sample-spark-job-using-fsx-for-lustre","content":"Amazon FSx for Lustre is a fully managed shared storage option built on the world‚Äôs most popular high-performance file system. You can use FSx to store shuffle files and also to store intermediate data processing tasks in a data pipeline. You can read more about FSX for Lustre in documentation and learn how to use this storage with EMR on EKS in our best practices guide In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage. There are two ways to use FSx for Lustre using static FSx for Lustre volumesusing dynamically created FSx for Lustre volumes fsx-staticfsx-dynamic Execute Spark Job by using FSx for Lustre with statically provisioned volume and compute optimized Karpenter provisioner. Fsx for Lustre Terraform module is disabled by default. Follow the customizing add-ons steps before running Spark jobs. Execute the Spark job using the below shell script. This script requires input parameters which can be extracted from terraform apply output values. caution This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job. cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-static-pvc-shuffle-storage ./fsx-static-spark.sh Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the provisioner templates before running the Spark Jobs. Nodes will be drained with once the job is completed Verify the job execution events kubectl get pods --namespace=emr-data-team-a -w This will show the mounted /data directory with FSx DNS name kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- df -h kubectl exec -ti ny-taxi-trip-static-exec-1 -c analytics-kubernetes-executor -n emr-data-team-a -- ls -lah /static  "},{"title":"Running Sample Spark job using Apache YuniKorn Batch Scheduler‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#running-sample-spark-job-using-apache-yunikorn-batch-scheduler","content":"Apache YuniKorn is an open-source, universal resource scheduler for managing distributed big data processing workloads such as Spark, Flink, and Storm. It is designed to efficiently manage resources across multiple tenants in a shared, multi-tenant cluster environment. Some of the key features of Apache YuniKorn include: Flexibility: YuniKorn provides a flexible and scalable architecture that can handle a wide variety of workloads, from long-running services to batch jobs.Dynamic Resource Allocation: YuniKorn uses a dynamic resource allocation mechanism to allocate resources to workloads on an as-needed basis, which helps to minimize resource wastage and improve overall cluster utilization.Priority-based Scheduling: YuniKorn supports priority-based scheduling, which allows users to assign different levels of priority to their workloads based on business requirements.Multi-tenancy: YuniKorn supports multi-tenancy, which enables multiple users to share the same cluster while ensuring resource isolation and fairness.Pluggable Architecture: YuniKorn has a pluggable architecture that allows users to extend its functionality with custom scheduling policies and pluggable components. Apache YuniKorn is a powerful and versatile resource scheduler that can help organizations efficiently manage their big data workloads while ensuring high resource utilization and workload performance. Apache YuniKorn Architecture Apache YuniKorn Gang Scheduling with Karpenter Apache YuniKorn Scheduler add-on is disabled by default. Follow the steps to deploy the Apache YuniKorn add-on and execute the Spark job. Update the analytics/terraform/emr-eks-karpenter/variables.tf file with the following variable &quot;enable_yunikorn&quot; { default = true description = &quot;Enable Apache YuniKorn Scheduler&quot; type = bool }  Execute terrafrom apply again. This will deploy FSx for Lustre add-on and all the necessary reosurces. terraform apply -auto-approve  This example demonstrates the Apache YuniKorn Gang Scheduling with Karpenter Autoscaler. cd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-yunikorn-gangscheduling ./execute_emr_eks_job.sh  Verify the job executionApache YuniKorn Gang Scheduling will create pause pods for total number of executors requested. kubectl get pods --namespace=emr-data-team-a -w  Verify the driver and executor pods prefix with tg- indicates the pause pods. These pods will be replaced with the actual Spark Driver and Executor pods once the Nodes are scaled and ready by the Karpenter.  Delta Lake Table Format "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"EMR on EKS with Karpenter","url":"/data-on-eks/docs/amazon-emr-on-eks/emr-eks-karpenter#cleanup","content":"Cleanup caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/data-on-eks/docs/intro","content":"Introduction Data on Amazon EKS(DoEKS) - A tool for building aws managed and self-managed scalable data platforms on Amazon EKS. With DoEKS, You have access to: Robust Deployment Infrastructure as Code (IaC) Templates using Terraform and AWS CDK, among otherBest Practices for Deploying Data Solutions on Amazon EKSDetailed Performance Benchmark ReportsHands-on Samples of Apache Spark/ML Jobs and various other frameworksIn-depth Reference Architectures and Data Blogs to keep you ahead of the curve Architecture The diagram displays the open source data tools, k8s operators and frameworks that runs on Kubernetes covered in DoEKS. AWS Data Analytics managed services integration with Data on EKS OSS tools. Main Features üöÄ EMR on EKS üöÄ Open Source Spark on EKS üöÄ Custom Kubernetes Schedulers (e.g., Apache YuniKorn, Volcano) üöÄ Job Schedulers (e.g., Apache Airflow, Argo Workflows) üöÄ AI/ML on Kubernetes (e.g., KubeFlow, MLFlow, Tensorflow, PyTorch etc.) üöÄ Distributed Databases (e.g., Cassandra, CockroachDB, MongoDB etc.) üöÄ Streaming Platforms (e.g., Apache Kafka, Apache Flink, Apache Beam etc.) Getting Started Checkout the documentation for each section to deploy infrastructure and run sample Spark/ML jobs.","keywords":""},{"title":"Job Schedulers","type":0,"sectionRef":"#","url":"/data-on-eks/docs/job-schedulers","content":"Job Schedulers Job schedulers are an essential component of many organizations' infrastructure, helping to automate and manage complex workflows. When deployed on Kubernetes, job schedulers can take advantage of the platform's features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability. Tools like Apache Airflow, Argo Workflow, and Amazon MWAA provide a simple and efficient way to manage and schedule jobs on a Kubernetes cluster. These tools are well-suited for a wide range of use cases, including data pipelines, machine learning workflows, and batch processing. By leveraging the power of Kubernetes, organizations can simplify and automate the management of their job schedulers, freeing up resources to focus on other areas of the business. With its growing ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running job schedulers in production. The following are the most popular job scheduling tools used with data workloads. This section provides deployment patterns for the following tools and examples to trigger Spark/ML jobs using these schedulers. Apache AirflowAmazon Managed Workflows for Apache Airflow (MWAA)Argo WorkflowPrefect","keywords":""},{"title":"Argo Workflows on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks","content":"","keywords":""},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraformArgo WorkflowCLI "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#deploy","content":"To provision this example: git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/argo-workflow region=&lt;your region&gt; # set region variable for following commands terraform init terraform apply -var region=$region #defaults to us-west-2  Enter yes at command prompt to apply The following components are provisioned in your environment: A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server, cluster autoscaler, Spark Operator and yunikorn schedulerK8s roles and rolebindings for argo workflows and argo events  "},{"title":"Validate‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment. "},{"title":"Run update-kubeconfig command:‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#run-update-kubeconfig-command","content":"aws eks --region us-west-2 update-kubeconfig --name argoworkflows-eks  "},{"title":"List the nodes‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#list-the-nodes","content":"kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-1-131-99.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326 ip-10-1-16-117.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326 ip-10-1-80-41.us-west-2.compute.internal Ready &lt;none&gt; 26h v1.23.9-eks-ba74326  "},{"title":"List the namespaces in EKS cluster‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#list-the-namespaces-in-eks-cluster","content":"kubectl get ns # Output should look like below NAME STATUS AGE argo-events Active 28h data-team-a Active 73m argo-workflows Active 28h default Active 30h kube-node-lease Active 30h kube-public Active 30h kube-system Active 30h spark-operator Active 30h yunikorn Active 30h  "},{"title":"Access Argo Workflow WebUI‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#access-argo-workflow-webui","content":"kubectl -n argo-workflows port-forward deployment.apps/argo-workflows-server 2746:2746 argo auth token # get login token # result: Bearer k8s-aws-v1.aHR0cHM6Ly9zdHMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vP0FjdGlvbj1HZXRDYWxsZXJJZGVudGl0eSZWZXJzaW9uPTIwMTEtMDYtMTUmWC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNWNFhDV1dLUjZGVTRGMiUyRjIwMjIxMDEzJTJGdXMtd2VzdC0yJTJGc3RzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMjEwMTNUMDIyODAyWiZYLUFtei1FeHBpcmVzPTYwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCUzQngtazhzLWF3cy1pZCZYLUFtei1TaWduYXR1cmU9NmZiNmMxYmQ0MDQyMWIwNTI3NjY4MzZhMGJiNmUzNjg1MTk1YmM0NDQzMjIyMTg5ZDNmZmE1YzJjZmRiMjc4OA  Open browser and enter http://localhost:2746/ and paste the token  "},{"title":"Submit Spark Job with Argo Workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#submit-spark-job-with-argo-workflow","content":"Modify workflow-example/argo-spark.yaml with your eks api server url kubectl apply -f workflow-example/argo-spark.yaml kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Running 8s  You can also check the workflow status from Web UI  "},{"title":"Submit Spark Job with Spark Operator and Argo Workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#submit-spark-job-with-spark-operator-and-argo-workflow","content":"kubectl apply -f workflow-example/argo-spark-operator.yaml kubectl get wf -n argo-workflows NAME STATUS AGE MESSAGE spark Succeeded 3m58s spark-operator Running 5s  The workflow status from web UI  "},{"title":"Trigger a workflow to create a spark job based on SQS message‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#trigger-a-workflow-to-create-a-spark-job-based-on-sqs-message","content":""},{"title":"Install argo events controllers‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#install-argo-events-controllers","content":"kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml  "},{"title":"Install eventbus which is for event transmission in argo events‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#install-eventbus-which-is-for-event-transmission-in-argo-events","content":"kubectl apply -f argo-events/eventbus.yaml  "},{"title":"Deploy eventsource-sqs.yaml to link with external SQS‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#deploy-eventsource-sqsyaml-to-link-with-external-sqs","content":"kubectl apply -f argo-events/eventsource-sqs.yaml  In this case, we configure a EventSource to license to the queue test1 in region us-east-1. Let's create that queue in your account if you don't have one. # create a queue aws sqs create-queue --queue-name test1 --region us-east-1 # get your queue arn aws sqs get-queue-attributes --queue-url &lt;your queue url&gt; --attribute-names QueueArn #Replace the following values in argo-events/sqs-accesspolicy.json #&lt;your queue arn&gt; #&lt;your event irsa arn&gt; (you can get from terraform output) aws sqs set-queue-attributes --queue-url &lt;your queue url&gt; --attributes file://argo-events/sqs-accesspolicy.json --region us-east-1  "},{"title":"Deploy sensor-rbac.yaml and sensor-sqs-spark-crossns.yaml for triggering workflow‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#deploy-sensor-rbacyaml-and-sensor-sqs-spark-crossnsyaml-for-triggering-workflow","content":"kubectl apply -f argo-events/sensor-rbac.yaml kubectl apply -f argo-events/sensor-sqs-sparkjobs.yaml  "},{"title":"Verify argo-events namespace‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#verify-argo-events-namespace","content":"kubectl get all,eventbus,EventSource,sensor,sa,role,rolebinding -n argo-events   "},{"title":"Test from SQS‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#test-from-sqs","content":"Send a message from SQS: {&quot;message&quot;: &quot;hello&quot;}  Argo Events would capture the message and trigger Argo Workflows to create a workflow for spark jobs. kubectl get wf -A NAMESPACE NAME STATUS AGE MESSAGE argo-workflows aws-sqs-spark-workflow-p57qx Running 9s  "},{"title":"Destroy‚Äã","type":1,"pageTitle":"Argo Workflows on EKS","url":"/data-on-eks/docs/job-schedulers/argo-workflows-eks#destroy","content":"To teardown and remove the resources created in this example: kubectl delete -f argo-events/. terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -target=&quot;module.irsa_argo_events&quot; -auto-approve -var region=$region terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve -var region=$region terraform destroy -auto-approve -var region=$region  "},{"title":"Self-managed Apache Airflow deployment for EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow","content":"","keywords":""},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/schedulers/terraform/self-managed-airflow terraform init  Set AWS_REGION and Run terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; terraform plan  Deploy the pattern terraform apply  Enter yes to apply. info Rerun terraform apply if your execution timed out. "},{"title":"Verify the resources‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#verify-the-resources","content":""},{"title":"Create kubectl config‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#create-kubectl-config","content":"aws eks --region us-west-2 update-kubeconfig --name self-managed-airflow  "},{"title":"Describe the EKS Cluster‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#describe-the-eks-cluster","content":"aws eks describe-cluster --name self-managed-airflow  "},{"title":"Verify the EFS PV and PVC created by this deployment‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#verify-the-efs-pv-and-pvc-created-by-this-deployment","content":"kubectl get pvc -n airflow NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE airflow-dags Bound pvc-157cc724-06d7-4171-a14d-something 10Gi RWX efs-sc 73m kubectl get pv -n airflow NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-157cc724-06d7-4171-a14d-something 10Gi RWX Delete Bound airflow/airflow-dags efs-sc 74m  "},{"title":"Verify the EFS Filesystem‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#verify-the-efs-filesystem","content":"aws efs describe-file-systems --query &quot;FileSystems[*].FileSystemId&quot; --output text  "},{"title":"Verify S3 bucket created for Airflow logs‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#verify-s3-bucket-created-for-airflow-logs","content":"aws s3 ls | grep airflow-logs-  "},{"title":"Verify the Airflow deployment‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#verify-the-airflow-deployment","content":"kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1/1 1 1 77m airflow-scheduler 2/2 2 2 77m airflow-statsd 1/1 1 1 77m airflow-triggerer 1/1 1 1 77m airflow-webserver 2/2 2 2 77m  "},{"title":"Fetch Postgres RDS password‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#fetch-postgres-rds-password","content":"Amazon Postgres RDS database password can be fetched from the Secrets manager Login to AWS console and open secrets managerClick on postgres secret nameClick on Retrieve secret value button to verify the Postgres DB master password "},{"title":"Login to Airflow Web UI‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#login-to-airflow-web-ui","content":"This deployment creates an Ingress object with public LoadBalancer(internet-facing) for demo purpose For production workloads, you can modify values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port. Execute the following command to get the ALB DNS name kubectl get ingress -n airflow NAME CLASS HOSTS ADDRESS PORTS AGE airflow-airflow-ingress alb * k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com 80 88m  The above ALB URL will be different for you deployment. So use your URL and open it in a brower e.g., Open URL http://k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com/ in a browser By default, Airflow creates a default user with admin and password as admin Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user "},{"title":"Create S3 Connection from Airflow Web UI‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#create-s3-connection-from-airflow-web-ui","content":"This step is critical for writing the Airflow logs to S3 bucket. Login to Airflow WebUI with admin and password as admin using ALB URLSelect Admin dropdown and Click on ConnectionsClick on &quot;+&quot; button to add a new recordEnter Connection Id as aws_s3_conn, Connection Type as Amazon Web Services and Extra as {&quot;region_name&quot;: &quot;&lt;ENTER_YOUR_REGION&gt;&quot;}Click on Save button  "},{"title":"Execute Sample Airflow Job‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#execute-sample-airflow-job","content":"Login to Airflow WebUIClick on DAGs link on the top of the page. This will show two dags pre-created by the GitSync featureExecute the first DAG by clicking on Play button (&gt;)Verify the DAG execution from Graph linkAll the Tasks will go green after few minutesClick on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3 "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Self-managed Apache Airflow deployment for EKS","url":"/data-on-eks/docs/job-schedulers/self-managed-airflow#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.db&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  Make sure all the S3 buckets are empty and deleted once your test is finished  "},{"title":"Amazon Managed Workflows for Apache Airflow (MWAA)","type":0,"sectionRef":"#","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow","content":"","keywords":""},{"title":"Considerations‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#considerations","content":"Ideally we recommend adding the steps to sync requirements/sync dags to the MWAA S3 Bucket as part of a CI/CD pipeline. Generally Dags development have a different lifecycle than the Terraform code to provision infrastructure. For simplicity, we are providing steps for that using Terraform running AWS CLI commands on null_resource. "},{"title":"Prerequisites:‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#prerequisites","content":"Ensure that you have the following tools installed locally: aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#deploy","content":"To provision this example: git clone https://github.com/awslabs/data-on-eks.git cd data-on-eks/schedulers/terraform/managed-airflow-mwaa terraform init terraform apply -var region=us-west-2 # Change according to your region  Enter yes at command prompt to apply Once done, you will see terraform output like below.  The following components are provisioned in your environment: A sample VPC, 3 Private Subnets and 3 Public SubnetsInternet gateway for Public Subnets and NAT Gateway for Private SubnetsEKS Cluster Control plane with one managed node groupEKS Managed Add-ons: VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_DriverK8S metrics server and cluster autoscalerA MWAA environment in version 2.2.2An EMR virtual cluster registered with the newly created EKSA S3 bucket with DAG code "},{"title":"Validate‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#validate","content":"The following command will update the kubeconfig on your local machine and allow you to interact with your EKS Cluster using kubectl to validate the deployment. "},{"title":"Run update-kubeconfig command‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#run-update-kubeconfig-command","content":"aws eks --region us-west-2 update-kubeconfig --name managed-airflow-mwaa  "},{"title":"List the nodes‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#list-the-nodes","content":"kubectl get nodes # Output should look like below NAME STATUS ROLES AGE VERSION ip-10-0-0-42.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-22-71.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326 ip-10-0-44-63.ec2.internal Ready &lt;none&gt; 5h15m v1.23.9-eks-ba74326  "},{"title":"List the namespaces in EKS cluster‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#list-the-namespaces-in-eks-cluster","content":"kubectl get ns # Output should look like below default Active 4h38m emr-mwaa Active 4h34m kube-node-lease Active 4h39m kube-public Active 4h39m kube-system Active 4h39m mwaa Active 4h30m  namespace emr-mwaa will be used by EMR for running spark jobs. namespace mwaa will be used by MWAA directly. "},{"title":"Trigger jobs from MWAA‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#trigger-jobs-from-mwaa","content":""},{"title":"Log into Apache Airflow UI‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#log-into-apache-airflow-ui","content":"Open the Environments page on the Amazon MWAA consoleChoose an environmentUnder the Details section, click the link for the Airflow UI  Note: You will see red error message once login. That is because the EMR connection has not been setup. The message will be gone after following the steps below to set up the connection and login again. "},{"title":"Trigger the DAG workflow to execute job in EMR on EKS‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-emr-on-eks","content":"First, you need to set up the connection to EMR virtual cluster in MWAA  Click Add button, Make sure use emr_eks as Connection Id Amazon Web Services as Connection Type Replace the value in Extra based on your terraform output {&quot;virtual_cluster_id&quot;:&quot;&lt;emrcontainers_virtual_cluster_id in terraform output&gt;&quot;, &quot;job_role_arn&quot;:&quot;&lt;emr_on_eks_role_arn in terraform output&gt;&quot;}  Go back to Airflow UI main page, enable the example DAG emr_eks_pi_job and then trigger the job.  While it is running, use the following command to verify the spark jobs: kubectl get all -n emr-mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE pod/000000030tk2ihdmr8g-psstj 3/3 Running 0 90s pod/pythonpi-a8051f83b415c911-exec-1 2/2 Running 0 14s pod/pythonpi-a8051f83b415c911-exec-2 2/2 Running 0 14s pod/spark-000000030tk2ihdmr8g-driver 2/2 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/spark-000000030tk2ihdmr8g-ee64be83b4151dd5-driver-svc ClusterIP None &lt;none&gt; 7078/TCP,7079/TCP,4040/TCP 57s NAME COMPLETIONS DURATION AGE job.batch/000000030tk2ihdmr8g 0/1 92s 92s  You can also check the job status in Amazon EMR console. Under the Virtual clusters section, click on Virtual cluster  "},{"title":"Trigger the DAG workflow to execute job in EKS‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#trigger-the-dag-workflow-to-execute-job-in-eks","content":"In the Airflow UI, enable the example DAG kubernetes_pod_example and then trigger it.   Verify that the pod was executed successfully After it runs and completes successfully, use the following command to verify the pod: kubectl get pods -n mwaa  You should see output similar to the following: NAME READY STATUS RESTARTS AGE mwaa-pod-test.4bed823d645844bc8e6899fd858f119d 0/1 Completed 0 25s  "},{"title":"Destroy‚Äã","type":1,"pageTitle":"Amazon Managed Workflows for Apache Airflow (MWAA)","url":"/data-on-eks/docs/job-schedulers/aws-managed-airflow#destroy","content":"To teardown and remove the resources created in this example: terraform destroy -auto-approve   "},{"title":"Streaming Platforms on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/streaming-platforms","content":"Streaming Platforms on EKS info Note: The blueprints for streaming platforms are currently in the process of development. Documentation will be updated once a deployment example has been added to the repository. Running streaming platforms on Kubernetes can provide a number of benefits for organizations looking to process and analyze real-time data streams. Kubernetes provides features such as automatic scaling, rolling updates, and self-healing capabilities to ensure high availability and reliability of streaming platforms. There are a number of popular streaming platforms that have emerged to support this use case, including Apache Kafka, Apache Flink, and Apache Pulsar. These platforms make it easy to process and analyze real-time data streams in a containerized environment, and provide features such as real-time data processing, event-driven architecture, and fault-tolerance. By leveraging the power of Kubernetes, organizations can focus on building and processing their streaming data pipelines, rather than worrying about the underlying infrastructure. With its robust ecosystem of tools and support for a wide range of use cases, Kubernetes is becoming an increasingly popular choice for running streaming platforms in production. KafkaFlink","keywords":""},{"title":"Flink Operator on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/streaming-platforms/flink","content":"","keywords":""},{"title":"Introduction to Apache Flink‚Äã","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/streaming-platforms/flink#introduction-to-apache-flink","content":"Apache Flink is an open-source, unified stream processing and batch processing framework that was designed to process large amounts of data. It provides fast, reliable, and scalable data processing with fault tolerance and exactly-once semantics. Some of the key features of Flink are: Distributed Processing: Flink is designed to process large volumes of data in a distributed fashion, making it horizontally scalable and fault-tolerant.Stream Processing and Batch Processing: Flink provides APIs for both stream processing and batch processing. This means you can process data in real-time, as it's being generated, or process data in batches.Fault Tolerance: Flink has built-in mechanisms for handling node failures, network partitions, and other types of failures.Exactly-once Semantics: Flink supports exactly-once processing, which ensures that each record is processed exactly once, even in the presence of failures.Low Latency: Flink's streaming engine is optimized for low-latency processing, making it suitable for use cases that require real-time processing of data.Extensibility: Flink provides a rich set of APIs and libraries, making it easy to extend and customize to fit your specific use case. "},{"title":"Architecture‚Äã","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/streaming-platforms/flink#architecture","content":"Flink Architecture high level design with EKS.  "},{"title":"Flink Kubernetes Operator‚Äã","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/streaming-platforms/flink#flink-kubernetes-operator","content":"Flink Kubernetes Operator is a powerful tool for managing Flink clusters on Kubernetes. Flink Kubernetes Operator (Operator) acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. The Operator can be installed on a Kubernetes cluster using Helm. The core responsibility of the Flink operator is to manage the full production lifecycle of Flink applications. Running, suspending and deleting applicationsStateful and stateless application upgradesTriggering and managing savepointsHandling errors, rolling-back broken upgrades Flink Operator defines two types of Custom Resources(CR) which are the extensions of the Kubernetes API. FlinkDeploymentFlinkSessionJob FlinkDeployment FlinkDeployment CR defines Flink Application and Session Cluster deployments. Application deployments manage a single job deployment on a dedicated Flink cluster in Application mode. Session clusters allows you to run multiple Flink Jobs on an existing Session cluster. FlinkDeployment in Application modes, Click to toggle content! apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment metadata: namespace: default name: basic-example spec: image: flink:1.16 flinkVersion: v1_16 flinkConfiguration: taskmanager.numberOfTaskSlots: &quot;2&quot; serviceAccount: flink jobManager: resource: memory: &quot;2048m&quot; cpu: 1 taskManager: resource: memory: &quot;2048m&quot; cpu: 1 job: jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar parallelism: 2 upgradeMode: stateless state: running  info Session clusters use a similar spec to Application clusters with the only difference that job is not defined in the yaml spec. info According to the Flink documentation, it is recommended to use FlinkDeployment in Application mode for production environments. On top of the deployment types the Flink Kubernetes Operator also supports two modes of deployments: Native and Standalone. NativeStandalone Native Native cluster deployment is the default deployment mode and uses Flink‚Äôs built in integration with Kubernetes when deploying the cluster.Flink cluster communicates directly with Kubernetes and allows it to manage Kubernetes resources, e.g. dynamically allocate and de-allocate TaskManager pods.Flink Native can be useful for advanced users who want to build their own cluster management system or integrate with existing management systems.Flink Native allows for more flexibility in terms of job scheduling and execution.For standard Operator use running your own Flink Jobs Native mode is recommended apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment ... spec: ... mode: native  "},{"title":"Best Practices for Running Flink Jobs on Kubernetes‚Äã","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/streaming-platforms/flink#best-practices-for-running-flink-jobs-on-kubernetes","content":"To get the most out of Flink on Kubernetes, here are some best practices to follow: Use the Kubernetes Operator: Install and use the Flink Kubernetes Operator to automate the deployment and management of Flink clusters on Kubernetes.Deploy in dedicated namespaces: Create a separate namespace for the Flink Kubernetes Operator and another one for Flink jobs/workloads. This ensures that the Flink jobs are isolated and have their own resources.Use high-quality storage: Store Flink checkpoints and savepoints in high-quality storage such as Amazon S3 or another durable external storage. These storage options are reliable, scalable, and offer durability for large volumes of data.Optimize resource allocation: Allocate sufficient resources to Flink jobs to ensure optimal performance. This can be done by setting resource requests and limits for Flink containers.Proper network isolation: Use Kubernetes Network Policies to isolate Flink jobs from other workloads running on the same Kubernetes cluster. This ensures that Flink jobs have the required network access without being impacted by other workloads.Configure Flink optimally: Tune Flink settings according to your use case. For example, adjust Flink's parallelism settings to ensure that Flink jobs are scaled appropriately based on the size of the input data.Use checkpoints and savepoints: Use checkpoints for periodic snapshots of Flink application state and savepoints for more advanced use cases such as upgrading or downgrading the application.Store checkpoints and savepoints in the right places: Store checkpoints in distributed file systems or key-value stores like Amazon S3 or another durable external storage. Store savepoints in a durable external storage like Amazon S3. "},{"title":"Flink Upgrade‚Äã","type":1,"pageTitle":"Flink Operator on EKS","url":"/data-on-eks/docs/streaming-platforms/flink#flink-upgrade","content":"Flink Operator provides three upgrade modes for Flink jobs. Checkout the Flink upgrade docs for up-to-date information. stateless: Stateless application upgrades from empty statelast-state: Quick upgrades in any application state (even for failing jobs), does not require a healthy job as it always uses the latest checkpoint information. Manual recovery may be necessary if HA metadata is lost.savepoint: Use savepoint for upgrade, providing maximal safety and possibility to serve as backup/fork point. The savepoint will be created during the upgrade process. Note that the Flink job needs to be running to allow the savepoint to get created. If the job is in an unhealthy state, the last checkpoint will be used (unless kubernetes.operator.job.upgrade.last-state-fallback.enabled is set to false). If the last checkpoint is not available, the job upgrade will fail. info last-state or savepoint are recommended modes for production Deploying the Solution Execute Sample Flink job with Karpenter Execute Sample Flink job with Managed Node Groups and Cluster Autoscaler Cleanup caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"/data-on-eks/docs/streaming-platforms/kafka","content":"","keywords":""},{"title":"Strimzi for Apache Kafka‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#strimzi-for-apache-kafka","content":"Strimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. Strimzi combines security and simple configuration to deploy and manage Kafka on Kubernetes using kubectl and/or GitOps based on the Operator Pattern. "},{"title":"Architecture‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#architecture","content":"info Architecture diagram work in progress "},{"title":"Managed Alternatives‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#managed-alternatives","content":""},{"title":"Amazon Managed Streaming for Apache Kafka (MSK)‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#amazon-managed-streaming-for-apache-kafka-msk","content":"Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters. It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka. This means existing applications, tooling, and plugins from partners and the Apache Kafka community are supported. You can use Amazon MSK to create clusters that use any of the Apache Kafka versions listed under Supported Apache Kafka versions. Amazon MSK offers cluster-based or serverless deployment types. "},{"title":"Amazon Kinesis Data Streams (KDS)‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#amazon-kinesis-data-streams-kds","content":"Amazon Kinesis Data Streams (KDS) allows users to collect and process large streams of data records in real time. You can create data-processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services. Kinesis Data Streams support your choice of stream processing framework including Kinesis Client Library (KCL), Apache Flink, and Apache Spark Streaming. It is serverless, and scales automatically. "},{"title":"Storage considerations when self-managing Kafka‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#storage-considerations-when-self-managing-kafka","content":"The most common resource bottlenecks for Kafka clusters are network throughput, storage throughput, and network throughput between brokers and the storage backend for brokers using network attached storage such as Amazon Elastic Block Store (EBS). "},{"title":"Advantages to using EBS as persistent storage backend‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#advantages-to-using-ebs-as-persistent-storage-backend","content":"Improved flexibility and faster recovery: Fault tolerance is commonly achieved via broker (server) replication within the cluster and/or maintaining cross-AZ or region replicas. Since the lifecycle of EBS volumes is independent of Kafka brokers, if a broker fails and needs to be replaced, the EBS volume attached to the failed broker can be reattached to a replacement broker. Most of the replicated data for the replacement broker is already available in the EBS volume, and does not need to be copied over the network from another broker. This avoids most of the replication traffic required to bring the replacement broker up to speed with current operations.Just in time scale up: The characteristics of EBS volumes can be modified while they‚Äôre in use. Broker storage can be automatically scaled over time rather than provisioning storage for peak or adding additional brokers.Optimized for frequently-accessed-throughput-intensive workloads: Volume types such as st1 can be a good fit since these volumes are offered at a relatively low cost, support a large 1 MiB I/O block size, max IOPS of 500/volume, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB, and a maximum throughput of 500 MB/s per volume. "},{"title":"What EBS volumes should I use when self-managing Kafka on AWS?‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#what-ebs-volumes-should-i-use-when-self-managing-kafka-on-aws","content":"General purpose SSD volume gp3 with a balanced price and performance are widely used, and you can independently provision storage (up to 16TiB), IOPS (up to 16,000) and throughput (up to 1,000MiB/s)st1 is a low-cost HDD option for frequently accessed and throughput intensive workloads with up to 500 IOPS and 500 MiB/sFor critical applications such as Zookeeper, provisioned IOPS volumes (io2 Block Express, io2) provide higher durability "},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#deploying-the-solution","content":"In this example, you will provision the following resources to run Kafka Cluster on EKS. This example deploys an EKS Cluster with Kafka into a new VPC. Creates a new sample VPC, 3 Private Subnets and 3 Public Subnets.Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets.Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with two managed node groups.Deploys Metrics server, Cluster Autoscaler, self-managed ebs-csi-driver, Strimzi Kafka Operator, Grafana Operator.Strimzi Kafka Operator is a Kubernetes Operator for Apache Kafka deployed to strimzi-kafka-operator namespace. The operator by default watches and handles kafka in all namespaces. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraform "},{"title":"Deploy‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#deploy","content":"Clone the repository git clone https://github.com/awslabs/data-on-eks.git  Navigate into one of the example directories and run terraform init cd data-on-eks/streaming/kafka terraform init  Run Terraform plan to verify the resources created by this execution. export AWS_REGION=&quot;us-west-2&quot; # Select your own region terraform plan  Deploy the pattern terraform apply  Enter yes to apply. info This deployment may take between 20 to 30mins. "},{"title":"Verify the deployment‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#verify-the-deployment","content":""},{"title":"Create kube config‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#create-kube-config","content":"Create kube config file. aws eks --region us-west-2 update-kubeconfig --name kafka-on-eks  "},{"title":"Get nodes‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#get-nodes","content":"Check if the deployment has created 6 nodes. 3 nodes for Core Node group and 3 for Kafka brokers across 3 AZs. kubectl get nodes  Output NAME STATUS ROLES AGE VERSION ip-10-0-10-36.us-west-2.compute.internal Ready &lt;none&gt; 5h28m v1.24.7-eks-fb459a0 ip-10-0-10-47.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-11-218.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-11-223.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-12-202.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0 ip-10-0-12-50.us-west-2.compute.internal Ready &lt;none&gt; 5h20m v1.24.7-eks-fb459a0  "},{"title":"Verify Kafka Brokers and Zookeeper‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#verify-kafka-brokers-and-zookeeper","content":"Verify the Kafka Broker and Zookeeper pods and the status created by the Strimzi Operator. kubectl get strimzipodsets.core.strimzi.io -n kafka  Output NAME PODS READY PODS CURRENT PODS AGE cluster-kafka 3 3 3 4h35m cluster-zookeeper 3 3 3 4h36m  kubectl get kafka.kafka.strimzi.io -n kafka  Output NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS READY WARNINGS cluster 3 3 True  kubectl get kafkatopic.kafka.strimzi.io -n kafka  Output NAME CLUSTER PARTITIONS REPLICATION FACTOR READY consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a cluster 50 3 True strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55 cluster 1 3 True strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b cluster 1 3 True strimzi.cruisecontrol.metrics cluster 1 3 True strimzi.cruisecontrol.modeltrainingsamples cluster 32 2 True strimzi.cruisecontrol.partitionmetricsamples cluster 32 2 True  "},{"title":"Verify the running Kafka pods‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#verify-the-running-kafka-pods","content":"kubectl get pods -n kafka  Output NAME READY STATUS RESTARTS AGE cluster-cruise-control-79f6457f8d-sm8c2 1/1 Running 0 4h40m cluster-entity-operator-5594c965ff-t9nl4 3/3 Running 0 4h40m cluster-kafka-0 1/1 Running 0 4h41m cluster-kafka-1 1/1 Running 0 4h41m cluster-kafka-2 1/1 Running 0 4h41m cluster-kafka-exporter-9dbfdff54-wx8vq 1/1 Running 0 4h39m cluster-zookeeper-0 1/1 Running 0 4h42m cluster-zookeeper-1 1/1 Running 0 4h42m cluster-zookeeper-2 1/1 Running 0 4h42m  "},{"title":"Create Kafka Topic and run Sample test‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#create-kafka-topic-and-run-sample-test","content":"We will create one kafka topic and run sample producer script to produce new messages to the kafka topic. We can then verify the data in the topic using sample consumer script. "},{"title":"Create a kafka Topic‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#create-a-kafka-topic","content":"Run this command to create a new topic called test-topic under kafka namespace cd streaming/kafka/examples/ kubectl apply -f kafka-topics.yaml  Verify the status of the test-topic topic. kubectl exec -it cluster-kafka-0 -c kafka -n kafka -- /bin/bash -c &quot;/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092&quot;  Output __consumer_offsets __strimzi-topic-operator-kstreams-topic-store-changelog __strimzi_store_topic strimzi.cruisecontrol.metrics strimzi.cruisecontrol.modeltrainingsamples strimzi.cruisecontrol.partitionmetricsamples test-topic  "},{"title":"Execute sample Kafka Producer‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#execute-sample-kafka-producer","content":"Open two terminals one for Kafka producer and one for Kafka Consumer. Execute the following command and press enter twice until you see the &gt; prompt. Start typing some random content. This data will be written to the test-topic. kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list cluster-kafka-bootstrap:9092 --topic test-topic  "},{"title":"Execute sample Kafka Consumer‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#execute-sample-kafka-consumer","content":"Now, you can verify the data written to test-topic by running Kafka consumer pod in another terminal kubectl -n kafka run kafka-consumer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server cluster-kafka-bootstrap:9092 --topic test-topic  "},{"title":"Kafka Producer and Consumer output‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#kafka-producer-and-consumer-output","content":" "},{"title":"Grafana Dashboard for Kafka‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#grafana-dashboard-for-kafka","content":""},{"title":"Login to Grafana‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#login-to-grafana","content":"Login to Grafana dashboard by running the following command. kubectl port-forward svc/grafana-service 3000:3000 -n grafana  Open browser with local Grafana Web UI Enter username as admin and password can be extracted from the below command. kubectl get secrets/grafana-admin-credentials --template={{.data.GF_SECURITY_ADMIN_PASSWORD}} -n grafana | base64 -D  "},{"title":"Open Strimzi Kafka Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#open-strimzi-kafka-dashboard","content":"The below are builtin Kafka dashboards which created during the deployment.  "},{"title":"Open Strimzi Zookeeper Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#open-strimzi-zookeeper-dashboard","content":" "},{"title":"Open Strimzi Zookeeper Dashboard‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#open-strimzi-zookeeper-dashboard-1","content":"You can verify the test-topic with three partitions below.  "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Apache Kafka","url":"/data-on-eks/docs/streaming-platforms/kafka#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order with --target option to avoid destroy failures. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; -auto-approve terraform destroy -target=&quot;module.eks_blueprints&quot; -auto-approve terraform destroy -target=&quot;module.vpc&quot; -auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy -auto-approve  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ex. Delete kafka-on-eks EBS volumes "},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/data-on-eks/docs/troubleshooting","content":"","keywords":""},{"title":"Error: local-exec provisioner error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#error-local-exec-provisioner-error","content":"Error: local-exec provisioner error \\ with module.eks-blueprints.module.emr_on_eks[&quot;data_team_b&quot;].null_resource.update_trust_policy,\\ on .terraform/modules/eks-blueprints/modules/emr-on-eks/main.tf line 105, in resource &quot;null_resource&quot; \\ &quot;update_trust_policy&quot;:‚îÇ 105: provisioner &quot;local-exec&quot; {‚îÇ ‚îÇ Error running command 'set -e‚îÇ ‚îÇ aws emr-containers update-role-trust-policy \\ ‚îÇ --cluster-name emr-on-eks \\‚îÇ --namespace emr-data-team-b \\‚îÇ --role-name emr-on-eks-emr-eks-data-team-b  "},{"title":"Solution‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#solution","content":"emr-containers not present in cli version 2.0.41 Python/3.7.4. For more detailsThis is fixed in version 2.0.54.Action: aws cli version should be updated to 2.0.54 or later : Execute pip install --upgrade awscliv2  "},{"title":"Timeouts during Terraform Destroy‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#timeouts-during-terraform-destroy","content":"Customers who are deleting their environments using terraform destroy may see timeout errors when VPCs are being deleted. This is due to a known issue in the vpc-cni Customers may face a situation where ENIs that were attached to EKS managed nodes (same may apply to self-managed nodes) are not being deleted by the VPC CNI as expected which leads to IaC tool failures, such as: ENIs are left on subnetsEKS managed security group which is attached to the ENI can‚Äôt be deleted by EKS "},{"title":"Solution‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#solution-1","content":"The current recommendation is to execute cleanup in the following order: delete all pods that have been created in the cluster. add delay/ wait delete VPC CNI delete nodes delete cluster "},{"title":"Forbidden! Configured service account doesn't have access‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#forbidden-configured-service-account-doesnt-have-access","content":"Error: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PATCH at: https://kubernetes.default.svc/api/v1/namespaces/emr-team-a/pods/createnosaprocessedactions-772b9c81ae56a93d-exec-394. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods &quot;createnosaprocessedactions-772b9c81ae56a93d-exec-394&quot; is forbidden: User &quot;system:serviceaccount:emr-team-a:emr-containers-sa-spark-driver-682942051493-76simz7hn0n7qw78flb3z0c1ldt10ou9nmbeg8sh29&quot; cannot patch resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;emr-team-a&quot;.  "},{"title":"Solution‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#solution-2","content":"The following script patches the Kubernetes roles created by EMR job execution for given namespace. This is a mandatory fix for EMR6.6/Spark3.2 for missing permissions. This issue will be resolved in future release e.g., EMR6.7 and the patch script may not be required Repeat the above tests after applying the patch. This script needs to be run for all the namespaces used by by EMR on EKS Jobs cd analytics/emr-eks-fsx-lustre/fsx_lustre python3 emr-eks-sa-fix.py -n &quot;emr-data-team-a&quot;  "},{"title":"Error: could not download chart‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#error-could-not-download-chart","content":"‚îÇ Error: could not download chart: failed to download &quot;oci://public.ecr.aws/karpenter/karpenter&quot; at version &quot;v0.18.1&quot; ‚îÇ ‚îÇ with module.eks_blueprints_kubernetes_addons.module.karpenter[0].module.helm_addon.helm_release.addon[0], ‚îÇ on .terraform/modules/eks_blueprints_kubernetes_addons/modules/kubernetes-addons/helm-addon/main.tf line 1, in resource &quot;helm_release&quot; &quot;addon&quot;: ‚îÇ 1: resource &quot;helm_release&quot; &quot;addon&quot; { ‚îÇ  "},{"title":"Solution‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/data-on-eks/docs/troubleshooting#solution-3","content":"Looks like there is a bug in Terraform while doing Karpenter install. Until this is fixed, you can authenticate with ECR and re-run terraform apply aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws terraform apply --auto-approve  "},{"title":"Apache NiFi on EKS","type":0,"sectionRef":"#","url":"/data-on-eks/docs/streaming-platforms/nifi","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#introduction","content":"Apache NiFi is an open-source data integration and management system designed to automate and manage the flow of data between systems. It provides a web-based user interface for creating, monitoring, and managing data flows in real-time. With its powerful and flexible architecture, Apache NiFi can handle a wide range of data sources, cloud platforms, and formats, including structured and unstructured data, and can be used for a variety of data integration scenarios, such as data ingest, data processing (low to medium level), data routing, data transformation, and data dissemination. Apache NiFi provides a GUI based interface for building and managing data flows, making it easier for non-technical users. It also offers robust security features, including SSL, SSH, and fine-grained access control, to ensure the safe and secure transfer of sensitive data. Whether you are a data analyst, a data engineer, or a data scientist, Apache NiFi provides a comprehensive solution for managing and integrating your data on AWS and other platforms. caution This blueprint should be considered as experimental and should only be used for proof of concept. This example deploys an EKS Cluster running the Apache NiFi cluster. In the example, Apache NIfi is streaming data from the AWS Kinesis Data Stream to an Amazon DynamoDB table after some format transformation. Creates a new sample VPC, 3 Private Subnets and 3 Public SubnetsCreates Internet gateway for Public Subnets and NAT Gateway for Private SubnetsCreates EKS Cluster Control plane with public endpoint (for demo reasons only) with one managed node groupDeploys Apache NiFi, AWS Load Balancer Controller, Cert Manager and External DNS (optional) add-onsDeploys Apache NiFi cluster in the nifi namespace "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#prerequisites","content":"Ensure that you have installed the following tools on your machine. aws clikubectlterraformjq Additionally, for end-to-end configuration of Ingress, you will need to provide the following: A Route53 Public Hosted Zone configured in the account where you are deploying this example. E.g. &quot;example.com&quot;An ACM Certificate in the account + region where you are deploying this example. A wildcard certificate is preferred, e.g. &quot;*.example.com&quot; "},{"title":"Deploy the EKS Cluster with Apache NiFi‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#deploy-the-eks-cluster-with-apache-nifi","content":""},{"title":"Clone the repository‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#clone-the-repository","content":"git clone https://github.com/awslabs/data-on-eks.git  "},{"title":"Initialize Terraform‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#initialize-terraform","content":"Navigate into the example directory and run terraform init cd data-on-eks/streaming/nifi/ terraform init  "},{"title":"Terraform Plan‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#terraform-plan","content":"Run Terraform plan to verify the resources created by this execution. Provide a Route53 Hosted Zone hostname and a corresponding ACM Certificate; export TF_VAR_eks_cluster_domain=&quot;&lt;CHANGEME - example.com&gt;&quot; export TF_VAR_acm_certificate_domain=&quot;&lt;CHANGEME - *.example.com&gt;&quot; export TF_VAR_nifi_sub_domain=&quot;nifi&quot; export TF_VAR_nifi_username=&quot;admin&quot;  "},{"title":"Deploy the pattern‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#deploy-the-pattern","content":"terraform plan terraform apply  Enter yes to apply. Outputs: configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name nifi-on-eks&quot;  "},{"title":"Verify Deployment‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#verify-deployment","content":"Update kubeconfig aws eks --region us-west-2 update-kubeconfig --name nifi-on-eks  Verify all pods are running. NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch aws-cloudwatch-metrics-7fbcq 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-82c9v 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-blrmt 1/1 Running 1 (43h ago) 2d amazon-cloudwatch aws-cloudwatch-metrics-dhpl7 1/1 Running 0 19h amazon-cloudwatch aws-cloudwatch-metrics-hpw5k 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-7d57b6576b-c52dw 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-cainjector-86f7f4749-hs7d9 1/1 Running 1 (43h ago) 2d cert-manager cert-manager-webhook-66c85f8577-rxms8 1/1 Running 1 (43h ago) 2d external-dns external-dns-57bb948d75-g8kbs 1/1 Running 0 41h grafana grafana-7f5b7f5d4c-znrqk 1/1 Running 1 (43h ago) 2d kube-system aws-load-balancer-controller-7ff998fc9b-86gql 1/1 Running 1 (43h ago) 2d kube-system aws-load-balancer-controller-7ff998fc9b-hct9k 1/1 Running 1 (43h ago) 2d kube-system aws-node-4gcqk 1/1 Running 1 (43h ago) 2d kube-system aws-node-4sssk 1/1 Running 0 19h kube-system aws-node-4t62f 1/1 Running 1 (43h ago) 2d kube-system aws-node-g4ndt 1/1 Running 1 (43h ago) 2d kube-system aws-node-hlxmq 1/1 Running 1 (43h ago) 2d kube-system cluster-autoscaler-aws-cluster-autoscaler-7bd6f7b94b-j7td5 1/1 Running 1 (43h ago) 2d kube-system cluster-proportional-autoscaler-coredns-6ccfb4d9b5-27xsd 1/1 Running 1 (43h ago) 2d kube-system coredns-5c5677bc78-rhzkx 1/1 Running 1 (43h ago) 2d kube-system coredns-5c5677bc78-t7m5z 1/1 Running 1 (43h ago) 2d kube-system ebs-csi-controller-87c4ff9d4-ffmwh 6/6 Running 6 (43h ago) 2d kube-system ebs-csi-controller-87c4ff9d4-nfw28 6/6 Running 6 (43h ago) 2d kube-system ebs-csi-node-4mkc8 3/3 Running 0 19h kube-system ebs-csi-node-74xqs 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-8cw8t 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-cs9wp 3/3 Running 3 (43h ago) 2d kube-system ebs-csi-node-ktdb7 3/3 Running 3 (43h ago) 2d kube-system kube-proxy-4s72m 1/1 Running 0 19h kube-system kube-proxy-95ptn 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-bhrdk 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-nzvb6 1/1 Running 1 (43h ago) 2d kube-system kube-proxy-q9xkc 1/1 Running 1 (43h ago) 2d kube-system metrics-server-fc87d766-dd647 1/1 Running 1 (43h ago) 2d kube-system metrics-server-fc87d766-vv8z9 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-b5vqg 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-pklhr 1/1 Running 0 19h logging aws-for-fluent-bit-rq2nc 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-tnmtl 1/1 Running 1 (43h ago) 2d logging aws-for-fluent-bit-zzhfc 1/1 Running 1 (43h ago) 2d nifi nifi-0 5/5 Running 0 41h nifi nifi-1 5/5 Running 0 41h nifi nifi-2 5/5 Running 0 41h nifi nifi-registry-0 1/1 Running 0 41h nifi nifi-zookeeper-0 1/1 Running 0 41h nifi nifi-zookeeper-1 1/1 Running 0 41h nifi nifi-zookeeper-2 1/1 Running 0 18h prometheus prometheus-alertmanager-655fcb46df-2qh8h 2/2 Running 2 (43h ago) 2d prometheus prometheus-kube-state-metrics-549f6d74dd-wwhtr 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-5cpzk 1/1 Running 0 19h prometheus prometheus-node-exporter-8jhbk 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-nbd42 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-str6t 1/1 Running 1 (43h ago) 2d prometheus prometheus-node-exporter-zkf5s 1/1 Running 1 (43h ago) 2d prometheus prometheus-pushgateway-677c6fdd5-9tqkl 1/1 Running 1 (43h ago) 2d prometheus prometheus-server-7bf9cbb9cf-b2zgl 2/2 Running 2 (43h ago) 2d vpa vpa-recommender-7c6bbb4f9b-rjhr7 1/1 Running 1 (43h ago) 2d vpa vpa-updater-7975b9dc55-g6zf6 1/1 Running 1 (43h ago) 2d  Apache NiFi UI‚Äã The Apache NiFi Dashboard can be opened at the following url &quot;https://nifi.example.com/nifi&quot;  Run the command below to retrieve NiFi user's password and default username as admin aws secretsmanager get-secret-value --secret-id &lt;nifi_login_password_secret_name from terraform outputs&gt; --region &lt;region&gt; | jq '.SecretString' --raw-output   "},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#monitoring","content":"Apache Nifi can be monitored using metrics reported by PrometheusReportingTask. JVM metrics are disabled by default, let's enable the JVM metrics by navigating to Controller Settings by the clicking on the hamburger icon (three horizontal bars) in the top right corner.  Next click on the REPORTING TASK tab and then click the + icon and search for PrometheusReportingTask in the filter. Select the PrometheusReportingTask and click ADD button.  The prometheus reporting task is stopped by default.  Click on the pencil icon to edit the task and click on the PROPERTIES tab. Set the Send JVM metrics to true and click on Apply. Start the task by clicking on the play icon and ensure it's in running state.  This blueprint uses the prometheus and grafana to create a monitoring stack for getting visibility into your Apache NiFi cluster. aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name from terraform outputs&gt; --region &lt;region&gt; | jq '.SecretString' --raw-output  Run the command below and open the Grafana dashboard using the url &quot;http://localhost:8080&quot;. kubectl port-forward svc/grafana -n grafana 8080:80  Import Apache NiFi Grafana dashboard  "},{"title":"Example‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#example","content":"Create IAM policies for accessing Amazon DynamoDB and AWS Kinesis‚Äã Create an AWS IAM role: Create an AWS IAM role with permissions to access the AWS Kinesis data stream and assign this role to the AWS EKS cluster hosting Apache NiFi. Attach the IAM policy: Attach a policy to the IAM role that limits access to the Kinesis data stream to read-only and IAM policy to enable EKS role to write Amazon DynamoDB table. Here's an example policy: { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;Nifi-access-to-Kinesis&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;kinesis:DescribeStream&quot;, &quot;kinesis:GetRecords&quot;, &quot;kinesis:GetShardIterator&quot;, &quot;kinesis:ListStreams&quot; ], &quot;Resource&quot;: &quot;arn:aws:kinesis:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:stream/kds-stream-nifi-on-EKS&quot; } ] }  { &quot;Sid&quot;: &quot;DynamoDBTableAccess&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;dynamodb:BatchGetItem&quot;, &quot;dynamodb:BatchWriteItem&quot;, &quot;dynamodb:ConditionCheckItem&quot;, &quot;dynamodb:PutItem&quot;, &quot;dynamodb:DescribeTable&quot;, &quot;dynamodb:DeleteItem&quot;, &quot;dynamodb:GetItem&quot;, &quot;dynamodb:Scan&quot;, &quot;dynamodb:Query&quot;, &quot;dynamodb:UpdateItem&quot; ], &quot;Resource&quot;: &quot;arn:aws:dynamodb:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:table/NifiStreamingTable&quot; }  Create AWS Kinesis Data Stream‚Äã Create an AWS Kinesis data stream: Log in to the AWS Management Console, and create a Kinesis data stream in the region where you want to collect your data or use the below command line to create one. aws kinesis create-stream --stream-name kds-stream-nifi-on-EKS  Create Amazon DynamoDB table‚Äã Create a Amazon DynamoDB in the same AWS Account using the AWS console or the command line. Create a JSON file with Amazon DynamoDb table information called JSONSchemaDynamoDBTABLE.json  &quot;TableName&quot;: &quot;NifiStreamingTable&quot;, &quot;KeySchema&quot;: [ { &quot;AttributeName&quot;: &quot;Name&quot;, &quot;KeyType&quot;: &quot;HASH&quot; }, { &quot;AttributeName&quot;: &quot;Age&quot;, &quot;KeyType&quot;: &quot;RANGE&quot; }}, { &quot;AttributeName&quot;: &quot;Location&quot;, &quot;KeyType&quot;: &quot;RANGE&quot; } ], &quot;AttributeDefinitions&quot;: [ { &quot;AttributeName&quot;: &quot;Name&quot;, &quot;KeyType&quot;: &quot;S&quot; }, { &quot;AttributeName&quot;: &quot;Age&quot;, &quot;KeyType&quot;: &quot;S&quot; }}, { &quot;AttributeName&quot;: &quot;Location&quot;, &quot;KeyType&quot;: &quot;S&quot; } ], &quot;ProvisionedThroughput&quot;: { &quot;ReadCapacityUnits&quot;: 5, &quot;WriteCapacityUnits&quot;: 5 } }  Execute the command line to create the Amazon DynamoDB table from the JSON file. aws dynamodb create-table --cli-input-json JSONSchemaDynamoDBTABLE.json  Open the Apache Nifi on the EKS UI using the endpoint, create a process group, and name it NifiStreamingExample.   Double-click on the Nifi-on-EKS-process-group and enter the process to create the data flow. Drag the processer icon from the top left, type Kinesis into the search window, and select the ConsumeKinesisStream processor. To create a Kinesis Consumer, click ADD.¬†  Double click on the Kinesis processor, select the properties tab, and fill in the information for the configuration below. a. Amazon Kinesis Stream Name b. Application Name c. Region d. AWS Credentials Provider Service - Select AWSCredentialsProviderControllerService and create one.  Create AWS credential setup‚Äã Setup the AWS credentials to access the AWS resource in the account using the AWS Credentials Provider Service. In this example, we are using the access key and secret key. Note : Other options are IAM role-based, assumed role options to authenticate an AWS resources.    Drag the processer icon from the top left, type &quot;dynamoDB&quot; into the search window, and select the &quot;PutDynamoDBRecord processor. Click on ADD to create an Amazon DynamoDB writer. Configure the processor using the fields below. a. Record Reader - Change it to JSONTreeReader b. AWS Credentials Provider Service - select the previously created configuration c. Region b. Table Name d. Partition Key Field - select the partition field  Hover over the Kinesis consumer and drag it to the DynamoDB writer. The connection will be made, and the success queue will be created.  For the Kinesis Consumer and DynamoDB, create an error route to a funnel. This is to route the unprocessed, failed, and successful records for further processing. Note: Under the Relationship tab, you can see all the options for each processor. For the DynamoDB writer, success should always point to a funnel.  Check that none of the processors have any Hazard symbols. Right-click on the grid and click &quot;run the data flow.&quot; You can start seeing the data flowing in. "},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Apache NiFi on EKS","url":"/data-on-eks/docs/streaming-platforms/nifi#cleanup","content":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC terraform destroy -target=&quot;module.eks_blueprints_kubernetes_addons&quot; --auto-approve terraform destroy -target=&quot;module.eks&quot; --auto-approve terraform destroy -target=&quot;module.vpc&quot; --auto-approve  Finally, destroy any additional resources that are not in the above modules terraform destroy --auto-approve  "}]