{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf05030c-f6a6-464a-ae42-52d4d8f5013b",
   "metadata": {},
   "source": [
    "# Using Iceberg and Query Engines\n",
    "\n",
    "This notebook demonstrates how you can use Iceberg and Spark / Trino to query your data.\n",
    "\n",
    "We will:\n",
    "- Configure Spark with Iceberg runtime libraries\n",
    "- Explore how iceberg works\n",
    "- Explore data we just generated\n",
    "- Come up with a plan to run a Spark job with Spark Operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175d793-7264-4e15-9797-0c39ca654554",
   "metadata": {},
   "source": [
    "## Setting up Spark and Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d11268-89cb-4b55-9fd5-cf200872669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# These packages are necessary for running this section on EKS. We have made these jars available within the container image, therefore the line below is commented out.\n",
    "# packages = \"org.apache.iceberg:iceberg-spark:1.10.0,org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.0,org.apache.iceberg:iceberg-spark-extensions-4.0_2.13:1.10.0,com.amazonaws:aws-java-sdk-bundle:1.12.791,software.amazon.awssdk:bundle:2.34.0,org.apache.hadoop:hadoop-aws:3.4.1\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergInspector\") \\\n",
    "    .config(\"spark.sql.catalog.workshop\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.workshop.type\", \"glue\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"workshop\") \\\n",
    "    .config(\"spark.sql.catalog.workshop.warehouse\", \"s3a://${S3_BUCKET}/iceberg-warehouse/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\")\\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.sql.catalog.workshop.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec19e0d-7e36-471a-a382-c412f002cb2c",
   "metadata": {},
   "source": [
    "## Explore Iceberg Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ebc36-9834-4046-8831-e647f846ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read database information from the catalog\n",
    "spark.sql(\"show databases\").show()\n",
    "# Then use the database for all queries going forward.\n",
    "spark.sql(\"use data_on_eks\")\n",
    "# Show all tables available with in the database\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd614b-83eb-400c-9a40-57a944a74f3c",
   "metadata": {},
   "source": [
    "# Peeking Under the Hood: Exploring Metadata Tables\n",
    "One of Iceberg's most powerful features is its transparency. Unlike other table formats where the underlying structure is hidden, Iceberg allows us to directly query its metadata to understand exactly what's going on.\n",
    "We'll use special sub-tables like .files, .history, and .snapshots to inspect the table's physical layout and history.\n",
    "\n",
    "Let's start by looking at the actual data files on disk. Every time we write data, Iceberg creates one or more files (in this case, Parquet files). The .files metadata table gives us a complete list of every data file that makes up the current snapshot of the table.\n",
    "This demonstrates how small, frequent writes can lead to multiple small files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16193234-adb3-4ac0-b4d4-f2bba988eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    file_path, file_format, file_size_in_bytes, record_count\n",
    "FROM\n",
    "    data_on_eks.cat_locations_raw.files\n",
    "ORDER BY file_path DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3fc74-a0e7-4747-abbc-e65a2060b85b",
   "metadata": {},
   "source": [
    "## The History Table\n",
    "The .history table is like a commit log for your data. Every change made to the table—like our INSERT statements—is recorded as an entry. This provides a clear, chronological audit trail of how the table has evolved. The query below will show the timeline of when each snapshot became the current version of the table. C. Show the Snapshot Details (.snapshots) While .history shows the timeline, the .snapshots table gives us the rich details for each snapshot. A snapshot is an immutable view of the table's complete state at a specific point in time. The query below shows what operation created each snapshot (append, overwrite, etc.) and a summary of the changes, like how many records and files were added. This table is the key that enables powerful features like time travel.\n",
    "\n",
    "The `is_current_ancestor` column is a boolean flag in the history table that answers a simple question: \"Is this historical snapshot part of the direct timeline that leads to the table's current state?\"\n",
    "\n",
    "  * If the value is **`true`**, the snapshot is a direct ancestor.\n",
    "  * If the value is **`false`**, the snapshot belongs to an abandoned branch of history, most often created after a table rollback.\n",
    "\n",
    "The diagrams below illustrate how this works.\n",
    "\n",
    "-----\n",
    "\n",
    "### Scenario 1: Linear History\n",
    "\n",
    "Initially, your table has three commits (snapshots). Each is a direct ancestor of the current version (`S3`), so `is_current_ancestor` is **true** for all of them.\n",
    "\n",
    "```\n",
    "(Main Timeline)\n",
    "+-------------+      +-------------+      +-------------+\n",
    "| Snapshot S1 |----->| Snapshot S2 |----->| Snapshot S3 |\n",
    "| ancestor: T |      | ancestor: T |      | ancestor: T |\n",
    "+-------------+      +-------------+      +-------------+\n",
    "                                                    ^\n",
    "                                                    |\n",
    "                                                 (current)\n",
    "```\n",
    "\n",
    "\n",
    "### Scenario 2: After Rolling Back to S2\n",
    "\n",
    "Now, you roll the table back to `S2`. The main timeline is now shorter, and `S2` is the new current version.\n",
    "\n",
    "Snapshot `S3` still exists in the table's history, but it's now on an **abandoned branch**. Its `is_current_ancestor` flag flips to **false**.\n",
    "\n",
    "```\n",
    "(Main Timeline)                               (Abandoned Branch)\n",
    "+-------------+      +-------------+                 +-------------+\n",
    "| Snapshot S1 |----->| Snapshot S2 |                 | Snapshot S3 |\n",
    "| ancestor: T |      | ancestor: T |                 | ancestor: F |\n",
    "+-------------+      +-------------+                 +-------------+\n",
    "                           ^\n",
    "                           |\n",
    "                        (current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35285816-a143-4c0f-9807-63f8d218b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    data_on_eks.cat_locations_raw.history\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dd70d-d346-4b3e-8c21-e4940b3af006",
   "metadata": {},
   "source": [
    "### Now that we understand how to view the metadata, let's enrich the raw data\n",
    "\n",
    "**Be sure to replace the S3_BUCKET with your bucket id**\n",
    "\n",
    "\n",
    "#### ETL Pipeline: Daily Cat Summary\n",
    "\n",
    "This cell executes a Spark batch job that performs a daily ETL (Extract, Transform, Load) process to create an aggregated summary of cat activity.\n",
    "\n",
    "*   **Extract**: Reads raw data from the `cat_wellness_raw`, `cat_interactions_raw`, and `cats_profiles_raw` Iceberg tables.\n",
    "*   **Transform**: Aggregates metrics for each cat by day, calculating:\n",
    "    *   Average activity level\n",
    "    *   Maximum hours since last drink\n",
    "    *   Total interaction count\n",
    "    *   Total \"like\" interactions\n",
    "*   **Load**: Joins the aggregated data to create a comprehensive daily summary, which is then saved to the `daily_cat_summary` Iceberg table for easier querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3a77c-16e2-401d-b3c5-141061439c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, avg, max, count, sum, when\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration\n",
    "# ==============================================================================\n",
    "# These should be configured to match your environment\n",
    "S3_WAREHOUSE_PATH = os.getenv('S3_WAREHOUSE_PATH', 's3a://${S3_BUCKET}/iceberg-warehouse/')\n",
    "ICEBERG_CATALOG_NAME = 'workshop'\n",
    "GLUE_DATABASE_NAME = 'data_on_eks'\n",
    "\n",
    "# PostgreSQL connection settings\n",
    "DB_URL = os.getenv('DB_URL', 'jdbc:postgresql://postgresql-0.postgresql.workshop.svc.cluster.local:5432/workshop')\n",
    "DB_USER = os.getenv('DB_USER', 'workshop')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD', 'workshop')\n",
    "DB_DRIVER = \"org.postgresql.Driver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648b0c7-4743-4310-938c-a8b9a729157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Spark batch job that reads from raw Iceberg tables, enriches the data with\n",
    "profile information from PostgreSQL, creates a daily summary for each cat,\n",
    "and displays the result.\n",
    "\"\"\"\n",
    "# ==============================================================================\n",
    "# Spark Session Initialization\n",
    "# ==============================================================================\n",
    "print(\"Starting Spark session for Enriched Daily Cat Summary job...\")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EnrichedDailyCatSummary\")\n",
    "    # Add Iceberg and PostgreSQL packages and configurations\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.workshop.type\", \"glue\")\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.warehouse\", S3_WAREHOUSE_PATH)\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark session created.\")\n",
    "\n",
    "try:\n",
    "    # ==============================================================================\n",
    "    # EXTRACT - Read data from Iceberg and PostgreSQL\n",
    "    # ==============================================================================\n",
    "    print(\"Reading raw data sources...\")\n",
    "\n",
    "    # Read raw wellness and interactions data from Iceberg\n",
    "    wellness_table = f\"{ICEBERG_CATALOG_NAME}.{GLUE_DATABASE_NAME}.cat_wellness_raw\"\n",
    "    interactions_table = f\"{ICEBERG_CATALOG_NAME}.{GLUE_DATABASE_NAME}.cat_interactions_raw\"\n",
    "    wellness_df = spark.table(wellness_table)\n",
    "    interactions_df = spark.table(interactions_table)\n",
    "    print(f\"Successfully read from Iceberg tables: {wellness_table} and {interactions_table}\")\n",
    "\n",
    "    # Read cat and visitor profiles directly from PostgreSQL\n",
    "    print(\"Reading from PostgreSQL tables: 'cats' and 'visitors'\")\n",
    "    cats_profiles_df = (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", DB_URL)\n",
    "        .option(\"dbtable\", \"cats\")\n",
    "        .option(\"user\", DB_USER)\n",
    "        .option(\"password\", DB_PASSWORD)\n",
    "        .option(\"driver\", DB_DRIVER)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    visitors_profiles_df = (\n",
    "        spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", DB_URL)\n",
    "        .option(\"dbtable\", \"visitors\")\n",
    "        .option(\"user\", DB_USER)\n",
    "        .option(\"password\", DB_PASSWORD)\n",
    "        .option(\"driver\", DB_DRIVER)\n",
    "        .load()\n",
    "    )\n",
    "    print(f\"Successfully read {cats_profiles_df.count()} cat profiles and {visitors_profiles_df.count()} visitor profiles from PostgreSQL.\")\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # TRANSFORM - Perform aggregations and joins\n",
    "    # ==============================================================================\n",
    "    print(\"Transforming data: Aggregating daily wellness and interaction metrics...\")\n",
    "\n",
    "    # --- Aggregate wellness data by day (logic preserved from original script) ---\n",
    "    wellness_daily_df = (\n",
    "        wellness_df\n",
    "        .withColumn(\"day\", to_date(col(\"event_time\")))\n",
    "        .groupBy(\"cat_id\", \"day\")\n",
    "        .agg(\n",
    "            avg(\"activity_level\").alias(\"avg_activity_level\"),\n",
    "            max(\"hours_since_last_drink\").alias(\"max_hours_since_drink\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Aggregate interaction data by day (logic preserved from original script) ---\n",
    "    interactions_daily_df = (\n",
    "        interactions_df\n",
    "        .withColumn(\"day\", to_date(col(\"event_time\")))\n",
    "        .groupBy(\"cat_id\", \"day\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_interaction_count\"),\n",
    "            sum(when(col(\"interaction_type\") == \"like\", 1).otherwise(0)).alias(\"like_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Join the summaries together ---\n",
    "    print(\"Joining daily summaries and enriching with cat names from PostgreSQL...\")\n",
    "    daily_summary_df = (\n",
    "        wellness_daily_df\n",
    "        .join(interactions_daily_df, [\"cat_id\", \"day\"], \"full_outer\")\n",
    "        # Join with profiles from PostgreSQL to get the cat's name\n",
    "        .join(cats_profiles_df.select(\"cat_id\", \"name\"), \"cat_id\", \"left\")\n",
    "        .na.fill(0) # Fill nulls with 0 for counts/metrics where one side of the join had no data\n",
    "        .select(\n",
    "            \"day\",\n",
    "            \"cat_id\",\n",
    "            \"name\",\n",
    "            \"avg_activity_level\",\n",
    "            \"max_hours_since_drink\",\n",
    "            \"total_interaction_count\",\n",
    "            \"like_count\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # LOAD - Display the final summary table\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Daily Cat Summary (Enriched) ---\")\n",
    "    print(\"This table shows the daily aggregated metrics for each cat.\")\n",
    "    daily_summary_df.show(10)\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # CELL 6: BONUS - Show enrichment of raw interactions with visitor names\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Bonus: Raw Cat Interactions with Visitor Names ---\")\n",
    "    print(\"This table shows the raw interaction events joined with visitor names from PostgreSQL.\")\n",
    "    if 'visitor_id' in interactions_df.columns:\n",
    "        enriched_interactions_df = (\n",
    "            interactions_df.join(\n",
    "                visitors_profiles_df.select(\"visitor_id\", \"name\").withColumnRenamed(\"name\", \"visitor_name\"),\n",
    "                \"visitor_id\",\n",
    "                \"left\"\n",
    "            )\n",
    "            .select(\"event_time\", \"cat_id\", \"visitor_id\", \"visitor_name\", \"interaction_type\")\n",
    "        )\n",
    "        enriched_interactions_df.show(10)\n",
    "    else:\n",
    "        print(\"Skipping visitor enrichment: 'visitor_id' column not found in the interactions table.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the Spark job: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    # ==============================================================================\n",
    "    # CELL 7: Stop the Spark Session\n",
    "    # ==============================================================================\n",
    "    print(\"\\nBatch job complete. Stopping Spark session.\")\n",
    "    spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bac6f5-41ef-404a-ad39-b317835b2966",
   "metadata": {},
   "source": [
    "### ETL Jobs\n",
    "\n",
    "While running Spark code in a notebook is excellent for development and interactive analysis, you wouldn't typically run a scheduled job this way.\n",
    "\n",
    "In a real-world scenario, this ETL logic would be packaged and defined as a `SparkApplication` custom resource for Kubernetes. This allows the Spark on Kubernetes Operator to\n",
    "manage the job's entire lifecycle, including scheduling, resource allocation, and retries, making it a robust and automated part of a data pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "Great work! Now that the summary table has been created, please **return to your terminal** to proceed with the next module of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaef0ec-1c38-4c70-b713-9a891171231a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
