# EMR Spark Operator Example - Spark Pi Calculation
# This example demonstrates using EMR Spark Operator for declarative Spark job management
#
# Prerequisites:
# 1. EMR Spark Operator must be installed (enable_emr_spark_operator = true)
# 2. Ensure emr-data-team-a namespace exists with proper IAM roles
#
# To execute: kubectl apply -f taxi-trip-spark-operator.yaml
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-pi-operator
  namespace: emr-data-team-a
spec:
  type: Scala
  mode: cluster

  # EMR optimized runtime image
  image: "895885662937.dkr.ecr.us-west-2.amazonaws.com/spark/emr-7.12.0:latest"
  imagePullPolicy: Always

  # Main application - Spark Pi example from EMR image
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///usr/lib/spark/examples/jars/spark-examples.jar

  arguments:
    - "1000"

  # Hadoop configuration for EMRFS
  hadoopConf:
    fs.s3.customAWSCredentialsProvider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider
    fs.s3.impl: com.amazon.ws.emr.hadoop.fs.EmrFileSystem
    fs.AbstractFileSystem.s3.impl: org.apache.hadoop.fs.s3.EMRFSDelegate
    fs.s3.buffer.dir: /mnt/s3
    fs.s3.getObject.initialSocketTimeoutMilliseconds: "2000"
    mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem: "2"
    mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem: "true"

  # Spark configuration
  sparkConf:
    # Kubernetes client timeout settings (critical for stability)
    spark.kubernetes.local.dirs.tmpfs: "true"
    spark.kubernetes.submission.connectionTimeout: "60000000"
    spark.kubernetes.submission.requestTimeout: "60000000"
    spark.kubernetes.driver.connectionTimeout: "60000000"
    spark.kubernetes.driver.requestTimeout: "60000000"

    # Event logging for Spark History Server (optional)
    spark.eventLog.enabled: "false"
    spark.kubernetes.driver.pod.name: driver-spark-pi-operator

    # EMR Runtime classpaths
    spark.driver.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*
    spark.driver.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native
    spark.executor.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*
    spark.executor.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native

    # EMRFS optimized committer
    spark.sql.parquet.output.committer.class: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
    spark.sql.parquet.fs.optimized.committer.optimization-enabled: "true"
    spark.sql.emr.internal.extensions: com.amazonaws.emr.spark.EmrSparkSessionExtensions

    # JVM options
    spark.executor.defaultJavaOptions: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'
    spark.driver.defaultJavaOptions: -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70


  restartPolicy:
    type: Never

  # Driver configuration
  driver:
    cores: 2
    memory: "8g"
    # Service account created by EMR virtual cluster module
    # Format: emr-containers-sa-spark-driver-<account-id>-<deployment-id>
    # Get actual name: kubectl get sa -n emr-data-team-a | grep driver
    # serviceAccount: emr-containers-sa-spark-driver-123456789-wu45qgcjmov3ud6ripe7a74zr24rc3cbnf0pxdup

    # Use Graviton compute-optimized nodes
    nodeSelector:
      NodeGroupType: "SparkGravitonComputeOptimized"

    labels:
      app: spark-pi-operator
      role: driver

  # Executor configuration
  executor:
    cores: 4
    instances: 1
    memory: "16g"
    # Service account created by EMR virtual cluster module
    # Format: emr-containers-sa-spark-executor-<account-id>-<deployment-id>
    # Get actual name: kubectl get sa -n emr-data-team-a | grep executor
    # serviceAccount: emr-containers-sa-spark-executor-123456789-wu45qgcjmov3ud6ripe7a74zr24rc3cbnf0pxdup

    # Use Graviton compute-optimized nodes
    nodeSelector:
      NodeGroupType: "SparkGravitonComputeOptimized"

    labels:
      app: spark-pi-operator
      role: executor
